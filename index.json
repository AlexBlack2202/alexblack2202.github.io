[{"categories":null,"content":" Cân bằng Nash: Khái niệm và ứng dụng Ứng dụng chia thưởng cho nhân viên Cân bằng Nash: Khái niệm và ứng dụng 1. Cân bằng Nash là gì? Cân bằng Nash (Nash Equilibrium) là một khái niệm trong lý thuyết trò chơi (game theory), được đặt theo tên của nhà toán học John Nash, người đã chứng minh sự tồn tại của cân bằng này trong các trò chơi không hợp tác (non-cooperative games). Cân bằng Nash mô tả một trạng thái mà mỗi người chơi trong trò chơi đều chọn chiến lược tối ưu nhất cho mình, dựa trên giả định rằng các đối thủ sẽ không thay đổi chiến lược của họ.\nĐịnh nghĩa chính xác: Một tập hợp các chiến lược của tất cả người chơi được gọi là cân bằng Nash nếu không có bất kỳ người chơi nào có thể cải thiện kết quả của mình bằng cách đơn phương thay đổi chiến lược của riêng mình. 2. Đặc điểm của cân bằng Nash Tính ổn định: Trong trạng thái cân bằng Nash, không ai có động lực để thay đổi hành vi của mình vì họ đã đạt được lợi ích tốt nhất có thể. Không phải luôn tối ưu Pareto: Cân bằng Nash không nhất thiết phải là giải pháp tốt nhất cho tất cả mọi người. Đôi khi, nó có thể dẫn đến \u0026ldquo;kết cục bi kịch\u0026rdquo; (tragedy of the commons) hoặc \u0026ldquo;hiệu ứng tù nhân\u0026rdquo; (Prisoner\u0026rsquo;s Dilemma). Sự tồn tại: Theo định lý của Nash, mọi trò chơi hữu hạn với số lượng người chơi và chiến lược hữu hạn đều có ít nhất một cân bằng Nash (có thể ở dạng hỗn hợp, tức là sử dụng chiến lược ngẫu nhiên). 3. Ví dụ về cân bằng Nash Ví dụ 1: Trò chơi \u0026ldquo;Chicken Game\u0026rdquo; (Trò chơi Gà con)\nHai tài xế lái xe lao thẳng vào nhau. Nếu cả hai từ chối nhường đường, họ sẽ đâm nhau và thiệt hại nặng nề. Nếu một người nhường đường (chạy thoát), người kia sẽ thắng cuộc.\nBảng trả thưởng:\nNgười B nhường Người B không nhường Người A nhường (0, 0) (-1, +1) Người A không nhường (+1, -1) (-10, -10) Nếu cả hai chọn \u0026ldquo;không nhường\u0026rdquo;, họ rơi vào tình huống tệ nhất (-10, -10). Đây không phải là cân bằng Nash. Nếu một người chọn \u0026ldquo;nhường\u0026rdquo; và người kia chọn \u0026ldquo;không nhường\u0026rdquo;, thì người không nhường sẽ có lợi (+1). Tuy nhiên, đây cũng không phải là cân bằng Nash vì người nhường có thể thay đổi chiến lược để tránh thiệt hại. Cân bằng Nash xảy ra khi một người nhường và người kia không nhường, ví dụ: (Người A nhường, Người B không nhường) hoặc ngược lại. Ví dụ 2: Trò chơi \u0026ldquo;Prisoner’s Dilemma\u0026rdquo; (Dilemma của Tù nhân)\nHai tù nhân bị bắt giữ và bị thẩm vấn riêng biệt. Họ có hai lựa chọn: \u0026ldquo;hợp tác\u0026rdquo; (giữ im lặng) hoặc \u0026ldquo;phản bội\u0026rdquo; (tố cáo người kia).\nBảng trả thưởng:\nNgười B hợp tác Người B phản bội Người A hợp tác (-1, -1) (-10, 0) Người A phản bội (0, -10) (-5, -5) Nếu cả hai hợp tác, họ nhận án nhẹ (-1, -1). Nếu một người phản bội và người kia hợp tác, người phản bội được tự do (0) còn người hợp tác chịu án nặng (-10). Cân bằng Nash xảy ra khi cả hai phản bội (-5, -5), vì dù bên kia làm gì, phản bội vẫn là chiến lược tối ưu cá nhân. 4. Ứng dụng của cân bằng Nash Cân bằng Nash có nhiều ứng dụng trong các lĩnh vực khác nhau, bao gồm:\n4.1. Kinh tế học Đấu giá và cạnh tranh thị trường: Các công ty trong thị trường cạnh tranh thường tìm cách đạt được cân bằng Nash bằng cách đưa ra mức giá và sản lượng tối ưu để tối đa hóa lợi nhuận. Chiến lược định giá: Các công ty lớn như Amazon, Walmart, hay các hãng hàng không thường sử dụng cân bằng Nash để dự đoán hành vi của đối thủ và điều chỉnh chiến lược của mình. 4.2. Chính trị và quan hệ quốc tế Đàm phán và xung đột: Cân bằng Nash giúp phân tích các tình huống đàm phán giữa các quốc gia hoặc các bên trong xung đột, ví dụ như thỏa thuận cắt giảm vũ khí hạt nhân. Chiến tranh lạnh: Cân bằng Nash giải thích tại sao các siêu cường như Mỹ và Liên Xô duy trì trạng thái \u0026ldquo;cân bằng khủng bố\u0026rdquo; (Mutually Assured Destruction - MAD), nơi không bên nào dám tấn công trước. 4.3. Sinh học tiến hóa Hành vi của động vật: Cân bằng Nash được sử dụng để giải thích các hành vi tiến hóa của động vật, chẳng hạn như việc chọn bạn đời hoặc phân bổ nguồn lực trong môi trường sống. 4.4. Công nghệ và mạng máy tính Giao thông mạng: Cân bằng Nash giúp tối ưu hóa lưu lượng mạng, đảm bảo rằng các luồng dữ liệu được phân phối hiệu quả và tránh tắc nghẽn. Blockchain: Trong các giao dịch tiền điện tử, cân bằng Nash được áp dụng để đảm bảo rằng các nút mạng hoạt động trung thực và không phá vỡ hệ thống. 4.5. Tâm lý học và xã hội học Quyết định cá nhân: Cân bằng Nash cung cấp một khung lý thuyết để hiểu cách con người đưa ra quyết định trong các tình huống tương tác xã hội phức tạp. 5. Hạn chế của cân bằng Nash Không duy nhất: Một trò chơi có thể có nhiều cân bằng Nash, khiến việc dự đoán kết quả trở nên khó khăn. Không phải luôn công bằng: Cân bằng Nash đôi khi dẫn đến kết quả không mong muốn hoặc không hiệu quả về mặt xã hội (ví dụ: Prisoner\u0026rsquo;s Dilemma). Giả định lý tính: Cân bằng Nash giả định rằng tất cả người chơi đều hành động hợp lý và biết rõ chiến lược của đối thủ, điều này không phải lúc nào cũng đúng trong thực tế. Ứng dụng chia thưởng cho nhân viên Chúng ta sẽ phân tích một tình huống cụ thể trong doanh nghiệp, nơi trưởng phòng cần quyết định cách chia thưởng giữa các nhân viên dựa trên hiệu suất làm việc. Điều này liên quan đến lý thuyết trò chơi và cân bằng Nash khi các nhân viên có thể điều chỉnh hành vi của mình để tối đa hóa lợi ích cá nhân.\nTình huống Một trưởng phòng quản lý hai nhân viên (A và B). Trưởng phòng có một khoản tiền thưởng cố định là 10 triệu đồng để chia cho hai nhân viên dựa trên mức độ đóng góp của họ vào dự án. Tuy nhiên, trưởng phòng không thể giám sát hoàn toàn năng suất thực tế của từng người, mà chỉ có thể đánh giá qua kết quả báo cáo của họ.\nChiến lược của nhân viên:\nLàm việc chăm chỉ (C): Đòi hỏi nỗ lực cao nhưng có khả năng đạt kết quả tốt. Làm việc lười biếng (L): Tiết kiệm công sức nhưng kết quả kém hơn. Quy tắc chia thưởng:\nNếu cả hai cùng làm việc chăm chỉ (C, C), mỗi người nhận được 5 triệu đồng. Nếu một người làm việc chăm chỉ và người kia lười biếng (C, L hoặc L, C), người làm việc chăm chỉ nhận 3 triệu đồng, người lười biếng nhận 7 triệu đồng (do người lười biếng \u0026ldquo;lợi dụng\u0026rdquo; kết quả của người khác). Nếu cả hai đều lười biếng (L, L), mỗi người chỉ nhận được 2 triệu đồng (do kết quả dự án kém). Ma trận trả thưởng Bảng trả thưởng (đơn vị: triệu đồng):\nNhân viên B: Chăm chỉ (C) Nhân viên B: Lười biếng (L) Nhân viên A: Chăm chỉ (C) (5, 5) (3, 7) Nhân viên A: Lười biếng (L) (7, 3) (2, 2) Phân tích cân bằng Nash Chúng ta sẽ xác định chiến lược tối ưu cho từng nhân viên:\nNếu nhân viên B chọn \u0026ldquo;Chăm chỉ (C)\u0026rdquo;:\nNếu A chọn \u0026ldquo;Chăm chỉ (C)\u0026rdquo;, A nhận 5 triệu. Nếu A chọn \u0026ldquo;Lười biếng (L)\u0026rdquo;, A nhận 7 triệu. =\u0026gt; A sẽ chọn \u0026ldquo;Lười biếng (L)\u0026rdquo; vì 7 \u0026gt; 5. Nếu nhân viên B chọn \u0026ldquo;Lười biếng (L)\u0026rdquo;:\nNếu A chọn \u0026ldquo;Chăm chỉ (C)\u0026rdquo;, A nhận 3 triệu. Nếu A chọn \u0026ldquo;Lười biếng (L)\u0026rdquo;, A nhận 2 triệu. =\u0026gt; A sẽ chọn \u0026ldquo;Lười biếng (L)\u0026rdquo; vì 2 \u0026lt; 3. Kết luận: Dù B chọn gì, A luôn có động lực chọn \u0026ldquo;Lười biếng (L)\u0026rdquo;. Tương tự, nếu phân tích ngược lại từ góc độ của B, B cũng sẽ chọn \u0026ldquo;Lười biếng (L)\u0026rdquo;.\n=\u0026gt; Cân bằng Nash xảy ra khi cả A và B đều chọn \u0026ldquo;Lười biếng (L)\u0026rdquo;, dẫn đến mỗi người nhận 2 triệu đồng.\nÝ nghĩa trong doanh nghiệp Hiệu quả thấp: Mặc dù cả hai nhân viên đều có thể nhận được 5 triệu đồng nếu cùng làm việc chăm chỉ, họ lại rơi vào trạng thái cân bằng Nash với kết quả tệ hơn (2 triệu đồng mỗi người). Đây là một ví dụ điển hình của \u0026ldquo;Dilemma của Tù nhân\u0026rdquo; (Prisoner\u0026rsquo;s Dilemma). Nguyên nhân: Do thiếu sự phối hợp và lòng tin giữa các nhân viên, mỗi người chỉ nghĩ đến lợi ích cá nhân thay vì lợi ích tập thể. Giải pháp: Trưởng phòng có thể áp dụng các biện pháp như: Xây dựng hệ thống giám sát: Đảm bảo rằng năng suất thực tế của nhân viên được đo lường chính xác. Khuyến khích tinh thần đồng đội: Tạo môi trường làm việc khuyến khích hợp tác và chia sẻ lợi ích. Thưởng theo nhóm: Thay vì chia thưởng cá nhân, trưởng phòng có thể thưởng dựa trên kết quả chung của cả nhóm, giảm động cơ lười biếng cá nhân. Bài viết dưới góc nhìn của một con IT quèn, thằng IT lỏ, viết về một vấn đề kinh tế, bà con chuyên ngành thấy sai thì hoan hỉ còm mên nhẹ nhàng, đừng buôn lời cay đắng.\nCảm ơn bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\nNguồn tham khảo\ngõ từ khoá Nash Equilibrium\nNash, John F. (1950). \u0026ldquo;Equilibrium Points in N-Person Games.\u0026rdquo; Proceedings of the National Academy of Sciences . -\u0026gt; Bài báo của John Nash, giới thiệu khái niệm cân bằng Nash.\nMyerson, Roger B. (1997). Game Theory: Analysis of Conflict. Harvard University Press. Sách giáo khoa\nOsborne, Martin J., \u0026amp; Rubinstein, Ariel. (1994). A Course in Game Theory. MIT Press. Sách giáo khoa\nGibbons, Robert. (1992). A Primer in Game Theory. Pearson Education. - Sách về lý thuyết trò chơi, với nhiều ví dụ thực tế, thích hợp cho người mới tìm hiểu .\n","date":"Apr 11, 2025","img":"https://unsplash.it/1920/1080?image=230","permalink":"/blog/2025-04-11-hash-equilibrium/","series":null,"tags":["Game Theory"],"title":"Cân Bằng Nash - Nash Equilibrium"},{"categories":null,"content":" Giới thiệu về Prisoner’s Dilemma (Song đề tù nhân) Ứng dụng của Prisoner’s Dilemma Ứng dụng Chia thưởng giữa các nhân viên* Ứng dụng thực tế trong doanh nghiệp: Ý nghĩa tổng quát Giới thiệu về Prisoner’s Dilemma (Song đề tù nhân) Prisoner’s Dilemma là một trong những khái niệm quan trọng nhất trong lý thuyết trò chơi (Game Theory), được phát triển bởi hai nhà toán học Merrill Flood và Melvin Dresher vào năm 1950, và sau đó được Albert W. Tucker đặt tên và minh họa bằng câu chuyện về hai tù nhân.\nCâu chuyện điển hình của Prisoner’s Dilemma Hai nghi phạm (A và B) bị bắt vì liên quan đến một vụ án. Cảnh sát không có đủ bằng chứng để kết tội cả hai nếu họ không khai báo gì. Do đó, cảnh sát tách họ ra và đưa ra các lựa chọn như sau:\nNếu cả hai im lặng (hợp tác với nhau), mỗi người sẽ chỉ phải ngồi tù 1 năm vì một tội nhẹ. Nếu một người khai báo (phản bội) và người kia im lặng, người khai báo sẽ được miễn án tù (0 năm) trong khi người im lặng sẽ chịu án 3 năm. Nếu cả hai cùng khai báo (cả hai phản bội nhau), mỗi người sẽ phải chịu án 2 năm. Kết quả này được biểu diễn dưới dạng ma trận thưởng phạt:\nB Im Lặng B Khai Báo A Im Lặng (-1, -1) (-3, 0) A Khai Báo (0, -3) (-2, -2) Trong đó, số đầu tiên là số năm tù của A, số thứ hai là số năm tù của B.\nPhân tích chiến lược Chiến lược hợp tác (Im lặng): Cả hai đều im lặng và nhận 1 năm tù. Chiến lược phản bội (Khai báo): Một người phản bội và người kia hợp tác → người phản bội được tự do, người hợp tác chịu án nặng. Cả hai phản bội: Cả hai đều nhận án 2 năm tù. Nếu xét từ góc độ cá nhân:\nMỗi người đều có động lực để khai báo (phản bội) vì điều này mang lại lợi ích cá nhân tốt hơn (miễn án hoặc ít nhất là 2 năm tù thay vì 3 năm). Tuy nhiên, nếu cả hai cùng phản bội, kết quả cuối cùng (2 năm tù cho mỗi người) lại tồi tệ hơn so với trường hợp cả hai cùng hợp tác (1 năm tù). Đây chính là \u0026ldquo;dilemma\u0026rdquo; (song đề): lợi ích cá nhân dẫn đến kết quả tập thể kém hiệu quả.\nỨng dụng của Prisoner’s Dilemma Prisoner’s Dilemma không chỉ là một bài toán lý thuyết mà còn có nhiều ứng dụng thực tế trong các lĩnh vực khác nhau:\n1. Kinh tế và Kinh doanh Cạnh tranh giá cả: Hai công ty cạnh tranh có thể chọn giữa việc giữ giá cao (hợp tác) hoặc giảm giá để giành thị phần (phản bội). Nếu cả hai giảm giá, lợi nhuận của cả hai sẽ giảm sút. Quảng cáo: Hai công ty có thể chọn chi nhiều tiền cho quảng cáo (phản bội) hoặc cùng hạn chế ngân sách quảng cáo (hợp tác). Nếu cả hai cùng tăng chi tiêu, lợi nhuận ròng có thể giảm. OPEC và sản lượng dầu mỏ: Các quốc gia thành viên OPEC có thể chọn tuân thủ thỏa thuận cắt giảm sản lượng (hợp tác) hoặc tăng sản lượng để kiếm lợi nhuận cá nhân (phản bội). Nếu tất cả tăng sản lượng, giá dầu sẽ giảm, gây thiệt hại chung. 2. Chính trị và Ngoại giao Giải trừ vũ khí hạt nhân: Các quốc gia có thể chọn giữa giải trừ vũ khí (hợp tác) hoặc tiếp tục phát triển vũ khí (phản bội). Nếu một nước phản bội, họ có lợi thế quân sự; nếu cả hai cùng phát triển, nguy cơ chiến tranh tăng lên. Hiệp định môi trường: Các quốc gia có thể hợp tác để giảm phát thải khí nhà kính hoặc tiếp tục phát triển công nghiệp mà không quan tâm đến môi trường. Nếu tất cả phản bội, biến đổi khí hậu sẽ trở nên nghiêm trọng hơn. 3. Xã hội và Tâm lý học Hành vi xã hội: Trong các mối quan hệ cá nhân, người ta thường đối mặt với lựa chọn giữa hợp tác (giúp đỡ người khác) hoặc phản bội (tự bảo vệ lợi ích cá nhân). Ví dụ: đóng góp vào quỹ cộng đồng hay giữ tiền cho bản thân. Tin tưởng và lòng trung thành: Song đề tù nhân giúp giải thích tại sao con người đôi khi chọn hợp tác dù có nguy cơ bị phản bội – điều này liên quan đến xây dựng niềm tin lâu dài. 4. Sinh học và Tiến hóa Hợp tác trong tự nhiên: Trong sinh học tiến hóa, các loài động vật có thể hợp tác để săn mồi hoặc bảo vệ lãnh thổ. Tuy nhiên, hành vi phản bội (ăn trộm thức ăn của đồng loại) cũng tồn tại. Lựa chọn di truyền: Các cá thể trong một quần thể có thể chọn hợp tác để đảm bảo sự sống còn của cả nhóm hoặc cạnh tranh để tối đa hóa khả năng sinh sản cá nhân. 5. Công nghệ và An ninh mạng Phòng chống tấn công mạng: Hai tổ chức có thể hợp tác để bảo vệ hệ thống mạng hoặc cố gắng tiết kiệm chi phí bằng cách bỏ qua các biện pháp an ninh. Nếu cả hai đều bỏ qua, cả hai đều dễ bị tấn công. Chia sẻ thông tin: Các công ty công nghệ có thể chọn chia sẻ dữ liệu để cải thiện sản phẩm hoặc giữ bí mật để duy trì lợi thế cạnh tranh. 6. Trò chơi lặp đi lặp lại (Iterated Prisoner’s Dilemma) Trong phiên bản lặp đi lặp lại của song đề tù nhân, các bên tương tác nhiều lần. Điều này mở ra cơ hội để xây dựng niềm tin và áp dụng chiến lược hợp tác lâu dài. Một chiến lược nổi tiếng là \u0026ldquo;Tit-for-Tat\u0026rdquo; (ăn miếng trả miếng):\nBắt đầu bằng việc hợp tác. Sau đó, làm theo hành động của đối phương ở vòng trước. Chiến lược này đã được chứng minh là rất hiệu quả trong việc khuyến khích hợp tác. Ứng dụng Chia thưởng giữa các nhân viên* Hãy tưởng tượng một tình huống trong một công ty, nơi ông trưởng phòng cần chia thưởng dựa trên hiệu suất làm việc của hai nhân viên (A và B). Tiền thưởng có thể được phân bổ theo cách hợp tác hoặc cạnh tranh, tùy thuộc vào hành vi của hai nhân viên. Điều này tạo ra một tình huống tương tự như Prisoner’s Dilemma.\nTình huống cụ thể: Ông trưởng phòng thông báo rằng sẽ có một khoản tiền thưởng lớn cho nhóm nếu cả hai nhân viên cùng hợp tác để hoàn thành một dự án quan trọng. Tuy nhiên, nếu một người cố gắng \u0026ldquo;phản bội\u0026rdquo; bằng cách chỉ tập trung vào lợi ích cá nhân (chẳng hạn, làm việc ít hơn nhưng vẫn cố gắng nhận phần thưởng lớn), thì người kia sẽ chịu thiệt thòi. Nếu cả hai cùng phản bội (cả hai đều lười biếng hoặc không hợp tác), dự án sẽ thất bại và cả hai đều không nhận được thưởng. Ma trận thưởng phạt: Chúng ta có thể biểu diễn tình huống này dưới dạng ma trận thưởng phạt:\nB Hợp Tác B Không Hợp Tác A Hợp Tác (5 triệu, 5 triệu) (1 triệu, 8 triệu) A Không Hợp Tác (8 triệu, 1 triệu) (2 triệu, 2 triệu) Trong đó:\nSố đầu tiên là số tiền thưởng của A. Số thứ hai là số tiền thưởng của B. Phân tích chiến lược: Cả hai hợp tác:\nCả A và B đều làm việc chăm chỉ và hoàn thành dự án tốt. Mỗi người nhận được 5 triệu đồng. Đây là kết quả tối ưu cho cả hai. Một người hợp tác, người kia không hợp tác:\nNếu A hợp tác và B không hợp tác: A nhận 1 triệu (do phải gánh vác phần việc của B), trong khi B nhận 8 triệu (vì không làm gì nhưng vẫn được thưởng). Ngược lại, nếu B hợp tác và A không hợp tác: B nhận 1 triệu, A nhận 8 triệu. Cả hai không hợp tác:\nDự án thất bại vì cả hai đều lười biếng. Mỗi người chỉ nhận được 2 triệu đồng. Song đề trong tình huống này: Nếu xét từ góc độ cá nhân:\nMỗi nhân viên đều có động lực để không hợp tác (phản bội) vì điều này mang lại lợi ích cá nhân cao hơn trong ngắn hạn (8 triệu so với 5 triệu nếu hợp tác). Tuy nhiên, nếu cả hai cùng không hợp tác, kết quả cuối cùng (2 triệu cho mỗi người) lại kém hơn nhiều so với trường hợp cả hai cùng hợp tác (5 triệu cho mỗi người). Từ góc độ tập thể:\nKết quả tốt nhất cho cả nhóm là cả hai cùng hợp tác, nhưng động cơ cá nhân khiến họ dễ dàng rơi vào tình trạng không hợp tác. Ứng dụng thực tế trong doanh nghiệp: Khuyến khích hợp tác:\nÔng trưởng phòng có thể thiết kế hệ thống thưởng sao cho việc hợp tác trở nên hấp dẫn hơn. Ví dụ: Thưởng thêm nếu cả nhóm hoàn thành mục tiêu chung. Áp dụng hình phạt nếu một người không đóng góp đủ (ví dụ: giảm lương hoặc cắt thưởng cá nhân). Xây dựng lòng tin:\nNếu hai nhân viên đã từng hợp tác thành công trong quá khứ, họ sẽ có xu hướng tin tưởng nhau hơn và tiếp tục hợp tác trong tương lai. Minh bạch hóa quy trình:\nCông khai mức độ đóng góp của từng người để tránh tình trạng \u0026ldquo;ăn bám\u0026rdquo; hoặc lợi dụng người khác. Áp dụng chiến lược dài hạn (Iterated Prisoner’s Dilemma):\nNếu hai nhân viên phải làm việc cùng nhau nhiều lần, họ sẽ nhận ra rằng hợp tác lâu dài mang lại lợi ích lớn hơn. Điều này khuyến khích họ chọn hợp tác thay vì phản bội. Ý nghĩa tổng quát Prisoner’s Dilemma là một mô hình đơn giản nhưng sâu sắc, giúp chúng ta hiểu rõ sự xung đột giữa lợi ích cá nhân và lợi ích tập thể. Nó giải thích tại sao con người và tổ chức thường khó đạt được hợp tác hoàn hảo, ngay cả khi điều đó có lợi cho tất cả. Đồng thời, nó cũng gợi ý rằng các cơ chế khuyến khích hợp tác (như thỏa thuận, luật pháp, hoặc lòng tin) là cần thiết để vượt qua những tình huống tương tự trong cuộc sống.\nBài viết dưới góc nhìn của một con IT quèn, thằng IT lỏ, viết về một vấn đề kinh tế, bà con chuyên ngành thấy sai thì hoan hỉ còm mên nhẹ nhàng, đừng buôn lời cay đắng.\nCảm ơn bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\nNguồn tham khảo\ngõ từ khoá Prisoner’s Dilemma\nAxelrod, Robert. (1984). The Evolution of Cooperation . Basic Books. - Sách kinh điển, nên đọc\nDixit, Avinash K., \u0026amp; Nalebuff, Barry J. (2008). The Art of Strategy: A Game Theorist\u0026rsquo;s Guide to Success in Business and Life . W.W. Norton \u0026amp; Company. - Sách chứa nhiều ví dụ thực tế , có thể áp dụng vào kinh doanh và cuộc sống.\nOsborne, Martin J. (2003). An Introduction to Game Theory . Oxford University Press. - Giáo trình lý thuyết toàn diện, phân tích chi tiết, và các mô hình Một giáo trình toàn diện về lý thuyết trò chơi, bao gồm phân tích chi tiết về Prisoner’s Dilemma và các mô hình liên quan.\nPoundstone, William. (1992). Prisoner’s Dilemma: John von Neumann, Game Theory, and the Puzzle of the Bomb . Anchor Books. - lịch sử và ý nghĩa của Prisoner’s Dilemma,kết nối với các vấn đề chính trị và xã hội.\n","date":"Apr 10, 2025","img":"https://unsplash.it/1920/1080?image=230","permalink":"/blog/2025-04-10-prisoner-dilemma/","series":null,"tags":["Game Theory"],"title":"Prisoner’s Dilemma - Song Đề Tù Nhân"},{"categories":null,"content":" 1. Đề bài 2. Kết quả 2.1. Về hiệu suất 2.2. Về chuyên môn 2.3. Về tính cộng đồng 2.4. Kết quả nổi bật khác: 1. Đề bài Công ty Procter \u0026amp; Gamble (P\u0026amp;G), một công ty hàng tiêu dùng đóng gói toàn cầu. Công ty có khoảng 7,000 chuyên gia R\u0026amp;D trên toàn thế giới, hoạt động của công ty bao gồm phát triển sản phẩm từ đầu đến cuối (R\u0026amp;D to product), có quy trình R\u0026amp;D có cấu trúc tốt và lực lượng lao động có kỹ năng cao\nĐề tài công ty đặt ra: \u0026ldquo;Đánh giá tác động của Trí tuệ nhân tạo thế hệ mới (GenAI) đến hiệu quả hợp tác nhóm trong môi trường làm việc tri thức, tập trung vào ba khía cạnh chính: hiệu suất làm việc, chia sẻ chuyên môn và tương tác cộng đồng .\u0026rdquo; hay nói cách khác, Thí nghiệm này được thiết kế để kiểm tra tác động của AI đến ba khía cạnh chính của teamwork: hiệu suất, chia sẻ chuyên môn và tương tác cộng đồng trong bối cảnh làm việc thực tế\nCụ thể, nghiên cứu này nhằm trả lời các câu hỏi chính sau:\nLiệu GenAI có thể cung cấp những lợi ích về hiệu suất so với cách làm truyền thống của teamwork hay không?\nGenAI có giúp mở rộng phạm vi chuyên môn của nhân viên ngay cả khi họ thiếu một số kiến thức và kỹ năng chuyên môn nhất định không?\nGenAI có thể cung cấp loại hình tương tác cộng đồng nào mà chúng ta thường liên tưởng đến sự hợp tác giữa con người không?\nQuy mô công ty thực hiện thí nghiệm được mô tả như sau:\nSố lượng người tham gia: 811 nhân viên P\u0026amp;G\nNgười tham gia là các chuyên gia từ hai lĩnh vực: R\u0026amp;D và Commercial\nĐược phân ngẫu nhiên vào các điều kiện thí nghiệm\nPhân bổ:\n776 người được phân ngẫu nhiên vào 4 điều kiện thí nghiệm\n35 người không được phân ngẫu nhiên do vào muộn hoặc cấp bậc cao hơn band 3\nCác nhóm làm việc từ xa qua Microsoft Teams\nChia làm 4 nhóm:\nCá nhân không sử dụng AI\nNhóm 2 người không sử dụng AI\nCá nhân sử dụng AI\nNhóm 2 người sử dụng AI\nCông cụ AI được sử dụng:\nDựa trên GPT-4 và truy cập thông qua Microsoft Azure\nNgười tham gia ở điều kiện có AI được đào tạo 1 giờ về cách tương tác với công cụ GenAI\nPhạm vi tổ chức:\nĐược thực hiện ở 4 đơn vị kinh doanh: Chăm sóc trẻ em, Chăm sóc phụ nữ, Chăm sóc cá nhân và Chăm sóc răng miệng Tại 2 khu vực địa lý: Châu Âu và Châu Mỹ Việc chọn P\u0026amp;G với quy mô lớn và phạm vi toàn cầu như vậy giúp tăng tính đại diện và độ tin cậy của kết quả nghiên cứu. Thời gian:\nThí nghiệm được thực hiện trong khoảng thời gian từ tháng 5 đến tháng 7 năm 2024 Dưới hình thức hội thảo phát triển sản phẩm ảo kéo dài một ngày Cách thức thu thập dữ liệu:\nThu thập trước thí nghiệm: thông tin cá nhân người tham gia Trong quá trình: ghi lại tất cả các lệnh và phản hồi của GenAI, bản ghi tương tác nhóm Sau thí nghiệm: khảo sát và phỏng vấn một số người tham gia Biến số đo lường:\nHiệu suất: Chất lượng giải pháp (thang điểm 1-10) Chuyên môn: Tính kỹ thuật của giải pháp (thang điểm 1-7) Tính cộng đồng : Cảm xúc tích cực và tiêu cực (thang điểm 1-7) 2. Kết quả Một vài kết quả mình rút ra khi đọc bài viết\n2.1. Về hiệu suất Nhóm không AI cải thiện 0.24 độ lệch chuẩn so với cá nhân không AI (p \u0026lt; 0.05) Cá nhân có AI cải thiện 0.37 độ lệch chuẩn so với cá nhân không AI (p \u0026lt; 0.01) Nhóm có AI cải thiện 0.39 độ lệch chuẩn so với cá nhân không AI (p \u0026lt; 0.01) Cá nhân có AI và nhóm có AI có chất lượng giải pháp tương đương Cá nhân có AI giảm 16.4% thời gian hoàn thành nhiệm vụ Nhóm có AI giảm 12.7% thời gian hoàn thành nhiệm vụ Nhóm có AI tạo ra giải pháp dài hơn đáng kể 2.2. Về chuyên môn Nhân viên làm việc một mình với AI đạt hiệu suất tương đương nhóm có ít nhất một nhân viên core-job ( 1 kỹ thuật + 1 chuyên môn) AI giúp xóa nhòa ranh giới giữa ý tưởng kỹ thuật và thương mại Sự khác biệt về tính kỹ thuật/thương mại giữa nhân viên R\u0026amp;D và Commercial biến mất khi sử dụng AI Không có sự khác biệt về điểm chất lượng dựa trên định hướng kỹ thuật của giải pháp 2.3. Về tính cộng đồng Cá nhân sử dụng AI tăng 0.457 độ lệch chuẩn cảm xúc tích cực (p \u0026lt; 0.01) Nhóm sử dụng AI tăng 0.635 độ lệch chuẩn cảm xúc tích cực (p \u0026lt; 0.01) Cá nhân sử dụng AI giảm 0.233 độ lệch chuẩn cảm xúc tiêu cực (p \u0026lt; 0.05) Nhóm sử dụng AI giảm 0.235 độ lệch chuẩn cảm xúc tiêu cực (p \u0026lt; 0.05) 2.4. Kết quả nổi bật khác: Nhóm có AI có khả năng tạo ra giải pháp trong top 10% cao hơn 9.2 điểm phần trăm so với nhóm kiểm soát Phân phối ý tưởng của nhóm có AI cân bằng hơn, giảm ảnh hưởng thống trị Nhiều người tham gia giữ lại hơn 75% nội dung do AI tạo ra trong giải pháp cuối cùng Một vài thông số trong bài báo\nCòn nhiều thông số quan trọng khác, các bạn có thể đọc bài báo để có thông tin chi tiết.\nCảm ơn các bạn đã đọc bài, hẹn gặp lại các bạn ở bài viết tiếp theo\nNguồn tham khảo\nhttps://www.hbs.edu/faculty/Pages/item.aspx?num=67197\nhttps://www.nber.org/papers/w33641\n","date":"Apr 7, 2025","img":"https://unsplash.it/1920/1080?image=228","permalink":"/blog/2025-04-07-generative-ai-expertise-teamwork/","series":null,"tags":["Artificial intelligence","Teamwork","Human-machine interaction","Productivity","Skills","Innovation","Field experiment"],"title":"Song Kiếm Hợp Bích Giữa Generative Ai Và Chuyên Viên. Nấc Thang Lên Thiên Đường"},{"categories":null,"content":" Lý thuyết và bối cảnh ra đời của The Dictator Game 7. Ví dụ trong doanh nghiệp Bối cảnh Các kịch bản phân bổ tiền thưởng Phân tích hành vi của ông trưởng phòng Ứng dụng của ví dụ này trong doanh nghiệp 8. So sánh với các mô hình khác Lý thuyết và bối cảnh ra đời của The Dictator Game 1. Lý thuyết nền tảng The Dictator Game là một biến thể đơn giản của các mô hình thử nghiệm trong lý thuyết trò chơi (Game Theory), một lĩnh vực nghiên cứu về cách con người đưa ra quyết định trong các tình huống tương tác chiến lược. Lý thuyết trò chơi tập trung vào việc phân tích các lựa chọn mà con người thực hiện khi kết quả không chỉ phụ thuộc vào hành động của họ mà còn phụ thuộc vào hành động của những người khác.\nTrong The Dictator Game, trọng tâm là nghiên cứu hành vi vị tha (altruism) và sự công bằng (fairness) – hai yếu tố quan trọng trong kinh tế học hành vi (Behavioral Economics). Mặc dù lý thuyết kinh tế truyền thống giả định rằng con người luôn hành động vì lợi ích cá nhân tối đa (rational self-interest), các thí nghiệm như The Dictator Game đã chứng minh rằng con người thường có xu hướng quan tâm đến lợi ích của người khác hoặc tuân theo các chuẩn mực xã hội.\n2. Bối cảnh ra đời The Dictator Game được phát triển từ Ultimatum Game, một mô hình thử nghiệm nổi tiếng trong lý thuyết trò chơi. Ultimatum Game yêu cầu hai người chơi tham gia: một người đề xuất cách chia sẻ một khoản tiền (Proposer) và người kia có quyền chấp nhận hoặc từ chối đề xuất đó (Responder). Nếu Responder từ chối, cả hai sẽ không nhận được gì.\nTuy nhiên, Ultimatum Game có một nhược điểm lớn: phản ứng của Responder có thể ảnh hưởng đến quyết định của Proposer, dẫn đến việc Proposer đưa ra các đề xuất \u0026ldquo;công bằng\u0026rdquo; hơn để tránh bị từ chối. Điều này làm phức tạp việc đánh giá liệu hành vi của Proposer có xuất phát từ lòng vị tha thật sự hay chỉ là một chiến lược để đạt được mục tiêu cá nhân.\nĐể khắc phục vấn đề này, Daniel Kahneman, Jack Knetsch và Richard Thaler đã giới thiệu The Dictator Game vào đầu những năm 1980. Trò chơi này loại bỏ vai trò của Responder, khiến quyết định của người cho (Dictator) trở nên hoàn toàn tự do và không bị ảnh hưởng bởi áp lực xã hội hoặc khả năng bị từ chối.\n3. Cách thức hoạt động: Hai người chơi được chọn: Người cho (Dictator) và Người nhận (Recipient). Người cho được trao một số tiền cố định (ví dụ: 100 USD). Người cho có quyền quyết định cách phân chia số tiền này giữa mình và người nhận. Người nhận không có quyền phản hồi, thương lượng hay từ chối. Kết quả cuối cùng phụ thuộc hoàn toàn vào quyết định của người cho. Ví dụ:\nNếu người cho quyết định giữ 80 USD và tặng 20 USD cho người nhận, thì kết quả là người cho giữ 80% tài sản, người nhận giữ 20%. Nếu người cho quyết định giữ toàn bộ số tiền, người nhận sẽ không nhận được gì. 4. Mục tiêu nghiên cứu ban đầu Mục tiêu chính của The Dictator Game là kiểm tra:\nLòng vị tha thật sự: Liệu con người có sẵn sàng chia sẻ tài sản với người khác ngay cả khi không có bất kỳ áp lực nào? Ảnh hưởng của chuẩn mực xã hội: Hành vi của con người có bị chi phối bởi các quy tắc văn hóa, đạo đức hoặc xã hội không? Sự công bằng trong phân phối tài sản: Con người có xu hướng ưu tiên sự công bằng hay lợi ích cá nhân? Kết quả ban đầu từ các thí nghiệm cho thấy rằng:\nPhần lớn người chơi vẫn chia sẻ một phần tài sản với người nhận, mặc dù họ không bắt buộc phải làm vậy. Tuy nhiên, mức độ chia sẻ thường ít hơn so với Ultimatum Game, điều này cho thấy rằng một phần hành vi vị tha trong Ultimatum Game có thể xuất phát từ áp lực xã hội. 5. Thí nghiệm Thí nghiệm The Dictator Game ban đầu được thực hiện tại Hoa Kỳ và Canada là những quốc gia phát triển với nền văn hóa cá nhân chủ nghĩa (individualistic culture), phù hợp để kiểm tra giả thuyết về lợi ích cá nhân tối đa. V ới tập mẫu chủ yếu là sinh viên đại học Princeton và University of British Columbia.\nSau đó, thí nghiệm được mở rộng sang nhiều quốc gia trên thế giới, bao gồm các nước phát triển và đang phát triển, để nghiên cứu rộng hơn về sự ảnh hưởng của văn hóa, kinh tế và tôn giáo.\nMở rộng sang các khu vực khác Nghiên cứu toàn cầu: Sau khi thí nghiệm trở nên phổ biến, các nhà nghiên cứu đã tiến hành nó ở nhiều quốc gia khác nhau, bao gồm cả các nước đang phát triển. Một số ví dụ nổi bật:\nChâu Âu: Các nghiên cứu tại Anh, Đức, Hà Lan nhằm so sánh hành vi vị tha giữa các nền văn hóa Tây Âu. Châu Á: Nhật Bản, Trung Quốc, Ấn Độ được nghiên cứu để xem xét tác động của văn hóa tập thể (collectivist culture). Châu Phi và Mỹ Latinh: Các nghiên cứu tại Kenya, Uganda, Brazil nhằm tìm hiểu hành vi chia sẻ trong các cộng đồng có thu nhập thấp. Phát hiện về sự khác biệt văn hóa:\nTại các nước cá nhân chủ nghĩa (như Hoa Kỳ, Canada), tỷ lệ chia sẻ thường thấp hơn, phản ánh xu hướng ưu tiên lợi ích cá nhân. Tại các nước tập thể chủ nghĩa (như Nhật Bản, Trung Quốc), tỷ lệ chia sẻ thường cao hơn, do ảnh hưởng của chuẩn mực xã hội và giá trị cộng đồng. Nghiên cứu trong các cộng đồng đặc thù Cộng đồng nông thôn và đô thị:\nCác nghiên cứu so sánh hành vi của người dân sống ở nông thôn và đô thị, nhằm đánh giá tác động của môi trường sống. Kết quả cho thấy rằng người dân nông thôn thường chia sẻ nhiều hơn, do mức độ gắn kết cộng đồng cao hơn. Cộng đồng thu nhập thấp:\nCác thí nghiệm tại các quốc gia đang phát triển hoặc khu vực nghèo khó thường cho thấy tỷ lệ chia sẻ cao hơn, dù số tiền ban đầu rất nhỏ. Điều này phản ánh vai trò của lòng tốt và sự tương trợ trong các cộng đồng khó khăn. 6. Kết quả Ảnh hưởng của văn hóa: Văn hóa đóng vai trò quan trọng trong việc định hình hành vi của Dictator. Ví dụ:\nỞ các quốc gia Tây phương (cá nhân chủ nghĩa), Dictator thường giữ lại phần lớn tài sản. Ở các quốc gia Đông phương (tập thể chủ nghĩa), Dictator có xu hướng chia sẻ nhiều hơn. Ảnh hưởng của kinh tế: Thu nhập bình quân đầu người cũng ảnh hưởng đến hành vi chia sẻ. Trong các cộng đồng thu nhập thấp, lòng vị tha thường cao hơn do sự tương trợ lẫn nhau là yếu tố sống còn.\nẢnh hưởng của tôn giáo: Các nghiên cứu tại các quốc gia có truyền thống tôn giáo mạnh mẽ (ví dụ: Hồi giáo, Phật giáo) thường cho thấy tỷ lệ chia sẻ cao hơn, do các giá trị đạo đức khuyến khích lòng tốt và sự công bằng.\nKinh tế học cổ điển giả định rằng con người luôn hành động vì lợi ích cá nhân tối đa, nhưng các nghiên cứu thực nghiệm như The Dictator Game đã chỉ ra rằng con người thường hành xử theo cách phi lý trí (irrational) hoặc chịu ảnh hưởng bởi các yếu tố xã hội, cảm xúc và đạo đức.\nNhững phát hiện từ The Dictator Game đã góp phần củng cố nền tảng cho các lĩnh vực như:\nKinh tế học hành vi: Nghiên cứu cách con người đưa ra quyết định trong thực tế, thay vì dựa trên các giả định lý thuyết. Tâm lý học xã hội: Hiểu rõ hơn về động lực thúc đẩy lòng vị tha, sự công bằng và lòng tham. Chính sách công: Thiết kế các chương trình phúc lợi và hỗ trợ xã hội phù hợp với hành vi thực tế của con người. 7. Ví dụ trong doanh nghiệp Hãy tưởng tượng một tình huống thực tế trong một doanh nghiệp, nơi ông trưởng phòng (giả sử tên là ông A) được giao quyền quyết định phân bổ một khoản tiền thưởng (ví dụ: 100 triệu đồng) cho nhóm nhân viên của mình. Đây là một ví dụ điển hình có thể được so sánh với The Dictator Game, trong đó ông trưởng phòng đóng vai trò là \u0026ldquo;Dictator\u0026rdquo; và các nhân viên là \u0026ldquo;Recipient\u0026rdquo;.\nBối cảnh Khoản tiền thưởng: 100 triệu đồng. Người quyết định: Ông trưởng phòng A. Nhóm nhân viên: Gồm 5 người (B, C, D, E, F). Mục tiêu của doanh nghiệp: Khoản tiền thưởng nhằm ghi nhận đóng góp của nhóm trong dự án vừa qua. Quyền lực của ông A: Ông A có toàn quyền quyết định cách phân bổ khoản tiền này mà không cần tham khảo ý kiến của nhân viên. Các kịch bản phân bổ tiền thưởng Kịch bản 1: Phân bổ công bằng Ông A quyết định chia đều khoản tiền cho 5 nhân viên:\nMỗi người nhận 20 triệu đồng. Lý do: Ông A muốn đảm bảo sự công bằng và tránh xung đột nội bộ. Ông tin rằng việc chia đều sẽ tạo động lực làm việc cho cả nhóm trong tương lai. Kịch bản 2: Phân bổ theo đóng góp Ông A đánh giá mức độ đóng góp của từng nhân viên và quyết định phân bổ như sau:\nNhân viên B: 30 triệu đồng (đóng góp nhiều nhất).\nNhân viên C: 25 triệu đồng (đóng góp quan trọng nhưng ít hơn B).\nNhân viên D: 20 triệu đồng (đóng góp trung bình).\nNhân viên E: 15 triệu đồng (đóng góp ít hơn).\nNhân viên F: 10 triệu đồng (đóng góp ít nhất).\nLý do: Ông A muốn khuyến khích nhân viên làm việc chăm chỉ hơn bằng cách khen thưởng những người có đóng góp lớn.\nKịch bản 3: Giữ lại phần lớn cho mình Ông A quyết định giữ lại 60 triệu đồng cho bản thân và chỉ chia 40 triệu đồng còn lại cho nhóm:\nNhân viên B, C, D: Mỗi người nhận 10 triệu đồng.\nNhân viên E, F: Mỗi người nhận 5 triệu đồng.\nLý do: Ông A cho rằng mình là người quản lý dự án và chịu trách nhiệm chính nên xứng đáng nhận phần lớn tiền thưởng.\nKịch bản 4: Chia sẻ hoàn toàn Ông A quyết định tặng toàn bộ 100 triệu đồng cho nhóm và không giữ lại bất kỳ khoản nào:\nMỗi nhân viên nhận 20 triệu đồng. Lý do: Ông A muốn thể hiện lòng vị tha và xây dựng tinh thần đoàn kết trong nhóm. Ông tin rằng việc đầu tư vào nhân viên sẽ mang lại lợi ích lâu dài cho doanh nghiệp. Phân tích hành vi của ông trưởng phòng Nếu ông A chọn Kịch bản 1 hoặc 4:\nHành vi của ông A phản ánh lòng vị tha và mong muốn duy trì sự công bằng trong nhóm. Điều này có thể giúp tăng cường niềm tin và động lực làm việc của nhân viên, vì họ cảm thấy được tôn trọng và ghi nhận. Nếu ông A chọn Kịch bản 2:\nHành vi của ông A phản ánh sự công bằng dựa trên đóng góp. Cách phân bổ này có thể khuyến khích nhân viên làm việc chăm chỉ hơn trong tương lai, nhưng cũng có thể gây ra xung đột nếu nhân viên cảm thấy đánh giá không công bằng. Nếu ông A chọn Kịch bản 3:\nHành vi của ông A phản ánh lợi ích cá nhân tối đa. Điều này có thể dẫn đến bất mãn trong nhóm nhân viên, giảm tinh thần làm việc và thậm chí gây ra thái độ tiêu cực đối với ông A và doanh nghiệp. Ứng dụng của ví dụ này trong doanh nghiệp Đánh giá năng lực lãnh đạo:\nCách ông A phân bổ tiền thưởng có thể được sử dụng để đánh giá phong cách lãnh đạo của ông. Ví dụ: Một nhà lãnh đạo công bằng và vị tha thường được nhân viên yêu mến và tôn trọng. Một nhà lãnh đạo ích kỷ có thể gây ra xung đột nội bộ và làm giảm hiệu suất làm việc của nhóm. Xây dựng văn hóa doanh nghiệp:\nNếu doanh nghiệp khuyến khích sự công bằng và hợp tác, các nhà quản lý nên học hỏi từ Kịch bản 1 hoặc 4. Ngược lại, nếu doanh nghiệp tập trung vào hiệu suất cá nhân, Kịch bản 2 có thể phù hợp hơn. Tăng cường lòng trung thành của nhân viên:\nKhi nhân viên cảm thấy được ghi nhận và đối xử công bằng, họ có xu hướng gắn bó lâu dài với doanh nghiệp. Kiểm tra tính minh bạch:\nDoanh nghiệp có thể sử dụng tình huống này để kiểm tra mức độ minh bạch và công bằng trong các chính sách lương thưởng. Ví dụ về việc ông trưởng phòng chia thưởng cho nhân viên là một minh họa rõ ràng cho The Dictator Game trong bối cảnh doanh nghiệp. Cách ông trưởng phòng phân bổ tiền thưởng không chỉ ảnh hưởng đến mối quan hệ giữa ông và nhân viên mà còn tác động đến văn hóa và hiệu suất làm việc của cả nhóm.\n8. So sánh với các mô hình khác Ultimatum Game vs. The Dictator Game Đặc điểm Ultimatum Game The Dictator Game Số người tham gia 2 (Proposer và Responder) 2 (Dictator và Recipient) Quyền lực của Responder Có quyền chấp nhận hoặc từ chối Không có quyền phản hồi Áp lực xã hội Có (Proposer sợ bị từ chối) Không Kết quả Thường công bằng hơn Thường ít công bằng hơn Bài viết dưới góc nhìn của một con IT quèn, thằng IT lỏ, viết về một vấn đề kinh tế, bà con chuyên ngành thấy sai thì hoan hỉ còm mên nhẹ nhàng, đừng buôn lời cay đắng.\nCảm ơn bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\nNguồn tham khảo\ngõ từ khoá The Dictator Game và tìm các cuốn sau, mỗi cuốn thì tác giả sẽ sử dụng lý thuyết cho các tập mẫu khác nhau và đưa ra kết luận, mấy sách đó toàn có bản quyền nên hơi khó kiếm. Có thể tìm qua ResearchGate hoặc gửi liên hệ với tác giả.\nDaniel Kahneman, Jack Knetsch và Richard Thaler (1986) Fairness and the Assumptions of Economics.\nErnst Fehr và Klaus Schmidt (1999) Ernst Fehr và Klaus Schmidt (1999) A Theory of Fairness, Competition, and Cooperation\nJoseph Henrich at el. (2005) Economic Man\u0026rsquo; in Cross-Cultural Perspective: Behavioral Experiments in 15 Small-Scale Societies.\nColin Camerer (2003) Behavioral Game Theory: Experiments in Strategic Interaction\nRichard Thaler (2015) Misbehaving: The Making of Behavioral Economics\nHerbert Gintis (2000) Game Theory Evolving: A Problem-Centered Introduction to Modeling Strategic Interaction\nDan Ariely (2008) Predictably Irrational: The Hidden Forces That Shape Our Decisions . HarperCollins.\n","date":"Apr 7, 2025","img":"https://unsplash.it/1920/1080?image=229","permalink":"/blog/2025-04-07-the-dictator-game/","series":null,"tags":["Game Theory"],"title":"The Dictator Game - Trò Chơi Độc Tài"},{"categories":null,"content":" 1. Lịch sử hình thành \u0026amp; phát triển 2. Ngôn ngữ lập trình 3. Phân tích Ưu và Nhược điểm 4. So sánh kiến trúc 5. Gợi ý nên sử dụng loại nào? 6. Các con số biết nói 1. Lịch sử hình thành \u0026 phát triển Redis\nLịch sử: Được phát triển lần đầu bởi Salvatore Sanfilippo (antirez) vào năm 2009. Từ đó, Redis nhanh chóng trở thành một trong những hệ thống lưu trữ dữ liệu trong bộ nhớ phổ biến nhất nhờ hiệu suất cao và tính linh hoạt trong việc xử lý các cấu trúc dữ liệu phong phú. Đội ngũ phát triển: Ban đầu do antirez tạo ra, hiện nay được duy trì và phát triển bởi Redis Labs cùng với sự đóng góp mạnh mẽ từ cộng đồng mã nguồn mở. DragonflyDB\nLịch sử: Ra đời khoảng năm 2022, DragonflyDB được xây dựng nhằm khắc phục một số hạn chế của Redis (như kiến trúc đơn luồng) bằng cách tận dụng sức mạnh của các CPU đa lõi và tối ưu hóa việc quản lý bộ nhớ. Đội ngũ phát triển: Được phát triển bởi DragonflyDB Inc., với sự lãnh đạo của Itamar Haber và đội ngũ chuyên sâu về tối ưu hóa hiệu năng và khả năng mở rộng. Valkey\nLịch sử: Xuất hiện vào khoảng năm 2023 dưới dạng một fork của Redis. Valkey được tạo ra nhằm duy trì cam kết hoàn toàn mã nguồn mở sau khi Redis có những thay đổi về giấy phép, đảm bảo tính tương thích API với Redis. Đội ngũ phát triển: Được hỗ trợ chủ yếu bởi Linux Foundation cùng với sự đóng góp của cộng đồng mã nguồn mở, tạo nên một nền tảng thay thế đáng tin cậy cho Redis. 2. Ngôn ngữ lập trình Redis: Được viết chủ yếu bằng C, cho phép tối ưu hóa hiệu suất ở mức hệ thống. DragonflyDB: Phát triển bằng C++, nhằm tận dụng kiến trúc đa luồng và cải thiện hiệu năng trên hệ thống đa lõi. Valkey: Giữ nguyên công nghệ của Redis, được viết bằng C, giúp dễ dàng duy trì sự tương thích với API của Redis. 3. Phân tích Ưu và Nhược điểm Redis Ưu điểm: Mã nguồn trưởng thành và ổn định: Được sử dụng rộng rãi trong nhiều ứng dụng sản xuất với hệ sinh thái phong phú (nhiều module, công cụ hỗ trợ, thư viện…). Hiệu suất cao: Mặc dù sử dụng kiến trúc đơn luồng, Redis vẫn đạt được hiệu năng ấn tượng nhờ tối ưu hóa cho các tác vụ I/O. Độ tin cậy: Được kiểm chứng qua thời gian với cộng đồng người dùng lớn và nhiều tài liệu hỗ trợ. Nhược điểm: Kiến trúc đơn luồng: Hạn chế khả năng tận dụng tối đa sức mạnh của các CPU đa lõi, đặc biệt dưới tải cao. Cluster phức tạp: Việc cấu hình và quản lý các cụm Redis đôi khi khá phức tạp. Thách thức giấy phép: Những thay đổi về giấy phép trong các phiên bản gần đây đã gây lo ngại cho một bộ phận người dùng. DragonflyDB Ưu điểm: Kiến trúc đa luồng: Tận dụng tối đa sức mạnh của CPU đa lõi, mang lại hiệu suất xử lý truy vấn và giảm độ trễ đáng kể. Quản lý bộ nhớ hiệu quả: Các thuật toán tối ưu giúp giảm mức tiêu thụ bộ nhớ dưới tải cao. Triển khai đơn giản: Không cần cấu hình clustering phức tạp như Redis, phù hợp với những ứng dụng cần hiệu năng cao mà không muốn đầu tư quá nhiều vào hạ tầng. Nhược điểm: Mới trên thị trường: Hệ sinh thái, tài liệu và cộng đồng hỗ trợ vẫn đang trong quá trình phát triển. Chưa kiểm chứng rộng rãi: Cần thêm thời gian để chứng minh tính ổn định và khả năng mở rộng trong môi trường production. Hạn chế về clustering: Một số tính năng clustering chưa đạt đến mức độ hoàn thiện như Redis. Valkey Ưu điểm: Tương thích API với Redis: Giúp chuyển đổi dễ dàng từ Redis mà không cần thay đổi mã nguồn. Hoàn toàn mở nguồn: Không gặp ràng buộc về giấy phép thương mại, phù hợp với các tổ chức ưu tiên giải pháp mã nguồn mở. Ổn định từ nền tảng Redis: Dựa trên công nghệ đã được kiểm chứng qua thời gian, đảm bảo tính ổn định. Nhược điểm: Ít cải tiến về hiệu suất: Không có nhiều cải tiến vượt trội so với Redis, chỉ tập trung vào việc duy trì tính mở nguồn. Cộng đồng nhỏ: Hệ sinh thái và tài liệu hướng dẫn vẫn chưa phong phú bằng Redis hoặc các giải pháp mới như DragonflyDB. Sự cạnh tranh khốc liệt: Phải đối mặt với các dự án cải tiến khác nhằm tối ưu hiệu năng và khả năng mở rộng. 4. So sánh kiến trúc Dưới đây là bảng so sánh các điểm mới trong thiết kế hệ thống của DragonflyDB so với Valkey và Redis: Chào bạn,\nDưới đây là bảng so sánh các điểm mới trong thiết kế hệ thống của DragonflyDB so với Valkey và Redis, với cột \u0026ldquo;Đặc điểm\u0026rdquo; được tách riêng:\nĐặc điểm DragonflyDB (Điểm mới) Valkey Redis (Hiện tại) Kiến trúc xử lý – Xây dựng theo kiến trúc đa luồng toàn diện, tận dụng tối đa sức mạnh của CPU đa lõi.\n– Xử lý song song nhiều truy vấn cùng lúc, giảm độ trễ dưới tải cao. - Sử dụng Multi-threaded với kiến trúc \u0026ldquo;shared-nothing\u0026rdquo;, chia dữ liệu thành các shard độc lập, mỗi shard xử lý bởi một thread riêng, giảm tranh chấp lock – Áp dụng kiến trúc đa luồng cải tiến cho xử lý I/O và thực thi lệnh.\n– Cho phép xử lý song song nhiều yêu cầu, cải thiện throughput và giảm độ trễ. – Sử dụng mô hình đơn luồng với event loop.\n– Xử lý tuần tự, tận dụng I/O phi đồng bộ nhưng không xử lý song song nhiều lệnh cùng lúc. - Dễ gây nghẽn cổ chai khi tải cao Quản lý bộ nhớ – Áp dụng các cấu trúc dữ liệu tối ưu ( DashTable thay thế Redis Dictionary, giảm metadata) và thuật toán caching tiên tiến, giảm mức tiêu thụ bộ nhớ.\n– Hiệu quả khi xử lý khối lượng dữ liệu lớn. – Cải tiến cấu trúc từ điển nội bộ để sử dụng bộ nhớ hiệu quả hơn.\n– Giảm chi phí tài nguyên và đảm bảo hiệu năng dưới tải cao. – Sử dụng các cấu trúc dữ liệu truyền thống trong C.\n– Hoạt động tốt nhưng ít tối ưu cho môi trường đa luồng và xử lý tải cao. Xử lý I/O – Hỗ trợ I/O bất đồng bộ kết hợp với batching, giảm overhead khi chuyển đổi ngữ cảnh giữa các luồng. – Sử dụng kiến trúc đa luồng trong xử lý I/O, cải thiện hiệu suất và giảm độ trễ. – Dựa vào I/O bất đồng bộ qua event loop, xử lý từng lệnh một, dẫn đến giới hạn khi đối mặt với tải lớn. Triển khai \u0026amp; Mở rộng – Thiết kế để đơn giản hóa việc triển khai trên single node với hiệu năng cao, hạn chế sự phức tạp của cluster.\n– Dễ dàng mở rộng quy mô nhờ vào kiến trúc đa luồng nội bộ. - vertical scale – Tích hợp các cải tiến về clustering như tự động chuyển đổi dự phòng và phân bổ dữ liệu thông minh.\n– Dễ dàng mở rộng quy mô trong môi trường phân tán. – Hỗ trợ Redis Cluster và Sentinel, nhưng yêu cầu cấu hình và quản lý khá phức tạp. - horizontal scale Công nghệ \u0026amp; Ngôn ngữ – Được xây dựng bằng C++ hiện đại, cho phép tận dụng các tính năng tối ưu từ ngôn ngữ và thư viện tiên tiến. – Được viết bằng C, giữ nguyên giấy phép BSD 3-clause, đảm bảo tính mở nguồn hoàn toàn.\n– Tập trung vào hiệu năng và khả năng mở rộng. – Được viết bằng C, mang lại độ ổn định cao nhưng hạn chế một số tối ưu hóa hiện đại. Những cải tiến trên giúp DragonflyDB và Valkey nâng cao hiệu năng, tối ưu hóa việc sử dụng tài nguyên và giảm độ trễ, phục vụ tốt hơn cho các ứng dụng thời gian thực và xử lý tải lớn. Tuy nhiên, mỗi hệ thống có những đặc điểm riêng, phù hợp với các nhu cầu và môi trường triển khai khác nhau.\nValkey là một dự án mã nguồn mở, được phát triển như một fork của Redis sau khi Redis chuyển sang giấy phép nguồn mở có điều kiện. Valkey giữ nguyên giấy phép BSD 3-clause, đảm bảo tính mở nguồn hoàn toàn. DragonflyDB được phát triển bởi một công ty thương mại và sử dụng giấy phép nguồn mở có điều kiện, cho phép sử dụng miễn phí nhưng hạn chế việc cung cấp như một dịch vụ đám mây thương mại. Redis, ban đầu được phát triển bởi Salvatore Sanfilippo, hiện đã chuyển sang giấy phép nguồn mở có điều kiện, hạn chế việc sử dụng trong một số trường hợp thương mại.\n5. Gợi ý nên sử dụng loại nào? Nếu bạn cần một giải pháp đã được kiểm chứng, với hệ sinh thái rộng lớn và sự hỗ trợ từ cộng đồng mạnh mẽ: Redis là lựa chọn phù hợp, đặc biệt với các ứng dụng truyền thống về bộ nhớ đệm, quản lý phiên và xử lý dữ liệu thời gian thực. Tuy nhiên, nếu bạn không ngại giải quyết các vấn đề liên quan đến cấu hình cluster hoặc một số hạn chế về kiến trúc đơn luồng, Redis vẫn là lựa chọn đáng tin cậy.\nNếu hiệu năng, khả năng tận dụng CPU đa lõi và triển khai đơn giản là ưu tiên hàng đầu của bạn: DragonflyDB có thể là lựa chọn tối ưu. Nó đem lại tốc độ xử lý vượt trội và quản lý bộ nhớ hiệu quả, phù hợp với các ứng dụng cần tốc độ cao mà không muốn phức tạp với việc cấu hình cluster.\nNếu bạn ưu tiên hoàn toàn mã nguồn mở và muốn tránh các ràng buộc giấy phép thương mại, đồng thời vẫn cần sự tương thích với Redis: Valkey là giải pháp đáng cân nhắc. Mặc dù về hiệu năng có thể không cải thiện vượt trội so với Redis, nhưng Valkey mang lại sự an tâm về mặt pháp lý và hỗ trợ cộng đồng mở.\nViệc lựa chọn giữa Redis, DragonflyDB và Valkey còn phụ thuộc vào yêu cầu cụ thể của dự án:\nRedis phù hợp với những ứng dụng cần sự ổn định, hỗ trợ đa dạng từ cộng đồng và một hệ sinh thái phong phú. DragonflyDB là giải pháp tiên tiến, tận dụng công nghệ đa luồng để cung cấp hiệu năng vượt trội trên hệ thống đa lõi, thích hợp với các ứng dụng mới đòi hỏi tốc độ cao. Valkey là lựa chọn lý tưởng nếu bạn muốn duy trì hoàn toàn tính mở nguồn và tương thích API với Redis, mặc dù có thể thiếu những cải tiến vượt trội về mặt hiệu năng. 6. Các con số biết nói Trong thử nghiệm cùng phần cứng EC2 c6gn.16xlarge DragonflyDB đạt thông lượng 3,8 triệu yêu cầu mỗi giây, cao hơn rất nhiều so với redis\nTrong các thử nghiệm với dung lượng lưu trữ 5GB, DragonflyDB yêu cầu ít hơn 30% bộ nhớ so với Redis\nHy vọng với bảng so sánh và phân tích trên, bạn có thể đưa ra quyết định phù hợp nhất cho nhu cầu dự án của mình.\nNguồn tham khảo:\nhttps://medium.com/%40mohitdehuliya/dragonflydb-vs-redis-a-deep-dive-towards-the-next-gen-caching-infrastructure-23186397b3d3\nhttps://www.dragonflydb.io/guides/valkey-vs-redis?utm_source=phamduytung.com\nhttps://redisson.org/articles/valkey-vs-redis-comparision.html?utm_source=phamduytung.com\nhttps://en.wikipedia.org/wiki/Valkey?utm_source=phamduytung.com\nhttps://db-engines.com/en/system/Dragonfly%3BKeyDB%3BValkey?utm_source=phamduytung.com\n","date":"Feb 17, 2025","img":"https://unsplash.it/1920/1080?image=206","permalink":"/blog/2025-02-17-redis-dragonflydb-valkey/","series":null,"tags":["DragonflyDB","Redis","Valkey"],"title":"Compare DragonflyDB vs Redis vs Valkey"},{"categories":null,"content":" 1. Hỗ trợ torch.compile cho Python 3.13 2. Giới thiệu torch.compiler.set_stance 3. Tăng cường AOTInductor 4. Hỗ trợ FP16 trên CPU X86 5. Cải thiện trải nghiệm người dùng PyTorch trên GPU Intel 6. Giới thiệu torch.library.triton_op 7. FlexAttention cho LLMs trên CPU X86 8. Dim.AUTO 9. Thay đổi trong tham số weights_only của torch.load 10. Ngừng hỗ trợ kênh Anaconda chính thức của PyTorch Kết luận Tài liệu tham khảo PyTorch 2.6, được phát hành vào ngày 29 tháng 1 năm 2025, mang đến nhiều cải tiến và tính năng mới so với các phiên bản trước đó. Dưới đây là tổng quan về những điểm nổi bật trong phiên bản này:\n1. Hỗ trợ torch.compile cho Python 3.13 Trước đây, torch.compile chỉ hỗ trợ đến phiên bản Python 3.12. Trong phiên bản 2.6, PyTorch đã mở rộng hỗ trợ cho Python 3.13, cho phép người dùng tối ưu hóa mô hình với torch.compile trên phiên bản Python mới nhất.\nĐánh giá: Việc mở rộng này giúp cộng đồng người dùng Python cập nhật và sử dụng các tính năng mới nhất của ngôn ngữ mà không gặp trở ngại về tương thích với PyTorch.\n2. Giới thiệu torch.compiler.set_stance Tính năng này cho phép người dùng chỉ định các hành vi khác nhau (\u0026ldquo;stances\u0026rdquo;) mà torch.compile có thể thực hiện giữa các lần gọi hàm đã biên dịch. Một trong những stance, chẳng hạn như \u0026ldquo;eager_on_recompile\u0026rdquo;, hướng dẫn PyTorch thực thi eagerly khi cần biên dịch lại, tái sử dụng mã đã biên dịch được lưu trong bộ nhớ cache khi có thể.\nĐánh giá: Tính năng này cung cấp sự linh hoạt cho người dùng trong việc kiểm soát quá trình biên dịch, giúp tối ưu hóa hiệu suất và quản lý tài nguyên hiệu quả hơn.\n3. Tăng cường AOTInductor Phiên bản 2.6 giới thiệu một định dạng gói mới, \u0026ldquo;PT2 archive\u0026rdquo;, chứa tất cả các tệp cần thiết cho AOTInductor, cho phép người dùng gửi mọi thứ cần thiết đến các môi trường khác. Ngoài ra, còn có chức năng đóng gói nhiều mô hình vào một artifact và lưu trữ thêm metadata bên trong gói.\nĐánh giá: Những cải tiến này giúp việc triển khai và phân phối mô hình trở nên dễ dàng và linh hoạt hơn, đặc biệt hữu ích trong các môi trường sản xuất và khi làm việc với nhiều mô hình.\n4. Hỗ trợ FP16 trên CPU X86 Một điểm nổi bật khác của phiên bản này là hỗ trợ FP16 trên CPU X86, mở rộng khả năng tính toán số học dấu phẩy động 16-bit trên các CPU phổ biến.\nĐánh giá: Điều này có thể cải thiện hiệu suất cho các mô hình yêu cầu tính toán dấu phẩy động 16-bit, đặc biệt hữu ích cho các ứng dụng yêu cầu hiệu suất cao trên phần cứng CPU.\n5. Cải thiện trải nghiệm người dùng PyTorch trên GPU Intel Phiên bản 2.6 mang lại trải nghiệm người dùng được cải thiện trên GPU Intel, đặc biệt trên Windows. Điều này bao gồm thiết lập phần mềm dễ dàng hơn, các binary Windows được cải thiện và mở rộng phạm vi của các toán tử Aten trên GPU Intel với các kernel SYCL.\nĐánh giá: Những cải tiến này làm cho PyTorch trở nên thân thiện hơn với người dùng sử dụng GPU Intel, mở rộng phạm vi phần cứng được hỗ trợ và cải thiện hiệu suất trên các thiết bị này.\n6. Giới thiệu torch.library.triton_op torch.library.triton_op cung cấp một cách tiêu chuẩn để tạo các toán tử tùy chỉnh được hỗ trợ bởi các kernel triton do người dùng định nghĩa. Khi người dùng chuyển các kernel triton do họ định nghĩa thành các toán tử tùy chỉnh, torch.library.triton_op cho phép torch.compile xem xét vào việc triển khai, cho phép torch.compile tối ưu hóa kernel triton bên trong nó.\nĐánh giá: Tính năng này mở ra khả năng mở rộng và tùy chỉnh cao hơn cho người dùng, cho phép họ tích hợp các kernel triton tùy chỉnh một cách liền mạch và tối ưu hóa chúng trong quá trình biên dịch.\n7. FlexAttention cho LLMs trên CPU X86 PyTorch 2.6 giới thiệu FlexAttention, một cải tiến đáng kể cho việc xử lý mô hình ngôn ngữ lớn (LLMs) trên CPU X86. FlexAttention giúp tối ưu hóa việc tính toán attention, giảm độ trễ và tăng tốc độ suy luận cho các mô hình Transformer trên phần cứng CPU. Điều này đặc biệt quan trọng đối với những hệ thống không có GPU mạnh hoặc cần chạy mô hình trên các môi trường tiết kiệm chi phí.\nCụ thể, FlexAttention tận dụng các tối ưu hóa về phần cứng trên kiến trúc X86, giúp cải thiện việc quản lý bộ nhớ đệm (cache) và tăng hiệu quả xử lý ma trận trong cơ chế attention. Những cải tiến này giúp giảm đáng kể thời gian suy luận của các mô hình ngôn ngữ lớn như GPT, LLaMA khi chạy trên CPU.\nĐánh giá: Việc hỗ trợ FlexAttention trên CPU X86 là một bước tiến quan trọng, giúp mở rộng khả năng chạy mô hình ngôn ngữ lớn mà không cần phụ thuộc vào GPU. Điều này mang lại lợi ích lớn cho các doanh nghiệp và nhà nghiên cứu muốn triển khai AI trong môi trường hạn chế tài nguyên. Tuy nhiên, để đạt hiệu suất tối ưu, người dùng vẫn cần tinh chỉnh các tham số mô hình phù hợp với phần cứng cụ thể của mình.\n8. Dim.AUTO PyTorch 2.6 giới thiệu Dim.AUTO, một tính năng mới giúp người dùng viết mã linh hoạt hơn khi làm việc với tensor có kích thước động. Thay vì phải chỉ định rõ kích thước của một chiều (dimension) trong một số trường hợp nhất định, Dim.AUTO cho phép PyTorch tự động xác định kích thước phù hợp dựa trên ngữ cảnh.\nĐánh giá: Đây là một cải tiến nhỏ nhưng hữu ích, giúp đơn giản hóa mã nguồn và giảm thiểu lỗi do việc xử lý kích thước tensor phức tạp, đặc biệt trong các kiến trúc mạng sâu có cấu trúc động.\n9. Thay đổi trong tham số weights_only của torch.load Trước đây, torch.load có tham số weights_only=True, giúp người dùng chỉ tải trọng số của mô hình, bỏ qua các metadata khác. Tuy nhiên, trong PyTorch 2.6, giá trị mặc định của tham số này được thay đổi để tránh lỗi tiềm ẩn khi tải mô hình.\nĐánh giá: Việc điều chỉnh giá trị mặc định giúp đảm bảo tính nhất quán khi tải mô hình, tránh các trường hợp mất metadata quan trọng. Tuy nhiên, điều này có thể ảnh hưởng đến một số pipeline đã sử dụng weights_only=True trong các phiên bản trước.\n10. Ngừng hỗ trợ kênh Anaconda chính thức của PyTorch PyTorch 2.6 chính thức thông báo ngừng cung cấp gói cài đặt thông qua kênh Anaconda chính thức. Thay vào đó, người dùng được khuyến nghị sử dụng pip hoặc conda-forge để cài đặt PyTorch.\nĐánh giá: Đây là một thay đổi quan trọng ảnh hưởng đến người dùng Anaconda, đặc biệt là những ai quen với việc cài đặt PyTorch từ kênh chính thức. Tuy nhiên, quyết định này giúp tập trung vào các phương thức cài đặt phổ biến hơn, đảm bảo tính nhất quán và cập nhật nhanh hơn.\nKết luận PyTorch 2.6 mang lại nhiều cải tiến đáng chú ý, bao gồm hỗ trợ Python 3.13, cải thiện hiệu suất trên GPU Intel, hỗ trợ FP16 trên CPU X86, và giới thiệu các tính năng mới như Dim.AUTO, torch.compiler.set_stance, hay torch.library.triton_op. Những thay đổi này giúp PyTorch trở nên linh hoạt hơn, tối ưu hóa hiệu suất tốt hơn, và hỗ trợ mạnh mẽ hơn cho các mô hình AI/ML.\nDù có một số thay đổi có thể ảnh hưởng đến cách cài đặt và sử dụng PyTorch (như việc ngừng hỗ trợ Anaconda), hầu hết các cập nhật đều mang lại lợi ích lớn cho cộng đồng người dùng.\nTài liệu tham khảo PyTorch 2.6 Release Notes Phoronix: PyTorch 2.6 Features GitHub: PyTorch Release 2.6 Bài viết trên đã tổng hợp và phân tích các điểm mới trong PyTorch 2.6 so với các phiên bản trước, giúp bạn đọc hiểu rõ hơn về những thay đổi quan trọng. Nếu bạn có bất kỳ câu hỏi hoặc ý kiến nào, hãy để lại bình luận bên dưới!\n","date":"Jan 31, 2025","img":"https://unsplash.it/1920/1080?image=222","permalink":"/blog/2025-01-31-pytorch2.6/","series":null,"tags":["pytorch"],"title":"Pytorch 2.6"},{"categories":null,"content":" Ngữ cảnh, chẩn bị dữ liệu giả lập Mô tả dữ liệu: Mô tả Dữ Liệu Thiếu: Code mẫu bằng python Phương Pháp tái tạo Dữ Liệu Dựa Trên Decision Tree Regession Đánh giá hiệu quả của việc tái tạo dữ liệu 1.Statistical Comparison - So sánh thống kê 2.Autocorrelation 3. Phân tích xu hướng và mùa vụ - STL Decomposition (Trend and Seasonality) So Sánh Xu Hướng (Trend Comparison) So Sánh Tính Mùa Vụ (Seasonality Comparison) Hạn Chế Của tái tạo Dữ Liệu Bằng Hồi Quy Tuyến Tính Kết luận Tài liệu tham khảo Ở phần trước đó, chúng ta đã nêu lên bài toán dữ liệu chuỗi thời gian có 10% data bị NA và sử dụng linear regression để tái tạo các điểm dữ liệu NA trên. Ở bài toán này, chúng ta đi vào luôn phân tích sử dụng Decision Tree Regression thay thế cho liner regression và xem thử việc thay thế nào có mang cho dữ liệu của chúng ta tốt hơn hay không\nNgữ cảnh, chẩn bị dữ liệu giả lập Mô tả dữ liệu: Một chuỗi thời gian từ ngày 1 tháng 1 năm 2025 đến ngày 30 tháng 4 năm 2025 được tạo ra với các khoảng thời gian 10 phút. chuỗi thời gian có chu kỳ ngày-đêm: cao vào ban ngày (từ 6 AM đến 6 PM) và thấp vào ban đêm. Mô tả Dữ Liệu Thiếu: 10% giá trị năng lượng được chọn ngẫu nhiên và thay thế bằng NaN để mô phỏng dữ liệu bị thiếu. Code mẫu bằng python 1import pandas as pd 2import numpy as np 3from datetime import datetime 4import matplotlib.pyplot as plt 5 6# Simulate mock energy production dataset 7def simulate_energy_data(start_date, end_date, freq=\u0026#39;10min\u0026#39;): 8 # Create a datetime index with 10-minute intervals 9 datetime_index = pd.date_range(start=start_date, end=end_date, freq=freq) 10 11 # Simulate energy production with day-night cycles 12 np.random.seed(42) # For reproducibility 13 hours = datetime_index.hour 14 day_energy = np.random.normal(loc=300, scale=30, size=len(hours)) 15 night_energy = np.random.normal(loc=50, scale=15, size=len(hours)) 16 energy_values = np.where((hours \u0026gt;= 6) \u0026amp; (hours \u0026lt;= 18), day_energy, night_energy) 17 18 # Introduce missing values (10% of the dataset) 19 num_missing = int(0.1 * len(energy_values)) # 10% of data will be missing 20 missing_indices = np.random.choice(len(energy_values), num_missing, replace=False) 21 energy_values[missing_indices] = np.nan # Set randomly selected indices to NaN 22 23 # Create DataFrame with the simulated energy data 24 energy_data = pd.DataFrame({ 25 \u0026#39;Datetime\u0026#39;: datetime_index, 26 \u0026#39;Energy_Production\u0026#39;: energy_values 27 }) 28 29 return energy_data 30 31 32def plot_origin_data(energy_data_with_missing): 33 plt.figure(figsize=(24, 6)) 34 plt.plot(energy_data_with_missing[\u0026#39;Datetime\u0026#39;], energy_data_with_missing[\u0026#39;Energy_Production\u0026#39;], label=\u0026#39;Energy Production\u0026#39;) 35 plt.xlabel(\u0026#39;Datetime\u0026#39;) 36 plt.ylabel(\u0026#39;Energy Production\u0026#39;) 37 plt.title(\u0026#39;Simulated Energy Production Over Time\u0026#39;) 38 plt.legend() 39 40 plt.grid(True) 41 plt.show() 42# Main script 43if __name__ == \u0026#34;__main__\u0026#34;: 44 start_date = datetime(2025, 1, 1) 45 end_date = datetime(2025, 4, 40) 46 47 # Generate the simulated dataset 48 energy_data_with_missing = simulate_energy_data(start_date, end_date) 49 50 # Display the first few rows of the dataset 51 print(energy_data_with_missing.head()) 52 53 54 plot_origin_data(energy_data_with_missing) Phương Pháp tái tạo Dữ Liệu Dựa Trên Decision Tree Regession 1 2from sklearn.tree import DecisionTreeRegressor 3 4# Impute missing values using linear regression 5def impute_missing_values(data): 6 # Extract the non-missing data 7 non_missing_data = data.dropna() 8 9 # Prepare the features (X) and target (y) 10 X = non_missing_data.index.values.reshape(-1, 1) 11 y = non_missing_data[\u0026#39;Energy_Production\u0026#39;].values 12 13 # Fit the linear regression model 14 model = DecisionTreeRegressor(max_depth=5, random_state=42) 15 model.fit(X, y) 16 17 # Predict the missing values 18 missing_data = data[data[\u0026#39;Energy_Production\u0026#39;].isna()] 19 X_missing = missing_data.index.values.reshape(-1, 1) 20 predicted_values = model.predict(X_missing) 21 22 # Create a DataFrame for the imputed data 23 imputed_data = data.copy() 24 imputed_data.loc[data[\u0026#39;Energy_Production\u0026#39;].isna(), \u0026#39;Energy_Production\u0026#39;] = predicted_values 25 26 return imputed_data 27 28 29def plot_full_data(energy_data_imputed,energy_data_with_missing): 30 plt.figure(figsize=(24, 6)) 31 32 plt.plot(energy_data_imputed[\u0026#39;Datetime\u0026#39;], energy_data_imputed[\u0026#39;Energy_Production\u0026#39;], label=\u0026#39;Imputed Data using Decision regression\u0026#39;, color=\u0026#39;blue\u0026#39; ) 33 plt.plot(energy_data_with_missing[\u0026#39;Datetime\u0026#39;], energy_data_with_missing[\u0026#39;Energy_Production\u0026#39;], label=\u0026#39;Original Data\u0026#39;, color=\u0026#39;red\u0026#39;) 34 plt.xlabel(\u0026#39;Datetime\u0026#39;) 35 plt.ylabel(\u0026#39;Energy Production\u0026#39;) 36 plt.title(\u0026#39;Simulated Energy Production Over Time (Original vs Imputed)\u0026#39;) 37 plt.legend() 38 plt.grid(True) 39 plt.show() 40 41# Main script 42if __name__ == \u0026#34;__main__\u0026#34;: 43 44 start_date = datetime(2024, 1, 1) 45 end_date = datetime(2024, 4, 30) 46 47 # Generate the simulated dataset 48 energy_data_with_missing = simulate_energy_data(start_date, end_date) 49 50 51 plot_origin_data(energy_data_with_missing) 52 53 54 # Impute the missing values 55 energy_data_imputed = impute_missing_values(energy_data_with_missing.copy()) 56 57 # Display the first few rows of the dataset 58 print(energy_data_imputed.head()) 59 60 plot_full_data(energy_data_imputed,energy_data_with_missing) Nhìn đồ thị, chúng ta thấy rằng giá trị của các dữ liệu tái tạo khá tương đồng với dữ liệu gốc. Một số khoảng trống được tái tạo tốt, một số dữ liệu ở điểm cực trị cũng được tái tạo. Cần các phân tích chuyên sâu hơn\nĐánh giá hiệu quả của việc tái tạo dữ liệu 1.Statistical Comparison - So sánh thống kê So sánh các chỉ số thống kê (trung bình, độ lệch chuẩn, giá trị nhỏ nhất, giá trị lớn nhất) để đảm bảo dữ liệu tái tạo phù hợp với phân phối dữ liệu gốc.\n1 2# Statistical comparison function using describe 3def statistical_comparison(original_data, imputed_data): 4 original_stats = original_data[\u0026#39;Energy_Production\u0026#39;].describe() 5 imputed_stats = imputed_data[\u0026#39;Energy_Production\u0026#39;].describe() 6 comparison = pd.DataFrame({ 7 \u0026#39;Metric\u0026#39;: original_stats.index, 8 \u0026#39;Original Data\u0026#39;: original_stats.values, 9 \u0026#39;Imputed Data\u0026#39;: imputed_stats.values 10 }) 11 return comparison 12 13comparison = statistical_comparison(energy_data_with_missing.dropna(), energy_data_imputed) 14print(comparison) Kết quả\n1 Metric Original Data Imputed Data 20 count 15553.000000 17281.000000 31 mean 185.155737 185.170568 42 std 127.062030 120.682665 53 min -16.984058 -16.984058 64 25% 51.260972 53.345570 75 50% 257.313693 185.457881 86 75% 302.393653 298.754461 97 max 415.581945 415.581945 So sánh với Linear Regression ở bài trước đó\n1 Metric Original Data Imputed Data Linear Regression 20 count 15553.000000 17281.000000 31 mean 185.155737 185.155865 42 std 127.062030 120.541640 53 min -16.984058 -16.984058 64 25% 51.260972 53.436820 75 50% 257.313693 185.404682 86 75% 302.393653 298.653209 97 max 415.581945 415.581945 Từ bảng so sánh thống kê ở trên , chúng ta có thể rút ra những kết luận sau:\nSố Lượng (Count): Bộ dữ liệu tái tạo chứa nhiều điểm dữ liệu hơn, 17281, so với bộ dữ liệu gốc, 15553, do việc tái tạo các giá trị thiếu để đảm bảo dữ liệu đầy đủ.\nTrung Bình (Mean): Trung bình của dữ liệu tái tạo là 185.17, gần giống với giá trị trung bình của dữ liệu gốc 185.15, điều này có nghĩa là xu hướng trung tâm của dữ liệu đã được duy trì trong quá trình tái tạo dữ liệu. Tuy nhiên, Linear Regression cho giá trị trung bình gần với tập dữ liệu gốc hơn.\nĐộ Lệch Chuẩn (Standard Deviation): Dữ liệu tái tạo có sự phân tán thấp hơn (độ lệch chuẩn = 120.6 so với 127.06 của dữ liệu gốc). Điều này có thể cho thấy rằng trong quá trình tái tạo dữ liệu, các giá trị đã trở nên mượt mà hơn, giảm bớt sự phân tán\nGiá Trị Tối Thiểu và Tối Đa: Giá trị tối thiểu (-16.984058) và tối đa (415.581945) là giống nhau, điều này gợi ý rằng quá trình tái tạo dữ liệu không tạo ra các giá trị ngoại lai cực đoan hoặc không đi ra ngoài phạm vi của dữ liệu gốc.\nCác Phân Vị (Quartiles 25%, 50%, 75%):\nPhân vị thứ 25 (quartile thấp) cao hơn một chút trong dữ liệu tái tạo , 53.345570 so với 51.26 của dữ liệu gốc, cho thấy các giá trị tái tạo đã lấp đầy nhiều khoảng trống ở phạm vi thấp.\nTrung vị (50%) đã thay đổi đáng kể từ 257.31 trong dữ liệu gốc thành 185.45 trong dữ liệu tái tạo , cho thấy các giá trị tái tạo đã giảm độ lệch dữ liệu của các phần tử dữ liệu cao.\nPhân vị thứ 75 (quartile cao) 302.39 và 298.75 , gần như tương đương nhau, phản ánh rằng các giá trị cao đã được bảo tồn tốt.\nKết luận: Phương pháp hồi quy Decision Tree đã bảo tồn phân phối tổng thể và phạm vi của dữ liệu, trong khi làm giảm sự biến thiên và làm mượt dữ liệu một chút.\n2.Autocorrelation Kiểm tra xem tự tương quan của chuỗi có được duy trì sau khi tái tạo dữ liệu hay không.\n1 2 3# Autocorrelation analysis function 4def autocorrelation_analysis(original_data, imputed_data): 5 fig, axes = plt.subplots(1, 2, figsize=(15, 6)) 6 plot_acf(original_data[\u0026#39;Energy_Production\u0026#39;].dropna(), ax=axes[0], title=\u0026#39;ACF of Original Data\u0026#39;) 7 plot_acf(imputed_data[\u0026#39;Energy_Production\u0026#39;], ax=axes[1], title=\u0026#39;ACF of Imputed Data\u0026#39;) 8 plt.tight_layout() 9 plt.show() Quan sát đồ thị ACF trên, chúng ta có thể rút ra các ý chính sau\nBảo Tồn Các Phụ Thuộc - Preservation of Temporal Dependencies:\nHình dạng và sự suy giảm của ACF trong dữ liệu tái tạo có sự tương đồng đáng kể với dữ liệu gốc. Điều này cho thấy rằng các phụ thuộc thời gian đã được bảo tồn khá tốt sau quá trình tái tạo dữ liệu.\nHiệu Ứng Làm Mịn - Slight Smoothing Effect:\nACF trên dữ liệu tái tạo cho thấy giá trị ở một số độ trễ (lags) thấp hơn so với dữ liệu gốc. Điều này có thể là do mô hình hồi quy tuyến tính làm mịn các cực trị, dẫn đến giảm nhẹ mức độ biến động.\nCác Mẫu Chu Kỳ Trong ACF - Cyclic Patterns:\nCác đỉnh chu kỳ trong ACF, chẳng hạn như tính mùa vụ hàng ngày, dường như được duy trì giữa dữ liệu gốc và dữ liệu tái tạo . Điều này cho thấy rằng quá trình tái tạo dữ liệu đã bảo tồn các hành vi tuần hoàn trong tập dữ liệu.\nTính Ổn Định Chung:\nSự tương đồng giữa hai biểu đồ ACF là một dấu hiệu tích cực, cho thấy phương pháp tái tạo dữ liệu bằng hồi quy tuyến tính đã giữ lại tốt cấu trúc cốt lõi trong dữ liệu.\nKết Luận:\nPhương pháp hồi quy tuyến tính không chỉ bảo tồn mối liên hệ thời gian trong dữ liệu mà còn duy trì các mẫu chu kỳ, mặc dù có một chút hiệu ứng làm mịn.\n3. Phân tích xu hướng và mùa vụ - STL Decomposition (Trend and Seasonality) So Sánh Xu Hướng (Trend Comparison) 1 2# STL decomposition function to extract and plot trend component 3def stl_decomposition_trend(original_data, imputed_data, period): 4 stl_original = STL(original_data[\u0026#39;Energy_Production\u0026#39;] , period=period) 5 result_original = stl_original.fit() 6 trend_original = result_original.trend 7 8 stl_imputed = STL(imputed_data[\u0026#39;Energy_Production\u0026#39;], period=period) 9 result_imputed = stl_imputed.fit() 10 trend_imputed = result_imputed.trend 11 12 plt.figure(figsize=(24, 6)) 13 plt.plot(original_data[\u0026#39;Datetime\u0026#39;], trend_original, label=\u0026#39;Trend of Original Data\u0026#39;, color=\u0026#39;red\u0026#39;) 14 plt.plot(imputed_data[\u0026#39;Datetime\u0026#39;], trend_imputed, label=\u0026#39;Trend of Imputed Data\u0026#39;, color=\u0026#39;blue\u0026#39;) 15 plt.xlabel(\u0026#39;Datetime\u0026#39;) 16 plt.ylabel(\u0026#39;Trend\u0026#39;) 17 plt.title(\u0026#39;STL Decomposition Trend of Original and Imputed Data\u0026#39;) 18 plt.legend() 19 plt.grid(True) 20 plt.show() 21 22stl_decomposition_trend(energy_data_with_missing.dropna(), energy_data_imputed, period=144) # Daily seasonality (144 10-min intervals in a day) Bảo Tồn Xu Hướng Dài Hạn:\nĐường Xu hướng tái tạo (màu xanh dương) nhìn chung phù hợp với xu hướng gốc (màu đỏ), cho thấy phương pháp tái tạo hồi quy tuyến tính đã bảo tồn được chuyển động dài hạn trong dữ liệu.\nHiệu Ứng Làm Mịn:\nĐường Xu hướng tái tạo mượt mà hơn so với xu hướng gốc, đặc biệt ở những nơi xu hướng gốc có nhiều biến động. Điều này là đặc điểm của hồi quy tuyến tính, vốn có xu hướng không phản ánh tốt các biến động mạnh và làm mịn các cực trị.\nSo Sánh Tính Mùa Vụ (Seasonality Comparison) 1 2 3def stl_decomposition_seasonality(original_data, imputed_data, period): 4 stl_original = STL(original_data[\u0026#39;Energy_Production\u0026#39;], period=period) 5 result_original = stl_original.fit() 6 seasonality_original = result_original.seasonal 7 8 stl_imputed = STL(imputed_data[\u0026#39;Energy_Production\u0026#39;], period=period) 9 result_imputed = stl_imputed.fit() 10 seasonality_imputed = result_imputed.seasonal 11 12 plt.figure(figsize=(24, 6)) 13 plt.plot(original_data[\u0026#39;Datetime\u0026#39;], seasonality_original, label=\u0026#39;Seasonality of Original Data\u0026#39;, color=\u0026#39;red\u0026#39;) 14 plt.plot(imputed_data[\u0026#39;Datetime\u0026#39;], seasonality_imputed, label=\u0026#39;Seasonality of Imputed Data\u0026#39;, color=\u0026#39;blue\u0026#39;) 15 plt.xlabel(\u0026#39;Datetime\u0026#39;) 16 plt.ylabel(\u0026#39;Seasonality\u0026#39;) 17 plt.title(\u0026#39;STL Decomposition Seasonality of Original and Imputed Data\u0026#39;) 18 plt.legend() 19 plt.grid(True) 20 plt.show() 21stl_decomposition_seasonality(energy_data_with_missing.dropna(), energy_data_imputed, period=144) # Daily seasonality (144 10-min intervals in a day) Bảo Tồn tính Chu Kỳ:\nĐường mùa vụ tái tạo thể hiện ra các mẫu chu kỳ tương đối gần giống với chu kỳ của đường mùa vụ gốc, phản ánh rằng dữ liệu các chu kỳ sản xuất ngày-đêm đã được duy trì khá tốt.\nGiảm Biên Độ Chu Kỳ:\nBiên độ của đường mùa vụ tái tạo bị giảm nhẹ so với gốc, đặc biệt tại các đỉnh và đáy. Điều này cho thấy quá trình tái tạo dữ liệu đã làm cho các biến động chu kỳ ở cực trị trở nên ít mạnh mẽ hơn.\nHạn Chế Của tái tạo Dữ Liệu Bằng Hồi Quy Tuyến Tính Làm Mịn Các cực trị:\nTừ so sánh thống kê, độ lệch chuẩn thấp hơn ở dữ liệu tái tạo cho thấy tính biến động đã bị giảm.\nTừ so sánh xu hướng, xu hướng tái tạo mượt hơn và thiếu một số biến động gốc trong các mẫu dài hạn.\nGiả Định Tuyến Tính:\nHiệu ứng làm mịn trong so sánh xu hướng cho thấy phương pháp này khó bắt kịp các thay đổi phi tuyến tính trong dữ liệu, đặc biệt tại các giai đoạn có sự thay đổi đột ngột. Giảm Biên Độ Mùa Vụ:\nSo sánh tính mùa vụ cho thấy biên độ chu kỳ tái tạo thấp hơn so với gốc, với các đỉnh và đáy bị làm mịn. Điều này phù hợp với xu hướng của hồi quy trong việc kéo các giá trị cực đoan về trung bình. Kết luận Phương pháp tái tạo hồi quy tuyến tính bảo tồn được các xu hướng tổng thể và các mẫu chu kỳ, nhưng làm giảm tính biến động và làm mịn các giá trị cực đoan. Ngoài ra, nó thể hiện sự giảm nhẹ cường độ của các chu kỳ mùa vụ, điều này được phản ánh trong cả phân tích thống kê và phân rã dữ liệu.\nTrong bài toán này, sự khác biệt của Decision Tree và linear regression không rõ ràng lắm, cả hai đều đáp ứng tốt nhu cầu tái tạo dữ liệu.\nCảm ơn các bạn đã theo dõi bài viết. Xin cảm ơn và hẹn gặp lại.\nTài liệu tham khảo https://www.geeksforgeeks.org/managing-missing-data-in-linear-regression/\nhttps://towardsdatascience.com/missing-data-in-time-series-machine-learning-techniques-6b2273ff8b45\nhttps://www.geeksforgeeks.org/dataset-for-linear-regression/\nhttps://www.geeksforgeeks.org/ml-handling-missing-values/\nhttps://codezup.com/mastering-linear-regression-time-series-forecasting/\n","date":"Jan 12, 2025","img":"https://unsplash.it/1920/1080?image=204","permalink":"/blog/2025-01-12-data-missing-time-serial-decision-tree-regression/","series":null,"tags":["Missing data","time-serials"],"title":"Xử Lý Dữ Liệu Khiếm Khuyết Trong Dữ Liệu Chuỗi Thời Gian Sử Dụng Phương Pháp Decision Tree Regression - Machine Learning Techniques for Mising Data in Time-Serials Using Decision Tree Regression"},{"categories":null,"content":" Sử Dụng Học Máy Cho Việc tái tạo Dữ Liệu Thiếu Trong Chuỗi Thời Gian Khi Nào sử dụng học máy 1. Mẫu dữ liệu Phi Tuyến Tính: 2. Bộ Dữ Liệu Có Nhiều Chiều: 3. Khoảng Trống Lớn Trong Dữ Liệu: 4. Dữ Liệu bị Thiếu Không Ngẫu Nhiên: 5. Tính Chắc Chắn: Những Đánh Đổi khi sử dụng học máy thay cho các phương pháp truyền thống: Ngữ cảnh, chẩn bị dữ liệu giả lập Mô tả dữ liệu: Mô tả Dữ Liệu Thiếu: Code mẫu bằng python Phương Pháp tái tạo Dữ Liệu Dựa Trên Hồi Quy Đánh giá hiệu quả của việc tái tạo dữ liệu 1.Statistical Comparison - So sánh thống kê 2.Autocorrelation 3. Phân tích xu hướng và mùa vụ - STL Decomposition (Trend and Seasonality) So Sánh Xu Hướng (Trend Comparison) So Sánh Tính Mùa Vụ (Seasonality Comparison) Hạn Chế Của tái tạo Dữ Liệu Bằng linear regression Kết luận Tài liệu tham khảo Dữ liệu bị thiếu trong phân tích chuỗi thời gian là một thách thức phổ biến, thường xảy ra do cảm biến hỏng, lỗi truyền dữ liệu hoặc các vấn đề bảo trì. Những khoảng trống trong dữ liệu này có thể làm gián đoạn dự báo và làm lệch kết quả phân tích, khiến cho thông tin trở nên không đáng tin cậy.\nCác kỹ thuật đơn giản như forward fill hoặc interpolation thường là giải pháp mặc định để xử lý dữ liệu thiếu. Tuy nhiên, khi đối mặt với các mẫu dữ liệu phức tạp, xu hướng phi tuyến tính hoặc độ biến thiên cao, các phương pháp này thường thất bại và tạo ra kết quả không ổn định.\nSử Dụng Học Máy Cho Việc tái tạo Dữ Liệu Thiếu Trong Chuỗi Thời Gian Học máy (ML) cung cấp một phương pháp mạnh mẽ để tái tạo dữ liệu thiếu bằng cách nhận diện các mẫu và mối quan hệ trong dữ liệu. Khác với các phương pháp truyền thống thường dựa vào các giả định về xu hướng tuyến tính, học máy có thể phát hiện các mối quan hệ phi tuyến tính và đa biến phức tạp, dẫn đến việc tái tạo dữ liệu thiếu chính xác hơn.\nKhi Nào sử dụng học máy 1. Mẫu dữ liệu Phi Tuyến Tính: Các phương pháp truyền thống thường giả định rằng dữ liệu có xu hướng tuyến tính, nhưng nhiều bộ dữ liệu chuỗi thời gian thực tế có các mẫu phi tuyến mà các phương pháp này không thể nắm bắt. Các thuật toán học máy có thể học và mô hình hóa những mẫu phức tạp này, cải thiện độ chính xác của việc tái tạo dữ liệu thiếu.\n2. Bộ Dữ Liệu Có Nhiều Chiều: Khi làm việc với các bộ dữ liệu có nhiều đặc trưng hoặc biến số, các phương pháp tái tạo dữ liệu truyền thống có thể gặp khó khăn. Các kỹ thuật học máy có thể tận dụng các đặc trưng bổ sung trong bộ dữ liệu có nhiều chiều, giúp tái tạo dữ liệu thiếu hiệu quả hơn bằng cách xem xét đồng thời nhiều biến.\n3. Khoảng Trống Lớn Trong Dữ Liệu: Đối với các bộ dữ liệu có khoảng trống lớn, các phương pháp đơn giản như tái tạo giá trị trước có thể không đủ hiệu quả. Học máy có thể học được xu hướng tổng thể và các mẫu trong dữ liệu, giúp tái tạo các khoảng trống lớn một cách có ý nghĩa và chính xác hơn.\n4. Dữ Liệu bị Thiếu Không Ngẫu Nhiên: Trong các trường hợp dữ liệu thiếu không ngẫu nhiên (MNAR), các phương pháp tái tạo dữ liệu truyền thống, vốn giả định rằng dữ liệu thiếu là ngẫu nhiên, có thể dẫn đến kết quả sai lệch. Các phương pháp học máy có thể học cách xử lý những sai lệch này bằng cách hiểu các mối quan hệ tiềm ẩn trong dữ liệu, làm cho chúng trở nên mạnh mẽ và chính xác hơn.\n5. Tính Chắc Chắn: Các phương pháp học máy thường mạnh mẽ hơn, đặc biệt khi dữ liệu có các mẫu phức tạp hoặc dữ liệu thiếu tuân theo các cơ chế không xác định. Chúng có thể thích nghi với các loại dữ liệu thiếu khác nhau, giúp xử lý các nhiệm vụ tái tạo dữ liệu thiếu trong chuỗi thời gian đầy thử thách. Những Đánh Đổi khi sử dụng học máy thay cho các phương pháp truyền thống: Mặc dù các phương pháp học máy thường mang lại kết quả chính xác hơn, nhưng chúng yêu cầu nhiều tài nguyên tính toán hơn so với các phương pháp truyền thống. Tuy nhiên, tính linh hoạt và sức mạnh của chúng khiến chúng trở thành lựa chọn lý tưởng để xử lý các nhiệm vụ tái tạo dữ liệu thiếu trong chuỗi thời gian, nơi độ chính xác là yếu tố quan trọng.\nNgữ cảnh, chẩn bị dữ liệu giả lập Mô tả dữ liệu: Một chuỗi thời gian từ ngày 1 tháng 1 năm 2025 đến ngày 30 tháng 4 năm 2025 được tạo ra với các khoảng thời gian 10 phút. chuỗi thời gian có chu kỳ ngày-đêm: cao vào ban ngày (từ 6 AM đến 6 PM) và thấp vào ban đêm. Mô tả Dữ Liệu Thiếu: 10% giá trị năng lượng được chọn ngẫu nhiên và thay thế bằng NaN để mô phỏng dữ liệu bị thiếu. Code mẫu bằng python 1import pandas as pd 2import numpy as np 3from datetime import datetime 4import matplotlib.pyplot as plt 5 6# Simulate mock energy production dataset 7def simulate_energy_data(start_date, end_date, freq=\u0026#39;10min\u0026#39;): 8 # Create a datetime index with 10-minute intervals 9 datetime_index = pd.date_range(start=start_date, end=end_date, freq=freq) 10 11 # Simulate energy production with day-night cycles 12 np.random.seed(42) # For reproducibility 13 hours = datetime_index.hour 14 day_energy = np.random.normal(loc=300, scale=30, size=len(hours)) 15 night_energy = np.random.normal(loc=50, scale=15, size=len(hours)) 16 energy_values = np.where((hours \u0026gt;= 6) \u0026amp; (hours \u0026lt;= 18), day_energy, night_energy) 17 18 # Introduce missing values (10% of the dataset) 19 num_missing = int(0.1 * len(energy_values)) # 10% of data will be missing 20 missing_indices = np.random.choice(len(energy_values), num_missing, replace=False) 21 energy_values[missing_indices] = np.nan # Set randomly selected indices to NaN 22 23 # Create DataFrame with the simulated energy data 24 energy_data = pd.DataFrame({ 25 \u0026#39;Datetime\u0026#39;: datetime_index, 26 \u0026#39;Energy_Production\u0026#39;: energy_values 27 }) 28 29 return energy_data 30 31 32def plot_origin_data(energy_data_with_missing): 33 plt.figure(figsize=(24, 6)) 34 plt.plot(energy_data_with_missing[\u0026#39;Datetime\u0026#39;], energy_data_with_missing[\u0026#39;Energy_Production\u0026#39;], label=\u0026#39;Energy Production\u0026#39;) 35 plt.xlabel(\u0026#39;Datetime\u0026#39;) 36 plt.ylabel(\u0026#39;Energy Production\u0026#39;) 37 plt.title(\u0026#39;Simulated Energy Production Over Time\u0026#39;) 38 plt.legend() 39 40 plt.grid(True) 41 plt.show() 42# Main script 43if __name__ == \u0026#34;__main__\u0026#34;: 44 start_date = datetime(2025, 1, 1) 45 end_date = datetime(2025, 4, 40) 46 47 # Generate the simulated dataset 48 energy_data_with_missing = simulate_energy_data(start_date, end_date) 49 50 # Display the first few rows of the dataset 51 print(energy_data_with_missing.head()) 52 53 54 plot_origin_data(energy_data_with_missing) Phương Pháp tái tạo Dữ Liệu Dựa Trên Hồi Quy Phương pháp tái tạo dữ liệu dựa trên hồi quy sử dụng các mô hình dự đoán — một lớp thuật toán học máy, chẳng hạn như hồi quy tuyến tính hoặc các bộ hồi quy cây quyết định — để ước tính giá trị từ các mối quan hệ đã biết giữa các đặc trưng khác hoặc các mẫu thời gian, như giá trị trễ trong chuỗi thời gian.\nMô hình tái tạo vào các khoảng trống bằng cách sử dụng các xu hướng và mối quan hệ tiềm ẩn đã học từ dữ liệu.\nỞ bài viết này, chúng ta sẽ tìm hiểu cách tái tạo dữ liệu bị thiếu sử dụng mô hình hồi quy tuyến tính với thư viện LinearRegression của sklearn\n1 2from sklearn.linear_model import LinearRegression 3 4# Impute missing values using linear regression 5def impute_missing_values(data): 6 # Extract the non-missing data 7 non_missing_data = data.dropna() 8 9 # Prepare the features (X) and target (y) 10 X = non_missing_data.index.values.reshape(-1, 1) 11 y = non_missing_data[\u0026#39;Energy_Production\u0026#39;].values 12 13 # Fit the linear regression model 14 model = LinearRegression() 15 model.fit(X, y) 16 17 # Predict the missing values 18 missing_data = data[data[\u0026#39;Energy_Production\u0026#39;].isna()] 19 X_missing = missing_data.index.values.reshape(-1, 1) 20 predicted_values = model.predict(X_missing) 21 22 # Create a DataFrame for the imputed data 23 imputed_data = data.copy() 24 imputed_data.loc[data[\u0026#39;Energy_Production\u0026#39;].isna(), \u0026#39;Energy_Production\u0026#39;] = predicted_values 25 26 return imputed_data 27 28 29def plot_full_data(energy_data_imputed,energy_data_with_missing): 30 plt.figure(figsize=(24, 6)) 31 32 plt.plot(energy_data_imputed[\u0026#39;Datetime\u0026#39;], energy_data_imputed[\u0026#39;Energy_Production\u0026#39;], label=\u0026#39;Imputed Data\u0026#39;, color=\u0026#39;blue\u0026#39; ) 33 plt.plot(energy_data_with_missing[\u0026#39;Datetime\u0026#39;], energy_data_with_missing[\u0026#39;Energy_Production\u0026#39;], label=\u0026#39;Original Data\u0026#39;, color=\u0026#39;red\u0026#39;) 34 plt.xlabel(\u0026#39;Datetime\u0026#39;) 35 plt.ylabel(\u0026#39;Energy Production\u0026#39;) 36 plt.title(\u0026#39;Simulated Energy Production Over Time (Original vs Imputed)\u0026#39;) 37 plt.legend() 38 plt.grid(True) 39 plt.show() 40 41# Main script 42if __name__ == \u0026#34;__main__\u0026#34;: 43 44 start_date = datetime(2024, 1, 1) 45 end_date = datetime(2024, 4, 30) 46 47 # Generate the simulated dataset 48 energy_data_with_missing = simulate_energy_data(start_date, end_date) 49 50 51 plot_origin_data(energy_data_with_missing) 52 53 54 # Impute the missing values 55 energy_data_imputed = impute_missing_values(energy_data_with_missing.copy()) 56 57 # Display the first few rows of the dataset 58 print(energy_data_imputed.head()) 59 60 plot_full_data(energy_data_imputed,energy_data_with_missing) Code đơn giản phải không các bạn, nhìn vào đồ thị thì chúng ta tháy rằng xu hướng của missing data đã giống gần như i sì với xu hướng của dữ liệu gốc rồi đó.\nMặc dù chúng ta có thể quan sát rằng các giá trị tái tạo theo xu hướng chung của dữ liệu gốc, nhưng điều này không đủ để đánh giá mức độ hiệu quả của phương pháp tái tạo dữ liệu. Chúng ta cần đưa ra các phương pháp định lượng để chứng minh mô hình này ổn, ở mức chấp nhận được\nĐánh giá hiệu quả của việc tái tạo dữ liệu 1.Statistical Comparison - So sánh thống kê So sánh các chỉ số thống kê (trung bình, độ lệch chuẩn, giá trị nhỏ nhất, giá trị lớn nhất) để đảm bảo dữ liệu tái tạo phù hợp với phân phối dữ liệu gốc.\n1 2# Statistical comparison function using describe 3def statistical_comparison(original_data, imputed_data): 4 original_stats = original_data[\u0026#39;Energy_Production\u0026#39;].describe() 5 imputed_stats = imputed_data[\u0026#39;Energy_Production\u0026#39;].describe() 6 comparison = pd.DataFrame({ 7 \u0026#39;Metric\u0026#39;: original_stats.index, 8 \u0026#39;Original Data\u0026#39;: original_stats.values, 9 \u0026#39;Imputed Data\u0026#39;: imputed_stats.values 10 }) 11 return comparison 12 13comparison = statistical_comparison(energy_data_with_missing.dropna(), energy_data_imputed) 14print(comparison) Kết quả\n1 Metric Original Data Imputed Data 20 count 15553.000000 17281.000000 31 mean 185.155737 185.155865 42 std 127.062030 120.541640 53 min -16.984058 -16.984058 64 25% 51.260972 53.436820 75 50% 257.313693 185.404682 86 75% 302.393653 298.653209 97 max 415.581945 415.581945 Từ bảng so sánh thống kê ở trên , chúng ta có thể rút ra những kết luận sau:\nSố Lượng (Count): Bộ dữ liệu tái tạo chứa nhiều điểm dữ liệu hơn, 17281, so với bộ dữ liệu gốc, 15553, do việc tái tạo các giá trị thiếu để đảm bảo dữ liệu đầy đủ.\nTrung Bình (Mean): Trung bình của dữ liệu tái tạo là 185.1558, gần giống với giá trị trung bình của dữ liệu gốc 185.1557, điều này có nghĩa là xu hướng trung tâm của dữ liệu đã được duy trì trong quá trình tái tạo dữ liệu.\nĐộ Lệch Chuẩn (Standard Deviation): Dữ liệu tái tạo có sự phân tán thấp hơn (độ lệch chuẩn = 120.54 so với 127.06 của dữ liệu gốc). Điều này có thể cho thấy rằng trong quá trình tái tạo dữ liệu, các giá trị đã trở nên mượt mà hơn, giảm bớt sự phân tán. Hồi quy tuyến tính đã làm \u0026ldquo;phép màu dự đoán\u0026rdquo; của mình bằng cách luôn trả về các giá trị gần với trung bình.\nGiá Trị Tối Thiểu và Tối Đa: Giá trị tối thiểu (-16.984058) và tối đa (415.581945) là giống nhau, điều này gợi ý rằng quá trình tái tạo dữ liệu không tạo ra các giá trị ngoại lai cực đoan hoặc không đi ra ngoài phạm vi của dữ liệu gốc.\nCác Phân Vị (Quartiles 25%, 50%, 75%):\nPhân vị thứ 25 (quartile thấp) cao hơn một chút trong dữ liệu tái tạo , 53.43 so với 51.26 của dữ liệu gốc, cho thấy các giá trị tái tạo đã lấp đầy nhiều khoảng trống ở phạm vi thấp. Trung vị (50%) đã thay đổi đáng kể từ 257.31 trong dữ liệu gốc thành 185.40 trong dữ liệu tái tạo , cho thấy các giá trị tái tạo đã giảm độ lệch dữ liệu của các phần tử dữ liệu cao. Phân vị thứ 75 (quartile cao) 302.39 và 298.65 , gần như tương đương nhau, phản ánh rằng các giá trị cao đã được bảo tồn tốt. Kết luận: Phương pháp hồi quy tuyến tính đã bảo tồn phân phối tổng thể và phạm vi của dữ liệu, trong khi làm giảm sự biến thiên và làm mượt dữ liệu một chút.\n2.Autocorrelation Kiểm tra xem tự tương quan của chuỗi có được duy trì sau khi tái tạo dữ liệu hay không.\n1 2 3# Autocorrelation analysis function 4def autocorrelation_analysis(original_data, imputed_data): 5 fig, axes = plt.subplots(1, 2, figsize=(15, 6)) 6 plot_acf(original_data[\u0026#39;Energy_Production\u0026#39;].dropna(), ax=axes[0], title=\u0026#39;ACF of Original Data\u0026#39;) 7 plot_acf(imputed_data[\u0026#39;Energy_Production\u0026#39;], ax=axes[1], title=\u0026#39;ACF of Imputed Data\u0026#39;) 8 plt.tight_layout() 9 plt.show() Quan sát đồ thị ACF trên, chúng ta có thể rút ra các ý chính sau\nBảo Tồn Các Phụ Thuộc - Preservation of Temporal Dependencies:\nHình dạng và sự suy giảm của ACF trong dữ liệu tái tạo có sự tương đồng đáng kể với dữ liệu gốc. Điều này cho thấy rằng các phụ thuộc thời gian đã được bảo tồn khá tốt sau quá trình tái tạo dữ liệu.\nHiệu Ứng Làm Mịn - Slight Smoothing Effect:\nACF trên dữ liệu tái tạo cho thấy giá trị ở một số độ trễ (lags) thấp hơn so với dữ liệu gốc. Điều này có thể là do mô hình hồi quy tuyến tính làm mịn các cực trị, dẫn đến giảm nhẹ mức độ biến động.\nCác Mẫu Chu Kỳ Trong ACF - Cyclic Patterns:\nCác đỉnh chu kỳ trong ACF, chẳng hạn như tính mùa vụ hàng ngày, dường như được duy trì giữa dữ liệu gốc và dữ liệu tái tạo . Điều này cho thấy rằng quá trình tái tạo dữ liệu đã bảo tồn các hành vi tuần hoàn trong tập dữ liệu.\nTính Ổn Định Chung:\nSự tương đồng giữa hai biểu đồ ACF là một dấu hiệu tích cực, cho thấy phương pháp tái tạo dữ liệu bằng hồi quy tuyến tính đã giữ lại tốt cấu trúc cốt lõi trong dữ liệu.\nKết Luận:\nPhương pháp hồi quy tuyến tính không chỉ bảo tồn mối liên hệ thời gian trong dữ liệu mà còn duy trì các mẫu chu kỳ, mặc dù có một chút hiệu ứng làm mịn.\n3. Phân tích xu hướng và mùa vụ - STL Decomposition (Trend and Seasonality) So Sánh Xu Hướng (Trend Comparison) 1 2# STL decomposition function to extract and plot trend component 3def stl_decomposition_trend(original_data, imputed_data, period): 4 stl_original = STL(original_data[\u0026#39;Energy_Production\u0026#39;] , period=period) 5 result_original = stl_original.fit() 6 trend_original = result_original.trend 7 8 stl_imputed = STL(imputed_data[\u0026#39;Energy_Production\u0026#39;], period=period) 9 result_imputed = stl_imputed.fit() 10 trend_imputed = result_imputed.trend 11 12 plt.figure(figsize=(24, 6)) 13 plt.plot(original_data[\u0026#39;Datetime\u0026#39;], trend_original, label=\u0026#39;Trend of Original Data\u0026#39;, color=\u0026#39;red\u0026#39;) 14 plt.plot(imputed_data[\u0026#39;Datetime\u0026#39;], trend_imputed, label=\u0026#39;Trend of Imputed Data\u0026#39;, color=\u0026#39;blue\u0026#39;) 15 plt.xlabel(\u0026#39;Datetime\u0026#39;) 16 plt.ylabel(\u0026#39;Trend\u0026#39;) 17 plt.title(\u0026#39;STL Decomposition Trend of Original and Imputed Data\u0026#39;) 18 plt.legend() 19 plt.grid(True) 20 plt.show() 21 22stl_decomposition_trend(energy_data_with_missing.dropna(), energy_data_imputed, period=144) # Daily seasonality (144 10-min intervals in a day) Bảo Tồn Xu Hướng Dài Hạn:\nĐường Xu hướng tái tạo (màu xanh dương) nhìn chung phù hợp với xu hướng gốc (màu đỏ), cho thấy phương pháp tái tạo hồi quy tuyến tính đã bảo tồn được chuyển động dài hạn trong dữ liệu.\nHiệu Ứng Làm Mịn:\nĐường Xu hướng tái tạo mượt mà hơn so với xu hướng gốc, đặc biệt ở những nơi xu hướng gốc có nhiều biến động. Điều này là đặc điểm của hồi quy tuyến tính, vốn có xu hướng không phản ánh tốt các biến động mạnh và làm mịn các cực trị.\nSo Sánh Tính Mùa Vụ (Seasonality Comparison) 1 2 3def stl_decomposition_seasonality(original_data, imputed_data, period): 4 stl_original = STL(original_data[\u0026#39;Energy_Production\u0026#39;], period=period) 5 result_original = stl_original.fit() 6 seasonality_original = result_original.seasonal 7 8 stl_imputed = STL(imputed_data[\u0026#39;Energy_Production\u0026#39;], period=period) 9 result_imputed = stl_imputed.fit() 10 seasonality_imputed = result_imputed.seasonal 11 12 plt.figure(figsize=(24, 6)) 13 plt.plot(original_data[\u0026#39;Datetime\u0026#39;], seasonality_original, label=\u0026#39;Seasonality of Original Data\u0026#39;, color=\u0026#39;red\u0026#39;) 14 plt.plot(imputed_data[\u0026#39;Datetime\u0026#39;], seasonality_imputed, label=\u0026#39;Seasonality of Imputed Data\u0026#39;, color=\u0026#39;blue\u0026#39;) 15 plt.xlabel(\u0026#39;Datetime\u0026#39;) 16 plt.ylabel(\u0026#39;Seasonality\u0026#39;) 17 plt.title(\u0026#39;STL Decomposition Seasonality of Original and Imputed Data\u0026#39;) 18 plt.legend() 19 plt.grid(True) 20 plt.show() 21stl_decomposition_seasonality(energy_data_with_missing.dropna(), energy_data_imputed, period=144) # Daily seasonality (144 10-min intervals in a day) Bảo Tồn tính Chu Kỳ:\nĐường mùa vụ tái tạo thể hiện ra các mẫu chu kỳ tương đối gần giống với chu kỳ của đường mùa vụ gốc, phản ánh rằng dữ liệu các chu kỳ sản xuất ngày-đêm đã được duy trì khá tốt.\nGiảm Biên Độ Chu Kỳ:\nBiên độ của đường mùa vụ tái tạo bị giảm nhẹ so với gốc, đặc biệt tại các đỉnh và đáy. Điều này cho thấy quá trình tái tạo dữ liệu đã làm cho các biến động chu kỳ ở cực trị trở nên ít mạnh mẽ hơn.\nHạn Chế Của tái tạo Dữ Liệu Bằng linear regression Làm Mịn Các cực trị:\nTừ so sánh thống kê, độ lệch chuẩn thấp hơn ở dữ liệu tái tạo cho thấy tính biến động đã bị giảm.\nTừ so sánh xu hướng, xu hướng tái tạo mượt hơn và thiếu một số biến động gốc trong các mẫu dài hạn.\nGiả Định Tuyến Tính:\nHiệu ứng làm mịn trong so sánh xu hướng cho thấy phương pháp này khó bắt kịp các thay đổi phi tuyến tính trong dữ liệu, đặc biệt tại các giai đoạn có sự thay đổi đột ngột. Giảm Biên Độ Mùa Vụ:\nSo sánh tính mùa vụ cho thấy biên độ chu kỳ tái tạo thấp hơn so với gốc, với các đỉnh và đáy bị làm mịn. Điều này phù hợp với xu hướng của hồi quy trong việc kéo các giá trị cực đoan về trung bình. Kết luận Phương pháp tái tạo hồi quy tuyến tính bảo tồn được các xu hướng tổng thể và các mẫu chu kỳ, nhưng làm giảm tính biến động và làm mịn các giá trị cực đoan. Ngoài ra, nó thể hiện sự giảm nhẹ cường độ của các chu kỳ mùa vụ, điều này được phản ánh trong cả phân tích thống kê và phân rã dữ liệu.\nCảm ơn các bạn đã theo dõi bài viết. Xin cảm ơn và hẹn gặp lại.\nTài liệu tham khảo https://www.geeksforgeeks.org/managing-missing-data-in-linear-regression/\nhttps://towardsdatascience.com/missing-data-in-time-series-machine-learning-techniques-6b2273ff8b45\nhttps://www.geeksforgeeks.org/dataset-for-linear-regression/\nhttps://www.geeksforgeeks.org/ml-handling-missing-values/\nhttps://codezup.com/mastering-linear-regression-time-series-forecasting/\n","date":"Jan 10, 2025","img":"https://unsplash.it/1920/1080?image=203","permalink":"/blog/2025-01-10-data-missing-time-serial-linear-regression/","series":null,"tags":["Missing data","time-serials"],"title":"Xử Lý Dữ Liệu Khiếm Khuyết Trong Dữ Liệu Chuỗi Thời Gian Sử Dụng Phương Pháp Linear Regression - Machine Learning Techniques for Mising Data in Time-Serials Using Liner Regression"},{"categories":null,"content":" I. Bloom Filters là gì? II. Nguyên lý Bloom Filters hoạt động Cấu trúc dữ liệu Xác suất dương tính sai Công thức ước lượng số phần tử và số hàm hash III. Ưu điểm của bộ lọc Bloom 1. Tiết kiệm bộ nhớ 2. Kiểm tra thành viên nhanh chóng 3. Không có false negative 4. Dễ dàng mở rộng 5. Thiết kế đơn giản 6. Thân thiện với hệ thống phân tán 7. Ứng dụng rộng rãi 8. Hỗ trợ tính toán song song 9. Ứng dụng trong bảo mật và quyền riêng tư 10. Dễ bảo trì Khi nào nên sử dụng Bloom Filter? Hạn chế của bộ lọc Bloom Filter 1. False Positive (Kết quả dương tính giả) 2. Không hỗ trợ xóa phần tử 3. Không lưu trữ dữ liệu gốc 4. Khó điều chỉnh tỷ lệ false positive 5. Yêu cầu chọn hàm băm phù hợp 6. Không hiệu quả với tập dữ liệu nhỏ 7. Không thể mở rộng một cách đơn giản 8. Không hỗ trợ kiểm tra phủ định (No False Negatives) 9. Khó triển khai và bảo trì trong hệ thống lớn 10. Không phù hợp với dữ liệu động Khi nào không nên sử dụng Bloom Filter? Một số biến thể của Bloom Filter Double Hashing Bloom Filter: Cách hoạt động của Double Hashing Bloom Filter Ưu điểm của Double Hashing Bloom Filter Hạn chế của Double Hashing Bloom Filter Ứng dụng của Double Hashing Bloom Filter Triển khai Double Hashing Bloom Filter bằng Python Tóm tắt Partitioning Bloom Filter: Cách hoạt động của Partitioning Bloom Filter Ưu điểm của Partitioning Bloom Filter Hạn chế của Partitioning Bloom Filter Ứng dụng của Partitioning Bloom Filter Triển khai Partitioning Bloom Filter bằng Python Tóm tắt Counting Bloom Filter: Cách hoạt động của Counting Bloom Filter Ưu điểm của Counting Bloom Filter Hạn chế của Counting Bloom Filter Ứng dụng của Counting Bloom Filter Triển khai bằng Python Tóm tắt Scalable Bloom Filter Vấn đề với Bloom Filter truyền thống Scalable Bloom Filter giải quyết vấn đề như thế nào? Cấu trúc và cách hoạt động của Scalable Bloom Filter Ưu điểm của Scalable Bloom Filter Hạn chế của Scalable Bloom Filter Ứng dụng của Scalable Bloom Filter Triển khai Scalable Bloom Filter Mã giả Scalable Bloom Filter Striped Bloom Filter Đặc điểm của Striped Bloom Filter Cách hoạt động của Striped Bloom Filter Ưu điểm của Striped Bloom Filter Hạn chế của Striped Bloom Filter Triển khai bằng Python Quotient Filter (Bộ lọc thương số) Nguyên lý hoạt động Ưu điểm của Quotient Filter Hạn chế của Quotient Filter Ứng dụng của Quotient Filter So sánh với Bloom Filter Ví dụ mã giả bằng Python Kết luận Cuckoo Filter Nguyên lý hoạt động Thao tác chính trong Cuckoo Filter Ưu điểm của Cuckoo Filter Hạn chế của Cuckoo Filter Ứng dụng của Cuckoo Filter So sánh với Bloom Filter Ví dụ mã giả Cuckoo Filter bằng Python Kết luận Ứng Dụng của Bloom Filter trong Các Lĩnh Vực 1. Phát Hiện Gian Lận Tài Chính (Financial Fraud Detection) 2. Đặt Quảng Cáo (Ad Placement - Retail, Advertising) 3. Kiểm Tra Tên Người Dùng (SaaS, Content Publishing Platforms) 4. Các Ứng Dụng Khác Của Bloom Filter Bài tập Financial Fraud Detection Spell Checker Recommendation Systems Bloom Filters Giả sử bạn muốn lập một tài khoảng phở bò, username là phamduytung, bạn hăng hái hăm hở gõ vào cái tên đó trong ô username và .. bùm, phở bò báo lại cho bạn rằng username đó đã được sử dụng, bạn cố gắng thử với vài trường hợp như nhét năm sinh của bạn vào, nhét thêm chữ viết tắt của trường đại học vào, nhưng phở bò vẫn trả lời lại là tên username đó đã được sử dụng, thật là bực bội phải không?\nKhoan khoan trút nỗi bực bội hoặc tìm cách đặt tên ở đây, chúng ta trở lại bản chất của vấn đề là hệ thống search username hoạt động như thế nào?\nPhương án a: tìm kiếm tuyến tính, duyệt tất cả các username, nếu gặp 1 username trùng với username mình nhập vào -\u0026gt; báo username đó đã tồn tại\nPhương án b: tìm kiếm nhị phân binary search, so sánh tên người dùng với tên ở giữa danh sách, nếu khớp thì trả ra đã có , nếu không khớp thì xem tên người dùng lớn hơn hay nhỏ hơn tên ở giữa, nếu lớn hơn thì chúng ta sẽ chỉ lặp lại việc tìm kiếm như trên nhưng ở phạm vi còn 1 phần 2 từ tên ở giữa đến hết. Nếu nhỏ hơn thì phạm vi tìm kiếm cũng còn là 1 phần 2 từ đầu đến tên ở giữa, lặp đi lặp lại việc này đến khi tìm thấy ( trả ra tên đã sử dụng) hoặc hết phạm vi tìm kiếm (trả ra tên khả dụng). Cách này ổn nhưng việc tìm kiếm cũng trải qua khá nhiều bước.\nPhương án c: lưu toàn bộ user dưới dạng 1 cái cây rồi duyệt node. Cách này ổn, nhưng khá tốn bộ nhớ khi tên username dài\nCòn phương án nào khác không?\nCó, tất nhiên là có rồi, đó chính là Bloom Filters\nI. Bloom Filters là gì? Bloom filter, được phát minh bởi Burton Howard Bloom năm 1970, là một cấu trúc dữ liệu xác suất dựa trên thuật toán hasing. Nó được sử dụng để kiểm tra xem một phần tử có phải thuộc về một tập hợp hay không. Tất nhiên, người ta cũng có thể sử dụng các cấu trúc dữ liệu khác để thực hiện việc này, nhưng Bloom filter có ưu điểm về hiệu quả về không gian và thời gian.\nII. Nguyên lý Bloom Filters hoạt động Cấu trúc dữ liệu Bloom Filters được cấu thành từ 2 thành phần, thứ nhất là một mảng N bit , mỗi phần tử trong mảng mang giá trị 0 hoặc 1, giá trị khởi tạo ban đầu là 0. Thành phần thứ 2 là k thuật toán hash khác nhau, mỗi hàm hash sẽ được chia lấy dư cho N, và kích hoạt ô nhớ tương ứng với giá trị sau chia dư của hash.\nGiả sử\nchúng ta có N = 5, k =3 nghĩa là có 3 hàm hash , đặt tên là hash1, hash2, hash 3, từ khoá cần check và nếu không có thì thêm vào là chữ \u0026ldquo;duy\u0026rdquo; và chữ \u0026ldquo;tung\u0026rdquo;\nỞ thời điểm bắt đầu , chúng ta có 1 mảng 5 phần tử đều mang giá trị 0\n[0,0,0,0,0]\nkiểm tra chữ \u0026ldquo;duy\u0026rdquo;\nhash1(\u0026ldquo;duy\u0026rdquo;) %5 = 1 hash2(\u0026ldquo;duy\u0026rdquo;) %5 = 2 hash3(\u0026ldquo;duy\u0026rdquo;) %5 = 4\ncheck các giá trị ở vị trí 1,2,4, chúng ta có toàn số 0, do dó từ khoá chưa có trong mảng, thêm vào mảng.\nvậy mảng chúng ta thu được sau khi thêm chữ \u0026ldquo;duy\u0026rdquo; sẽ là [0,1,1,0,1]\nkiểm tra chữ \u0026ldquo;tung\u0026rdquo;\nhash1(\u0026ldquo;tung\u0026rdquo;) %5 = 1 hash2(\u0026ldquo;tung\u0026rdquo;) %5 = 3 hash3(\u0026ldquo;tung\u0026rdquo;) %5 = 4\ncheck các giá trị ở bị trí 1,3,4, chúng ta thấy ở 1 và 4 đã là 1, nhưng ở 3 là 0, vậy là chữ \u0026ldquo;tung\u0026rdquo; chưa có, thêm vào mảng\nvậy mảng chúng ta có sau khi thêm chữ \u0026ldquo;tung\u0026rdquo; sẽ là [0,1,1,1,1]\ngiờ giả sử thêm chữ \u0026ldquo;pham\u0026rdquo; nha\nkiểm tra chữ \u0026ldquo;pham\u0026rdquo;\nhash1(\u0026ldquo;tung\u0026rdquo;) %5 = 2 hash2(\u0026ldquo;tung\u0026rdquo;) %5 = 3 hash3(\u0026ldquo;tung\u0026rdquo;) %5 = 4\ncheck các giá trị ở bị trí 2,3,4, chúng ta thấy cả 3 vị trí 2 , 3, 4 đều là 1 -\u0026gt; chữ \u0026ldquo;pham\u0026rdquo; đã có, thông báo với người dùng là đã có , không thêm vào\nKhoan khoan, ủa gì kỳ vậy, rõ ràng chữ \u0026ldquo;pham\u0026rdquo; chưa có mà\nXác suất dương tính sai Đây, là vấn đề, cấu trúc này tồn tại một cái gọi là Xác suất dương tính sai, nghĩa là phần tử chưa có nhưng báo có.\nĐể hạn chế cái này, chúng ta có công thức tính, phần chứng minh xác xuất đụng độ thì chắc các bạn đọc wiki để hiểu thêm, do nó khá rõ ràng và dễ hiểu đối với các bạn đã học toán cơ bản rồi, còn bạn nào chưa học thì bỏ qua nó đi, chứ mình đem gõ lại mấy công thức này thì các bạn chưa học cũng chưa chắc sẽ hiểu\nhttps://en.wikipedia.org/wiki/Bloom_filter\nCông thức ước tính số lượng phần tử còn lại có thể được lưu trữ\n$$ [ n* =- \\frac{m}{k}ln\\left[ 1-\\frac{X}{n} \\right] ] $$\nTrong đó:\nn* là số lượng phần tử ước tính còn lại có thể được lưu trữ\nk là số lượng hàm hash\nm là chiều dài của mảng\nX là số lượng phần tử đã được gán 1\nCông thức ước lượng số phần tử và số hàm hash Có nhiều cách thức, nhưng theo wiki, phần Probability of false positives thì chúng ta sẽ có, các bạn nên đọc kỹ\n$$ [ m =-n * ln(p)/ln(2)^2 ] $$\n$$ [ k =- \\frac{m}{n}ln2 ] $$\nTrong đó:\nk là số hàm hash\nm là chiều dài mảng\nn là số lượng cần lưu trữ, ví dụ facebook mình thiết kế cho 20 tỷ username, thì set n = 20 tỷ\np là Xác suất dương tính sai, ví dụ là 0.1% thì p= 0.001\nChúng ta tính được m = 383,402,335 và k = 14\nIII. Ưu điểm của bộ lọc Bloom Bloom Filter là một cấu trúc dữ liệu nhỏ gọn và hiệu quả, thường được sử dụng để kiểm tra thành viên (membership) trong tập hợp. Dưới đây là các ưu điểm nổi bật của Bloom Filter:\n1. Tiết kiệm bộ nhớ Kích thước nhỏ gọn: Bloom Filter sử dụng ít bộ nhớ hơn nhiều so với các cấu trúc dữ liệu khác như bảng băm (hash table) hoặc danh sách. Thích hợp cho dữ liệu lớn: Đặc biệt hữu ích khi làm việc với tập dữ liệu khổng lồ, nơi mà việc lưu trữ đầy đủ các phần tử không khả thi. 2. Kiểm tra thành viên nhanh chóng Độ phức tạp O(1): Bloom Filter có thể kiểm tra một phần tử có khả năng nằm trong tập hợp hay không trong thời gian hằng số. Không lưu trữ phần tử: Điều này giúp Bloom Filter hoạt động nhanh hơn và giảm chi phí lưu trữ. 3. Không có false negative Đảm bảo độ chính xác khi kiểm tra sự tồn tại: Nếu Bloom Filter xác nhận rằng một phần tử nằm trong tập hợp, điều đó luôn đúng (không có false negative). Kiểm soát false positive: False positive (khi phần tử không thuộc tập nhưng lại được báo là thuộc) có thể được giảm bằng cách điều chỉnh kích thước mảng bit và số lượng hàm băm. 4. Dễ dàng mở rộng Điều chỉnh linh hoạt: Có thể thay đổi kích thước mảng bit hoặc số hàm băm để tối ưu hóa hiệu suất hoặc giảm tỷ lệ false positive. Các biến thể mạnh mẽ: Scalable Bloom Filter và Counting Bloom Filter cho phép Bloom Filter mở rộng hoặc hỗ trợ cập nhật dữ liệu dễ dàng. 5. Thiết kế đơn giản Không cần xử lý xung đột: Không như bảng băm, Bloom Filter không cần các cơ chế xử lý xung đột phức tạp. Không cần thay đổi kích thước: Bloom Filter không yêu cầu \u0026ldquo;resize\u0026rdquo; như các cấu trúc dữ liệu khác khi dữ liệu tăng lên. 6. Thân thiện với hệ thống phân tán Hiệu quả trên mạng: Bloom Filter có thể được truyền qua mạng với dung lượng nhỏ, giúp đồng bộ dữ liệu giữa các hệ thống phân tán hiệu quả. Ứng dụng trong hệ thống lưu trữ: Giảm số lần truy cập không cần thiết vào cơ sở dữ liệu phân tán. 7. Ứng dụng rộng rãi Đa dạng ứng dụng: Được sử dụng trong các hệ thống cơ sở dữ liệu, mạng, công cụ tìm kiếm, bảo mật, và nhiều lĩnh vực khác. Ví dụ sử dụng: Cơ sở dữ liệu: Kiểm tra nhanh sự tồn tại của khóa để giảm chi phí truy cập ổ đĩa. Bộ lọc web: Loại bỏ nhanh các URL trùng lặp hoặc không hợp lệ. Phát hiện thư rác: Xác định địa chỉ email hoặc domain trong danh sách đen. 8. Hỗ trợ tính toán song song Tính toán hàm băm độc lập: Các hàm băm có thể được tính toán song song, giúp Bloom Filter tận dụng được hệ thống đa lõi hoặc phân tán. Tăng tốc bằng phần cứng: Có thể triển khai trên phần cứng (như FPGA, GPU) để đạt hiệu năng cao. 9. Ứng dụng trong bảo mật và quyền riêng tư Truy vấn ẩn danh: Hỗ trợ các giao thức truy vấn thông tin riêng tư mà không làm lộ dữ liệu. Phát hiện nhanh chóng: Xác định IP hoặc hành vi đáng ngờ mà không cần lưu trữ toàn bộ dữ liệu nhạy cảm. 10. Dễ bảo trì Không lưu dữ liệu thô: Vì Bloom Filter không lưu trữ chính xác dữ liệu thô, chi phí quản lý và bảo trì thấp hơn. Hoạt động tĩnh: Một Bloom Filter được tạo trước có thể được sử dụng lặp lại mà không cần cập nhật. Khi nào nên sử dụng Bloom Filter? Bộ nhớ hạn chế: Khi không gian lưu trữ là vấn đề quan trọng, chẳng hạn trong các hệ thống nhúng hoặc thiết bị IoT. Cần kiểm tra nhanh: Khi tốc độ kiểm tra thành viên quan trọng hơn độ chính xác tuyệt đối. Chấp nhận false positive: Các ứng dụng có thể chịu được một số trường hợp false positive, ví dụ như bộ lọc spam. Hạn chế của bộ lọc Bloom Filter Mặc dù Bloom Filter có nhiều ưu điểm vượt trội, nhưng cũng tồn tại một số hạn chế cần xem xét trước khi sử dụng. Dưới đây là các nhược điểm chính:\n1. False Positive (Kết quả dương tính giả) Không chính xác tuyệt đối: Bloom Filter có thể báo rằng một phần tử nằm trong tập hợp mặc dù thực tế không phải vậy. Điều này xảy ra do bản chất xác suất của cấu trúc dữ liệu. Không phù hợp cho các ứng dụng yêu cầu chính xác tuyệt đối: Ví dụ, không thể sử dụng Bloom Filter để lưu trữ dữ liệu nhạy cảm hoặc khi cần đảm bảo 100% độ tin cậy. 2. Không hỗ trợ xóa phần tử Không thể xóa trong phiên bản cơ bản: Một phần tử đã được thêm vào Bloom Filter không thể bị xóa, vì việc thay đổi bất kỳ bit nào có thể ảnh hưởng đến các phần tử khác đã được băm vào cùng bit. Giải pháp: Sử dụng Counting Bloom Filter, nhưng điều này đòi hỏi nhiều bộ nhớ hơn. 3. Không lưu trữ dữ liệu gốc Không thể trích xuất lại dữ liệu: Bloom Filter chỉ lưu dấu vết của phần tử thông qua mảng bit, vì vậy không thể truy xuất lại các phần tử thực tế từ Bloom Filter. Ứng dụng hạn chế: Không thể sử dụng Bloom Filter trong các hệ thống cần lưu trữ hoặc quản lý dữ liệu thực tế. 4. Khó điều chỉnh tỷ lệ false positive Cần thiết kế trước: Tỷ lệ false positive phụ thuộc vào kích thước mảng bit, số lượng hàm băm, và số lượng phần tử. Nếu các tham số này không được thiết kế cẩn thận từ đầu, Bloom Filter có thể hoạt động không hiệu quả. Không linh hoạt: Việc thay đổi các tham số (như kích thước hoặc số hàm băm) thường đòi hỏi phải tạo lại toàn bộ Bloom Filter. 5. Yêu cầu chọn hàm băm phù hợp Hiệu suất phụ thuộc vào hàm băm: Nếu các hàm băm không được chọn tốt, chúng có thể tạo ra các xung đột lớn, dẫn đến tỷ lệ false positive cao. Hao tổn tài nguyên: Tính toán các hàm băm phức tạp có thể tiêu tốn tài nguyên CPU, đặc biệt khi sử dụng nhiều hàm băm. 6. Không hiệu quả với tập dữ liệu nhỏ Quá phức tạp so với bài toán nhỏ: Khi tập dữ liệu nhỏ, việc sử dụng Bloom Filter có thể phức tạp và tốn tài nguyên hơn so với các giải pháp khác như danh sách liên kết hoặc bảng băm. 7. Không thể mở rộng một cách đơn giản Khó thêm phần tử mới: Khi tập dữ liệu lớn hơn dự kiến, Bloom Filter ban đầu có thể không đủ để lưu trữ thêm phần tử mà không tăng tỷ lệ false positive. Giải pháp: Sử dụng Scalable Bloom Filter, nhưng điều này làm tăng độ phức tạp và chi phí. 8. Không hỗ trợ kiểm tra phủ định (No False Negatives) Chỉ kiểm tra thành viên: Bloom Filter chỉ xác nhận rằng phần tử \u0026ldquo;có thể có\u0026rdquo; hoặc \u0026ldquo;chắc chắn không có\u0026rdquo; trong tập hợp, và không thể sử dụng để so sánh hoặc tìm kiếm dữ liệu thực tế. Ứng dụng giới hạn: Không phù hợp cho các bài toán yêu cầu thông tin chính xác về phần tử (như vị trí, giá trị cụ thể). 9. Khó triển khai và bảo trì trong hệ thống lớn Cần đồng bộ hóa: Trong hệ thống phân tán, Bloom Filter cần được cập nhật hoặc đồng bộ liên tục, điều này có thể gây phức tạp khi dữ liệu thay đổi nhanh. Chi phí bộ nhớ: Mặc dù Bloom Filter tiết kiệm bộ nhớ, nhưng khi yêu cầu tỷ lệ false positive thấp, kích thước mảng bit có thể trở nên lớn, làm giảm lợi ích của nó. 10. Không phù hợp với dữ liệu động Không tối ưu cho dữ liệu thay đổi thường xuyên: Khi tập dữ liệu thay đổi liên tục (thêm hoặc xóa phần tử), Bloom Filter cơ bản không phù hợp vì không hỗ trợ xóa và tái sử dụng không gian. Khi nào không nên sử dụng Bloom Filter? Khi yêu cầu kết quả chính xác tuyệt đối (không chấp nhận false positive). Khi dữ liệu thay đổi liên tục và cần cập nhật (thêm hoặc xóa phần tử). Khi tập dữ liệu nhỏ, Bloom Filter có thể phức tạp và tốn tài nguyên hơn các giải pháp đơn giản khác. Một số biến thể của Bloom Filter Double Hashing Bloom Filter: Double Hashing Bloom Filter là một biến thể của Bloom Filter truyền thống sử dụng double hashing để tính toán nhiều giá trị băm từ một cặp hàm băm cơ sở thay vì sử dụng một tập hợp các hàm băm độc lập. Điều này giúp giảm độ phức tạp và tối ưu hóa hiệu suất khi triển khai.\nCách hoạt động của Double Hashing Bloom Filter Ý tưởng chính của Double Hashing:\nSử dụng hai hàm băm cơ bản, ( h_1(x) ) và ( h_2(x) ), để tạo ra ( k ) hàm băm cho Bloom Filter. Các giá trị băm được tính theo công thức: [ g_i(x) = (h_1(x) + i \\cdot h_2(x)) \\mod m ] ( g_i(x) ) là giá trị băm thứ ( i ) cho phần tử ( x ). ( m ) là kích thước của mảng bit. ( i ) là chỉ số (0 đến ( k-1 )). Thêm phần tử (Insert):\nTính ( k ) giá trị băm từ hai hàm băm ( h_1(x) ) và ( h_2(x) ). Đặt các bit tại các vị trí tương ứng trong mảng thành 1. Kiểm tra phần tử (Check):\nTính ( k ) giá trị băm tương tự. Kiểm tra xem tất cả các bit tương ứng đã được đặt thành 1 chưa. Ưu điểm của Double Hashing Bloom Filter Giảm số hàm băm cần thiết:\nChỉ cần hai hàm băm thay vì ( k ), giúp đơn giản hóa triển khai. Tăng hiệu quả tính toán:\nViệc tính toán các giá trị băm sử dụng ( h_1(x) ) và ( h_2(x) ) là nhanh chóng và dễ dàng. Tối ưu hóa bộ nhớ:\nKhông cần lưu trữ hoặc triển khai nhiều hàm băm riêng lẻ. Hạn chế của Double Hashing Bloom Filter Độ chính xác phụ thuộc vào hàm băm cơ sở:\nNếu ( h_1(x) ) và ( h_2(x) ) không tốt, phân phối giá trị băm có thể không đều. False positive vẫn tồn tại:\nGiống Bloom Filter truyền thống, nó không thể tránh hoàn toàn false positive. Ứng dụng của Double Hashing Bloom Filter Cơ sở dữ liệu:\nGiảm chi phí kiểm tra sự tồn tại của các khóa trong hệ thống lưu trữ. Cache và bộ lọc web:\nXác định nhanh chóng xem một URL có nằm trong danh sách chặn hay không. Hệ thống mạng:\nTheo dõi và lọc gói tin hoặc địa chỉ IP. Triển khai Double Hashing Bloom Filter bằng Python 1import hashlib 2 3class DoubleHashingBloomFilter: 4 def __init__(self, size, num_hashes): 5 self.size = size 6 self.num_hashes = num_hashes 7 self.bit_array = [0] * size 8 9 def _hashes(self, item): 10 h1 = int(hashlib.md5(item.encode()).hexdigest(), 16) % self.size 11 h2 = int(hashlib.sha256(item.encode()).hexdigest(), 16) % self.size 12 hashes = [(h1 + i * h2) % self.size for i in range(self.num_hashes)] 13 return hashes 14 15 def add(self, item): 16 indices = self._hashes(item) 17 for idx in indices: 18 self.bit_array[idx] = 1 19 20 def contains(self, item): 21 indices = self._hashes(item) 22 return all(self.bit_array[idx] for idx in indices) 23 24 25# Example Usage 26dbf = DoubleHashingBloomFilter(size=100, num_hashes=5) 27 28# Add elements 29dbf.add(\u0026#34;hello\u0026#34;) 30dbf.add(\u0026#34;world\u0026#34;) 31 32# Check elements 33print(dbf.contains(\u0026#34;hello\u0026#34;)) # True 34print(dbf.contains(\u0026#34;world\u0026#34;)) # True 35print(dbf.contains(\u0026#34;python\u0026#34;)) # False Tóm tắt Double Hashing Bloom Filter giảm số lượng hàm băm cần thiết bằng cách sử dụng hai hàm băm cơ sở để tạo ( k ) giá trị băm. Phù hợp với các ứng dụng yêu cầu hiệu suất cao, dễ triển khai và bảo toàn tính chính xác của Bloom Filter. Mã giả ở trên minh họa cách triển khai bằng cả Python và Golang. Nếu cần giải thích thêm hoặc có yêu cầu cụ thể, hãy cho mình biết nhé! 😊\nPartitioning Bloom Filter: Partitioning Bloom Filter (PBF) là một biến thể của Bloom Filter, trong đó mảng bit được chia thành các phân đoạn riêng biệt (partitions). Mỗi hàm băm chỉ ảnh hưởng đến một phân đoạn cụ thể thay vì toàn bộ mảng. Phương pháp này cải thiện khả năng phân tán và giảm khả năng các hàm băm khác nhau ghi đè lẫn nhau (collision) trong mảng bit.\nCách hoạt động của Partitioning Bloom Filter Chia mảng bit thành nhiều phân đoạn (partitions):\nMảng bit tổng thể được chia thành ( k ) phân đoạn, trong đó ( k ) là số hàm băm. Mỗi hàm băm chỉ hoạt động trên một phân đoạn riêng biệt. Thêm phần tử (Insert):\nKhi một phần tử được thêm vào, các hàm băm ánh xạ nó đến các vị trí trong từng phân đoạn. Chỉ các bit trong phân đoạn tương ứng được thiết lập. Kiểm tra phần tử (Check):\nĐể kiểm tra sự tồn tại, áp dụng các hàm băm để kiểm tra các vị trí trong các phân đoạn tương ứng. Giảm xung đột giữa các hàm băm:\nVì mỗi hàm băm chỉ làm việc trong một phân đoạn, khả năng ghi đè bit của nhau (collision) giảm đáng kể so với Bloom Filter thông thường. Ưu điểm của Partitioning Bloom Filter Phân phối đồng đều hơn:\nViệc phân chia mảng giúp giảm xung đột giữa các hàm băm và cải thiện độ chính xác. Kiểm soát false positive:\nXác suất false positive có thể giảm so với Bloom Filter thông thường nếu các phân đoạn được thiết kế tối ưu. Dễ dàng mở rộng:\nCó thể tăng số phân đoạn hoặc kích thước từng phân đoạn để phù hợp với yêu cầu cụ thể. Hạn chế của Partitioning Bloom Filter Tăng phức tạp quản lý:\nCần đảm bảo rằng mỗi hàm băm chỉ hoạt động trong phân đoạn tương ứng, tăng độ phức tạp khi triển khai. Bộ nhớ không linh hoạt:\nMỗi phân đoạn phải có kích thước giống nhau, dẫn đến việc sử dụng bộ nhớ không linh hoạt nếu dữ liệu không đồng đều. Ứng dụng của Partitioning Bloom Filter Hệ thống lưu trữ và cache:\nTheo dõi sự tồn tại của các phần tử trong các vùng dữ liệu riêng biệt. Quản lý tải trong mạng:\nChia nhỏ dữ liệu theo các nhóm (partitions) để giảm xung đột khi lưu trữ. Phân vùng cơ sở dữ liệu:\nGiúp phân tán truy vấn và dữ liệu trong các cụm (cluster) cơ sở dữ liệu. Triển khai Partitioning Bloom Filter bằng Python 1import hashlib 2 3class PartitioningBloomFilter: 4 def __init__(self, total_size, num_hashes): 5 self.num_hashes = num_hashes 6 self.partition_size = total_size // num_hashes 7 self.bit_array = [0] * total_size 8 9 def _hashes(self, item): 10 hashes = [] 11 for i in range(self.num_hashes): 12 hash_value = int(hashlib.md5((str(item) + str(i)).encode()).hexdigest(), 16) 13 # Map hash to the partition 14 partition_start = i * self.partition_size 15 index = partition_start + (hash_value % self.partition_size) 16 hashes.append(index) 17 return hashes 18 19 def add(self, item): 20 indices = self._hashes(item) 21 for idx in indices: 22 self.bit_array[idx] = 1 23 24 def contains(self, item): 25 indices = self._hashes(item) 26 return all(self.bit_array[idx] for idx in indices) 27 28 29# Example Usage 30pbf = PartitioningBloomFilter(total_size=100, num_hashes=5) 31 32# Add elements 33pbf.add(\u0026#34;hello\u0026#34;) 34pbf.add(\u0026#34;world\u0026#34;) 35 36# Check elements 37print(pbf.contains(\u0026#34;hello\u0026#34;)) # True 38print(pbf.contains(\u0026#34;world\u0026#34;)) # True 39print(pbf.contains(\u0026#34;python\u0026#34;)) # False Tóm tắt Partitioning Bloom Filter cải thiện Bloom Filter bằng cách chia mảng bit thành các phân đoạn độc lập. Nó giảm xung đột giữa các hàm băm và có thể cải thiện độ chính xác. Thích hợp cho các ứng dụng yêu cầu truy vấn nhanh và đồng thời trong các vùng dữ liệu riêng biệt. Counting Bloom Filter: Counting Bloom Filter (CBF) là một biến thể của Bloom Filter hỗ trợ thêm khả năng xóa (delete) phần tử khỏi cấu trúc dữ liệu. Trong khi Bloom Filter truyền thống chỉ có thể thêm và kiểm tra sự tồn tại của phần tử, Counting Bloom Filter cho phép cả thêm, xóa, và kiểm tra với độ chính xác tương đối.\nCách hoạt động của Counting Bloom Filter Thay vì mảng bit, sử dụng mảng đếm (counting array):\nMỗi vị trí trong mảng không còn là một bit (0 hoặc 1) mà là một số nguyên (counter). Bộ đếm ở mỗi vị trí cho biết số lần một phần tử hoặc nhiều phần tử khác nhau ánh xạ đến vị trí đó. Thêm phần tử (Insert):\nKhi thêm một phần tử, tăng giá trị của các bộ đếm tại các chỉ mục được tính bởi các hàm băm. Xóa phần tử (Delete):\nKhi xóa một phần tử, giảm giá trị của các bộ đếm tại các chỉ mục được tính bởi các hàm băm. Nếu bất kỳ bộ đếm nào giảm về 0, vị trí đó được coi là trống. Kiểm tra phần tử (Check):\nPhần tử được coi là có thể tồn tại nếu tất cả các chỉ mục băm tương ứng có giá trị lớn hơn 0. Ưu điểm của Counting Bloom Filter Hỗ trợ xóa phần tử:\nKhắc phục hạn chế lớn của Bloom Filter truyền thống là không hỗ trợ xóa. Giữ được tính gọn nhẹ:\nChỉ cần một lượng nhỏ bộ nhớ bổ sung (các bộ đếm thay vì bit). Hiệu quả với các tập dữ liệu động:\nRất phù hợp trong các ứng dụng mà các phần tử thường xuyên được thêm và xóa. Hạn chế của Counting Bloom Filter False positive:\nGiống như Bloom Filter, CBF vẫn có nguy cơ false positive (trả về \u0026ldquo;có thể tồn tại\u0026rdquo; cho phần tử không tồn tại). Không hỗ trợ kiểm tra \u0026ldquo;chắc chắn xóa\u0026rdquo;:\nKhi một phần tử bị xóa, các phần tử khác có thể ánh xạ đến cùng chỉ mục vẫn giữ các bộ đếm. Tăng yêu cầu bộ nhớ:\nMỗi vị trí trong mảng cần lưu trữ một số nguyên thay vì một bit, dẫn đến tăng đáng kể dung lượng bộ nhớ. Ứng dụng của Counting Bloom Filter Quản lý cache:\nTheo dõi các phần tử trong cache, cho phép xóa khi hết hạn hoặc khi không còn cần thiết. Hệ thống mạng:\nTheo dõi các gói tin hoặc lưu lượng mạng trong khoảng thời gian nhất định. Cơ sở dữ liệu:\nHỗ trợ trong các hệ thống lưu trữ, nơi các phần tử cần được thêm và xóa thường xuyên. Triển khai bằng Python 1import hashlib 2 3class CountingBloomFilter: 4 def __init__(self, size, num_hashes): 5 self.size = size 6 self.num_hashes = num_hashes 7 self.count_array = [0] * size 8 9 def _hashes(self, item): 10 hashes = [] 11 for i in range(self.num_hashes): 12 hash_value = int(hashlib.md5((str(item) + str(i)).encode()).hexdigest(), 16) 13 hashes.append(hash_value % self.size) 14 return hashes 15 16 def add(self, item): 17 indices = self._hashes(item) 18 for idx in indices: 19 self.count_array[idx] += 1 20 21 def delete(self, item): 22 indices = self._hashes(item) 23 for idx in indices: 24 if self.count_array[idx] \u0026gt; 0: 25 self.count_array[idx] -= 1 26 27 def contains(self, item): 28 indices = self._hashes(item) 29 return all(self.count_array[idx] \u0026gt; 0 for idx in indices) 30 31 32# Example Usage 33cbf = CountingBloomFilter(size=100, num_hashes=3) 34 35# Add elements 36cbf.add(\u0026#34;hello\u0026#34;) 37cbf.add(\u0026#34;world\u0026#34;) 38 39# Check elements 40print(cbf.contains(\u0026#34;hello\u0026#34;)) # True 41print(cbf.contains(\u0026#34;world\u0026#34;)) # True 42print(cbf.contains(\u0026#34;python\u0026#34;)) # False 43 44# Delete an element 45cbf.delete(\u0026#34;hello\u0026#34;) 46print(cbf.contains(\u0026#34;hello\u0026#34;)) # False Tóm tắt Counting Bloom Filter cho phép thêm, xóa và kiểm tra phần tử, mở rộng tính năng của Bloom Filter truyền thống. Nó phù hợp với các ứng dụng yêu cầu thao tác động với tập dữ liệu, nhưng vẫn giữ các ưu điểm về hiệu suất và bộ nhớ của Bloom Filter. Nếu cần hỗ trợ thêm, hãy cho mình biết nhé! 😊\nScalable Bloom Filter Scalable Bloom Filter (SBF) là một biến thể của Bloom Filter được thiết kế để khắc phục hạn chế cố hữu của Bloom Filter truyền thống: kích thước cố định. SBF có thể mở rộng động khi số lượng phần tử tăng mà không làm mất tính hiệu quả hoặc chính xác của Bloom Filter.\nVấn đề với Bloom Filter truyền thống Giới hạn kích thước: Bloom Filter truyền thống yêu cầu kích thước mảng bit cố định khi khởi tạo, dựa trên số phần tử ước tính và tỷ lệ false positive mong muốn. Nếu số phần tử vượt quá dự đoán ban đầu, tỷ lệ false positive tăng đáng kể. Không thể thay đổi kích thước mà vẫn giữ nguyên các phần tử đã được thêm. Scalable Bloom Filter giải quyết vấn đề như thế nào? SBF khắc phục vấn đề này bằng cách:\nTăng kích thước động: Khi số phần tử vượt quá một ngưỡng (threshold), SBF thêm một Bloom Filter mới với kích thước lớn hơn. Giảm false positive: Mỗi Bloom Filter mới được tạo ra sử dụng một tỷ lệ false positive thấp hơn so với các Bloom Filter trước đó. Điều này giúp giảm nguy cơ tổng thể của false positive. Cấu trúc và cách hoạt động của Scalable Bloom Filter Cấu trúc:\nSBF bao gồm một chuỗi các Bloom Filter được tạo ra khi cần mở rộng. Mỗi Bloom Filter có kích thước và tỷ lệ false positive khác nhau, tăng dần theo thời gian. Chèn phần tử (Insert):\nPhần tử mới được thêm vào Bloom Filter hiện tại (bloom filter cuối cùng trong chuỗi). Nếu Bloom Filter hiện tại đạt ngưỡng tối đa, một Bloom Filter mới được tạo. Kiểm tra phần tử (Check):\nLần lượt kiểm tra phần tử trong từng Bloom Filter, từ đầu đến cuối chuỗi. Nếu phần tử được tìm thấy trong bất kỳ Bloom Filter nào, kết quả trả về là \u0026ldquo;có thể tồn tại\u0026rdquo;. Ngưỡng mở rộng (Threshold):\nSBF sử dụng một ngưỡng kiểm soát (thường dựa trên tải trọng, như số lượng phần tử đã thêm) để quyết định khi nào cần tạo một Bloom Filter mới. Ưu điểm của Scalable Bloom Filter Khả năng mở rộng (Scalability):\nSBF không giới hạn số lượng phần tử có thể thêm vào. Tăng kích thước động mà không cần định cấu hình trước. Giảm false positive hiệu quả:\nMỗi Bloom Filter mới có tỷ lệ false positive thấp hơn, làm giảm tỷ lệ tổng thể. Tiết kiệm bộ nhớ:\nSBF sử dụng kích thước bộ nhớ nhỏ hơn so với việc tạo một Bloom Filter rất lớn ngay từ đầu. Phù hợp với dữ liệu thay đổi:\nSBF rất hữu ích trong các ứng dụng nơi số lượng phần tử khó dự đoán. Hạn chế của Scalable Bloom Filter Tăng độ phức tạp:\nViệc duy trì nhiều Bloom Filter khiến kiểm tra phần tử tốn thời gian hơn, đặc biệt khi chuỗi Bloom Filter dài. Overhead bộ nhớ:\nDù SBF tiết kiệm bộ nhớ hơn khi so sánh với Bloom Filter truyền thống không được tối ưu hóa, nó vẫn có overhead vì phải quản lý nhiều Bloom Filter. Không hỗ trợ xóa:\nNhư Bloom Filter truyền thống, SBF không hỗ trợ xóa phần tử. Ứng dụng của Scalable Bloom Filter Quản lý cache:\nDùng để theo dõi các phần tử trong cache với số lượng phần tử thay đổi động. Hệ thống lưu trữ và cơ sở dữ liệu:\nĐược sử dụng trong các hệ thống lưu trữ phân tán như Bigtable, HBase, hoặc Cassandra. Hệ thống phân tán:\nSBF giúp theo dõi trạng thái của các phần tử (như dữ liệu, gói tin) trong hệ thống mà kích thước tập hợp thay đổi liên tục. Dữ liệu lớn (Big Data):\nSBF phù hợp với các ứng dụng xử lý dữ liệu lớn, nơi số lượng phần tử không thể dự đoán chính xác. Triển khai Scalable Bloom Filter Một cách phổ biến để triển khai SBF:\nBắt đầu với một Bloom Filter ban đầu có kích thước cố định. Đặt ngưỡng tải trọng (load factor), ví dụ: khi số phần tử đạt 80% dung lượng tối đa. Mỗi khi thêm một Bloom Filter mới, tăng kích thước mảng bit và giảm tỷ lệ false positive bằng cách thay đổi số lượng hàm băm. Mã giả Scalable Bloom Filter 1 2import math 3import hashlib 4 5class BloomFilter: 6 def __init__(self, size, num_hashes): 7 self.size = size 8 self.num_hashes = num_hashes 9 self.bit_array = [0] * size 10 self.count = 0 11 12 def _hash(self, item, seed): 13 hash_value = int(hashlib.md5((str(item) + str(seed)).encode()).hexdigest(), 16) 14 return hash_value % self.size 15 16 def add(self, item): 17 for i in range(self.num_hashes): 18 index = self._hash(item, i) 19 self.bit_array[index] = 1 20 self.count += 1 21 22 def contains(self, item): 23 for i in range(self.num_hashes): 24 index = self._hash(item, i) 25 if self.bit_array[index] == 0: 26 return False 27 return True 28 29 30class ScalableBloomFilter: 31 def __init__(self, initial_size=100, fp_rate=0.05, growth_factor=2): 32 self.filters = [] 33 self.fp_rate = fp_rate 34 self.growth_factor = growth_factor 35 self.current_size = initial_size 36 self._add_new_filter() 37 38 def _optimal_num_hashes(self, size, items): 39 return max(1, math.ceil((size / items) * math.log(2))) 40 41 def _add_new_filter(self): 42 num_hashes = self._optimal_num_hashes(self.current_size, -math.log(self.fp_rate)) 43 self.filters.append(BloomFilter(self.current_size, num_hashes)) 44 self.current_size *= self.growth_factor 45 46 def add(self, item): 47 if self.filters[-1].count \u0026gt;= self.filters[-1].size: 48 self._add_new_filter() 49 self.filters[-1].add(item) 50 51 def contains(self, item): 52 return any(filter.contains(item) for filter in self.filters) 53 54 55# Example Usage: 56sbf = ScalableBloomFilter(initial_size=10, fp_rate=0.1) 57sbf.add(\u0026#34;hello\u0026#34;) 58print(sbf.contains(\u0026#34;hello\u0026#34;)) # True 59print(sbf.contains(\u0026#34;world\u0026#34;)) # False Striped Bloom Filter Striped Bloom Filter là một biến thể của Bloom Filter được thiết kế để hỗ trợ truy cập đồng thời (concurrent access) hiệu quả trong môi trường đa luồng (multi-threaded). Mục tiêu chính của Striped Bloom Filter là giảm thiểu việc khóa toàn bộ cấu trúc dữ liệu khi các luồng thực hiện thêm hoặc kiểm tra phần tử, qua đó cải thiện hiệu năng.\nĐặc điểm của Striped Bloom Filter Phân chia thành các phân đoạn độc lập (Stripes):\nMảng bit của Bloom Filter được chia thành nhiều phân đoạn (segments) độc lập. Mỗi phân đoạn được bảo vệ bởi một khóa riêng biệt (lock). Các thao tác trên từng phân đoạn có thể thực hiện đồng thời mà không ảnh hưởng lẫn nhau. Tăng hiệu suất truy cập đồng thời:\nBằng cách phân chia mảng và sử dụng nhiều khóa, các luồng chỉ cần khóa một phân đoạn thay vì toàn bộ mảng. Điều này giảm thiểu độ trễ trong các hệ thống đa luồng. Hoạt động giống Bloom Filter truyền thống:\nStriped Bloom Filter vẫn đảm bảo các thuộc tính cơ bản của Bloom Filter như tỷ lệ false positive và sử dụng các hàm băm để xác định các chỉ mục. Cách hoạt động của Striped Bloom Filter 1. Chèn phần tử ( Tính các chỉ mục băm (hash indices) của phần tử. Xác định các phân đoạn tương ứng dựa trên chỉ mục. Khóa các phân đoạn liên quan để thêm bit. 2. Kiểm tra phần tử ( Tính các chỉ mục băm của phần tử. Truy cập các phân đoạn tương ứng mà không cần khóa (nếu chỉ kiểm tra). Nếu tất cả các bit tại các chỉ mục được đặt là 1, phần tử có thể tồn tại. 3. Phân đoạn (Segment): Mảng bit được chia thành ( S ) phân đoạn. Một hàm băm bổ sung được sử dụng để ánh xạ một phần tử đến một hoặc nhiều phân đoạn cụ thể. 4. Tăng hiệu suất đồng thời: Các luồng chỉ cần khóa phân đoạn cần thao tác, thay vì toàn bộ cấu trúc dữ liệu. Ưu điểm của Striped Bloom Filter Hỗ trợ đồng thời: Nhiều luồng có thể thêm hoặc kiểm tra phần tử đồng thời mà không gây xung đột. Giảm độ trễ: Phân đoạn độc lập giúp giảm thời gian chờ của các luồng khi khóa. Hiệu quả bộ nhớ: Dù chia thành nhiều phân đoạn, tổng bộ nhớ sử dụng vẫn tương tự Bloom Filter truyền thống. Hạn chế của Striped Bloom Filter Độ phức tạp quản lý khóa: Cần quản lý nhiều khóa hơn, làm tăng độ phức tạp trong triển khai. False positive vẫn tồn tại: Như Bloom Filter truyền thống, Striped Bloom Filter vẫn không loại bỏ được false positive. Không phù hợp với ứng dụng đơn luồng: Trong môi trường đơn luồng, cơ chế phân đoạn và khóa trở nên dư thừa. Triển khai bằng Python 1import threading 2import hashlib 3 4class StripedBloomFilter: 5 def __init__(self, num_bits, num_hashes, num_stripes): 6 self.num_bits = num_bits 7 self.num_hashes = num_hashes 8 self.num_stripes = num_stripes 9 self.segment_size = num_bits // num_stripes 10 self.bit_array = [0] * num_bits 11 self.locks = [threading.Lock() for _ in range(num_stripes)] 12 13 def _hashes(self, item): 14 hashes = [] 15 for i in range(self.num_hashes): 16 hash_value = int(hashlib.md5((str(item) + str(i)).encode()).hexdigest(), 16) 17 hashes.append(hash_value % self.num_bits) 18 return hashes 19 20 def add(self, item): 21 indices = self._hashes(item) 22 locked_segments = set() 23 24 # Lock relevant segments 25 for idx in indices: 26 segment = idx // self.segment_size 27 if segment not in locked_segments: 28 self.locks[segment].acquire() 29 locked_segments.add(segment) 30 31 try: 32 for idx in indices: 33 self.bit_array[idx] = 1 34 finally: 35 # Unlock segments 36 for segment in locked_segments: 37 self.locks[segment].release() 38 39 def contains(self, item): 40 indices = self._hashes(item) 41 for idx in indices: 42 if self.bit_array[idx] == 0: 43 return False 44 return True 45 46 47# Example Usage 48sbf = StripedBloomFilter(num_bits=1024, num_hashes=3, num_stripes=4) 49 50# Adding elements 51sbf.add(\u0026#34;hello\u0026#34;) 52sbf.add(\u0026#34;world\u0026#34;) 53 54# Checking elements 55print(sbf.contains(\u0026#34;hello\u0026#34;)) # True 56print(sbf.contains(\u0026#34;world\u0026#34;)) # True 57print(sbf.contains(\u0026#34;python\u0026#34;)) # False Quotient Filter (Bộ lọc thương số) Quotient Filter (QF) là một cấu trúc dữ liệu xác suất tương tự Bloom Filter, được thiết kế để kiểm tra thành viên (membership) trong tập hợp với hiệu quả về mặt bộ nhớ. QF hoạt động dựa trên kỹ thuật băm (hashing) và chia thương số, cung cấp một giải pháp nhỏ gọn cho các bài toán kiểm tra thành viên.\nNguyên lý hoạt động Băm và chia thương số:\nTương tự Bloom Filter, QF sử dụng hàm băm để tạo ra một giá trị băm cho phần tử. Giá trị băm được chia thành hai phần: Thương số (Quotient): Xác định chỉ số của bucket (ô nhớ) trong một mảng cố định. Phần dư (Remainder): Lưu lại như một chữ ký duy nhất trong mảng để kiểm tra. Buckets và mảng:\nThương số được dùng để xác định vị trí lưu phần dư trong mảng. Trường hợp xung đột (collisions) được xử lý bằng cách kiểm tra tuần tự các ô liền kề (linear probing). Lưu trữ nhỏ gọn:\nChỉ phần dư được lưu trong mảng, giúp tiết kiệm bộ nhớ đáng kể so với bảng băm (hash table) truyền thống. Metadata (dữ liệu phụ) được sử dụng để theo dõi trạng thái của các bucket (đầy hay rỗng, phần tử bị dời chỗ). Hỗ trợ thêm/xóa/kiểm tra:\nQF hỗ trợ ba thao tác cơ bản: thêm phần tử, xóa phần tử, và kiểm tra thành viên, với hiệu suất cao và tiêu tốn ít bộ nhớ. Ưu điểm của Quotient Filter Tiết kiệm bộ nhớ:\nQF nhỏ gọn hơn bảng băm và tương đương Bloom Filter về mức độ sử dụng bộ nhớ. Hỗ trợ xóa phần tử:\nKhông như Bloom Filter cơ bản, QF hỗ trợ việc xóa phần tử mà không cần biến thể bổ sung như Counting Bloom Filter. Chỉ cần một hàm băm:\nQF chỉ cần một hàm băm duy nhất, giúp giảm chi phí tính toán so với Bloom Filter (yêu cầu nhiều hàm băm). Hiệu suất cao:\nThao tác thêm, xóa và kiểm tra nhanh chóng với độ phức tạp thấp. Khả năng mở rộng động:\nQF có thể mở rộng dung lượng hoặc tái cấu trúc mảng khi cần thiết mà không ảnh hưởng lớn đến hiệu suất. Hạn chế của Quotient Filter False Positive (Kết quả dương tính giả):\nTương tự Bloom Filter, QF có thể báo rằng một phần tử thuộc tập hợp dù thực tế không phải vậy. Tuy nhiên, không có false negative (kết quả âm tính giả). Phức tạp hơn Bloom Filter:\nViệc quản lý metadata (trạng thái bucket, phần tử bị dời) khiến QF phức tạp hơn trong triển khai. Cố định dung lượng:\nDung lượng của QF cần được thiết kế trước. Khi vượt quá giới hạn, việc mở rộng dung lượng sẽ yêu cầu tái cấu trúc mảng, gây tốn kém tài nguyên. Hiệu suất giảm với các tập dữ liệu phân tán:\nQF không phù hợp với các ứng dụng yêu cầu truy cập ngẫu nhiên, do cần kiểm tra tuần tự trong trường hợp xung đột. Ứng dụng của Quotient Filter Cơ sở dữ liệu: Lọc dữ liệu, kiểm tra thành viên trong chỉ mục hoặc giảm truy cập đĩa không cần thiết. Hệ thống mạng: Lọc gói tin hoặc kiểm tra thành viên của địa chỉ IP trong bảng định tuyến. Hệ thống phân tán: Đồng bộ dữ liệu giữa các nút hoặc kiểm tra tính nhất quán. Lưu trữ: Loại bỏ trùng lặp dữ liệu (deduplication) trong các hệ thống lưu trữ. So sánh với Bloom Filter Tính năng Bloom Filter Quotient Filter False Positive Có thể xảy ra Có thể xảy ra False Negative Không Không Hỗ trợ xóa Không (cần Counting BF) Có Bộ nhớ Hiệu quả cao Hiệu quả (kèm metadata) Hàm băm Nhiều Một Mở rộng động Cần Scalable Bloom Filter Hỗ trợ giới hạn Ví dụ mã giả bằng Python 1class QuotientFilter: 2 def __init__(self, size): 3 self.size = size 4 self.table = [None] * size 5 self.metadata = [False] * size # Trạng thái bucket 6 7 def _hash(self, value): 8 h = hash(value) 9 quotient = h // self.size 10 remainder = h % self.size 11 return quotient, remainder 12 13 def insert(self, value): 14 quotient, remainder = self._hash(value) 15 while self.metadata[quotient]: # Xử lý xung đột 16 quotient = (quotient + 1) % self.size 17 self.table[quotient] = remainder 18 self.metadata[quotient] = True 19 20 def query(self, value): 21 quotient, remainder = self._hash(value) 22 while self.metadata[quotient]: 23 if self.table[quotient] == remainder: 24 return True 25 quotient = (quotient + 1) % self.size 26 return False 27 28 def delete(self, value): 29 quotient, remainder = self._hash(value) 30 while self.metadata[quotient]: 31 if self.table[quotient] == remainder: 32 self.table[quotient] = None 33 self.metadata[quotient] = False 34 return True 35 quotient = (quotient + 1) % self.size 36 return False Kết luận Quotient Filter là một giải pháp mạnh mẽ, kết hợp hiệu quả của Bloom Filter và bảng băm, phù hợp cho các ứng dụng cần kiểm tra thành viên, hỗ trợ xóa, và tiết kiệm bộ nhớ. Tuy nhiên, cần xem xét các hạn chế về thiết kế và ứng dụng để sử dụng tối ưu trong các bài toán cụ thể.\nCuckoo Filter Cuckoo Filter là một cấu trúc dữ liệu xác suất được thiết kế để kiểm tra thành viên trong tập hợp (membership test) giống như Bloom Filter, nhưng với nhiều cải tiến. Nó sử dụng ý tưởng từ Cuckoo Hashing và cung cấp một số tính năng nổi bật như hỗ trợ xóa phần tử và hiệu quả về bộ nhớ.\nNguyên lý hoạt động Cuckoo Filter hoạt động dựa trên hai khái niệm chính:\nFingerprinting (Chữ ký):\nMỗi phần tử được băm để tạo ra một giá trị fingerprint ngắn, đại diện cho phần tử đó. Chỉ lưu trữ giá trị fingerprint trong mảng, giúp tiết kiệm bộ nhớ. Cuckoo Hashing:\nMỗi phần tử có thể được lưu ở một trong hai vị trí tiềm năng trong mảng (table). Khi thêm phần tử mới mà vị trí đã bị chiếm, một phần tử khác có thể bị \u0026ldquo;đẩy\u0026rdquo; đến vị trí thứ hai của nó, theo cách tương tự thuật toán Cuckoo Hashing. Thao tác chính trong Cuckoo Filter Thêm phần tử (Insert):\nTính hai vị trí băm (bucket) cho phần tử dựa trên giá trị fingerprint. Nếu một trong hai vị trí còn trống, lưu fingerprint tại đó. Nếu cả hai vị trí đều đầy, chọn ngẫu nhiên một fingerprint trong bucket và \u0026ldquo;đẩy\u0026rdquo; nó đến vị trí thứ hai của nó. Tiếp tục lặp lại cho đến khi chèn thành công hoặc vượt quá số lần thử (rehash nếu cần). Kiểm tra phần tử (Query):\nTính hai vị trí băm dựa trên giá trị fingerprint. Kiểm tra xem giá trị fingerprint có tồn tại tại một trong hai bucket không. Xóa phần tử (Delete):\nTính hai vị trí băm dựa trên giá trị fingerprint. Nếu giá trị fingerprint tồn tại tại một trong hai vị trí, xóa nó. Ưu điểm của Cuckoo Filter Hỗ trợ xóa phần tử:\nKhông giống Bloom Filter cơ bản, Cuckoo Filter hỗ trợ xóa phần tử dễ dàng mà không cần các biến thể phức tạp. Tỷ lệ false positive thấp:\nCuckoo Filter cung cấp tỷ lệ dương tính giả (false positive) thấp, đặc biệt khi tối ưu kích thước bucket và fingerprint. Tiết kiệm bộ nhớ:\nNhờ chỉ lưu trữ giá trị fingerprint ngắn, Cuckoo Filter thường tiết kiệm bộ nhớ hơn Bloom Filter trong nhiều trường hợp. Hiệu suất cao:\nThao tác thêm, xóa và kiểm tra nhanh chóng, thường có độ phức tạp ( O(1) ) trung bình. Hỗ trợ kiểm tra âm tính chính xác:\nKhông bao giờ xảy ra false negative (kết quả âm tính sai). Khả năng mở rộng:\nCuckoo Filter có thể mở rộng thông qua cơ chế rehash hoặc tăng kích thước mảng khi cần thiết. Hạn chế của Cuckoo Filter Khó khăn với chèn phần tử dày đặc:\nKhi mảng quá đầy, việc thêm phần tử mới có thể dẫn đến hiện tượng \u0026ldquo;cuckoo evictions\u0026rdquo; (đẩy vòng lặp không thành công), buộc phải tái cấu trúc (rehash) toàn bộ mảng. Cấu trúc phức tạp hơn Bloom Filter:\nSo với Bloom Filter, Cuckoo Filter yêu cầu quản lý nhiều bucket, cơ chế đẩy phần tử, và xử lý xung đột phức tạp hơn. Không tối ưu với số lượng phần tử lớn:\nVới tập dữ liệu quá lớn, Bloom Filter có thể hiệu quả hơn về mặt bộ nhớ so với Cuckoo Filter. Ứng dụng của Cuckoo Filter Hệ thống cơ sở dữ liệu: Kiểm tra dữ liệu tồn tại trong các chỉ mục hoặc giảm truy cập đĩa. Hệ thống mạng: Lọc gói tin hoặc kiểm tra địa chỉ IP trong bảng định tuyến. Hệ thống phân tán: Đồng bộ dữ liệu, kiểm tra tính nhất quán. Bộ nhớ đệm (Cache): Lọc nhanh các truy vấn trong bộ nhớ đệm. So sánh với Bloom Filter Tính năng Bloom Filter Cuckoo Filter False Positive Có thể xảy ra Có thể xảy ra False Negative Không Không Hỗ trợ xóa Không (cần Counting BF) Có Bộ nhớ Tiết kiệm cao Tiết kiệm tốt (nhưng phức tạp hơn) Thao tác thêm Đơn giản, không đẩy phần tử Phức tạp hơn do đẩy phần tử Khả năng mở rộng Cần Scalable Bloom Filter Có hỗ trợ mở rộng trực tiếp Ví dụ mã giả Cuckoo Filter bằng Python 1import random 2 3class CuckooFilter: 4 def __init__(self, size, bucket_size=2, fingerprint_size=4): 5 self.size = size 6 self.bucket_size = bucket_size 7 self.fingerprint_size = fingerprint_size 8 self.buckets = [[] for _ in range(size)] 9 10 def _hash(self, value): 11 return hash(value) % self.size 12 13 def _fingerprint(self, value): 14 return hash(value) % (2 ** self.fingerprint_size) 15 16 def insert(self, value): 17 fingerprint = self._fingerprint(value) 18 i1 = self._hash(value) 19 i2 = (i1 ^ hash(fingerprint)) % self.size 20 21 if len(self.buckets[i1]) \u0026lt; self.bucket_size: 22 self.buckets[i1].append(fingerprint) 23 return True 24 elif len(self.buckets[i2]) \u0026lt; self.bucket_size: 25 self.buckets[i2].append(fingerprint) 26 return True 27 28 # Eviction 29 i = random.choice([i1, i2]) 30 for _ in range(self.size): 31 evicted_fingerprint = self.buckets[i].pop(0) 32 self.buckets[i].append(fingerprint) 33 fingerprint = evicted_fingerprint 34 i = (i ^ hash(fingerprint)) % self.size 35 if len(self.buckets[i]) \u0026lt; self.bucket_size: 36 self.buckets[i].append(fingerprint) 37 return True 38 39 return False # Failed to insert after many attempts 40 41 def query(self, value): 42 fingerprint = self._fingerprint(value) 43 i1 = self._hash(value) 44 i2 = (i1 ^ hash(fingerprint)) % self.size 45 46 return fingerprint in self.buckets[i1] or fingerprint in self.buckets[i2] 47 48 def delete(self, value): 49 fingerprint = self._fingerprint(value) 50 i1 = self._hash(value) 51 i2 = (i1 ^ hash(fingerprint)) % self.size 52 53 if fingerprint in self.buckets[i1]: 54 self.buckets[i1].remove(fingerprint) 55 return True 56 elif fingerprint in self.buckets[i2]: 57 self.buckets[i2].remove(fingerprint) 58 return True 59 return False Kết luận Cuckoo Filter là một cấu trúc dữ liệu mạnh mẽ, hiệu quả và phù hợp cho các ứng dụng cần thêm, xóa, và kiểm tra thành viên nhanh chóng. Mặc dù phức tạp hơn Bloom Filter, nó mang lại nhiều tính năng hữu ích như hỗ trợ xóa và hiệu quả bộ nhớ cao hơn trong một số trường hợp.\nỨng Dụng của Bloom Filter trong Các Lĩnh Vực 1. Phát Hiện Gian Lận Tài Chính (Financial Fraud Detection) Mục đích: Xác định hành vi đáng ngờ trong thói quen mua sắm của người dùng.\nCách sử dụng:\nSử dụng một Bloom Filter riêng cho mỗi người dùng. Kiểm tra mọi giao dịch để trả lời câu hỏi: Người dùng này đã thanh toán từ địa điểm này trước đây chưa? Bloom Filter cung cấp phản hồi cực kỳ nhanh (độ trễ thấp). Nhân bản dữ liệu qua các khu vực để xử lý khi người dùng di chuyển. Lợi ích khi sử dụng Bloom Filter:\nGiao dịch hoàn tất nhanh chóng. Giảm nguy cơ giao dịch bị gián đoạn trong trường hợp mạng bị phân vùng (kết nối được giữ trong thời gian ngắn hơn). Tăng cường lớp bảo mật cho cả chủ thẻ tín dụng và nhà bán lẻ. Các câu hỏi khác mà Bloom Filter có thể hỗ trợ trong ngành tài chính:\nNgười dùng đã từng mua sản phẩm/dịch vụ trong danh mục này chưa? Có cần bỏ qua một số bước bảo mật khi mua sắm với các cửa hàng trực tuyến đáng tin cậy (như Amazon, Apple App Store)? Thẻ tín dụng này có bị báo mất hoặc đánh cắp không? Lợi ích bổ sung: Các tổ chức tài chính có thể trao đổi danh sách số thẻ bị mất/mất cắp mà không tiết lộ số thực. 2. Đặt Quảng Cáo (Ad Placement - Retail, Advertising) Mục đích: Hiển thị quảng cáo hoặc đề xuất sản phẩm một cách cá nhân hóa.\nCách sử dụng:\nSử dụng một Bloom Filter cho mỗi người dùng, lưu trữ danh sách sản phẩm đã mua. Khi hệ thống đề xuất sản phẩm mới: Nếu sản phẩm chưa có trong Bloom Filter, quảng cáo được hiển thị và sản phẩm được thêm vào Bloom Filter. Nếu sản phẩm đã có trong Bloom Filter, hệ thống tiếp tục kiểm tra sản phẩm khác cho đến khi tìm được sản phẩm chưa có. Lợi ích khi sử dụng Bloom Filter:\nGiải pháp chi phí thấp để tạo trải nghiệm cá nhân hóa gần như theo thời gian thực. Không cần đầu tư vào cơ sở hạ tầng đắt đỏ. 3. Kiểm Tra Tên Người Dùng (SaaS, Content Publishing Platforms) Mục đích: Xác định tên người dùng/email/domain name/slug đã được sử dụng chưa.\nCách sử dụng:\nSử dụng một Bloom Filter lưu tất cả các tên người dùng đã đăng ký. Khi người dùng nhập tên mong muốn: Nếu không có trong Bloom Filter, tài khoản được tạo và tên được thêm vào Bloom Filter. Nếu có, ứng dụng quyết định kiểm tra cơ sở dữ liệu chính hoặc từ chối tên đó. Lợi ích khi sử dụng Bloom Filter:\nPhương pháp rất nhanh và hiệu quả để thực hiện thao tác phổ biến. Không cần đầu tư vào cơ sở hạ tầng phức tạp. 4. Các Ứng Dụng Khác Của Bloom Filter Kiểm Tra Chính Tả (Spell Checker):\nTrong thời kỳ đầu, bộ kiểm tra chính tả được triển khai bằng Bloom Filter. Cơ Sở Dữ Liệu (Databases):\nNhiều cơ sở dữ liệu phổ biến sử dụng Bloom Filter để giảm số lần truy cập đĩa tốn kém cho các hàng/cột không tồn tại. Các hệ thống như PostgreSQL, Apache Cassandra, Cloud Bigtable sử dụng kỹ thuật này. Công Cụ Tìm Kiếm (Search Engines):\nBitFunnel, một thuật toán lập chỉ mục cho công cụ tìm kiếm, sử dụng Bloom Filter để lưu trữ chỉ mục tìm kiếm. An Ninh (Security):\nDùng Bloom Filter để phát hiện mật khẩu yếu, URL độc hại, và các nguy cơ an ninh khác. Bài tập Financial Fraud Detection xây dựng ứng dụng Financial Fraud Detection sử dụng Bloom Filter. Mục tiêu là kiểm tra xem người dùng đã thực hiện giao dịch từ một địa điểm cụ thể hay chưa để phát hiện hành vi gian lận. Nếu người dùng chưa thực hiện giao dịch ở 1 ban nào đó trong lịch sử thì cảnh báo lên\n1 2import hashlib 3 4class BloomFilter: 5 def __init__(self, size, num_hash_functions): 6 # Kích thước Bloom filter (số lượng bit) 7 self.size = size 8 # Số hàm băm 9 self.num_hash_functions = num_hash_functions 10 # Mảng bit để lưu trữ (khởi tạo với các bit = 0) 11 self.bit_array = [0] * size 12 13 def _hash(self, item, i): 14 \u0026#34;\u0026#34;\u0026#34;Hàm băm để tạo chỉ số từ item, với i là chỉ số của hàm băm.\u0026#34;\u0026#34;\u0026#34; 15 return int(hashlib.sha256(f\u0026#34;{item}{i}\u0026#34;.encode(\u0026#39;utf-8\u0026#39;)).hexdigest(), 16) % self.size 16 17 def add(self, item): 18 \u0026#34;\u0026#34;\u0026#34;Thêm một item vào Bloom filter.\u0026#34;\u0026#34;\u0026#34; 19 for i in range(self.num_hash_functions): 20 index = self._hash(item, i) 21 self.bit_array[index] = 1 22 23 def check(self, item): 24 \u0026#34;\u0026#34;\u0026#34;Kiểm tra một item có trong Bloom filter không.\u0026#34;\u0026#34;\u0026#34; 25 for i in range(self.num_hash_functions): 26 index = self._hash(item, i) 27 if self.bit_array[index] == 0: 28 return False # Nếu bất kỳ bit nào là 0, phần tử chắc chắn không có trong bộ lọc 29 return True # Nếu tất cả các bit đều là 1, có khả năng phần tử có trong bộ lọc 30 31# Khởi tạo Bloom filter với kích thước 1000 bit và 3 hàm băm 32bloom_filter = BloomFilter(size=1000, num_hash_functions=3) 33 34# Mô phỏng thông tin giao dịch của người dùng 35users_transactions = { 36 \u0026#34;tung\u0026#34;: [\u0026#34;New York\u0026#34;, \u0026#34;Los Angeles\u0026#34;, \u0026#34;Miami\u0026#34;], 37 \u0026#34;kim\u0026#34;: [\u0026#34;London\u0026#34;, \u0026#34;Paris\u0026#34;, \u0026#34;Berlin\u0026#34;], 38 \u0026#34;tuan\u0026#34;: [\u0026#34;Tokyo\u0026#34;, \u0026#34;Osaka\u0026#34;, \u0026#34;Kyoto\u0026#34;] 39} 40 41# Thêm tất cả các giao dịch vào Bloom filter 42for user, locations in users_transactions.items(): 43 for location in locations: 44 bloom_filter.add(f\u0026#34;{user}-{location}\u0026#34;) # Kết hợp tên người dùng và địa điểm giao dịch 45 46# Kiểm tra một giao dịch mới từ người dùng 47def check_fraud(user, location): 48 if not bloom_filter.check(f\u0026#34;{user}-{location}\u0026#34;): 49 return f\u0026#34;Warning: Transaction from {location} by {user} might be suspicious!\u0026#34; 50 else: 51 return f\u0026#34;Transaction from {location} by {user} is normal.\u0026#34; 52 53# Kiểm tra một số giao dịch 54test_transactions = [ 55 (\u0026#34;tung\u0026#34;, \u0026#34;Miami\u0026#34;), # Giao dịch hợp lệ 56 (\u0026#34;tung\u0026#34;, \u0026#34;Chicago\u0026#34;), # Giao dịch mới (không có trong Bloom filter) 57 (\u0026#34;kim\u0026#34;, \u0026#34;Paris\u0026#34;), # Giao dịch hợp lệ 58 (\u0026#34;tuan\u0026#34;, \u0026#34;Kyoto\u0026#34;) # Giao dịch hợp lệ 59] 60 61# Kiểm tra các giao dịch 62for user, location in test_transactions: 63 result = check_fraud(user, location) 64 print(result) Kết quả:\n1Transaction from Miami by tung is normal. 2Warning: Transaction from Chicago by tung might be suspicious! 3Transaction from Paris by kim is normal. 4Transaction from Kyoto by tuan is normal. Spell Checker xây dựng một Spell Checker sử dụng Bloom Filter. Mã này sẽ kiểm tra xem một từ có trong từ điển (dictionary) hay không và đưa ra kết quả.\n1 2import hashlib 3 4class BloomFilter: 5 def __init__(self, size, num_hash_functions): 6 # Kích thước của Bloom filter (số lượng bit) 7 self.size = size 8 # Số hàm băm 9 self.num_hash_functions = num_hash_functions 10 # Mảng bit để lưu trữ (được khởi tạo với tất cả các bit là 0) 11 self.bit_array = [0] * size 12 13 def _hash(self, item, i): 14 \u0026#34;\u0026#34;\u0026#34;Hàm băm để tạo chỉ số từ item, với i là chỉ số của hàm băm.\u0026#34;\u0026#34;\u0026#34; 15 # Dùng hàm băm SHA-256 và điều chỉnh với chỉ số i để tạo ra các chỉ số khác nhau 16 return int(hashlib.sha256(f\u0026#34;{item}{i}\u0026#34;.encode(\u0026#39;utf-8\u0026#39;)).hexdigest(), 16) % self.size 17 18 def add(self, item): 19 \u0026#34;\u0026#34;\u0026#34;Thêm một item vào Bloom filter.\u0026#34;\u0026#34;\u0026#34; 20 for i in range(self.num_hash_functions): 21 index = self._hash(item, i) 22 self.bit_array[index] = 1 23 24 def check(self, item): 25 \u0026#34;\u0026#34;\u0026#34;Kiểm tra một item có trong Bloom filter không.\u0026#34;\u0026#34;\u0026#34; 26 for i in range(self.num_hash_functions): 27 index = self._hash(item, i) 28 if self.bit_array[index] == 0: 29 return False # Nếu bất kỳ bit nào là 0, phần tử chắc chắn không có trong bộ lọc 30 return True # Nếu tất cả các bit đều là 1, có khả năng phần tử có trong bộ lọc 31 32# Tạo một Bloom filter với kích thước 1000 bit và 3 hàm băm 33bloom_filter = BloomFilter(size=1000, num_hash_functions=3) 34 35# Từ điển mẫu 36dictionary = [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;spell\u0026#34;, \u0026#34;check\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;bloom\u0026#34;, \u0026#34;filter\u0026#34;] 37 38# Thêm tất cả các từ vào Bloom filter 39for word in dictionary: 40 bloom_filter.add(word) 41 42# Kiểm tra chính tả của một số từ 43test_words = [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;java\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;flutter\u0026#34;] 44 45for word in test_words: 46 if bloom_filter.check(word): 47 print(f\u0026#34;\u0026#39;{word}\u0026#39; có thể là một từ đúng.\u0026#34;) 48 else: 49 print(f\u0026#34;\u0026#39;{word}\u0026#39; chắc chắn là một từ sai.\u0026#34;) Kết quả:\n1 2\u0026#39;hello\u0026#39; có thể là một từ đúng. 3\u0026#39;world\u0026#39; có thể là một từ đúng. 4\u0026#39;java\u0026#39; chắc chắn là một từ sai. 5\u0026#39;python\u0026#39; có thể là một từ đúng. Recommendation Systems Bloom Filters triển khai việc tránh gợi ý các sản phẩm mà người dùng đã tương tác trước đó bằng cách sử dụng Bloom Filters.\n1 2 3import hashlib 4 5class BloomFilter: 6 def __init__(self, size, num_hash_functions): 7 # Kích thước Bloom filter (số lượng bit) 8 self.size = size 9 # Số hàm băm 10 self.num_hash_functions = num_hash_functions 11 # Mảng bit để lưu trữ (khởi tạo tất cả các bit là 0) 12 self.bit_array = [0] * size 13 14 def _hash(self, item, i): 15 \u0026#34;\u0026#34;\u0026#34;Hàm băm để tạo chỉ số từ item, với i là chỉ số của hàm băm.\u0026#34;\u0026#34;\u0026#34; 16 return int(hashlib.sha256(f\u0026#34;{item}{i}\u0026#34;.encode(\u0026#39;utf-8\u0026#39;)).hexdigest(), 16) % self.size 17 18 def add(self, item): 19 \u0026#34;\u0026#34;\u0026#34;Thêm một item vào Bloom filter.\u0026#34;\u0026#34;\u0026#34; 20 for i in range(self.num_hash_functions): 21 index = self._hash(item, i) 22 self.bit_array[index] = 1 23 24 def check(self, item): 25 \u0026#34;\u0026#34;\u0026#34;Kiểm tra một item có trong Bloom filter không.\u0026#34;\u0026#34;\u0026#34; 26 for i in range(self.num_hash_functions): 27 index = self._hash(item, i) 28 if self.bit_array[index] == 0: 29 return False # Nếu bất kỳ bit nào là 0, phần tử chắc chắn không có trong bộ lọc 30 return True # Nếu tất cả các bit đều là 1, có khả năng phần tử có trong bộ lọc 31 32# Khởi tạo Bloom filter 33bloom_filter = BloomFilter(size=1000, num_hash_functions=3) 34 35# Mô phỏng danh sách sản phẩm người dùng đã tương tác 36user_interactions = { 37 \u0026#34;tung\u0026#34;: [\u0026#34;thịt bò\u0026#34;, \u0026#34;hành tây\u0026#34;, \u0026#34;khoai tây\u0026#34;], 38 \u0026#34;tuan\u0026#34;: [\u0026#34;thit heo\u0026#34;, \u0026#34;trứng\u0026#34;], 39 \u0026#34;canh\u0026#34;: [\u0026#34;thịt bò\u0026#34;, \u0026#34;trứng\u0026#34;, \u0026#34;sữa TH\u0026#34;, \u0026#34;kem\u0026#34;] 40} 41 42# Thêm các sản phẩm đã tương tác vào Bloom filter 43for user, items in user_interactions.items(): 44 for item in items: 45 bloom_filter.add(f\u0026#34;{user}-{item}\u0026#34;) # Kết hợp user và item để lưu trữ duy nhất 46 47# Hàm gợi ý sản phẩm 48def recommend_items(user, candidate_items): 49 \u0026#34;\u0026#34;\u0026#34;Đưa ra gợi ý các sản phẩm chưa tương tác.\u0026#34;\u0026#34;\u0026#34; 50 recommendations = [] 51 for item in candidate_items: 52 if not bloom_filter.check(f\u0026#34;{user}-{item}\u0026#34;): 53 recommendations.append(item) # Chỉ thêm sản phẩm nếu chưa tương tác 54 return recommendations 55 56# Danh sách các sản phẩm có thể gợi ý 57candidate_items = [\u0026#34;thịt bò\u0026#34;, \u0026#34;hành tây\u0026#34;, \u0026#34;khoai tây\u0026#34;,\u0026#34;trứng\u0026#34;, \u0026#34;sữa TH\u0026#34;, \u0026#34;kem\u0026#34;,\u0026#34;thit heo\u0026#34;] 58 59# Gợi ý sản phẩm cho tung 60user = \u0026#34;tung\u0026#34; 61recommendations = recommend_items(user, candidate_items) 62 63print(f\u0026#34;Recommendations for {user}: {recommendations}\u0026#34;) Kết quả:\n1Recommendations for tung: [\u0026#39;trứng\u0026#39;, \u0026#39;sữa TH\u0026#39;, \u0026#39;kem\u0026#39;, \u0026#39;thit heo\u0026#39;] https://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.html\nhttps://en.wikipedia.org/wiki/Bloom_filter\nCảm ơn các bạn đã theo dõi bài viết. Xin cảm ơn và hẹn gặp lại.\n","date":"Nov 24, 2024","img":"https://unsplash.it/1920/1080?image=202","permalink":"/blog/2024-11-24-system-design-top-10-interview-bloom-filters/","series":null,"tags":["System Design"],"title":"Top 10 Thuật Toán System Design Các Bạn Nên Biết Và Thường Được Hỏi Trong Phỏng Vấn - Top 3 Bloom Filters"},{"categories":null,"content":" I. Khái niệm II. Distributed Hash Table được sử dụng ở đâu 1. Peer-to-peer (P2P) networks 2. Distributed databases 3. Content delivery networks 4. Event Notification 5. Distributed File Systems III. Các yêu cầu của một lookup algorithm tốt Autonomy và decentralization Fault tolerance Scalability Load balance Low maintenance overhead IV. Điểm mạnh của Distributed Hash Table Scalability Efficiency Fault tolerance Decentralization Security V. Điểm không mạnh của Distributed Hash Table Complexity Performance Security Compatibility Limited functionality VI. Tham khảo Đến thời điểm hiện tại, cuối năm 2024, từ khoá Decentralization vẫn đang là một từ khoá hot, quang trọng. Từ việc bùng nổ , nở rộ của block chain, đến việc các data center của các tập đoàn lớn nước ngoài được đặt ở nhiều nơi, trong nước thì việc số hoá dữ liệu phát triển mạnh mẽ.\nTrong bài viết này, chúng ta sẽ cùng nhau hiểu khái niệm cơ bản của Distributed Hash Tables, ưu điểm và nhược điểm của nó\nI. Khái niệm Distributed Hash Tables - DHT là một hệ thống phân tán phi tập trung cung cấp dịch vụ tra cứu, tựa tựa như hash table.\nHash table: là một bảng dữ liệu key - value. Value được lưu trữ và truy vấn thông qua key. Key được sử dụng để xác định nơi lưu trữ value.\nVí dụ\nkey :a, value:/data/2024/02/01/12/01/01/a.txt key :b, value:/data/2024/02/01/12/01/01/b.txt\nDistributed Hash Tables: dữ liệu cũng dạng key - value, nhưng dữ liệu được lưu trữ phân tán trên nhiều node trong một network, khác với Hash table là chỉ lưu trữ trong 1 node.\ntrong Distributed Hash Tables, mỗi node chịu trách nhiệm lưu trữ một phần dữ liệu. Khi người dùng truy vấn hoặc lưu dữ liệu lên Distributed Hash Tables, người dùng sẽ đẩy dữ liệu lên network. Yêu cầu của người dùng sẽ được dẩy lên node tương ứng với khoá của dữ liệu. Node đó sẽ chịu trách nhiệm lưu trữ và truy vấn dữ liệu của người dùng.\nVậy nên, một Distributed Hash Tables cần có ít nhất 3 thành phần chính\nDistributed application: Chịu trách nhiệm giao tiếp với người dùng qua 2 phương thức là push ( key, value) để người dùng đẩy dữ liệu lên network và get(key) để người dùng lấy dữ liệu thông qua key. App có thể được lưu trữ phân tán ở trên nhiều node.\nDistributed hash table: hay còn gọi là DHash, chịu trách nhiệm lấy ra node đang lưu dữ liệu của key. Data có thể được lưu trữ trên nhiều node.\nLookup service: thành phần này Ở trên node, trả dữ liệu của key.\nII. Distributed Hash Table được sử dụng ở đâu là một hệ thống phân tán phi tập trung, Distributed Hash Table được sử dụng dưới nhiều mục đích khác nhau, gom nhóm lại thì đến thời điểm hiện tại, chúng ta có 4 nhóm chính\n1. Peer-to-peer (P2P) networks Ở đây, mình đề cập tới BitTorrent cho đơn giản hen\nVí dụ , mình muốn download file tên là abc.txt\nchúng ta sẽ dùng Distributed application như BitTorrent\nDistributed hash table:\nkey sẽ là hash (\u0026lsquo;abc.txt\u0026rsquo;)\nvalue là ip máy có chứa file \u0026lsquo;abc.txt\u0026rsquo;\nLookup service:\ngọi đến máy có ip do Distributed hash table trả về và kèm theo một số lệnh xác thực để lấy file abc.txt\n2. Distributed databases Ngày nay, với dự phát triển mạnh mẽ của big data, iot, một máy khủng long cũng có thể chưa đủ đáp ứng tải và tài nguyên để lưu trữ dữ liệu.\n3. Content delivery networks Chúng ta tưởng tượng hệ s3 của amazone á, chắc chắn nó phải được lưu trên nhiều node rồi.\n4. Event Notification Giống firebase.\n5. Distributed File Systems Quản lý file trong hệ thống lưu trữ dữ liệu phân tán\nIII. Các yêu cầu của một lookup algorithm tốt Autonomy và decentralization Các node tự động phối hợp với nhau tạo lên hệ thống, không cần node trung tâm\nFault tolerance Hệ thống đáng tin cậy, khi có một node trong hệ thống bị lỗi, thì hệ thống vẫn hoạt động bình thường\nScalability Hệ thống phải hoạt động hiệu quả ngay cả khi có hàng ngàn, hàng triệu node.\nLoad balance Các key cần phải phân bố đều giữa các node, tránh cho quá tải 1 node nào đó\nLow maintenance overhead Khi có 1 node mới tham gia vào hệ thống hoặc một node rời khỏi hệ thống, một vấn đề gặp phải là chúng ta sẽ tốn kha khá bandwidth để gửi thông báo tới các node còn lại rằng có node mới hoặc có node rời khỏi hệ thống. Nên, thay vì gửi thông báo tới toán bộ node trong network, chúng ta có thể chỉ gửi thông báo tới các neighbors thôi.\nIV. Điểm mạnh của Distributed Hash Table Scalability Distributed Hash Table có khả năng mở rộng cao vì chúng có thể lưu trữ và truy xuất lượng lớn dữ liệu mà không cần điều phối tập trung hoặc máy chủ để quản lý hệ thống. Distributed Hash Table phù hợp với các hệ thống phân tán quy mô lớn.\nEfficiency Distributed Hash Table cung cấp cách thức lưu trữ và truy vấn dữ liệu một cách hiệu quả, sử dụng khoá dữ liệu để xác định ví trị của dữ liệu trong hệ thống. Chính điều đó giúp cho Distributed Hash Table có thể xác định và truy vấn dữ liệu nhanh chóng mà không cần phải tìm kiếm trên toàn bộ node của hệ thống.\nFault tolerance Distributed Hash Table đảm bảo toàn vẹn dữ liệu, có thể quản lý và cô lập node lỗi mà không cần server trung tâm quản lý. Dữ liệu được lưu trữ phân tán trên các node nên khi có node bị lỗi, node sẽ bị cô lập và dữ liệu sẽ được trả về cho người dùng từ các node còn lại trong hệ thống\nDecentralization Distributed Hash Table là hệ quản lý phi tập trung, không cần central authority (CA) hoặc server quản lý trung tâm, do đó hệ thống ít bị khai thác lỗ hổng bảo mật hơn khi bị tấn công. Ít chứ không phải là không có\nSecurity Distributed Hash Table cung cấp các cơ chế bảo mật để lưu trữ và truy vấn dữ liệu, khi dữ liệu được lưu phân tán trên nhiều node của hệ thống thay vì chỉ lưu trên một node, điều này giúp giảm thiểu rủi ro khi kẻ gian muốn thay đổi dữ liệu vì mục đích không tốt.\nV. Điểm không mạnh của Distributed Hash Table Complexity Distributed Hash Table khá khoai khi triển khai và bảo trì, hệ thống cần một lượng lớn nodes để các chức năng hoạt động một cách trơn tru, hiệu quả. Do phải quản lý quá nhiều node, người quản lý sẽ gặp thách thức khi có sự cố xui xẻo xảy ra, ngoài ra người quản lý còn phải hiểu kỹ hệ thống của mình\nPerformance Trong một số trường hợp xấu, Distributed Hash Table có hiệu năng lởm hơn so với các hệ distributed systems khác, đặc biệt lởm khi hệ thống đang gần quá tải (heavy load) hoặc khi hệ thống quá lớn, người quản trị config số lượng neighbors hoặc số hop nhiều.\nSecurity Bản thân Distributed Hash Table có trang bị một vài cách thức bảo mật dữ liệu để đảm bảo toàn vẹn dữ liệu của người dùng khi lưu trữ và truy vấn dữ liệu, nhưng về mặt thiết kế thì hệ thống có thể tồn tại các lỗi hổng về bảo mật hoặc bị tấn công kiến trúc hệ thống, ví dụ như tấn công từ chối dịch vụ (DDoS) hoặc tấn công mạo nhận - Sybil attack, là hình thức tấn công vào các mạng lưới ngang hàng được thực hiện bằng cách tạo nhiều thực thể ảo (tài khoản, node hoặc máy tính) để chiếm quyền kiểm soát mạng lưới.\nCompatibility Distributed Hash Table có thể không tương thích với toàn bộ kiểu dữ liệu của ngừoi dùng. Một số kiến trúc yêu cầu một cấu trúc hoặc định dạng đặc biệt để hoạt động\nLimited functionality Distributed Hash Table được thiết để để lưu trữ và lấy dữ liệu, và không hỗ trợ các hàm bổ trợ\nVI. Tham khảo https://www.cs.princeton.edu/courses/archive/fall18/cos418/docs/L6-dhts.pdf\nhttps://www.cs.cmu.edu/%7Edga/15-744/S07/lectures/16-dht.pdf\nhttps://web.mit.edu/6.829/www/currentsemester/materials/chord.pdf\nhttps://www.tutorialspoint.com/distributed-hash-tables-dhts\nhttps://www.geeksforgeeks.org/distributed-hash-tables-with-kademlia/\nhttps://medium.com/the-code-vault/data-structures-distributed-hash-table-febfd01fc0af\nCảm ơn các bạn đã theo dõi bài viết. Xin cảm ơn và hẹn gặp lại.\n","date":"Nov 23, 2024","img":"https://unsplash.it/1920/1080?image=200","permalink":"/blog/2024-11-23-system-design-top-10-interview-distributed-hash-table/","series":null,"tags":["System Design"],"title":"Top 10 Thuật Toán System Design Các Bạn Nên Biết Và Thường Được Hỏi Trong Phỏng Vấn - Top 2 Distributed Hash Tables"},{"categories":null,"content":" I. Lý thuyết căn bản Reinforcement Learning Các thành phần cơ bản của Reinforcement Learning Lý thuyết toán học Q-Learning Các khái niệm trong Q-learning Cách Q-learning hoạt động Double Deep Q-Network 1. Vấn đề của Q-learning (Overestimation Bias): 2. Cải tiến của Double Deep Q-Network (DDQN): 3. Lợi ích của DDQN so với DQN/Q-learning: 4. Ví dụ trực quan về sự khác biệt: 5. Tóm tắt: II. Thực hành với chương trình mario Environment Khởi tạo môi trường Xử lý dữ liệu Agent Act Remember Learn Play Replay Kết quả III. Tham khảo Chào các bạn, sau một thời gian ở ẩn, chúng ta lại tiếp tục với việc thực chiến AI, ở bài viết này, chúng ta sẽ train mô hình AI Reinforcement Learning với tựa game đã đi vào bao nhiêu thế hệ trẻ thơ, Mario, tuy nhiên, để bắt đầu bài viết, mình sẽ note lại một vài ý về Reinforcement Learning, Q learning, và cải tiến của Deep Q-Network là Double Deep Q-Network , trong phần code mình sẽ sử dụng Double Deep Q-Network\nI. Lý thuyết căn bản Reinforcement Learning Các thành phần cơ bản của Reinforcement Learning Theo lý thuyết Reinforcement Learning, chúng ta cần các thành phần sau:\nAgent: là đối tượng giữ các hành động (Action), thực hiện các hành động\nEnvironment : Môi trường xung quanh nơi agent tương tác\nAction : Danh sách các hành động mà Agent thực hiện, ví dụ nhảy, chạy, đi lên trước 1 bước, đi lùi 1 bước, bắn đạn \u0026hellip; Khi Agent thực hiện các action, thì environment thay đổi\nState : Danh sách các trạng thái của environment khi có action từ agent\nOptimal Action-Value function : Hàm Q*(s,a), chữ Q có thể hiểu là viết tắt của từ quality\nReward : Agent nhận reward từ Environment khi có action\nVí dụ, Agent là con robot, Ation là [dậm chân, vỗ tay ], khi con robot dậm chân, môi trường thay đổi, đất lún hơn một chút, lúc này State là 1 bức tranh có 1 con rô bốt với chân con rô bốt hơi hơi lún một chút xuống đất, và environment sẽ trả về 1 giá trị Reward nào đó cho Agent sau hành động dậm chân của Agent, dễ hiểu phải không các bạn.\nReward của hành động dậm chân có thể sẽ có giá trị khác so với reward của hành động vỗ tay.\nVì chúng ta không biết khi nào hành động kết thúc, nên rewards sẽ là một chuỗi vô hạn các reward sau thời điểm action xảy ra, tính từ thời điểm t_0 ban đầu.\nChuỗi vô hạn không có hội tụ, nên người ta chế (trick) sẽ thêm 1 tham số là discount factor hay discount rate, để chuỗi này hội tụ.\nLý thuyết toán học đứng đằng sau là Markov decision process và sử dụng nền tản là phương trình Bellman, Markov decision process đã được đề xuất từ năm 1950s, bạn có thể tra google để tìm hiểu thêm. Giờ mình hiểu là có lý thuyết toán học đảm bảo chuỗi này hội tụ rồi, triển thôi.\nLý thuyết toán học Ở mục này mình đề cập một chút về Markov decision process và phương trình Bellman, các bạn có thể bỏ qua nếu thấy ngán, mình note lại để sau này khỏi mất công tìm\nPhương trình của Markov Decision Process (MDP) chính là biểu thức mô tả cách giá trị của các trạng thái hoặc hành động được cập nhật thông qua quá trình ra quyết định. Tuy nhiên, bản thân MDP không có một phương trình duy nhất cụ thể, mà thường được mô tả qua các thành phần cơ bản như tập trạng thái, hành động, xác suất chuyển trạng thái, phần thưởng, và hệ số chiết khấu.\nPhương trình chính xác nhất liên quan đến MDP là phương trình Bellman, mà chúng ta có thể viết theo hai dạng: dạng hàm giá trị trạng thái và dạng hàm giá trị hành động. Hai phương trình này thể hiện rõ cách tính toán tổng phần thưởng kỳ vọng.\nMarkov Decision Process (MDP) MDP là một khung toán học dùng để mô tả các bài toán ra quyết định trong môi trường không chắc chắn. Một MDP bao gồm các thành phần sau:\nS (State space): Tập hợp các trạng thái có thể xảy ra trong môi trường. A (Action space): Tập hợp các hành động mà người ra quyết định (agent) có thể thực hiện ở mỗi trạng thái. P (Transition probability): Xác suất chuyển trạng thái ( P(s\u0026rsquo;|s, a) ), biểu thị xác suất trạng thái kế tiếp ( s\u0026rsquo; ) xảy ra khi thực hiện hành động ( a ) tại trạng thái ( s ). R (Reward function): Hàm thưởng ( R(s, a) ), là phần thưởng tức thì nhận được khi thực hiện hành động ( a ) tại trạng thái ( s ). $(\\gamma) (Discount factor)$: Hệ số chiết khấu $( \\gamma \\in [0, 1] )$, xác định mức độ ưu tiên cho phần thưởng tức thì so với phần thưởng trong tương lai. Khi $( \\gamma )$ gần bằng 1, giá trị các phần thưởng trong tương lai càng được đánh giá cao. Mục tiêu của MDP là tìm ra chính sách tối ưu - optimal policy $( \\pi^* )$, tức là một chuỗi các hành động giúp tối đa hóa tổng phần thưởng kỳ vọng trong dài hạn.\nPhương trình Bellman Phương trình Bellman mô tả mối quan hệ đệ quy giữa giá trị của một trạng thái hoặc một hành động với các trạng thái kế tiếp hoặc hành động tiếp theo. Nó thường được sử dụng để tính toán giá trị kỳ vọng của các trạng thái hoặc hành động, giúp đánh giá và tìm ra chính sách tối ưu.\na. Phương trình Bellman cho hàm giá trị trạng thái ( V(s) )\nHàm giá trị trạng thái ( V(s) ) cho biết tổng phần thưởng kỳ vọng khi bắt đầu từ trạng thái ( s ) và theo chính sách tối ưu. Phương trình Bellman cho hàm giá trị trạng thái là:\n$$ [ V(s) = \\max_{a} \\left[ R(s, a) + \\gamma \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s, a)V(s\u0026rsquo;) \\right] ] $$\nỞ đây:\n( V(s) ) là giá trị của trạng thái ( s ). ( R(s, a) ) là phần thưởng nhận được khi thực hiện hành động ( a ) tại trạng thái ( s ). ( P(s\u0026rsquo;|s, a) ) là xác suất chuyển từ trạng thái ( s ) sang trạng thái ( s\u0026rsquo; ) khi thực hiện hành động ( a ). $( \\gamma )$ là hệ số chiết khấu, và $( \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s, a)V(s\u0026rsquo;) )$ là giá trị kỳ vọng của các trạng thái tiếp theo. b. Phương trình Bellman cho hàm giá trị hành động ( Q(s, a) )\nHàm giá trị hành động ( Q(s, a) ) biểu diễn tổng phần thưởng kỳ vọng khi bắt đầu từ trạng thái ( s ), thực hiện hành động ( a ), và sau đó tiếp tục theo chính sách tối ưu. Phương trình Bellman cho hàm giá trị hành động là:\n$$ [ Q(s, a) = R(s, a) + \\gamma \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s, a) \\max_{a\u0026rsquo;} Q(s\u0026rsquo;, a\u0026rsquo;) ] $$\nỞ đây:\n( Q(s, a) ) là giá trị của hành động ( a ) ở trạng thái ( s ). ( \\max_{a\u0026rsquo;} Q(s\u0026rsquo;, a\u0026rsquo;) ) là giá trị tối ưu của hành động tiếp theo ở trạng thái kế tiếp ( s\u0026rsquo; ). Q-Learning Q-learning là một thuật toán trong nhóm học tăng cường (reinforcement learning) , thuộc nhóm model-free, value-based, off-policy. Thuật toán sẽ tìm ra chuỗi hành động tốt nhất dựa trên trạng thái hiện tại của agent. “Q” đại diện cho chất lượng. Chất lượng biểu thị giá trị của hành động trong việc tối đa hóa (cực đại hóa) phần thưởng ở tương lai.\nCó một số key word cần làm rõ một chút.\nChúng ta có hai nhóm thuật toán là model-base và model-free\nmodel-base dùng 2 hàm là transition và reward để ước tính đường đi tối ưu, chúng ta phải vắt óc suy nghĩ 2 hàm này, tưởng tượng bạn chơi cờ và dự đoán trước các nước đi của đối thủ, biết được đối thủ sẽ đi như thế nào, nên ta có thể chọn những nước đi sao cho kết quả cuối cùng ta sẽ thắng.\nmodel-free học từ chuỗi hành động, rút ra kinh nghiệm, và vấp ngã đâu , đứng dậy ở đó, không cần định nghĩa transition function và reward function. Tưởng tượng bạn tự mình học cách đi xe đạp, bạn ngã, rút kinh nghiệm từ lỗi lầm và dần dần đi được mà không cần bản hướng dẫn chi tiết nào, cứ ôm xe đạp mà tập dần.\nTiếp tới, chúng ta có 2 loại phương thức là value-based và policy-based\nPhương pháp value-based , huấn luyện hàm giá trị, huấn luyện làm sao để hàm giá trị có thể tìm ra trạng thái mà trạng thái đó làm cho hàm giá trị đạt giá trị lớn nhất, đạt giá trị cực đại, từ đó quyết định sử dụng hành động đó. Nói cách khác, nó giúp agent hiểu xem ở trạng thái nào thì hành động nào sẽ mang lại phần thưởng cao nhất. Ví dụ, bạn chọn môn học có giá trị nhất để học trước nhằm đạt điểm số cao nhất.\nPhương pháp policy-based đưa ra các policy quy định ứng với từng state, ta sẽ đưa ra các action gì, nó học cách đưa ra quyết định tốt nhất trong từng trạng thái. Giống như khi bạn không chỉ học lý thuyết mà thực sự thực hành để biết cách hành động tốt nhất trong từng tình huống cụ thể.\nCuối cùng, có 2 cái chính sách đối lập là off-policy và on-policy\noff-policy thuật toán đánh giá và cập nhật lại policy mới, policy mới khác với policy đang thực hiện action. Nghĩa là nó không cần theo đúng chính sách hiện hành mà có thể học và cải thiện chính sách mới dựa trên dữ liệu và kinh nghiệm thu thập được, kiểu như là vừa chơi game vừa nghĩ ra chiến lược mới thay vì bám sát chiến lược cũ.\non-policy nó không chỉ dùng chính sách hiện tại mà còn điều chỉnh và cải thiện chính sách đó liên tục dựa trên những gì đã học được từ mỗi hành động. Như cách bạn tiếp tục hoàn thiện chiến lược chơi game của mình mỗi lần chơi dựa trên những gì đã trải qua.\nCác khái niệm trong Q-learning Kế thừa các key trong Reinforcement Learning, chúng ta có\nStates(s) : vị trí hiện tại của agent trong environment\nAction(a) : Hành động của agent trong một state cụ thể\nRewards : Giá trị phần thưởng hoặc giá trị phạt khi một Action xảy ra\nEpisodes: Kết thúc state, khi Agent không thể thực hiện một action mới. Episodes xảy ra khi agent phá đảo hoặc agent bị die\n$Q(S_t+1, a)$ : Giá trị kỳ vọng đạt được Q value ở state t+1 và hành động a\nCách Q-learning hoạt động Q-Table Q-Table về cơ bản là một bảng tra cứu, trong đó mỗi hàng đại diện cho một trạng thái có thể có, và mỗi cột đại diện cho một hành động có thể thực hiện. Bảng này lưu trữ các giá trị Q-values (phần thưởng kỳ vọng) cho mỗi cặp trạng thái-hành động. Theo thời gian, Agent sẽ cập nhật bảng này để học cách lựa chọn hành động tốt nhất trong mỗi trạng thái.\nQ-value Q-value đại diện cho phần thưởng tương lai, kỳ vọng mà Agent sẽ nhận được khi thực hiện một hành động nhất định từ trạng thái hiện tại, và sau đó thực hiện theo policy tốt nhất (tối đa hóa phần thưởng).\nQ-learning Function Là một model-free reinforcement learning, sử dụng phương trình Bellman, cập nhật bảng Q thông qua việc học từ sự tương tác với môi trường. Khi Agent thực hiện một hành động, nó sẽ quan sát phần thưởng và trạng thái mới mà nó chuyển đến. Thuật toán sau đó sẽ cập nhật giá trị Q cho cặp trạng thái-hành động đó theo quy tắc cập nhật sau:\n$$ [ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_a\u0026rsquo; Q(s\u0026rsquo;, a\u0026rsquo;) - Q(s, a) \\right) ] $$\nTrong đó:\n$( Q(s, a) )$ là giá trị Q cho trạng thái ( s ) và hành động ( a ) $( \\alpha )$ là tốc độ học (quyết định mức độ mà thông tin mới ghi đè thông tin cũ) $( r )$ là phần thưởng nhận được sau khi thực hiện hành động ( a ) ở trạng thái ( s ) $( \\gamma )$ là hệ số chiết khấu (xác định mức độ phần thưởng tương lai được tính đến) $( \\max_a\u0026rsquo; Q(s\u0026rsquo;, a\u0026rsquo;) )$ là phần thưởng kỳ vọng lớn nhất cho trạng thái tiếp theo ( s\u0026rsquo; ) sau hành động ( a\u0026rsquo; ) Theo thời gian, Agent sử dụng Q-learning sẽ dần dần hoàn thiện bảng Q của mình và học được cách thực hiện các hành động tối ưu cho mỗi trạng thái để tối đa hóa phần thưởng kỳ vọng.\nQ-learning algorithm Init Q_table -\u0026gt; Choose action -\u0026gt; Do action -\u0026gt; Mesure reward -\u0026gt; Update Q Table -\u0026gt; Choose action \u0026hellip;\nInit Q_table Xây dựng bảng bao gồm hàng là các state, cột là các action , đầu tiên có thể khởi tạo giá trị của bảng này là 0.\nChoose action Ở lần chạy đầu tiên, chúng ta có thể random action, ở lần chạy sau, chúng ta lấy action ở bảng Q Table ở trên\nDo action Thực hiện chọn hành động và thực hiện hành động đến khi quá trình train dừng lại.\nVới mỗi lần Choose action và Do action, chúng ta sẽ:\nỞ lần chạy đầu tiên, chúng ta lấy ngẫu nhiên 1 hành động, sau đó Agent sẽ thực hiện hành động và nhận reward, update Q Table sử dụng Q-learning Function đã nêu phía trên. Ở các lần chạy sau, chúng ta lấy ra hành động tốt nhất để Agent thực hiện hành động và chúng ta lại update Q Table tiếp.\nVì lý do là Agent cần tối đa hóa phần thưởng đạt được, nên ở đây xuất hiện 2 khái niệm là exploration (khám phá) và exploitation (khai thác), và cần cân bằng cả 2\nExploration (Khám phá):\nKhám phá là khi Agent thử những hành động mới hoặc chưa từng thử trước đó để tìm hiểu thêm về môi trường. Điều này có nghĩa là Agent có thể sẽ không chọn hành động có phần thưởng cao nhất dựa trên thông tin hiện tại mà thay vào đó thử các hành động chưa rõ kết quả. Lý do: Nếu Agent chỉ khai thác các hành động có phần thưởng cao hiện tại mà không khám phá các hành động khác, nó có thể bỏ lỡ những hành động tốt hơn ở tương lai. Môi trường có thể phức tạp và thay đổi, nên Agent cần tiếp tục tìm hiểu để có dữ liệu đầy đủ. Exploitation (Khai thác):\nKhai thác là khi Agent chọn hành động dựa trên thông tin mà nó đã học được để tối ưu hóa phần thưởng. Trong trường hợp này, Agent chọn hành động mà nó tin là có phần thưởng cao nhất dựa trên những gì nó đã trải nghiệm. Lý do: Sau khi đã tích lũy đủ thông tin về môi trường, Agent cần tập trung khai thác các hành động đã được biết là có lợi để tối đa hóa phần thưởng trong dài hạn. Tại sao cần có cả hai?\nCân bằng: Nếu chỉ khai thác mà không khám phá, Agent có thể rơi vào cái gọi là local optimum (cực đại cục bộ) mà bỏ lỡ cơ hội đạt được global optimum (cực đại toàn cục), tức là giải pháp tốt nhất. Mặt khác, nếu chỉ khám phá mà không khai thác, Agent sẽ không thể tận dụng những gì nó đã học được, dẫn đến không tối ưu hóa phần thưởng. Ví dụ:\nExploration: Bạn đi ăn ở một nhà hàng mới mà bạn chưa bao giờ thử, hy vọng tìm được món ăn ngon hơn. Exploitation: Bạn quay lại một nhà hàng quen thuộc mà bạn biết chắc món ăn ở đó rất ngon. Trong thực tế, các thuật toán như epsilon-greedy sử dụng một chiến lược kết hợp cả khám phá và khai thác, cho phép Agent thực hiện phần lớn các hành động khai thác nhưng đôi khi vẫn khám phá những hành động mới với một xác suất nhỏ (epsilon).\nTrong Q-learning, ở giai đoạn đầu, giá trị epsilon thường lớn để xác xuất Exploration xuất hiện nhiều, qua mỗi lần lặp, Agent càng ngày càng tự tin với các giá trị học được đã được cập nhật ở Q table, nên giá trị Exploration ở các lần lặp sau sẽ giảm bớt, nhỏ đần, từ đó xác xuất chọn action từ Q table sẽ lớn hơn.\nMeasuring the Rewards Sau khi thực hiện hành động, chúng ta sẽ thu được kết quả và phần thưởng\nCó nhiều cách cho thưởng, tùy , một dạng đơn giản nhất đó là\nNếu về đích , +1 điểm thưởng\nNếu thất bại, chưa về đích , 0 điểm\nUpdate Q Table Trong Q-learning, khi một Agent cập nhật giá trị Q cho một cặp trạng thái-hành động, quá trình này dựa trên sự kết hợp giữa giá trị Q cũ (former Q-value) và giá trị Q mới ước tính (new Q-value estimation). Đây là hai khía cạnh quan trọng của công thức cập nhật Q-value trong Q-learning:\nFormer Q-value (Giá trị Q cũ):\nĐây là giá trị Q hiện tại cho một cặp trạng thái-hành động cụ thể mà Agent đã ghi nhận trước đó. Nó thể hiện phần thưởng kỳ vọng mà Agent đã tính toán từ các lần tương tác trước đó với môi trường.\nTrong công thức cập nhật Q-learning:\n$$ [ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_a\u0026rsquo; Q(s\u0026rsquo;, a\u0026rsquo;) - Q(s, a) \\right) ] $$\nPhần $( Q(s, a) )$ bên phải của dấu mũi tên là giá trị Q cũ.\nNew Q-value estimation (Giá trị Q mới ước tính): Đây là giá trị Q được cập nhật dựa trên phần thưởng vừa nhận được và dự đoán phần thưởng trong tương lai (dựa trên trạng thái tiếp theo và hành động tốt nhất có thể thực hiện).\nPhần ( r + \\gamma \\max_a\u0026rsquo; Q(s\u0026rsquo;, a\u0026rsquo;) ) trong công thức là phần thưởng mới và giá trị kỳ vọng của trạng thái tiếp theo. Điều này đại diện cho sự ước tính mới về phần thưởng nếu Agent tiếp tục thực hiện chính sách tối ưu từ trạng thái tiếp theo.\nAlpha (α) - Hệ số học (Learning Rate):\n-Ý nghĩa: Alpha kiểm soát mức độ mà các giá trị Q hiện tại được cập nhật bằng thông tin mới. Nó quyết định xem tác nhân sẽ học nhanh chóng từ các trải nghiệm mới hay học dần dần.\nPhạm vi: $( 0 \\leq \\alpha \\leq 1 )$\nGiải thích:\nα = 1: Tác nhân hoàn toàn bỏ qua thông tin cũ và chỉ dùng giá trị mới ước tính. Nghĩa là mỗi khi có một trải nghiệm mới, giá trị Q cũ sẽ được thay thế hoàn toàn.\nα = 0: Tác nhân hoàn toàn không cập nhật giá trị Q cũ, có nghĩa là tác nhân sẽ không học gì từ trải nghiệm mới.\nGiá trị trung gian (0 \u0026lt; α \u0026lt; 1): Kết hợp giữa giá trị Q cũ và giá trị mới, tức là học tập từ cả kinh nghiệm cũ và mới một cách từ từ. Trong thực tế, alpha thường được chọn là một giá trị nhỏ (ví dụ: 0.1 hoặc 0.01) để tác nhân có thể học ổn định và không thay đổi quá đột ngột.\nGamma (γ) - Hệ số chiết khấu (Discount Factor):\nÝ nghĩa: Gamma xác định mức độ mà tác nhân coi trọng các phần thưởng trong tương lai. Nó cho phép tác nhân cân nhắc giữa việc nhận phần thưởng ngay lập tức và phần thưởng tiềm năng trong tương lai. Phạm vi: $( 0 \\leq \\gamma \\leq 1 )$ Giải thích: γ = 0: Tác nhân chỉ quan tâm đến phần thưởng tức thì mà không để ý đến phần thưởng tương lai. Điều này khiến tác nhân chỉ tối ưu hóa cho lợi ích ngắn hạn. γ = 1: Tác nhân đánh giá phần thưởng hiện tại và tương lai một cách cân bằng, tức là phần thưởng trong tương lai xa có cùng trọng số với phần thưởng ngay lập tức. Giá trị trung gian (0 \u0026lt; γ \u0026lt; 1): Đây là lựa chọn phổ biến trong các bài toán thực tế. Gamma sẽ giảm dần giá trị của các phần thưởng càng xa trong tương lai, nhưng vẫn đảm bảo rằng tác nhân quan tâm đến việc tối đa hóa phần thưởng dài hạn. Tóm lại, vai trò của α và γ:\nAlpha (α): Điều chỉnh tốc độ học, tức là mức độ cập nhật giá trị Q dựa trên thông tin mới. Gamma (γ): Điều chỉnh sự ưu tiên giữa phần thưởng hiện tại và phần thưởng trong tương lai. Cả hai tham số này ảnh hưởng trực tiếp đến hiệu quả học tập của tác nhân trong môi trường và cần được tinh chỉnh phù hợp cho từng bài toán cụ thể.\nDouble Deep Q-Network Sau khi tìm hiểu Q learning, chúng ta sẽ tìm hiểu 1 cải tiến của nó là Double Deep Q-Network\nDouble Deep Q-Network (DDQN) là một cải tiến của Q-learning (cụ thể là DQN - Deep Q-Network) nhằm giải quyết một số vấn đề quan trọng trong quá trình học tập. So với Q-learning, DDQN giúp giảm sự thiên lệch ước lượng (overestimation bias) và cải thiện độ chính xác trong việc chọn hành động. Dưới đây là chi tiết về các cải tiến của DDQN so với Q-learning:\n1. Vấn đề của Q-learning (Overestimation Bias): Q-learning tiêu chuẩn (bao gồm cả DQN, phiên bản mở rộng với mạng nơ-ron) có xu hướng gặp phải vấn đề gọi là thiên lệch ước lượng quá mức (overestimation bias). Khi tính toán giá trị Q, Q-learning chọn hành động dựa trên giá trị Q lớn nhất trong Q-table (hoặc mạng Q trong DQN). Tuy nhiên, do sự ngẫu nhiên trong môi trường và các lỗi nhỏ khi ước tính, tác nhân có thể đánh giá quá cao giá trị Q của một số hành động.\nCông thức cập nhật Q-learning:\n$$ [ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_a Q(s\u0026rsquo;, a\u0026rsquo;) - Q(s, a) \\right) ] $$\nTrong đó, ( \\max_a Q(s\u0026rsquo;, a\u0026rsquo;) ) chọn hành động có giá trị Q cao nhất cho trạng thái tiếp theo ( s\u0026rsquo; ). Việc sử dụng cùng một mạng để chọn và đánh giá hành động này có thể dẫn đến thiên lệch khi các giá trị Q bị phóng đại một cách không chính xác.\n2. Cải tiến của Double Deep Q-Network (DDQN): DDQN được phát triển để khắc phục vấn đề thiên lệch ước lượng quá mức trong Q-learning/DQN bằng cách tách biệt việc chọn hành động và đánh giá giá trị của hành động. Trong DDQN, hai mạng nơ-ron khác nhau được sử dụng để thực hiện hai nhiệm vụ này:\nMạng chính (main network): Được sử dụng để chọn hành động tốt nhất cho trạng thái tiếp theo. Mạng mục tiêu (target network): Được sử dụng để ước tính giá trị của hành động đó. Công thức cập nhật DDQN:\n$$ [ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma Q_{\\text{target}}(s\u0026rsquo;, \\arg\\max_a Q_{\\text{main}}(s\u0026rsquo;, a\u0026rsquo;)) - Q(s, a) \\right) ] $$\nTrong đó:\n( Q_{\\text{main}}(s\u0026rsquo;, a\u0026rsquo;) ): Mạng chính được dùng để chọn hành động tốt nhất tại trạng thái ( s\u0026rsquo; ) (tức là hành động có giá trị Q cao nhất).\n( Q_{\\text{target}}(s\u0026rsquo;, a\u0026rsquo;) ): Mạng mục tiêu được dùng để đánh giá giá trị Q của hành động đó.\nÝ tưởng chính: Bằng cách sử dụng hai mạng riêng biệt (một để chọn hành động, một để đánh giá), DDQN tránh được việc phóng đại giá trị Q do cùng một mạng chọn và đánh giá hành động trong Q-learning/DQN tiêu chuẩn. Điều này giúp giảm thiên lệch và cải thiện hiệu quả học tập.\n3. Lợi ích của DDQN so với DQN/Q-learning: Giảm thiên lệch ước lượng (Overestimation Bias): DDQN cải thiện độ chính xác của ước tính giá trị Q bằng cách tách rời nhiệm vụ chọn và đánh giá hành động. Học tập ổn định hơn: Việc giảm thiên lệch giúp DDQN ổn định hơn trong quá trình học tập, đặc biệt khi các tác nhân tương tác với những môi trường phức tạp và ngẫu nhiên. Cải thiện độ hội tụ (Convergence): Do các giá trị Q không bị phóng đại một cách sai lầm, quá trình học tập của tác nhân trở nên hiệu quả và nhanh hơn, giúp hệ thống hội tụ về giải pháp tốt hơn. 4. Ví dụ trực quan về sự khác biệt: DQN: Nếu có hai hành động A và B, và DQN đánh giá hành động A có giá trị Q là 10 (thực tế là 8) và B là 9 (thực tế là 7), DQN sẽ chọn A vì $( \\max(10, 9) = 10 )$. Tuy nhiên, giá trị thực của A chỉ là 8, dẫn đến đánh giá sai. DDQN: Trong DDQN, mạng chính sẽ chọn A, nhưng mạng mục tiêu sẽ đánh giá A dựa trên giá trị thực tế của nó, làm giảm khả năng phóng đại giá trị và giúp lựa chọn chính xác hơn. 5. Tóm tắt: Q-learning/DQN: Chỉ dùng một mạng để chọn và đánh giá, dễ gặp tình trạng ước lượng quá cao (overestimation). DDQN: Tách biệt việc chọn và đánh giá hành động, giúp giảm thiên lệch và cải thiện quá trình học tập. II. Thực hành với chương trình mario Ở bài thực hành này, mình kế thừa code từ blog chính chủ của pytorch\nTrain trò chơi mario sử dụng Reinforcement Learning\ncác nguyên liệu cần thiết\n1pip install gym==0.22.0 --update 2pip install gym-super-mario-bros==7.4.0 3pip install tensordict==0.3.0 4pip install torchrl==0.3.0 Các bạn lưu ý sử dụng đúng phiên bản để khỏi bị lỗi\nEnvironment Khởi tạo môi trường Trong trò chơi mario, chúng ta có nhiều đối tượng khi chơi, là cây nấm , các ống trụ màu xanh, các viên gạch \u0026hellip;\nKhi chúng ta thực hiện một hành động ( ấn nút trên Joypad ), trò chơi sẽ phản hồi lại next_state là hình ảnh của khung hình sau khi ta nhấn nút, reward, done, info\n1 2env = gym_super_mario_bros.make(\u0026#34;SuperMarioBros-1-1-v0\u0026#34;) 3 4# Limit the action-space to 5# 0. walk right 6# 1. jump right 7env = JoypadSpace(env, [[\u0026#34;right\u0026#34;], [\u0026#34;right\u0026#34;, \u0026#34;A\u0026#34;]]) 8 9env.reset() 10next_state, reward, done, info = env.step(action=0) 11print(f\u0026#34;{next_state.shape},\\n {reward},\\n {done},\\n {info}\u0026#34;) 12 13 14(240, 256, 3), 15 0.0, 16 False, 17 {\u0026#39;coins\u0026#39;: 0, \u0026#39;flag_get\u0026#39;: False, \u0026#39;life\u0026#39;: 2, \u0026#39;score\u0026#39;: 0, \u0026#39;stage\u0026#39;: 1, \u0026#39;status\u0026#39;: \u0026#39;small\u0026#39;, \u0026#39;time\u0026#39;: 400, \u0026#39;world\u0026#39;: 1, \u0026#39;x_pos\u0026#39;: 40, \u0026#39;y_pos\u0026#39;: 79} Xử lý dữ liệu Dữ liệu của state là một hình có kích thước (240, 256, 3) , hệ bgr, chúng ta sẽ convert về GrayScale (1 ,240, 256) và resize về hình vuông có kích thước 84x84 để tăng thời gian xử lý . Các bạn có thể thay đổi thành 112x112 hoặc 96x96 tùy thích.\nNgoài ra, do hình trước khi ấn và hình sau khi ấn nút thường sẽ gần gần giống nhau, nên chúng ta sẽ thêm một lớp SkipFrame, hiểu đúng như tên, chúng ta sẽ cộng dồn giá trị reward để trả ra cho mô hình thực hiện cập nhật trọng số. Ví dụ SkipFrame(4), nghĩa là ta sẽ cộng dồn giá trị reward của 4 hình liên tiếp thành tổng reward và cập nhật trọng số, cái này giúp cho mô hình chạy nhanh hơn xíu mà vẫn đảm bảo thông tin, tất nhiên số lượng frame bị skip cần be bé thôi\nChúng ta sẽ tạo các lớp , implement từ gym.Wrapper\n1 2class SkipFrame(gym.Wrapper): 3 def __init__(self, env, skip): 4 \u0026#34;\u0026#34;\u0026#34;Return only every `skip`-th frame\u0026#34;\u0026#34;\u0026#34; 5 super().__init__(env) 6 self._skip = skip 7 8 def step(self, action): 9 \u0026#34;\u0026#34;\u0026#34;Repeat action, and sum reward\u0026#34;\u0026#34;\u0026#34; 10 total_reward = 0.0 11 for i in range(self._skip): 12 # Accumulate reward and repeat the same action 13 obs, reward, done, trunk, info = self.env.step(action) 14 total_reward += reward 15 if done: 16 break 17 return obs, total_reward, done, trunk, info 18 19 20class GrayScaleObservation(gym.ObservationWrapper): 21 def __init__(self, env): 22 super().__init__(env) 23 obs_shape = self.observation_space.shape[:2] 24 self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8) 25 26 def permute_orientation(self, observation): 27 # permute [H, W, C] array to [C, H, W] tensor 28 observation = np.transpose(observation, (2, 0, 1)) 29 observation = torch.tensor(observation.copy(), dtype=torch.float) 30 return observation 31 32 def observation(self, observation): 33 observation = self.permute_orientation(observation) 34 transform = T.Grayscale() 35 observation = transform(observation) 36 return observation 37 38 39class ResizeObservation(gym.ObservationWrapper): 40 def __init__(self, env, shape): 41 super().__init__(env) 42 if isinstance(shape, int): 43 self.shape = (shape, shape) 44 else: 45 self.shape = tuple(shape) 46 47 obs_shape = self.shape + self.observation_space.shape[2:] 48 self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8) 49 50 def observation(self, observation): 51 transforms = T.Compose( 52 [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)] 53 ) 54 observation = transforms(observation).squeeze(0) 55 return observation 56 57 58# Apply Wrappers to environment 59env = SkipFrame(env, skip=4) 60env = GrayScaleObservation(env) 61env = ResizeObservation(env, shape=84) 62 63env = FrameStack(env, num_stack=4) Cuối cùng, chúng ta sẽ đóng các khai báo trên vào một FrameStack với số lượng lớp là 4, nghĩa là chúng ta sẽ đưa vào 4 hình có kích thước (240, 256, 3), kết quả là hình có kích thươc (4, 84, 84)\nAgent Chúng ta chơi mario, nên tạo 1 Agent tên là mario , theo lý thuyết, chúng ta sẽ có các hành động cho agent\nAct : Trả về 1 hành động tối ưu , trong danh sách các hành động dựa trên hình ảnh hiệnt tại\nRemember experiences. Experience = (current state, current action, reward, next state). Mario sẽ lưu lại các hành động (cache) và nhớ lại các hành động của mình để rút ra bài học\nLearn: Cập nhật trọng số\n1 2class Mario: 3 def __init__(): 4 pass 5 6 def act(self, state): 7 \u0026#34;\u0026#34;\u0026#34;Given a state, choose an epsilon-greedy action\u0026#34;\u0026#34;\u0026#34; 8 pass 9 10 def cache(self, experience): 11 \u0026#34;\u0026#34;\u0026#34;Add the experience to memory\u0026#34;\u0026#34;\u0026#34; 12 pass 13 14 def recall(self): 15 \u0026#34;\u0026#34;\u0026#34;Sample experiences from memory\u0026#34;\u0026#34;\u0026#34; 16 pass 17 18 def learn(self): 19 \u0026#34;\u0026#34;\u0026#34;Update online action value (Q) function with a batch of experiences\u0026#34;\u0026#34;\u0026#34; 20 pass Act Khi chơi mario, hành động chúng ta sẽ thực hiện sẽ là lấy ngẫu nhiên 1 hành động trong tập lệnh (explore), hoặc là thực hiện lệnh tối ưu do mô hình gợi ý (exploit). Để đạt được tính năng này, chúng ta sẽ sử dụng exploration_rate để điều khiển xác xuất chọn explore hay exploit.\nNgoài ra, do là mô hình AI, nên chúng ta cần xây dựng một lớp CNN tên là MarioNet để hàm learn cập nhật trọng số\n1 2class Mario: 3 def __init__(self, state_dim, action_dim, save_dir): 4 self.state_dim = state_dim 5 self.action_dim = action_dim 6 self.save_dir = save_dir 7 8 self.device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; 9 10 # Mario\u0026#39;s DNN to predict the most optimal action - we implement this in the Learn section 11 self.net = MarioNet(self.state_dim, self.action_dim).float() 12 self.net = self.net.to(device=self.device) 13 14 self.exploration_rate = 1 15 self.exploration_rate_decay = 0.99999975 16 self.exploration_rate_min = 0.1 17 self.curr_step = 0 18 19 self.save_every = 5e5 # no. of experiences between saving Mario Net 20 21 def act(self, state): 22 \u0026#34;\u0026#34;\u0026#34; 23 Given a state, choose an epsilon-greedy action and update value of step. 24 25 Inputs: 26 state(``LazyFrame``): A single observation of the current state, dimension is (state_dim) 27 Outputs: 28 ``action_idx`` (``int``): An integer representing which action Mario will perform 29 \u0026#34;\u0026#34;\u0026#34; 30 # EXPLORE 31 if np.random.rand() \u0026lt; self.exploration_rate: 32 action_idx = np.random.randint(self.action_dim) 33 34 # EXPLOIT 35 else: 36 state = state[0].__array__() if isinstance(state, tuple) else state.__array__() 37 state = torch.tensor(state, device=self.device).unsqueeze(0) 38 action_values = self.net(state, model=\u0026#34;online\u0026#34;) 39 action_idx = torch.argmax(action_values, axis=1).item() 40 41 # decrease exploration_rate 42 self.exploration_rate *= self.exploration_rate_decay 43 self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate) 44 45 # increment step 46 self.curr_step += 1 47 return action_idx Remember Phần này gồm 2 hàm là cache và recall. cache, hiểu đúng như tên, là lưu lại các giá trị state, next_state, action, reward, done. recall là lấy các giá trị đã được nhớ ra\n1 2class Mario(Mario): # subclassing for continuity 3 def __init__(self, state_dim, action_dim, save_dir): 4 super().__init__(state_dim, action_dim, save_dir) 5 self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\u0026#34;cpu\u0026#34;))) 6 self.batch_size = 32 7 8 def cache(self, state, next_state, action, reward, done): 9 \u0026#34;\u0026#34;\u0026#34; 10 Store the experience to self.memory (replay buffer) 11 12 Inputs: 13 state (``LazyFrame``), 14 next_state (``LazyFrame``), 15 action (``int``), 16 reward (``float``), 17 done(``bool``)) 18 \u0026#34;\u0026#34;\u0026#34; 19 def first_if_tuple(x): 20 return x[0] if isinstance(x, tuple) else x 21 state = first_if_tuple(state).__array__() 22 next_state = first_if_tuple(next_state).__array__() 23 24 state = torch.tensor(state) 25 next_state = torch.tensor(next_state) 26 action = torch.tensor([action]) 27 reward = torch.tensor([reward]) 28 done = torch.tensor([done]) 29 30 # self.memory.append((state, next_state, action, reward, done,)) 31 self.memory.add(TensorDict({\u0026#34;state\u0026#34;: state, \u0026#34;next_state\u0026#34;: next_state, \u0026#34;action\u0026#34;: action, \u0026#34;reward\u0026#34;: reward, \u0026#34;done\u0026#34;: done}, batch_size=[])) 32 33 def recall(self): 34 \u0026#34;\u0026#34;\u0026#34; 35 Retrieve a batch of experiences from memory 36 \u0026#34;\u0026#34;\u0026#34; 37 batch = self.memory.sample(self.batch_size).to(self.device) 38 state, next_state, action, reward, done = (batch.get(key) for key in (\u0026#34;state\u0026#34;, \u0026#34;next_state\u0026#34;, \u0026#34;action\u0026#34;, \u0026#34;reward\u0026#34;, \u0026#34;done\u0026#34;)) 39 return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze() Learn Ở phần init trên, chúng ta có cái khai báo MarioNet, ở đây, chúng ta sử dụng mô hình DDQN - Double Q-learning https://arxiv.org/pdf/1509.06461\nDDQN sử dụng hai CNN đặt tên là Q_online và Q_target. Hai mô hình cnn này độc lập với nhau\nChúng ta sẽ chia sẽ chung features của Q_online và Q_target, nhưng FC classifiers sẽ độc lập nhau, các giá trị trọng số của Q_target sẽ bị frozen để ngăng cập nhật trọng số từ backprop\n1 2class MarioNet(nn.Module): 3 \u0026#34;\u0026#34;\u0026#34;mini CNN structure 4 input -\u0026gt; (conv2d + relu) x 3 -\u0026gt; flatten -\u0026gt; (dense + relu) x 2 -\u0026gt; output 5 \u0026#34;\u0026#34;\u0026#34; 6 7 def __init__(self, input_dim, output_dim): 8 super().__init__() 9 c, h, w = input_dim 10 11 if h != 84: 12 raise ValueError(f\u0026#34;Expecting input height: 84, got: {h}\u0026#34;) 13 if w != 84: 14 raise ValueError(f\u0026#34;Expecting input width: 84, got: {w}\u0026#34;) 15 16 self.online = self.__build_cnn(c, output_dim) 17 18 self.target = self.__build_cnn(c, output_dim) 19 self.target.load_state_dict(self.online.state_dict()) 20 21 # Q_target parameters are frozen. 22 for p in self.target.parameters(): 23 p.requires_grad = False 24 25 def forward(self, input, model): 26 if model == \u0026#34;online\u0026#34;: 27 return self.online(input) 28 elif model == \u0026#34;target\u0026#34;: 29 return self.target(input) 30 31 def __build_cnn(self, c, output_dim): 32 return nn.Sequential( 33 nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4), 34 nn.ReLU(), 35 nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), 36 nn.ReLU(), 37 nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), 38 nn.ReLU(), 39 nn.Flatten(), 40 nn.Linear(3136, 512), 41 nn.ReLU(), 42 nn.Linear(512, output_dim), 43 ) Estimate Do chúng ta có 2 lớp cnn, nên chúng ta cần xây 2 hàm Estimate\nVới Q_online, chúng ta thực hiện infer, xong.\nVới Q_target, giá trị reward hơi phức tạp một chút, phân tích chúng\nchúng ta có giá trị reward hiện tại\nChúng ta cần kết hợp với reward của Q_target, nhưng action thì không biết, vậy nên chúng ta sẽ lấy action tối ưu từ Q_online với state hiện tại\n1 2class Mario(Mario): 3 def __init__(self, state_dim, action_dim, save_dir): 4 super().__init__(state_dim, action_dim, save_dir) 5 self.gamma = 0.9 6 7 def td_estimate(self, state, action): 8 current_Q = self.net(state, model=\u0026#34;online\u0026#34;)[ 9 np.arange(0, self.batch_size), action 10 ] # Q_online(s,a) 11 return current_Q 12 13 @torch.no_grad() 14 def td_target(self, reward, next_state, done): 15 next_state_Q = self.net(next_state, model=\u0026#34;online\u0026#34;) 16 best_action = torch.argmax(next_state_Q, axis=1) 17 next_Q = self.net(next_state, model=\u0026#34;target\u0026#34;)[ 18 np.arange(0, self.batch_size), best_action 19 ] 20 return (reward + (1 - done.float()) * self.gamma * next_Q).float() Cập nhật model Sử dụng cnn, nên chúng ta cần định nghĩa là loss và hàm optimizer, sau khi update trọng số của Q_online, chúng ta sẽ cập nhật trọng số đó cho Q_target\n1 2class Mario(Mario): 3 def __init__(self, state_dim, action_dim, save_dir): 4 super().__init__(state_dim, action_dim, save_dir) 5 self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025) 6 self.loss_fn = torch.nn.SmoothL1Loss() 7 8 def update_Q_online(self, td_estimate, td_target): 9 loss = self.loss_fn(td_estimate, td_target) 10 self.optimizer.zero_grad() 11 loss.backward() 12 self.optimizer.step() 13 return loss.item() 14 15 def sync_Q_target(self): 16 self.net.target.load_state_dict(self.net.online.state_dict()) Save checkpoint 1class Mario(Mario): 2 def save(self): 3 save_path = ( 4 self.save_dir / f\u0026#34;mario_net_{int(self.curr_step // self.save_every)}.chkpt\u0026#34; 5 ) 6 torch.save( 7 dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), 8 save_path, 9 ) 10 print(f\u0026#34;MarioNet saved to {save_path} at step {self.curr_step}\u0026#34;) Gom vào hàm learn 1 2class Mario(Mario): 3 def __init__(self, state_dim, action_dim, save_dir): 4 super().__init__(state_dim, action_dim, save_dir) 5 self.burnin = 1e4 # min. experiences before training 6 self.learn_every = 3 # no. of experiences between updates to Q_online 7 self.sync_every = 1e4 # no. of experiences between Q_target \u0026amp; Q_online sync 8 9 def learn(self): 10 if self.curr_step % self.sync_every == 0: 11 self.sync_Q_target() 12 13 if self.curr_step % self.save_every == 0: 14 self.save() 15 16 if self.curr_step \u0026lt; self.burnin: 17 return None, None 18 19 if self.curr_step % self.learn_every != 0: 20 return None, None 21 22 # Sample from memory 23 state, next_state, action, reward, done = self.recall() 24 25 # Get TD Estimate 26 td_est = self.td_estimate(state, action) 27 28 # Get TD Target 29 td_tgt = self.td_target(reward, next_state, done) 30 31 # Backpropagate loss through Q_online 32 loss = self.update_Q_online(td_est, td_tgt) 33 34 return (td_est.mean().item(), loss) Play Chúng ta thực hiện learn 40000 lần\n1 2use_cuda = torch.cuda.is_available() 3print(f\u0026#34;Using CUDA: {use_cuda}\u0026#34;) 4print() 5 6save_dir = Path(\u0026#34;checkpoints\u0026#34;) / datetime.datetime.now().strftime(\u0026#34;%Y-%m-%dT%H-%M-%S\u0026#34;) 7save_dir.mkdir(parents=True) 8 9mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir) 10 11logger = MetricLogger(save_dir) 12 13episodes = 40000 14for e in range(episodes): 15 16 state = env.reset() 17 18 # Play the game! 19 while True: 20 21 # Run agent on the state 22 action = mario.act(state) 23 24 # Agent performs action 25 next_state, reward, done, trunc, info = env.step(action) 26 27 # Remember 28 mario.cache(state, next_state, action, reward, done) 29 30 # Learn 31 q, loss = mario.learn() 32 33 # Logging 34 logger.log_step(reward, loss, q) 35 36 # Update state 37 state = next_state 38 39 # Check if end of game 40 if done or info[\u0026#34;flag_get\u0026#34;]: 41 break 42 43 logger.log_episode() 44 45 if (e % 20 == 0) or (e == episodes - 1): 46 logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step) Replay Ở hàm này, mình load model lên và cho auto chơi, sau vài vòng lặp cũng sẽ về đích được :)\nỞ đây, các bạn chú ý phiên bản gym 0.22.0, ở bài viết gốc họ xài gym 0.17.x nên không có hàm save video, phải tự viết lại, còn các bản cao hơn thì họ tách rõ biến done của env.step thành 2 biến nên nếu bạn nào xài code thì sẽ bị lỗi.\n1 2import random, datetime 3from pathlib import Path 4 5import gym 6import gym_super_mario_bros 7from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation 8from nes_py.wrappers import JoypadSpace 9 10from metrics import MetricLogger 11from agent import Mario 12from wrappers import ResizeObservation, SkipFrame 13 14 15word = 1 16state = 1 17env = gym_super_mario_bros.make(f\u0026#39;SuperMarioBros-{word}-{state}-v0\u0026#39;) 18 19 20 21env = JoypadSpace( 22 env, 23 [[\u0026#39;right\u0026#39;], 24 [\u0026#39;right\u0026#39;, \u0026#39;A\u0026#39;]] 25) 26 27env = SkipFrame(env, skip=4) 28env = GrayScaleObservation(env, keep_dim=False) 29env = ResizeObservation(env, shape=84) 30env = TransformObservation(env, f=lambda x: x / 255.) 31env = FrameStack(env, num_stack=4) 32env = gym.wrappers.RecordVideo(env=env, video_folder=\u0026#34;video\u0026#34;, name_prefix=f\u0026#34;mario_-{word}-{state}\u0026#34;) 33 34env.reset() 35 36 37 38# Start the recorder 39env.start_video_recorder() 40 41save_dir = Path(\u0026#39;checkpoints\u0026#39;) / \u0026#34;test\u0026#34; 42save_dir.mkdir(parents=True,exist_ok=True) 43 44checkpoint = Path(\u0026#39;checkpoints/2024-10-12T14-02-01/mario_net_15.chkpt\u0026#39;) 45mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint) 46mario.exploration_rate = mario.exploration_rate_min 47 48logger = MetricLogger(save_dir) 49 50episodes = 50 51 52for e in range(episodes): 53 54 state = env.reset() 55 56 while True: 57 58 env.render() 59 60 action = mario.act(state) 61 62 next_state, reward, done, info = env.step(action) 63 64 mario.cache(state, next_state, action, reward, done) 65 # print(next_state, reward, done, info) 66 67 logger.log_step(reward, None, None) 68 69 state = next_state 70 71 if done or info[\u0026#39;flag_get\u0026#39;]: 72 break 73 74 logger.log_episode() 75 76 if e % 20 == 0: 77 logger.record( 78 episode=e, 79 epsilon=mario.exploration_rate, 80 step=mario.curr_step 81 ) 82 83env.close_video_recorder() 84 85# Close the environment 86env.close() code chính chủ https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\nKết quả Đợi tầm 48h khi chạy bằng GPU, mình train bằng RTX 4060 TI 16G, khá lâu\nNếu train với phần cứng mạnh hơn, như RTX 4090, hoặc A100, hoặc đổi một model mạnh hơn như Proximal Policy Optimizatio, sẽ nhanh hơn\nModel trên mình train với level 1, để chạy auto cho level 2,3\u0026hellip; 32, mình phải chạy 32 lần train tương ứng cho mỗi level.\nMình thử để model chạy thử cho level 2,3 nhưng không về đích được, phải train lại\nYour browser does not support the video tag. Phần tiếp theo, mình sẽ train thử model PPO thay DDQN\nIII. Tham khảo https://arxiv.org/pdf/1509.06461\nhttps://www.geeksforgeeks.org/what-is-reinforcement-learning/\nhttps://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\nhttps://github.com/yfeng997/MadMario\nhttps://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292\nCảm ơn các bạn đã theo dõi bài viết. Xin cảm ơn và hẹn gặp lại.\n","date":"Oct 27, 2024","img":"https://unsplash.it/1920/1080?image=1","permalink":"/blog/2024-10-27-mario-reinfomation-learning-double-dqn/","series":null,"tags":["Reinformation Learning","DeepLearning"],"title":"Sử Dụng Mô Hình Double DQN  Huấn Luyện Mô Hình Reinforcement Learning Với Game Mario"},{"categories":null,"content":" Hashing là gì? Consistent Hashing là gì? Ứng dụng của Consistent Hashing Implement Consistent Hashing Ưu và nhược điểm của Consistent Hashing Hashing là gì? Hasing, tiếng việt có thể dịch là \u0026ldquo;băm\u0026rdquo; là quá trình chúng ta đưa một chuỗi vào một hàm băm và băm nó ra, rồi nén nó lại trong một vùng không gian, thu được vị trí của chuỗi đầu vào trong vùng không gian nén đó.\nCông thức biểu diễn\n1 2vi_tri = ham_bam(dau_vao)%kich_thuoc_khong_gian Ví dụ:\n1 2Giả sử chúng ta có kích thước không gian kich_thuoc_khong_gian = 10 3 4Chúng ta muốn lưu trữ chuỗi đầu vào là dau_vao = \u0026#34;Hello\u0026#34; 5 6Sử dụng hàm băm ham_bam(\u0026#34;Hello\u0026#34;) ra kết quả là 16 7 8ta có vi_tri = ham_bam(\u0026#34;Hello\u0026#34;) % 10 = 16 % 10 = 6 9 10Vậy chuỗi dữ liệu \u0026#34;Hello\u0026#34; sẽ được lưu trữ tại vị trí 6 trong không gian nhớ size 10. Quá trình này cho phép chúng ta biến một chuỗi dữ liệu thành một vị trí trong không gian nhớ, giúp cho việc lưu trữ và truy xuất dữ liệu trở nên hiệu quả hơn.\nConsistent Hashing là gì? Consistent Hashing (băm nhất quán) là kỹ thuật được sử dụng để phân phối các khoá (key) đều trên các cụm máy tính (clusters), mục tiêu là giảm thiểu số lượng các khoá cần di chuyển khi thêm nodes hoặc xoá nodes ( xoá trong trường hợp lỗi , thêm trong trường hợp muốn scale hệ thống), giảm số lượng các khoá cần di chuyển góp phần làm ổn định hệ thống, và giảm tác động tiêu cực của sự thay đổi này lên hệ thống\nMục tiêu:\nPhân phối các khóa (keys) đều trên một cụm các nút (nodes) trong hệ thống.\nGiảm thiểu số lượng khóa cần di chuyển khi thêm hoặc xóa nút khỏi cụm.\nCấu trúc:\nSử dụng một vòng ảo (hashring) để biểu diễn các yêu cầu của server/clients và các server nodes.\nSố lượng vị trí trên vòng không cố định, nhưng được coi là có vô số điểm.\nCác nút máy chủ có thể được đặt tại các vị trí ngẫu nhiên trên vòng bằng cách sử dụng hàm băm (hashing).\nCác yêu cầu (requests) từ người dùng, máy tính hoặc chương trình không có máy chủ cũng được đặt trên cùng một vòng bằng cách sử dụng cùng một hàm băm.\nLợi ích:\nKhi thêm hoặc xóa nút khỏi cụm, chỉ cần di chuyển một số nhỏ khóa đến các nút khác.\nGiảm thiểu tác động của việc thêm hoặc xóa nút đến toàn bộ hệ thống.\nCải thiện hiệu suất và độ tin cậy của hệ thống.\nCách thức hoạt động:\nKhi một yêu cầu được gửi đến hệ thống, nó sẽ được băm (hash) để tạo ra một giá trị băm.\nGiá trị băm này sẽ được sử dụng để xác định vị trí của yêu cầu trên vòng ảo.\nHệ thống sẽ tìm nút máy chủ gần nhất với vị trí của yêu cầu trên vòng và gửi yêu cầu đến nút đó.\nNếu nút máy chủ đó không khả dụng, hệ thống sẽ tìm nút máy chủ tiếp theo trên vòng và gửi yêu cầu đến nút đó.\nKỹ thuật Consistent Hashing giúp phân phối tải trọng đều trên các nút máy chủ và giảm thiểu tác động của việc thêm hoặc xóa nút đến toàn bộ hệ thống.\nỨng dụng của Consistent Hashing Ngày nay Consistent Hashing là một kỹ thuật phổ biến được sử dụng trong các hệ thống phân tán để giải quyết thách thức phân phối hiệu quả các khóa hoặc phần tử dữ liệu trên nhiều nút/máy chủ trong một mạng lưới.\nBằng cách sử dụng Consistent Hashing, các hệ thống phân tán có thể đạt được nhiều lợi ích, bao gồm:\nCải thiện khả năng mở rộng: Consistent Hashing cho phép các hệ thống phân tán mở rộng dễ dàng hơn, vì các nút mới có thể được thêm hoặc xóa mà không làm gián đoạn toàn bộ hệ thống.\nGiảm thiểu chi phí ánh xạ lại: Bằng cách giảm thiểu số lượng các phép ánh xạ lại cần thiết, Consistent Hashing giúp giảm thiểu chi phí liên quan đến việc thêm hoặc xóa nút, giúp duy trì hiệu suất của hệ thống.\nTăng cường khả năng chịu lỗi: Consistent Hashing giúp phân phối các phần tử dữ liệu trên nhiều nút, giúp tăng cường khả năng chịu lỗi của hệ thống và giảm thiểu rủi ro mất dữ liệu trong trường hợp nút bị lỗi.\nCân bằng tải tốt hơn: Consistent Hashing có thể giúp phân phối tải trên các nút một cách đồng đều hơn, giúp cải thiện hiệu suất của hệ thống và giảm thiểu rủi ro các điểm nóng.\nConsistent Hashing được sử dụng rộng rãi trong các hệ thống phân tán khác nhau, bao gồm:\nCơ sở dữ liệu phân tán: Consistent Hashing được sử dụng trong các cơ sở dữ liệu phân tán để phân phối các phần tử dữ liệu trên nhiều nút và cải thiện hiệu suất của hệ thống.\nHệ thống lưu trữ đệm: Consistent Hashing được sử dụng trong các hệ thống lưu trữ đệm để phân phối các phần tử đệm trên nhiều nút và cải thiện hiệu suất của hệ thống.\nMạng lưới phân phối nội dung (CDN): Consistent Hashing được sử dụng trong các mạng lưới phân phối nội dung để phân phối nội dung trên nhiều nút và cải thiện hiệu suất của hệ thống.\nHệ thống lưu trữ đám mây: Consistent Hashing được sử dụng trong các hệ thống lưu trữ đám mây để phân phối các phần tử dữ liệu trên nhiều nút và cải thiện hiệu suất của hệ thống.\nTóm lại, Consistent Hashing là một kỹ thuật mạnh mẽ giúp các hệ thống phân tán phân phối các khóa hoặc phần tử dữ liệu trên nhiều nút một cách hiệu quả, giảm thiểu số lượng các phép ánh xạ lại cần thiết khi thêm hoặc xóa nút, và cải thiện khả năng mở rộng, khả năng chịu lỗi và cân bằng tải của hệ thống.\nImplement Consistent Hashing Để triển khai một hệ thống sử dụng consistent hasing, chúng ta cần xác định 7 bước sau\nBước 1: Chọn hàm băm\nChọn một hàm băm tạo ra một dải giá trị băm phân bố đều. Các lựa chọn phổ biến bao gồm MD5, SHA-1 hoặc SHA-256. Bước 2: Định nghĩa vòng băm\nBiểu diễn dải giá trị băm như một vòng.\nVòng này nên bao phủ toàn bộ dải giá trị băm có thể và được phân bố đều.\nBước 3: Gán nút vào vòng\nGán mỗi nút trong hệ thống một vị trí trên vòng băm.\nĐiều này thường được thực hiện bằng cách băm định danh của nút bằng hàm băm đã chọn.\nBước 4: Ánh xạ khóa\nKhi cần lưu trữ hoặc truy xuất một khóa, băm khóa bằng hàm băm đã chọn để thu được giá trị băm.\nTìm vị trí trên vòng băm nơi giá trị băm rơi vào.\nĐi theo chiều kim đồng hồ trên vòng để tìm nút đầu tiên gặp phải. Nút này trở thành nút sở hữu khóa.\nBước 5: Thêm nút\nKhi một nút mới được thêm vào, tính toán vị trí của nó trên vòng băm bằng hàm băm.\nXác định dải khóa sẽ được sở hữu bởi nút mới. Điều này thường liên quan đến việc tìm nút tiền nhiệm trên vòng.\nCập nhật vòng để bao gồm nút mới và ánh xạ lại các khóa bị ảnh hưởng đến nút mới.\nBước 6: Xóa nút\nKhi một nút bị xóa, xác định vị trí của nó trên vòng băm.\nXác định dải khóa sẽ bị ảnh hưởng bởi việc xóa nút. Điều này thường liên quan đến việc tìm nút kế tiếp trên vòng.\nCập nhật vòng để loại trừ nút đã xóa và ánh xạ lại các khóa bị ảnh hưởng đến nút kế tiếp.\nBước 7: Cân bằng tải\nĐịnh kỳ kiểm tra tải trên mỗi nút bằng cách theo dõi số lượng khóa nó sở hữu.\nNếu có sự mất cân bằng, hãy xem xét việc phân phối lại một số khóa để đạt được sự phân bố đều hơn.\nDưới dây là code golang , coi như là demo example 7 bước trên\n1 2package main 3 4import ( 5\t\u0026#34;crypto/md5\u0026#34; 6\t\u0026#34;fmt\u0026#34; 7\t\u0026#34;sort\u0026#34; 8\t\u0026#34;strconv\u0026#34; 9\t\u0026#34;sync\u0026#34; 10) 11 12type ConsistentHashRing struct { 13\tring map[uint64]string 14\tsortedKeys []uint64 15\treplicas int 16\tmu sync.RWMutex 17} 18 19func NewConsistentHashRing(replicas int) *ConsistentHashRing { 20\treturn \u0026amp;ConsistentHashRing{ 21\tring: make(map[uint64]string), 22\tsortedKeys: make([]uint64, 0), 23\treplicas: replicas, 24\t} 25} 26 27// Bước 1: Chọn hàm băm, ở đây dùng md5. 28 29func (chr *ConsistentHashRing) getHash(value string) uint64 { 30\thash := md5.Sum([]byte(value)) 31\tvar hashValue uint64 32\tfor i := 0; i \u0026lt; 8; i++ { 33\thashValue = (hashValue \u0026lt;\u0026lt; 8) | uint64(hash[i]) 34\t} 35\treturn hashValue 36} 37 38// Bước 3: Gán nút vào vòng. 39func (chr *ConsistentHashRing) assignNodesToRing() { 40\tsort.Slice(chr.sortedKeys, func(i, j int) bool { 41\treturn chr.sortedKeys[i] \u0026lt; chr.sortedKeys[j] 42\t}) 43} 44 45// Bước 5: Thêm nút 46func (chr *ConsistentHashRing) AddNode(node string) { 47\tfor i := 0; i \u0026lt; chr.replicas; i++ { 48\treplicaKey := chr.getHash(node + \u0026#34;_\u0026#34; + strconv.Itoa(i)) 49\tchr.ring[replicaKey] = node 50\tchr.sortedKeys = append(chr.sortedKeys, replicaKey) 51 52\tfmt.Printf(\u0026#34;Added node: %s with hash: %d on replica %d\\n\u0026#34;, node, replicaKey, i) 53\t} 54\tchr.assignNodesToRing() 55} 56 57// Bước 6: Xóa nút 58func (chr *ConsistentHashRing) RemoveNode(node string) { 59\tfor i := 0; i \u0026lt; chr.replicas; i++ { 60\treplicaKey := chr.getHash(node + \u0026#34;_\u0026#34; + strconv.Itoa(i)) 61\tdelete(chr.ring, replicaKey) 62\tchr.sortedKeys = removeUint64Slice(chr.sortedKeys, replicaKey) 63\t} 64} 65 66// Bước 4: Ánh xạ khóa 67func (chr *ConsistentHashRing) KeyMap(key string) string { 68\tchr.mu.RLock() 69\tdefer chr.mu.RUnlock() 70 71\thashValue := chr.getHash(key) 72\tindex := sort.Search(len(chr.sortedKeys), func(i int) bool { 73\treturn chr.sortedKeys[i] \u0026gt;= hashValue 74\t}) 75 76\tif index == len(chr.sortedKeys) { 77\t// Wrap around to the beginning of the ring 78\treturn chr.ring[chr.sortedKeys[0]] 79\t} 80 81\treturn chr.ring[chr.sortedKeys[index]] 82} 83 84func removeUint64Slice(s []uint64, e uint64) []uint64 { 85\tfor i, a := range s { 86\tif a == e { 87\treturn append(s[:i], s[i+1:]...) 88\t} 89\t} 90\treturn s 91} 92 93// Bước 7: Cân bằng tải 94// LoadBalancing kiểm tra tải trên mỗi nút và phân phối lại khóa nếu cần 95func (chr *ConsistentHashRing) LoadBalancing() { 96\tchr.mu.Lock() 97\tdefer chr.mu.Unlock() 98 99\tnodeCount := make(map[string]int) 100\tfor _, node := range chr.ring { 101\tnodeCount[node]++ 102\t} 103 104\tavgCount := len(chr.ring) / len(nodeCount) 105\tfor node, count := range nodeCount { 106\tif count \u0026gt; avgCount { 107\t// Node này có tải quá cao, phân phối lại khóa 108\tfmt.Printf(\u0026#34;Node %s có tải quá cao, phân phối lại khóa\\n\u0026#34;, node) 109\t// ... 110\t} 111\t} 112} 113 114func (chr *ConsistentHashRing) PrintMap(key string) { 115\tnode := chr.KeyMap(key) 116 117\tfmt.Printf(\u0026#34;The key \u0026#39;%s\u0026#39; is mapped to node: %s\\n\u0026#34;, key, node) 118} 119func main() { 120\thashRing := NewConsistentHashRing(3) 121 122\t// Add nodes to the ring 123\thashRing.AddNode(\u0026#34;sw_hn\u0026#34;) 124\thashRing.AddNode(\u0026#34;sw_dn\u0026#34;) 125\thashRing.AddNode(\u0026#34;sw_hcm\u0026#34;) 126 127\t// // Get the node for a key 128\t// key := \u0026#34;Tác giả Phạm Duy Tùng\u0026#34; 129\thashRing.PrintMap(\u0026#34;Tác giả Phạm Duy Tùng\u0026#34;) 130\thashRing.PrintMap(\u0026#34;Ngày cập nhật 08/09/2024\u0026#34;) 131\thashRing.PrintMap(\u0026#34;Ngày mưa bão\u0026#34;) 132\thashRing.PrintMap(\u0026#34;Viết vào ngày bão, Tên cơn bão là Yagi\u0026#34;) 133\thashRing.PrintMap(\u0026#34;Tác giả Phạm Duy Tùng, Cảm ơn các bạn đã theo dõi bài viết\u0026#34;) 134\thashRing.LoadBalancing() 135 136} Lưu ý: code demo thôi, xài hàm hash đơn giản, và sử dụng tìm kiếm nhị phân để tìm vị trí của phần tử trong vòng. Thực tế thì chắc không ai chơi mấy hàm này :)\nKết quả:\n1\u0026gt;\u0026gt;\u0026gt;\u0026gt; go run consistent_hashing.go 2 3Added node: sw_hn with hash: 17429720091564777933 on replica 0 4Added node: sw_hn with hash: 6206559145603051050 on replica 1 5Added node: sw_hn with hash: 501148381563080863 on replica 2 6Added node: sw_dn with hash: 10372921504992544131 on replica 0 7Added node: sw_dn with hash: 10352104123016491672 on replica 1 8Added node: sw_dn with hash: 4947674849506040391 on replica 2 9Added node: sw_hcm with hash: 13712729030455601798 on replica 0 10Added node: sw_hcm with hash: 13299855957139837304 on replica 1 11Added node: sw_hcm with hash: 15146544336749671394 on replica 2 12The key \u0026#39;Tác giả Phạm Duy Tùng\u0026#39; is mapped to node: sw_dn 13The key \u0026#39;Ngày cập nhật 08/09/2024\u0026#39; is mapped to node: sw_dn 14The key \u0026#39;Ngày mưa bão\u0026#39; is mapped to node: sw_dn 15The key \u0026#39;Viết vào ngày bão, Tên cơn bão là Yagi\u0026#39; is mapped to node: sw_hn 16The key \u0026#39;Tác giả Phạm Duy Tùng, Cảm ơn các bạn đã theo dõi bài viết\u0026#39; is mapped to node: sw_dn Ưu và nhược điểm của Consistent Hashing Ưu điểm của việc sử dụng Consistent Hashing\nCân bằng tải: Consistent Hashing giúp phân phối tải trọng của mạng đều giữa các nút, bảo vệ hiệu suất và khả năng đáp ứng của hệ thống ngay cả khi lượng dữ liệu tăng lên và thay đổi theo thời gian.\nKhả năng mở rộng: Consistent Hashing rất linh hoạt và có thể thích nghi với sự thay đổi của số lượng nút hoặc lượng dữ liệu được xử lý mà không ảnh hưởng đến hiệu suất của toàn bộ hệ thống.\nTối thiểu hoá số lượng ánh xạ lại: Consistent Hashing giảm thiểu số lượng khóa cần ánh xạ lại khi thêm hoặc xóa nút, đảm bảo rằng hệ thống luôn ổn định và nhất quán ngay cả khi mạng thay đổi theo thời gian.\nTăng khả năng chịu lỗi: Consistent Hashing giúp dữ liệu luôn khả dụng và cập nhật, ngay cả trong trường hợp nút bị lỗi. Khả năng sao chép khóa trên nhiều nút và ánh xạ lại khóa đến nút khác trong trường hợp lỗi giúp tăng cường độ ổn định và tin cậy của toàn bộ hệ thống.\nĐơn giản hóa hoạt động: Consistent Hashing giúp đơn giản hóa quá trình thêm hoặc xóa nút khỏi mạng, giúp dễ dàng quản lý và duy trì hệ thống phân tán lớn.\nNhược điểm của việc sử dụng Consistent Hashing\nHàm băm: Hiệu suất của Consistent Hashing phụ thuộc vào việc sử dụng hàm băm phù hợp. Hàm băm phải tạo ra giá trị duy nhất cho mỗi khóa và phải là xác định để có hiệu quả. Sự phức tạp của hàm băm có thể ảnh hưởng đến hiệu suất và hiệu quả của toàn bộ hệ thống.\nTốn kém hiệu suất: Việc sử dụng Consistent Hashing có thể dẫn đến một số tốn kém hiệu suất do cần phải sử dụng tài nguyên tính toán để ánh xạ khóa đến nút, sao chép khóa và ánh xạ lại khóa trong trường hợp thêm hoặc xóa nút.\nThiếu linh hoạt: Trong một số trường hợp, giới hạn cố định của Consistent Hashing có thể hạn chế khả năng của hệ thống để thích nghi với sự thay đổi của nhu cầu hoặc điều kiện mạng.\nSử dụng tài nguyên cao: Trong một số trường hợp, việc sử dụng Consistent Hashing có thể dẫn đến sử dụng tài nguyên cao khi thêm hoặc xóa nút khỏi mạng, điều này có thể ảnh hưởng đến hiệu suất và hiệu quả của toàn bộ hệ thống.\nPhức tạp của quản lý: Việc quản lý và duy trì hệ thống sử dụng Consistent Hashing có thể phức tạp và đòi hỏi chuyên môn và kỹ năng đặc biệt.\nCảm ơn các bạn đã theo dõi bài viết. Xin cảm ơn và hẹn gặp lại. Bài sau sẽ nói về Distributed Hash Tables\n","date":"Sep 8, 2024","img":"https://unsplash.it/1920/1080?image=2","permalink":"/blog/2024-09-05-system-design-top-10-interview-consistent-hashing/","series":null,"tags":["System Design"],"title":"Top 10 Thuật Toán System Design Các Bạn Nên Biết Và Thường Được Hỏi Trong Phỏng Vấn - Top 1 Consistent Hashing"},{"categories":null,"content":" Netflix: Phát Triển Khắc Phục Sự Cố Big Data Picnic: Cải Thiện Truy Xuất Tìm Kiếm Uber: Cá Nhân Hóa Thông Tin Ngoài Ứng Dụng GitLab: Xác Thực và Kiểm Tra Mô Hình AI LinkedIn: Kết Nối Thành Viên với Sản Phẩm Cao Cấp Swiggy: Đề Xuất Sản Phẩm Cho Người Dùng Mới Careem: Giảm Gian Lận Bằng Tiền Tạm Ứng Slack: AI Cho Tin Nhắn Bảo Mật Trong Doanh Nghiệp Picnic: Hỗ Trợ Yêu Cầu Khách Hàng Foodpanda: Tối Ưu Hóa Cung và Cầu Etsy: Tìm Kiếm và Đề Xuất Bằng Hình Ảnh LinkedIn: Phát Hiện Hình Ảnh Do AI Tạo Ra Discord: Các Trường Hợp Sử Dụng AI Tạo Sinh Pinterest: Cải Thiện Hiệu Suất Quảng Cáo Expedia: Tìm Kiếm Ngữ Nghĩa Cho Du Lịch 15 Ví dụ Thực Tế về Ứng Dụng của LLM Trong Các Ngành Công Nghiệp Khác Nhau\nBởi Sana Hassan - 3 Tháng 7, 2024\nTrong thế giới công nghệ đầy biến động, các Mô Hình Ngôn Ngữ Lớn (LLM) đã trở nên quan trọng trong nhiều ngành công nghiệp khác nhau. Khả năng xử lý ngôn ngữ tự nhiên, tạo nội dung và phân tích dữ liệu của chúng đã mở đường cho nhiều ứng dụng. Hãy cùng khám phá 15 ví dụ chi tiết về cách các công ty tận dụng LLM trong các tình huống thực tế.\nNetflix: Phát Triển Khắc Phục Sự Cố Big Data Netflix đã chuyển từ các bộ phân loại dựa trên quy tắc truyền thống sang các hệ thống khắc phục tự động dựa trên học máy để xử lý các công việc big data bị lỗi. Sự chuyển đổi này đã cho phép Netflix tự động phát hiện, chẩn đoán và sửa các vấn đề trong các quy trình dữ liệu của mình, giảm đáng kể thời gian ngừng hoạt động và đảm bảo dịch vụ phát trực tuyến liền mạch. LLM giúp hiểu dữ liệu log, xác định các mẫu lỗi và đề xuất hoặc thực hiện các biện pháp sửa chữa, nâng cao hiệu quả và độ tin cậy của hoạt động.\nPicnic: Cải Thiện Truy Xuất Tìm Kiếm Picnic, một dịch vụ giao hàng tạp hóa trực tuyến, đã tích hợp LLM để cải thiện sự liên quan của kết quả tìm kiếm cho các danh sách sản phẩm. Việc sử dụng các mô hình ngôn ngữ lớn cho phép Picnic hiểu rõ hơn các truy vấn của người dùng và ngữ cảnh, mang lại kết quả tìm kiếm chính xác và cá nhân hóa hơn. Sự cải thiện này nâng cao trải nghiệm khách hàng và tăng tỷ lệ chuyển đổi bằng cách giúp khách hàng dễ dàng tìm thấy sản phẩm họ cần.\nUber: Cá Nhân Hóa Thông Tin Ngoài Ứng Dụng Hệ thống đề xuất tiên tiến của Uber cá nhân hóa thông tin ngoài ứng dụng để tăng cường sự tương tác của người dùng. Bằng cách sử dụng các thuật toán đề xuất dựa trên LLM tinh vi, Uber có thể điều chỉnh thông báo và gợi ý theo sở thích và hành vi của từng người dùng. Sự cá nhân hóa này mở rộng ra ngoài ứng dụng, đảm bảo người dùng nhận được các cập nhật và ưu đãi liên quan qua email, SMS và các kênh khác, từ đó cải thiện sự duy trì và sự hài lòng của người dùng.\nGitLab: Xác Thực và Kiểm Tra Mô Hình AI GitLab đã phát triển GitLab Duo, một nền tảng xác thực và kiểm tra các kết quả do AI tạo ra. Sáng kiến này sử dụng LLM để đánh giá chất lượng, độ chính xác và độ tin cậy của các mô hình AI ở quy mô lớn. GitLab Duo giúp xác định các thiên vị tiềm ẩn, lỗi và các khu vực cần cải thiện trong các mô hình AI, đảm bảo rằng các mô hình triển khai đạt tiêu chuẩn hiệu suất và độ tin cậy cao. Quy trình kiểm tra nghiêm ngặt này rất quan trọng để duy trì niềm tin vào các tính năng do AI điều khiển.\nLinkedIn: Kết Nối Thành Viên với Sản Phẩm Cao Cấp LinkedIn sử dụng LLM để đề xuất các sản phẩm cao cấp phù hợp cho người dùng. Bằng cách phân tích dữ liệu người dùng, bao gồm lịch sử công việc, sở thích và mô hình hoạt động, hệ thống đề xuất của LinkedIn có thể kết nối thành viên với các dịch vụ và sản phẩm cao cấp phù hợp nhất với nhu cầu của họ. Cách tiếp cận có mục tiêu này giúp LinkedIn nâng cao sự hài lòng của người dùng và thúc đẩy đăng ký vào các dịch vụ cao cấp của mình.\nSwiggy: Đề Xuất Sản Phẩm Cho Người Dùng Mới Swiggy, một nền tảng giao đồ ăn hàng đầu, sử dụng học tập liên miền phân cấp để cung cấp các đề xuất sản phẩm cho người dùng mới. Bằng cách phân tích dữ liệu từ các miền khác nhau và học hỏi từ các tương tác của người dùng, hệ thống đề xuất của Swiggy có thể đưa ra các gợi ý cá nhân hóa phù hợp với sở thích của người dùng mới. Cách tiếp cận này hiệu quả trong việc tiếp cận và giữ chân khách hàng mới.\nCareem: Giảm Gian Lận Bằng Tiền Tạm Ứng Careem, một dịch vụ gọi xe, sử dụng các mô hình học máy để giảm rủi ro gian lận thông qua các kỹ thuật tiền tạm ứng. Bằng cách áp dụng các khoản giữ tạm thời trên các giao dịch, Careem có thể phân tích các mẫu giao dịch và đánh dấu các hoạt động đáng ngờ theo thời gian thực. Cơ chế phát hiện gian lận chủ động này, được hỗ trợ bởi LLM, giúp giảm các vụ gian lận, bảo vệ công ty và người dùng khỏi các tổn thất tiềm ẩn.\nSlack: AI Cho Tin Nhắn Bảo Mật Trong Doanh Nghiệp Slack đã phát triển các khả năng AI để nâng cao bảo mật và riêng tư cho tin nhắn trong doanh nghiệp. Sử dụng LLM, các tính năng AI của Slack có thể xử lý và phân tích tin nhắn trong khi đảm bảo tiêu chuẩn cao về bảo mật và quyền riêng tư. Các tính năng này bao gồm tóm tắt tin nhắn tự động, trả lời thông minh và đề xuất theo ngữ cảnh được thiết kế để cải thiện hiệu quả giao tiếp mà không làm giảm bảo vệ dữ liệu.\nPicnic: Hỗ Trợ Yêu Cầu Khách Hàng Picnic đã vượt qua rào cản ngôn ngữ trong hỗ trợ khách hàng bằng cách sử dụng xử lý ngôn ngữ tự nhiên (NLP). Bằng cách chuyển hướng yêu cầu hỗ trợ đến các nhân viên phù hợp nhất và cung cấp dịch thuật theo thời gian thực, Picnic đảm bảo rằng khách hàng nhận được sự hỗ trợ kịp thời và chính xác bất kể ngôn ngữ. Hệ thống hỗ trợ dựa trên NLP này nâng cao chất lượng dịch vụ khách hàng và giúp Picnic phục vụ một cơ sở khách hàng đa dạng.\nFoodpanda: Tối Ưu Hóa Cung và Cầu Foodpanda sử dụng học máy để cân bằng cung và cầu cho các dịch vụ giao đồ ăn. Bằng cách sử dụng phân tích dự đoán và các thuật toán tiên tiến, Foodpanda có thể dự báo các mẫu nhu cầu và phân bổ tài nguyên. Việc tối ưu hóa này giúp quản lý thời gian giao hàng, giảm chi phí vận hành và đảm bảo trải nghiệm tốt hơn cho khách hàng và đối tác giao hàng.\nEtsy: Tìm Kiếm và Đề Xuất Bằng Hình Ảnh Etsy đã triển khai kỹ thuật học biểu diễn và đánh giá bằng hình ảnh cho tìm kiếm và đề xuất tương tự. Bằng cách tận dụng thị giác máy tính và LLM, hệ thống của Etsy có thể phân tích hình ảnh sản phẩm và cung cấp cho người dùng các mặt hàng tương tự về mặt hình ảnh. Tính năng này nâng cao trải nghiệm mua sắm bằng cách giúp người dùng dễ dàng tìm thấy các sản phẩm phù hợp với sở thích của họ dựa trên các thuộc tính hình ảnh.\nLinkedIn: Phát Hiện Hình Ảnh Do AI Tạo Ra LinkedIn đã phát triển các hệ thống để phát hiện hình ảnh do AI tạo ra (deepfake). Sử dụng các thuật toán nhận diện hình ảnh tiên tiến và LLM, LinkedIn có thể xác định và đánh dấu nội dung deepfake, đảm bảo tính toàn vẹn và đáng tin cậy của hồ sơ người dùng và nội dung trên nền tảng. Khả năng này rất quan trọng trong việc duy trì một môi trường người dùng an toàn và xác thực.\nDiscord: Các Trường Hợp Sử Dụng AI Tạo Sinh Discord, một nền tảng giao tiếp phổ biến, đã khám phá nhiều trường hợp sử dụng AI tạo sinh để tăng cường sự tương tác của người dùng. Bằng cách phát triển và tích hợp nhanh chóng các tính năng AI tạo sinh, Discord có thể cung cấp cho người dùng các công cụ sáng tạo như avatar do AI tạo ra, kiểm duyệt nội dung và phản hồi tự động. Các tính năng này tận dụng LLM để cải thiện trải nghiệm người dùng và thúc đẩy cộng đồng tương tác hơn.\nPinterest: Cải Thiện Hiệu Suất Quảng Cáo Pinterest đã phát triển các mô hình tối ưu hóa chuyển đổi quảng cáo để nâng cao hiệu suất quảng cáo. Bằng cách tận dụng LLM, Pinterest có thể phân tích hành vi và sở thích của người dùng để cung cấp các quảng cáo mục tiêu và liên quan. Việc tối ưu hóa này dẫn đến tỷ lệ chuyển đổi cao hơn, trải nghiệm người dùng tốt hơn và tăng doanh thu cho các nhà quảng cáo trên nền tảng.\nExpedia: Tìm Kiếm Ngữ Nghĩa Cho Du Lịch Expedia sử dụng các biểu diễn cho các khái niệm du lịch lưu trú để nâng cao khả năng tìm kiếm ngữ nghĩa. Bằng cách hiểu nghĩa ngữ cảnh của các truy vấn của người dùng, hệ thống tìm kiếm của Expedia có thể cung cấp các kết quả chính xác và liên quan hơn cho các khách sạn và chỗ ở du lịch. Chức\nnăng tìm kiếm ngữ nghĩa này, được hỗ trợ bởi LLM, cải thiện trải nghiệm đặt chỗ bằng cách giúp người dùng tìm thấy các tùy chọn tốt nhất dựa trên nhu cầu và sở thích của họ.\nTóm lại, các ví dụ này minh họa tác động chuyển đổi của LLM trên nhiều lĩnh vực, thúc đẩy đổi mới và hiệu quả. Khi công nghệ LLM tiến bộ, các ứng dụng của nó được dự đoán sẽ mở rộng, cung cấp các giải pháp phức tạp hơn cho các thách thức trong ngành. Các công ty nên cân nhắc tận dụng các nền tảng chuyên dụng như AI Drive Pro để quản lý và tối ưu hóa việc triển khai LLM của họ để đạt được kết quả tối ưu.\nTham khảo\nhttps://www.evidentlyai.com/ml-system-design\n","date":"Jul 6, 2024","img":"https://unsplash.it/1920/1080?image=3","permalink":"/blog/2024-07-06-llm-applications-in-real/","series":null,"tags":["bigdata"],"title":"Các Ứng Dụng Của LLM Trong Thực Tế"},{"categories":null,"content":"Giới thiệu Hiện nay, NVIDA đang là nhà sản xuất GPU hàng đầu thế giới, và cùng với sự phát triển của của mô hình AI, chip NVIDIA được sử dụng rộng rải vì các tính năng sau\n1. Hiệu Suất Cao Tận Dụng GPU\nCUDA được tối ưu hóa để tận dụng sức mạnh xử lý song song của GPU, cho phép thực hiện hàng ngàn tác vụ tính toán đồng thời. GPU có nhiều lõi hơn so với CPU, giúp tăng tốc độ tính toán đáng kể khi xử lý các tác vụ liên quan đến AI và học sâu. Tối Ưu Hóa Toán Học\nCUDA cung cấp các thư viện toán học hiệu suất cao như cuBLAS, cuDNN, và cuFFT, giúp tối ưu hóa các phép toán ma trận và phép biến đổi Fourier, các phép toán phổ biến trong các mô hình AI. 2. Thư Viện và Hệ Sinh Thái Phong Phú Thư Viện AI\nCác thư viện học sâu phổ biến như TensorFlow, PyTorch, và MXNet đều có hỗ trợ CUDA, giúp dễ dàng triển khai và tối ưu hóa các mô hình AI trên GPU. Các thư viện này tích hợp chặt chẽ với CUDA, cung cấp các công cụ và API mạnh mẽ để xây dựng, huấn luyện, và triển khai các mô hình AI. Cộng Đồng và Hỗ Trợ\nNVIDIA có một cộng đồng lớn các nhà phát triển và nhà nghiên cứu, cung cấp hỗ trợ qua các diễn đàn, tài liệu, và khóa học trực tuyến. Các công cụ phát triển như NVIDIA CUDA Toolkit, NVIDIA Nsight, và cuDNN Debugger giúp dễ dàng phát triển và gỡ lỗi ứng dụng AI. 3. Tính Tương Thích và Di Động Phần Cứng Tương Thích\nCUDA tương thích với hầu hết các GPU của NVIDIA, từ các dòng sản phẩm tiêu dùng đến các dòng sản phẩm chuyên dụng cho trung tâm dữ liệu. Điều này cho phép sử dụng các mô hình AI trên nhiều loại phần cứng, từ máy tính cá nhân đến các hệ thống máy chủ lớn. Tương Thích Phần Mềm\nCUDA hỗ trợ nhiều ngôn ngữ lập trình và framework, giúp dễ dàng tích hợp vào các dự án hiện có mà không cần thay đổi nhiều về mã nguồn. 4. Khả Năng Mở Rộng và Tính Linh Hoạt Huấn Luyện Phân Tán\nCUDA hỗ trợ các kỹ thuật huấn luyện phân tán, cho phép huấn luyện các mô hình lớn trên nhiều GPU hoặc thậm chí nhiều máy tính. Các framework như Horovod (do Uber phát triển) sử dụng CUDA để thực hiện huấn luyện phân tán hiệu quả. Khả Năng Tùy Chỉnh\nCUDA cung cấp khả năng tùy chỉnh cao, cho phép các nhà phát triển tối ưu hóa các thuật toán cụ thể cho ứng dụng của họ. CUDA cung cấp quyền truy cập trực tiếp vào phần cứng GPU, giúp tối ưu hóa hiệu suất theo yêu cầu cụ thể. 5. Hiệu Quả Kinh Tế Tối Ưu Chi Phí:\nSử dụng GPU và CUDA để huấn luyện các mô hình AI có thể giúp tiết kiệm chi phí bằng cách giảm thời gian huấn luyện so với việc sử dụng CPU. Tính hiệu quả cao của GPU giúp giảm tổng chi phí cho phần cứng và năng lượng. Các thư viện lập trình song song khác ngoài CUDA 1. OpenCL (Open Computing Language) Tổng quan:\nOpenCL là một tiêu chuẩn mở cho lập trình song song trên các nền tảng dị thể, bao gồm CPU, GPU, DSP và FPGA. Nó được quản lý bởi Khronos Group. Đặc điểm chính:\nĐộc lập với nền tảng: Hoạt động trên các phần cứng từ nhiều nhà cung cấp khác nhau, bao gồm AMD, Intel và NVIDIA. Tính toán song song: Hỗ trợ tính toán dựa trên tác vụ và dữ liệu. Hiệu suất: Thường có một chút chi phí hiệu suất so với CUDA do tính chất tổng quát của nó. Trường hợp sử dụng:\nTính toán khoa học Xử lý hình ảnh và video thời gian thực Mô hình tài chính Ưu điểm:\nTính di động: Viết một lần, chạy mọi nơi. Hỗ trợ phần cứng rộng rãi: Có thể chạy trên CPU, GPU và các bộ tăng tốc khác từ nhiều nhà cung cấp. Nhược điểm:\nHiệu suất: Có thể không tối ưu như CUDA trên GPU của NVIDIA. Phức tạp: API mức thấp có thể khó lập trình hơn CUDA. 2. AMD ROCm (Radeon Open Compute) Tổng quan:\nAMD ROCm là một nền tảng mã nguồn mở cho tính toán GPU. Nó cung cấp các công cụ để chuyển đổi các ứng dụng CUDA sang chạy trên GPU của AMD. Đặc điểm chính:\nHIP (Heterogeneous-Compute Interface for Portability): Một runtime và API cho phép mã CUDA được chuyển đổi để chạy trên phần cứng AMD. Hỗ trợ TensorFlow và PyTorch: Tích hợp cho các khung máy học phổ biến. Trường hợp sử dụng:\nHọc máy và AI Tính toán hiệu năng cao Trung tâm dữ liệu và điện toán đám mây Ưu điểm:\nMã nguồn mở: Được cộng đồng đóng góp với sự tham gia của nhiều tổ chức. Tương thích CUDA: Dễ dàng chuyển mã CUDA thông qua HIP. Nhược điểm:\nHạn chế phần cứng: Chủ yếu hỗ trợ GPU của AMD. Độ trưởng thành: Ít trưởng thành hơn so với CUDA, ít tài nguyên và công cụ hơn. 3. SYCL (C++ for Heterogeneous Computing) Tổng quan:\nSYCL là một mô hình lập trình mức cao dựa trên C++ cho tính toán dị thể, cho phép mã di động trên các phần cứng khác nhau bao gồm CPU, GPU và FPGA. Đặc điểm chính:\nLập trình nguồn đơn: Cho phép mã cho máy chủ và thiết bị được viết trong một tệp nguồn duy nhất. Tích hợp C++: Sử dụng các tính năng C++ hiện đại cho mã an toàn và biểu cảm hơn. Backend: Có thể biên dịch sang OpenCL, CUDA (thông qua hipSYCL), và nhiều hơn nữa. Trường hợp sử dụng:\nỨng dụng đa nền tảng Hệ thống thời gian thực Nghiên cứu khoa học Ưu điểm:\nTính di động: Tính di động cao trên các nền tảng phần cứng khác nhau. C++ hiện đại: Lợi ích từ sự an toàn và tính mạnh mẽ của C++. Nhược điểm:\nĐường cong học tập: Yêu cầu quen thuộc với cả C++ hiện đại và các khái niệm lập trình song song. Phụ thuộc vào công cụ: Hiệu suất và tính năng có thể phụ thuộc nhiều vào việc triển khai SYCL (ví dụ: DPC++, hipSYCL). 4. Vulkan Compute Tổng quan:\nVulkan Compute là một phần của API đồ họa Vulkan hỗ trợ các shader tính toán cho tính toán tổng quát trên GPU. Đặc điểm chính:\nKiểm soát mức thấp: Cung cấp kiểm soát chi tiết về các hoạt động của GPU. Đa nền tảng: Hoạt động trên nhiều hệ điều hành và nhà cung cấp phần cứng khác nhau. Trường hợp sử dụng:\nỨng dụng đồ họa và tính toán thời gian thực Phát triển trò chơi Mô phỏng và hình ảnh hóa Ưu điểm:\nHiệu suất: Hiệu quả cao nhờ truy cập mức thấp vào phần cứng GPU. Hỗ trợ đa nhà cung cấp: Tương thích với nhiều loại GPU. Nhược điểm:\nPhức tạp: API mức thấp yêu cầu hiểu biết chi tiết về kiến trúc GPU. Công sức phát triển: Đòi hỏi nhiều công sức để thiết lập và bảo trì so với các API mức cao hơn. 5. Intel oneAPI Tổng quan:\nIntel oneAPI là một mô hình lập trình hợp nhất thiết kế để đơn giản hóa việc phát triển trên các kiến trúc đa dạng như CPU, GPU, FPGA và bộ tăng tốc AI. Đặc điểm chính:\nDPC++ (Data Parallel C++): Một phần mở rộng của SYCL cho oneAPI, hỗ trợ mã chạy trên các phần cứng khác nhau. Thư viện tối ưu hóa: Cung cấp các thư viện hiệu năng cho toán học, phân tích dữ liệu, học sâu, v.v. Trường hợp sử dụng:\nTính toán hiệu năng cao AI và học máy Phân tích dữ liệu Ưu điểm:\nKiến trúc chéo: Cho phép một mã nguồn duy nhất chạy trên nhiều phần cứng Intel và không phải Intel. Hệ sinh thái: Hệ sinh thái mạnh với nhiều công cụ và thư viện. Nhược điểm:\nTập trung vào Intel: Chủ yếu tối ưu hóa cho phần cứng Intel, có thể không hiệu quả trên các thiết bị không phải của Intel. Mới: Vẫn đang phát triển, có thể ít tài nguyên so với CUDA. 6. OpenMP (Open Multi-Processing) Tổng quan:\nOpenMP là một API hỗ trợ lập trình đa nền tảng bộ nhớ chia sẻ đa xử lý trong C, C++ và Fortran. Các phiên bản gần đây bao gồm các chỉ thị để tính toán trên GPU. Đặc điểm chính:\nChỉ thị trình biên dịch: Đơn giản hóa lập trình song song với các chỉ thị trình biên dịch. Hỗ trợ CPU và GPU: Các phiên bản gần đây hỗ trợ tính toán trên GPU. Song song hóa dần dần: Cho phép song song hóa dần dần các mã nguồn hiện có. Trường hợp sử dụng:\nBộ nhớ chia sẻ đa xử lý Song song hóa mã CPU hiện có Tính toán khoa học hiệu năng cao Ưu điểm:\nDễ sử dụng: Mô hình song song đơn giản hơn so với lập trình đa luồng rõ ràng. Mã nguồn kế thừa: Tốt cho việc song song hóa các mã nguồn CPU hiện có. Nhược điểm:\nKhả năng mở rộng: Phù hợp nhất cho các hệ thống bộ nhớ chia sẻ, có thể không mở rộng tốt cho các hệ thống phân tán lớn. Hiệu suất: Tính toán trên GPU có thể kém hiệu quả hơn so với CUDA. Lời kết Các lựa chọn thay thế này đều có những điểm mạnh và yếu riêng, và lựa chọn tốt nhất thường phụ thuộc vào các yêu cầu cụ thể của ứng dụng và cơ sở hạ tầng hiện có của chính bạn. Cảm ơn các bạn đã theo dõi bài viết. Hẹn gặp lại ở các bài viết tiếp theo.\n","date":"Jun 12, 2024","img":"https://unsplash.it/1920/1080?image=15","permalink":"/blog/2024-06-12-cuda-alternate/","series":null,"tags":["Machine Learning","Parallel Computing","Cuda"],"title":"Một Số Thư Viện Tính Toán Song Song Thay Thế Cho Cuda"},{"categories":null,"content":" 1. Phân tích hồi quy (regression analysis) 1.1. Linear Regression: 1.2. Simple Linear Regression: 1.3. Multiple Linear Regression: 2. Phân tích nhân tố (Factor analysis) 3. Neural network 4. Phân tích cụm (Cluster analysis) 5. Phân tích tổ hợp - Phân tích theo nhóm (Cohort analysis) Cohort Dựa trên Thời gian Cohort Dựa trên lợi ích 6. Phân tích thuộc tính - Phân tích kết hợp (conjoint analysis) 7. Phân tích văn bản (Text analysis) 8. Phân tích chuỗi thời gian (time series analysis) 9. Khai thác dữ liệu (Data mining) 10. Cây quyết định (decision tree) Nguồn: Nhân dịp tết, rảnh rỗi chạy kpi viết bài để đảm bảo số lượng bài viết, chứ để cái website nó muốn mốc meo hết cả rồi. Cơ mà viết càng nhiều thì càng không đủ, cái gì cũng muốn viết, thành ra nó dài dòng, lê thê, ngồi đọc lại thấy chán ngán, nên phải ngồi tém tém nội dung lại. Bà con đọc thấy chỗ nào còn dài , cần tóm, tém, gọt thì vui lòng thảy cái commend hen.\n1. Phân tích hồi quy (regression analysis) Regression analysis là một phương pháp thống được sử dụng để ước lượng mối quan hệ giữa các biến phụ thuộc (hay còn được gọi là biến \u0026lsquo;outcome\u0026rsquo; hoặc biến \u0026lsquo;response\u0026rsquo; ) và một hoặc nhiều biến độc lập ( cũng được gọi với tên là \u0026lsquo;predictors\u0026rsquo;, \u0026lsquo;covariates\u0026rsquo;, \u0026rsquo;explanatory variables\u0026rsquo;, \u0026lsquo;features\u0026rsquo;). Chi tiết:\n1.1. Linear Regression: Hồi quy tuyến tính là hình thức phân tích hồi quy phổ biến nhất. Nó nhằm mục đích tìm ra đường thẳng khớp với dữ liệu nhất ( fitted line) theo một số tiêu chí toán học cụ thể nào đó.\nHồi quy tuyến tính giả định rằng các mối quan hệ giữa các biến là tuyến tính và thoả các giả định là normality of residuals và independence of errors. 1.2. Simple Linear Regression: Trong hồi quy tuyến tính đơn giản, chúng ta đánh giá mối quan hệ giữa một biến phụ thuộc duy nhất (Y) và một biến độc lập (X). Phương trình: Y = x +bX + epsilon\n1.3. Multiple Linear Regression: Là biến thể mở rộng của hồi quy tuyến tính đơn giản, với nhiều biến độc lập X Phương trình: Y = x + bX1 + cX2 + dX3 ... + epsilon 2. Phân tích nhân tố (Factor analysis) Factor analysis là một kỹ thuật thống kê, phân tích yếu tố nhận diện cấu trúc cơ bản của một tập hợp các biến và giải thích chúng dưới dạng một số lượng nhỏ hơn các yếu tố chung. Phân tích yếu tố giúp giảm chiều dữ liệu và sự phức tạp của nó, cũng như khám phá những yếu tố tiềm ẩn gây ra sự biến động chung của các biến quan sát.\nPhân loại:\nExploratory factor analysis (EFA): Loại phân tích này được sử dụng khi người phân tích không có hiểu biết gì về dữ liệu. Mục tiêu của phân tích này là tìm số factor tối ưu với điều kiện cực đại hoá các biến trong dữ liệu.\nConfirmatory factor analysis (CFA): Loại phân tích này được sử dụng khi người phân tích có mô hình lý thuêts hoặc giả thueyets về các factor và mối quan hệ giữa chúng.\nPrincipal component analysis (PCA): Dạng phân tích này thường nhầm lẫn với EFA, nhưng chúng khác mục tiêu và khác giả định. Mục tiêu của PCA là tìm ra sự kết hợp tuyến tính của các biến quan sát để thu được phương sai lớn nhất trong dữ liệu, mà không giả định về bất kỳ yếu tố tiềm ẩn nào. PCA thích hợp hơn cho việc giảm kích thước dữ liệu và tóm tắt, trong khi Phân tích Yếu tố Khám phá (EFA) thích hợp hơn cho việc tìm ra các khái niệm tiềm ẩn và mối quan hệ nguyên nhân.\nFactor analysis trải qua các nhiều bước sau:\nData preparation:Xem số dòng, số cột, phân phối của các biến, mối quan hệ giữa các biến.\nFactor extraction: Xác định số lượng các factor cần rút trích. Sử dụng principal component analysis, maximum likelihood, principal axis factoring, 3 chấm \u0026hellip;\nFactor rotation: Bước này dùng để cải thiện khả năng diễn giải và tăng tính rõ ràng của các yếu tố bằng cách thay đổi hướng và vị trí của chúng. Có hai loại chính : xoay góc và xoay chéo. Xoay góc giả định rằng các yếu tố không tương quan, trong khi xoay chéo cho phép một số tương quan giữa các yếu tố .\nFactor interpretation: đặt tên cho các factor ( bước này khá khó, do tên phải cover được dữ liệu mà nó đang handle).\nPhân tích yếu tố là một công cụ hữu ích và mạnh mẽ để khám phá và xác nhận cấu trúc của dữ liệu, nhưng nó cũng mang đến một số hạn chế và thách thức\nSubjectivity: Mỗi nhà phân tích có một chiến lược phân tích khác nhau, nên có thể sẽ có các báo cáo khác nhau, trên cùng một dữ liệu.\nComplexity: Phương pháp Factor analysis khá khó tiếp cận, đòi hỏi người phân tích có kiến thức chuyên sâu về dữ liệu họ đang có, và có kiến thức vững chắc về thống kê, giải định, có khả năng sử dụng các tool phân tích dữ liệu lớn.\nValidity: Phương pháp này không thể dứng minh mối quan hệ nhân quả , tính hợp lệ của các yếu tố. Mọi thông tin được rút ra từ trong dữ liệu dựa trên các tiêu chí thống kê và các giả đinh.\n3. Neural network Một mạng neural là một loại trí tuệ nhân tạo cố gắng mô phỏng cách não người hoạt động. Nó bao gồm nhiều đơn vị được kết nối gọi là neuron, chúng xử lý thông tin và học từ dữ liệu. Mạng neural có thể thực hiện nhiều nhiệm vụ khác nhau, như nhận dạng giọng nói, phân tích hình ảnh và xử lý ngôn ngữ tự nhiên. Dưới đây là một số khái niệm chính của mạng neural:\nMột mạng neural có nhiều lớp của các neuron, như là một lớp đầu vào, một hoặc nhiều lớp ẩn, và một lớp đầu ra. Mỗi lớp nhận đầu vào từ lớp trước đó và chuyển đầu ra cho lớp kế tiếp.\nMỗi neuron có một trọng số và một độ lệch, quyết định mức độ ảnh hưởng của nó đối với đầu ra. Trọng số và độ lệch được điều chỉnh trong quá trình huấn luyện, nơi mạng học từ dữ liệu và cải thiện hiệu suất.\nMỗi neuron cũng có một hàm kích hoạt, quyết định liệu neuron có được kích hoạt hay không dựa trên đầu vào. Một số hàm kích hoạt phổ biến bao gồm sigmoid, tanh, và ReLU.\nCó nhiều loại mạng neural khác nhau, như mạng neural feedforward, mạng neural hồi quy, mạng neural tích chập, và mạng neural sâu. Mỗi loại có kiến trúc, ưu điểm và ứng dụng riêng.\n4. Phân tích cụm (Cluster analysis) Phân tích cụm là một phương pháp phân tích dữ liệu nhóm các đối tượng dựa trên các thuộc tính chung của chúng. Nó có thể được sử dụng trong học máy, phân tích hình ảnh, khai thác dữ liệu và nhận dạng mẫu.\nTìm ra cấu trúc và số lượng cụng tối ưu phù hợp với dữ liệu. Có nhiều loại cụm như cụm cầu, cụm phân cấp, cụm dựa trên mật độ, cụm không gian con, và cụm dựa trên mô hình.\nPhân tích cụm đòi hỏi việc lựa chọn một thuật toán phân cụm phù hợp và cài đặt các tham số của nó. Một số thuật toán phân cụm phổ biến bao gồm K-means, phân cụm phân cấp, DBSCAN, phân cụm phổ, và mô hình hỗn hợp Gaussian.\nPhân tích cụm cũng yêu cầu kiểm định và diễn giải kết quả phân cụm. Điều này có thể được thực hiện bằng cách sử dụng các phương pháp khác nhau như kiểm định thống kê, so sánh với các lớp đã biết, hoặc các tiêu chí cụ thể cho từng lĩnh vực.\nPhân tích cụm là một công cụ hữu ích và mạnh mẽ để khám phá và xác nhận cấu trúc của dữ liệu, nhưng nó cũng có một số hạn chế và thách thức:\nTính chủ quan: Phân tích cụm liên quan đến nhiều quyết định và đánh giá từ phía nghiên cứu, như loại phân tích cụm, phương pháp phân cụm, số lượng cụm, và cách diễn giải cụm. Những lựa chọn này có thể ảnh hưởng đến kết quả và kết luận của phân tích, và các nhà nghiên cứu khác nhau có thể thu được kết quả khác nhau từ cùng một dữ liệu.\nĐộ phức tạp: Phân tích cụm có thể khó hiểu và áp dụng đúng, đặc biệt là đối với người mới học và người không chuyên về thống kê. Nó đòi hỏi sự hiểu biết tốt về lý thuyết cơ bản, giả định, phương pháp và công thức cơ bản, cũng như khả năng sử dụng phần mềm và công cụ phù hợp.\nTính hợp lệ: Phân tích cụm không chứng minh sự nhân quả hoặc tính hợp lệ của các cụm. Nó chỉ cung cấp một giải thích có thể về dữ liệu dựa trên tiêu chí thống kê và giả định. Người nghiên cứu luôn nên kiểm tra tính hợp lệ và độ tin cậy của các cụm bằng cách sử dụng các phương pháp khác như chỉ số alpha của Cronbach, tính hợp lệ xây dựng, tính hợp lệ hội tụ, tính hợp lệ phân loại.\n5. Phân tích tổ hợp - Phân tích theo nhóm (Cohort analysis) Phân tích nhóm là một kỹ thuật quan trọng trong lĩnh vực phân tích hành vi.\nPhân tích nhóm liên quan đến việc chia dữ liệu từ một bộ dữ liệu thành các nhóm liên quan, được gọi là cohort, thay vì xem xét dữ liệu như một đơn vị duy nhất.\nCác nhóm này có các đặc điểm tương tự, chẳng hạn như thời gian tham gia hoặc kích thước.\nCohort analysis thường được sử dụng trong nhiều lĩnh vực, ví như doanh nghiệp cung cấp dịch vụ đám mây, doanh nghiệp kinh doanh trò chơi , các nền tảng thương mại điện tử, các doanh nghiệp bán lẻ, bất động sản, ngân hàng \u0026hellip;.\nMục tiêu chính của phân tích nhóm là hiểu hành vi của khách hàng qua toàn bộ vòng đời của mỗi khách hàng.\nBằng cách nhóm khách hàng thành các nhóm quản lý được, doanh nghiệp có cái nhìn sâu sắc về xu hướng và mô hình theo thời gian.\nNó giúp điều chỉnh các ưu đãi sản phẩm và chiến lược tiếp thị cho các phân khúc khách hàng cụ thể.\nCohort Dựa trên Thời gian Các nhóm này bao gồm khách hàng đăng ký sử dụng sản phẩm hoặc dịch vụ trong một khoảng thời gian cụ thể (ví dụ, hàng tháng hoặc hàng quý).\nPhân tích nhóm dựa trên thời gian cho thấy cách hành vi của khách hàng thay đổi dựa trên thời điểm họ bắt đầu sử dụng sản phẩm của công ty.\nVí dụ, so sánh tỷ lệ giữ lại giữa đăng ký Q1 và Q2 có thể làm nổi bật các vấn đề tiềm ẩn hoặc thách thức từ đối thủ.\nNó cũng giúp đánh giá tỷ lệ chuyển đổi và xác định nguyên nhân đằng sau việc mất khách hàng.\nCohort Dựa trên lợi ích Hiểu Rõ Hành Vi Khách Hàng: Phân tích nhóm mang lại cái nhìn tỷ mị về cách các nhóm khách hàng khác nhau thể hiện hành vi qua thời gian.\nTối Ưu Hóa Tiếp Thị: Bằng cách hiểu hành vi nhóm, doanh nghiệp có thể điều chỉnh nỗ lực tiếp thị và chiến lược giao tiếp.\nCải Tiến Sản Phẩm: Các thông tin từ phân tích nhóm hướng dẫn cho sự cải tiến sản phẩm và phát triển tính năng.\nCâu nói ăn tiền: phân tích không chỉ đơn thuần là về những con số; nó là về việc hiểu những câu chuyện đằng sau những con số đó và ra quyết định thông tin dựa trên hành vi của khách hàng.\n6. Phân tích thuộc tính - Phân tích kết hợp (conjoint analysis) Phân tích kết hợp là một kỹ thuật thống kê được sử dụng trong nghiên cứu thị trường để hiểu cách khách hàng đánh giá các thuộc tính khác nhau của một sản phẩm hoặc dịch vụ.\nNó dựa trên nguyên tắc rằng bất kỳ sản phẩm nào cũng có thể phân rã thành một tập hợp các thuộc tính ảnh hưởng đến giá trị được người dùng cảm nhận đối với một mục hoặc dịch vụ.\nPhân tích kết hợp thường được thực hiện thông qua một cuộc khảo sát chuyên biệt yêu cầu người tiêu dùng xếp hạng sự quan trọng của các đặc điểm cụ thể. Phân tích kết quả cho phép công ty gán giá trị cho mỗi đặc điểm.\nCó nhiều loại phân tích kết hợp, bao gồm\nPhân tích Hội tụ Dựa trên Sự Lựa Chọn (CBC)\nPhân tích Hội tụ Thích ứng (ACA)\nPhân tích Hội tụ Toàn bộ\nPhân tích Hội tụ MaxDiff\nViệc các công ty sử dụng loại phân tích hội tụ nào, phụ thuộc vào mục tiêu định hình phân tích và loại sản phẩm hoặc dịch vụ đang được đánh giá.\nPhân tích hội tụ có thể giúp doanh nghiệp hiểu được những đặc tính nào của sản phẩm hoặc dịch vụ của họ được khách hàng đánh giá cao nhất, và gán một giá trị cụ thể cho mỗi đặc tính. Hiểu biết này cho phép xây dựng chiến lược có thông tin hơn từ lâu dài đến giá cả và bán hàng.\n7. Phân tích văn bản (Text analysis) Phân tích văn bản là quá trình trích xuất thông tin giá trị từ dữ liệu văn bản không có cấu trúc. Nó có thể được sử dụng cho nhiều mục đích khác nhau như hiểu phản hồi của khách hàng, tóm tắt tài liệu, xác định chủ đề và phân loại cảm xúc. Phân tích văn bản có thể thực hiện bằng cách sử dụng các phương pháp và kỹ thuật khác nhau, phụ thuộc vào loại văn bản và mục tiêu nghiên cứu. Dưới đây là một số phương pháp phổ biến:\nSentiment analysis: Phương pháp này xác định cảm xúc của văn bản, như tích cực, tiêu cực hoặc trung tính. Nó có thể giúp doanh nghiệp đo lường sự hài lòng của khách hàng, danh tiếng thương hiệu và đánh giá sản phẩm.\nPhân tích chủ đề: Phương pháp này xác định các chủ đề chính của văn bản, như thể thao, chính trị, hoặc giải trí. Nó có thể giúp doanh nghiệp tổ chức và phân loại lượng lớn dữ liệu văn bản như email, bài viết trên mạng xã hội và yêu cầu hỗ trợ.\nTrích xuất từ khóa: Phương pháp này trích xuất các từ hoặc cụm từ quan trọng nhất từ văn bản, như tên, địa điểm hoặc khái niệm. Nó có thể giúp doanh nghiệp tìm kiếm thông tin quan trọng như vấn đề của khách hàng, đặc điểm sản phẩm hoặc xu hướng thị trường.\nPhân tích văn bản có thể thực hiện thủ công hoặc tự động. Phân tích văn bản thủ công tốn thời gian, dễ chán và dễ gặp lỗi. Phân tích văn bản tự động sử dụng các kỹ thuật học máy để phân tích dữ liệu văn bản một cách nhanh chóng, chính xác và có thể mở rộng. Hiện nay, Có nhiều công cụ trực tuyến giúp thực hiện phân tích văn bản một cách tự động. Về tiếng việt thì chúng ta có thể sử dụng thư viện under the sea, hoặc nếu các bạn có dữ liệu lớn thì có thể implement lại các thuật toán đã public và train lại mô hình\n8. Phân tích chuỗi thời gian (time series analysis) Phân tích chuỗi thời gian là một cách cụ thể để phân tích một chuỗi điểm dữ liệu được thu thập trong một khoảng thời gian. Khác với việc thu thập dữ liệu ngẫu nhiên hoặc rải rác, phân tích chuỗi thời gian liên quan đến việc ghi lại các điểm dữ liệu ở các khoảng thời gian đều đặn trong một khoảng thời gian cố định. Sự khác biệt chính nằm ở cách các biến thay đổi theo thời gian. Dữ liệu chuỗi thời gian cung cấp thông tin quý giá về xu hướng, dự đoán.\nPhân tích chuỗi thời gian xử lý các điểm dữ liệu được sắp xếp theo thời gian. Ví dụ bao gồm chiều cao của đợt thủy triều, tốc độ gió trên biển, độ dày của sương mù, giá đóng cửa hàng ngày trên thị trường chứng khoán, để:\nHiểu rõ Xu hướng: Các tổ chức sử dụng phân tích chuỗi thời gian để hiểu nguyên nhân cơ bản của các xu hướng hoặc mô hình hệ thống theo thời gian. Các biểu đồ minh họa xu hướng theo mùa vụ, và các nền tảng phân tích hiện đại vượt xa các biểu đồ đường đơn giản.\nDự đoán: Dự báo chuỗi thời gian dự đoán giá trị tương lai dựa trên dữ liệu lịch sử. Nó giúp dự đoán các biến đổi, như mùa vụ hoặc hành vi chu kỳ.\nTài chính: Phân tích biến động tiền tệ, giá cổ phiếu và các chỉ số kinh tế.\nBán lẻ: Nghiên cứu dữ liệu bán hàng và mô hình yêu cầu.\nDự báo thời tiết: Dự đoán điều kiện thời tiết dựa trên dữ liệu lịch sử.\nChăm sóc sức khỏe: Giám sát các chỉ số quan trọng của bệnh nhân theo thời gian.\nKinh tế học: Theo dõi các chỉ số kinh tế như tăng trưởng GDP.\n9. Khai thác dữ liệu (Data mining) Khai thác dữ liệu là quá trình trích xuất và khám phá các mô hình trong các tập dữ liệu lớn liên quan đến các phương pháp ở sự giao lộ giữa học máy, thống kê và hệ thống cơ sở dữ liệu. Khai thác dữ liệu có thể được sử dụng cho nhiều mục đích, như hiểu cấu trúc và mô hình cơ bản của dữ liệu, phân tích hiệu suất của một công ty, hoặc dự đoán doanh thu và ảnh hưởng của quyết định kinh doanh. Khai thác dữ liệu phụ thuộc vào việc thu thập dữ liệu hiệu quả, lưu trữ và xử lý máy tính.\nCác bài toán trong data mining bao gồm: Classification, Clustering, Association rule mining, Sequential pattern mining, Anomaly detection\nKhai thác dữ liệu có thể được áp dụng trong nhiều lĩnh vực như tài chính, bán lẻ, dự báo thời tiết, chăm sóc sức khỏe và kinh tế. Khai thác dữ liệu có thể giúp tổ chức đạt được thông tin, đưa ra quyết định tốt hơn và cải thiện hiệu suất của họ. Tuy nhiên, khai thác dữ liệu cũng đặt ra một số thách thức và rủi ro, như chất lượng dữ liệu, quyền riêng tư, an ninh và đạo đức. Do đó, khai thác dữ liệu nên được thực hiện cẩn thận và tôn trọng đối với dữ liệu và những người liên quan.\n10. Cây quyết định (decision tree) Một cây quyết định là một biểu diễn của các duyệt định dứoi dạng cây. Nó có thể được sử dụng cho cả các nhiệm vụ phân loại và hồi quy trong học máy giám sát. Một cây quyết định bao gồm các nút, nhánh và lá tương ứng với các đặc trưng, quy tắc và dự đoán của dữ liệu. Một cây quyết định được xây dựng bằng cách chia dữ liệu thành các tập con dựa trên giá trị của các đặc trưng cho đến khi đạt đến một tiêu chí dừng. Tiêu chí chia thường dựa trên một độ đồng nhất hoặc phương sai, chẳng hạn như entropy hoặc chỉ số Gini\nMột số ưu điểm của cây quyết định bao gồm:\nDễ hiểu và giải thích, vì chúng giống như quá trình suy luận của con người.\nCó thể xử lý cả dữ liệu số và dữ liệu phân loại, cũng như có thể xử lý giá trị thiếu.\nChịu được ảnh hưởng từ nhiễu và giữ nguyên tính chất khi dữ liệu không cân bằng.\nMột số nhược điểm của cây quyết định là:\nDễ bị overfitting, cây càng sâu, càng phức tạp thì càng dễ bị overfitting.\nCó thể không ổn định, vì những thay đổi nhỏ trong dữ liệu có thể dẫn đến những thay đổi lớn trong cấu trúc cây.\nCó thể bị thiên vị, vì chúng có xu hướng ưa thích đặc trưng có nhiều cấp độ hoặc loại.\nNguồn: Regression analysis - Wikipedia. https://en.wikipedia.org/wiki/Regression_analysis.\nRegression Analysis - Formulas, Explanation, Examples and Definitions. https://corporatefinanceinstitute.com/resources/data-science/regression-analysis/.\nSimple Linear Regression | An Easy Introduction \u0026amp; Examples - Scribbr. https://www.scribbr.com/statistics/simple-linear-regression/.\nFactor analysis - Wikipedia. https://en.wikipedia.org/wiki/Factor_analysis.\nFactor Analysis Guide with an Example - Statistics By Jim. https://statisticsbyjim.com/basics/factor-analysis/.\nFactor Analysis - Steps, Methods and Examples - Research Method. https://researchmethod.net/factor-analysis/.\nFactor analysis - Wikipedia. https://en.wikipedia.org/wiki/Factor_analysis.\nFactor Analysis Guide with an Example - Statistics By Jim. https://statisticsbyjim.com/basics/factor-analysis/.\nFactor Analysis - Steps, Methods and Examples - Research Method. https://researchmethod.net/factor-analysis/.\nWhat are Neural Networks? | IBM. https://www.ibm.com/topics/neural-networks.\nNeural network - Wikipedia. https://en.wikipedia.org/wiki/Neural_network.\nWhat are Neural Networks? | IBM. https://www.ibm.com/topics/neural-networks.\nNeural network - Wikipedia. https://en.wikipedia.org/wiki/Neural_network.\nWhat Is a Neural Network? - Investopedia. https://www.investopedia.com/terms/n/neuralnetwork.asp.\nWhat is a neural network? A computer scientist explains - The Conversation. https://theconversation.com/what-is-a-neural-network-a-computer-scientist-explains-151897.\nCluster analysis - Wikipedia. https://en.wikipedia.org/wiki/Cluster_analysis.\nCluster Analysis - Types, Methods and Examples - Research Method. https://researchmethod.net/cluster-analysis/.\nWhat Is Cluster Analysis? (Examples + Applications) | Built In. https://builtin.com/data-science/cluster-analysis.\nCluster analysis - Wikipedia. https://en.wikipedia.org/wiki/Cluster_analysis.\nCluster Analysis - Types, Methods and Examples - Research Method. https://researchmethod.net/cluster-analysis/.\nWhat Is Cluster Analysis? (Examples + Applications) | Built In. https://builtin.com/data-science/cluster-analysis.\nGetty Images. https://www.gettyimages.com/detail/illustration/big-data-illustration-with-structuring-map-royalty-free-illustration/1139303464.\nhttps://online.hbs.edu/blog/post/what-is-conjoint-analysis\nWhat Is Conjoint Analysis \u0026amp; How Can You Use It? | HBS Online. https://online.hbs.edu/blog/post/what-is-conjoint-analysis.\nConjoint analysis - Wikipedia. https://en.wikipedia.org/wiki/Conjoint_analysis.\nWhat is a Conjoint Analysis? Types \u0026amp; Use Cases - Qualtrics. https://www.qualtrics.com/experience-management/research/types-of-conjoint/.\nen.wikipedia.org. https://en.wikipedia.org/wiki/Conjoint_analysis.\nText Analysis: Definition, Benefits \u0026amp; Examples - Qualtrics XM. https://www.qualtrics.com/experience-management/research/text-analysis/.\nTextual Analysis | Guide, 3 Approaches \u0026amp; Examples - Scribbr. https://www.scribbr.com/methodology/textual-analysis/.\nTime Series Analysis: Definition, Types \u0026amp; Techniques | Tableau. https://www.tableau.com/learn/articles/time-series-analysis.\nTime series - Wikipedia. https://en.wikipedia.org/wiki/Time_series.\nTime Series Analysis and Forecasting | Data-Driven Insights. https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-to-time-series-analysis/.\nData mining - Wikipedia. https://en.wikipedia.org/wiki/Data_mining.\nWhat Is Data Mining? How It Works, Benefits, Techniques, and Examples. https://www.investopedia.com/terms/d/datamining.asp.\nWhat is Data Mining? | IBM. https://www.ibm.com/topics/data-mining.\nDecision tree - Wikipedia. https://en.wikipedia.org/wiki/Decision_tree.\nDecision Tree - GeeksforGeeks. https://www.geeksforgeeks.org/decision-tree/.\nWhat is a Decision Tree | IBM. https://www.ibm.com/topics/decision-trees.\nen.wikipedia.org. https://en.wikipedia.org/wiki/Decision_tree.\n","date":"Feb 16, 2024","img":"https://unsplash.it/1920/1080?image=4","permalink":"/blog/2024-02-16-cac-phuong-phap-phan-tich-du-lieu-lon/","series":null,"tags":["bigdata"],"title":"Các Phương Pháp Phân Tích Dữ Liệu Lớn"},{"categories":null,"content":" 1. Mô hình thác nước (waterfall model) 2. Mô hình chữ V (V model) 3. Mô hình tiếp cận lặp (Interactive Model) 4. Mô hình xoắn ốc (Spiral model) 6. Nhóm mô hình Agile 6.1 Mô hình SCRUM 6.2 Mô hình KANBAN 6.3 Mô hình EXTREME PROGRAMMING - lập trình cực hạn - XP Chào tất cả các bạn, chúc các bạn năm mới an lành và hạnh phúc.\nHôm nay, mùng 4 tết, mình rảnh rỗi xíu nên chia sẽ với mọi người bài viết mới, tổng hợp nhỏ về các mô hình phát triển phần mềm\n1. Mô hình thác nước (waterfall model) Mô hình thác nước là một mô hình tuyến tính, trong đó các giai đoạn phát triển diễn ra theo một trình tự tuyến tính. Mỗi giai đoạn chỉ được thực hiện tiếp khi giai đoạn trước đã kết thúc. Mô hình này dễ sử dụng, dễ tiếp cận, nhưng rất khó để quay lại giai đoạn nào khi nó đã kết thúc và ít tính linh hoạt.\nCác giai đoạn đi theo từng bước:\nRequirements Gathering and Analysis \u0026raquo; System Design \u0026raquo; Implementation \u0026raquo; Testing Deployment \u0026raquo; Maintenance\n2. Mô hình chữ V (V model) là mô hình thác nước mở rộng, phát triển các bài kiểm tra và kiểm thử song song với từng giai đoạn phát triển, nhằm hạn chế những khuyết điểm của mô hình waterfall. Phù hợp với dự án vừa và nhỏ.\nVẽ hơi xấu\nRequirements Gathering and Analysis \u003c\u003c acceptance test design \u003e\u003e acceptance testing ↓ ↑ System Design \u003c\u003c system test design \u003e\u003eSystem testing ↓ ↑ Architecture Design \u003c\u003c integration test design \u003e\u003e integration testing ↓ ↑ Module design \u003c\u003c Unit test design \u003e\u003e Unit test ↓ ↑ Implementation Đối với dự án lớn, thì không nên áp dụng mô hình này.\n3. Mô hình tiếp cận lặp (Interactive Model) mỗi quy trình phát triển là một vòng lặp\nRequirement ==\u0026gt; Analytic and design ↑ ↓ init ==\u0026gt; plaining implement =\u0026gt; deploy ↑ ↓ evaluation \u0026lt;== testing\n4. Mô hình xoắn ốc (Spiral model) Mô hình linh hoạt kết hợp các khía cạnh của mô hình thác nước và phương pháp Agile. Mô hình này chú trọng vào phân tích rủi ro dự án, bắt đầu với yêu cầu/mục tiêu thiết kế và kết thúc với việc khách hàng kiểm tra tiến độ của từng giai đoạn. Mô hình này có tính linh hoạt cao, nhưng cũng tốn nhiều thời gian và tài nguyên\nƯu: đặt trọng tâm vào quản lý và giảm thiểu rủi ro\nNhược: chi phí cao\nthích hợp cho các dự án lớn, dự án phức tạp, dự án có nguy cơ thay đổi cao, dự án lần đường dò bước.\nMô hình tăng trưởng (Incremental model): Mô hình phát triển từng bước một, bổ sung các tính năng mới vào phiên bản cũ cho đến khi đạt được phiên bản hoàn chỉnh.\nMô hình này giúp sản xuất phần mềm làm việc ở giai đoạn trung gian, nhưng cũng có thể gây khó khăn trong việc tích hợp các phiên bản khác nhau.\n6. Nhóm mô hình Agile 6.1 Mô hình SCRUM Mô hình này hiện đang được rất nhiều công ty product và outsource sử dụng, gồm có các thành phần cơ bản sau\nTổ chức Organization : Product Owner, ScrumMaster, Development Team\nTài liệu (Atifacts): Product Backlog, Sprint Backlog, Estimation\nQui trình(Process): Sprint Planning meeting, Review, Daily Scrum Meeting\nƯu điểm của mô hình:\nMột người có thể thực hiện nhiều việc ví dụ như dev có thể test.\nPhát hiện lỗi sớm.\nCó khả năng áp dụng được cho những dự án mà yêu cầu khách hàng không rõ ràng ngay từ đầu.\nNhược điểm của mô hình:\nTrình độ của nhóm cần có một kỹ năng nhất định.\nPhải có sự hiểu biết về mô hình aglie.\nKhó khăn trong việc xác định ngân sách và thời gian.\nLuôn nghe ý kiến phản hồi từ khách hàng và thay đổi theo nên thời gian sẽ kéo dài.\n6.2 Mô hình KANBAN Mô hình này xây dựng 1 bảng tên là KANBAN để quản lý công việc, cái này khá đơn giản nhưng cực kỳ hiệu quả, chúng ta có thể xây dựng team KANBAN và member KANBAN (mỗi member tự xây dựng)\n6.3 Mô hình EXTREME PROGRAMMING - lập trình cực hạn - XP Khác với mô hình Scrum tập trung vào cấp độ quản lý dự án trọng tâm là ưu tiên công việc và lấy phản hồi, XP lại tập trung vào phát triển phần mềm chất lượng cao song song với chất lượng cuộc sống của nhóm phát triển\nCác vai trò trong CP: Huấn luyện viên(Coach), Khách hàng (Customer), Lập trình viên (Programmer), và Kiểm định viên (Tester).\nCác yếu tố cốt lõi của XP:\nGiá trị:\nGiao tiếp: XP nhấn mạnh các cuộc thảo luận trực tiếp kèn theo bảng và bút, nói chung là thảo luận face-to-face, không screen.\nĐơn giản: Tập trung vào giải pháp đơn giản nhất mà vẫn đáp ứng nhu cầu hoạt động. Tránh sự phức tạp không cần thiết, tránh sự dự đoán.\nPhản hồi: Liên tục thu thập phản hồi để cải thiện.\nDũng cảm: Hành động mạnh mẽ trước sự sợ hãi. Sẵn lòng nêu lên vấn đề tổ chức, dừng các thực hành không hiệu quả và chấp nhận phản hồi.\nTôn trọng: Các thành viên trong nhóm phải tôn trọng lẫn nhau để giao tiếp hiệu quả và cộng tác, tương tác tốt với nhau (hơi khó).\nCách thức thực hiện:\nPhát hành Thường xuyên: Chu kỳ phát triển ngắn, cập nhật thường xuyên.\nLập Trình Đôi: Nhà phát triển làm việc theo cặp, liên tục xem xét và cải thiện mã nguồn.\nKiểm thử Đơn vị: Kiểm thử tất cả mã nguồn để đảm bảo độ chính xác.\nThiết kế Đơn giản: Giữ cho thiết kế hệ thống càng đơn giản càng tốt.\nTương Tác với Khách hàng: Giao tiếp thường xuyên với khách hàng để hiểu rõ yêu cầu đang thay đổi.\nXP phù hợp cho:\nYêu cầu phần mềm thay đổi động.\nRủi ro trong các dự án có thời gian cố định sử dụng công nghệ mới.\nNhóm phát triển nhỏ, đặt tại cùng một địa điểm.\nCông nghệ cho phép kiểm thử tự động đơn vị và chức năng.\nNguồn:\nhttps://en.wikipedia.org/wiki/Extreme_programming\n","date":"Feb 13, 2024","img":"https://unsplash.it/1920/1080?image=5","permalink":"/blog/2024-02-13-mo-hinh-phat-trien-phan-mem/","series":null,"tags":["software"],"title":"Mô Hình Phát Triển Phần Mềm"},{"categories":null,"content":" I. Phân loại các loại data analytics 1. No analytics: 2. Descriptive analytics - Phân tích mô tả Một số ví dụ sử dụng Descriptive analytics 3 Diagnostic analytics - Phân tích chẩn đoán Tìm hiểu một số khái niệm cơ bản của Diagnostic analytics Một số ví dụ sử dụng diagnostic analytics 4 Predictive analytics: 5 Ví dụ của PREDICTIVE ANALYTICS trong thực tế 5 Prescriptive analytics: II. Sáu bước cơ bản bắt đầu một dự án Data Analytics Bước 0 : Prepair - Chuẩn bị Bước 1: Define analytics requirement - Tìm ra các câu hỏi cần trả lời Bước 2: Collecting data Bước 3: Clearning data Bước 4: Analyzing data Bước 5: Presenting Report III. Các bài toán thông dụng nhất của DA Đo lường tác động của thay đổi so với hiện tại ( Quan trọng nhất) Dự báo ( Quan trọng) Phân tích khách hàng Nhận dạng khách hàng ( Quan trọng nhất) Cross selling Customer journey Basket analytics Đo lường thời hạn tác động của một cải tiến / sale Thứ tự quan tâm của các công ty Sale -\u0026gt; marketing -\u0026gt; product -\u0026gt; hướng phát triển.\nHiện nay, các công ty thông thường sẽ tuyển các bạn Data Analytics là những bạn có \u0026ldquo;kiến thức ngành cứng\u0026rdquo; cộng với kỹ năng về data. Bởi vậy, nghề này có mức độ cạnh tranh khá khốc liệt. Một số ngành cứng hiện giờ mình có thể kể tên là marketting, quản lý chuỗi cung ứng, vận hành, kho bãi, tài chính, v.v\nI. Phân loại các loại data analytics Tóm tắt ngắn gọn, cho những ai lười đọc:\nNếu tổ chức của bạn chưa bao giờ phân tích dữ liệu, hãy bắt đầu tập làm quen với việc phân tích, bằng cách đưa ra những câu hỏi cần sự trả lời, đưa ra các quy trình cần sự tối ưu, thu thập dữ liệu xung quanh các câu hỏi, các quy trình và sử dụng một trong các kiểu phân tích bên dưới để vẽ lại bức tranh đầy đủ.\nDescriptive: Trend của data chỉ ra cái gì?\nDiagnostic: Yếu tố nào đóng góp vào các trend trên, tại sao trend lại xảy ra?\nPredictive: Nếu có thể, xác định khi nào trend là một yếu tố mà nó vẫn còn tiếp tục ( còn trend ở thời điểm hiện tại) hoặc trend sẽ lặp lại (chu kỳ)\nPrescriptive: Đào sâu vào phân tích.\nNếu chúng ta có các thuật toán độc quyền hoặc có các công cụ phân tích của bên thứ ba, chạy thuật toán đó trên dữ liệu của mình.\nNếu không có, hãy xây dựng manual analysis các \u0026ldquo;nước\u0026rdquo; phân tích dựa trên những khám phá của bạn về quy trình cần tối ưu hoặc về câu hỏi cần sự trả lời. iến hành phân tích thủ công các bước tiếp theo có thể thực hiện được dựa trên những gì bạn đã khám phá được về câu hỏi hoặc quy trình của mình. Mỗi lựa chọn \u0026ldquo;nước\u0026rdquo; đi đó sẽ tác động như thế nào đến kết quả của các tình huống và từ đó nó sẽ tác động như thế nào đến mục tiêu của bạn?\n1. No analytics: Không có phân tích, chạy theo cảm hứng và kinh nghiệm của một số người.\n2. Descriptive analytics - Phân tích mô tả Descriptive analytics là một phần trong lĩnh vực phân tích dữ liệu (data analytics) và nó tập trung vào việc tóm tắt, mô tả và hiểu sự thực tại của dữ liệu. Mục tiêu chính của descriptive analytics là cung cấp thông tin dựa trên dữ liệu lịch sử để giúp tổ chức hoặc người quản lý hiểu rõ tình hình hiện tại, khám phá mô hình hoặc xu hướng trong dữ liệu, và đưa ra các quyết định tương lai dựa trên kiến thức này.\nCác phương pháp và công cụ phổ biến trong descriptive analytics bao gồm:\nBáo cáo và Biểu đồ: Sử dụng biểu đồ, biểu đồ, và báo cáo để biểu thị dữ liệu và thể hiện mối quan hệ giữa các biến số. Các biểu đồ và báo cáo này giúp tạo ra cái nhìn tổng quan về dữ liệu.\nTóm tắt Thống kê: Tính toán các thống kê mô tả như trung bình, phương sai, tỷ lệ, và phân phối dữ liệu. Điều này giúp trong việc mô tả các tính chất quan trọng của dữ liệu.\nPhân tích dữ liệu lịch sử: Xem xét dữ liệu lịch sử để xác định xu hướng, biến động, và các sự kiện quan trọng trong quá khứ. Điều này có thể giúp dự đoán sự kiện tương lai dựa trên dữ liệu lịch sử.\nPhân loại và nhóm dữ liệu: Nhóm dữ liệu vào các phân loại để hiểu rõ các nhóm và sự tương quan giữa chúng.\nKhám phá dữ liệu (Data Exploration): Sử dụng các kỹ thuật khám phá dữ liệu để phát hiện thông tin mới và bất thường trong dữ liệu, chẳng hạn như việc sử dụng mô hình phân cụm.\nLọc và tìm kiếm dữ liệu: Tìm kiếm dữ liệu cụ thể hoặc lọc dữ liệu để tập trung vào các yếu tố quan trọng.\nDescriptive analytics thường được sử dụng để mô tả hiện tượng, hiểu rõ tình hình hiện tại, và xác định các vấn đề hoặc cơ hội cơ bản. Nó cung cấp nền tảng cho các giai đoạn phân tích tiếp theo như predictive analytics (dự đoán tương lai) và prescriptive analytics (đưa ra hướng dẫn và đề xuất hành động).\nMột số ví dụ sử dụng Descriptive analytics Traffic and Engagement Reports Ngữ cảnh là bạn đang có một website bán hàng, có lưu lại hành vi tương tác của khách hàng lên trên website sử dụng GA. Một số báo cáo bạn có thể xây dựng.\nBáo cáo kênh truyền thông nào đang thu hút nhiều lưu lượng truy cập trên trang web của bạn nhất.\nXác định số lượng người dùng từ mỗi nguồn\nSo sánh số người dùng ở thời điểm hiện tại với quá khứ từ cùng một nguồn.\nXem số lượng truy cập từ quảng cáo trả phí đang tăng lên bao nhiêu phần trăm\nCác yếu tố mình liệt kê ở trên là một trong các thông tin quan trọng để các loại phân tích khác bên dưới đào sâu hơn lý do.\nVì sao nguồn truy cập từ facebook lại tăng theo thời gian\nXu hướng tăng này có còn tiếp tục ở tương lai\nHành động tiếp theo của chúng ta là gì\nFinancial Statement Analysis Báo cáo tài chính là báo cáo định kỳ nêu chi tiết thông tin tài chính về một doanh nghiệp và cùng nhau đưa ra cái nhìn toàn diện về tình hình tài chính của công ty.\nCó một số loại báo cáo tài chính, bao gồm bảng cân đối kế toán, báo cáo kết quả hoạt động kinh doanh, báo cáo lưu chuyển tiền tệ và báo cáo vốn chủ sở hữu của cổ đông. Mỗi kênh phục vụ một đối tượng cụ thể và truyền tải những thông tin khác nhau về tài chính của công ty.\nPhân tích báo cáo tài chính có thể được thực hiện theo ba cách chính: dọc, ngang và tỷ lệ.\nPhân tích theo chiều dọc là việc đọc các dòng dữ liệu theo thứ tự từ trên xuống dưới và so sánh từng mục với các mục ở trên và dưới nó. Điều này giúp xác định mối quan hệ giữa các biến. Ví dụ: nếu mỗi chi tiết đơn hàng là một tỷ lệ phần trăm của tổng số thì việc so sánh chúng có thể cung cấp thông tin chi tiết về chi tiết đơn hàng nào chiếm tỷ lệ phần trăm lớn hơn và nhỏ hơn trong tổng số.\nPhân tích theo chiều ngang là việc đọc một báo cáo từ trái sang phải và so sánh từng mục với chính nó ở kỳ trước. Loại phân tích này xác định sự thay đổi theo thời gian.\nCuối cùng, phân tích tỷ lệ là việc việc so sánh một phần của báo cáo với phần khác dựa trên mối quan hệ của chúng với tổng thể. Điều này so sánh trực tiếp các mặt hàng qua các thời kỳ, cũng như tỷ lệ của công ty bạn với ngành để đánh giá xem công ty của bạn hoạt động hiệu quả hơn hay kém hơn.\nMỗi phương pháp phân tích báo cáo tài chính này là ví dụ về descriptive analytics vì chúng cung cấp thông tin về xu hướng và mối quan hệ giữa các biến dựa trên dữ liệu hiện tại và lịch sử.\nDemand Trends Descriptive analytics cũng có thể được sử dụng để xác định xu hướng trong sở thích và hành vi của khách hàng, đồng thời đưa ra các giả định về nhu cầu đối với các sản phẩm hoặc dịch vụ cụ thể.\nMột usecase thường được nhắc tới là Netflix’s. Team của họ đã thu thập một lượng lớn hành vi của người dùng trên nền tảng của họ. Họ phân tích dữ liệu này để xác định ra các chương trình TV và các bộ phim đang là trending ở thời điểm hiện tại và đưa ra các gợi ý phim trending ở trang chủ.\nKhông dừng lại ở đó, các dữ liệu này còn giúp Netflix biết được rằng loại phim nào, diễn viên nào, đạo diễn nào hiện tại đang được yêu thích. Và nó giúp đưa quyết định về nội dung các phim sắp tới sẽ được bấm máy, hợp đồng với nhà sản xuất phim, đưa ra các chiến dịch quản cáo, retargeting quản cáo.\nAggregated Survey Results Descriptive analytics cũng hữu ích trong nghiên cứu thị trường.\nVí dụ: bạn có thể tiến hành một cuộc khảo sát và xác định rằng khi độ tuổi của người trả lời tăng lên thì khả năng họ mua sản phẩm của bạn cũng tăng theo. Nếu bạn đã thực hiện khảo sát này nhiều lần trong nhiều năm, thì phân tích mô tả có thể cho bạn biết liệu mối tương quan giữa độ tuổi mua hàng này luôn tồn tại hay nó chỉ xảy ra trong năm nay.\nNhững hiểu biết sâu sắc như thế này có thể mở đường cho các phân tích chẩn đoán giải thích lý do tại sao một số yếu tố nhất định lại có mối tương quan với nhau. Sau đó, bạn có thể tận dụng các phân tích dự đoán và phân tích theo quy định để lập kế hoạch cải tiến sản phẩm hoặc chiến dịch tiếp thị trong tương lai dựa trên những xu hướng đó.\nProgress to Goals descriptive analytics có thể được áp dụng để theo dõi tiến trình đạt được mục tiêu (Progress to Goals). Các báo cáo về tiến độ của KPIs có thể giúp team của bạn hiểu được rằng công việc mình làm có đang đi đúng hướng, hay nó đang đi sai hướng và chúng ta cần điều chỉnh lại để đi đúng hướng.\nVí dụ: nếu tổ chức của bạn đặt mục tiêu đạt được 500.000 unique page views / month, bạn có thể sử dụng dữ liệu lưu lượng truy cập để theo dõi. Giả sử, trong nửa tháng, bạn đạt được 200.000 lượt xem, vậy là đi nữa chặn đường rồi nhưng chúng ta chưa đạt được 1 nữa mục tiêu. Phân tích dạng này chỉ cho chúng ta điều đó và chúng ta cần sử dụng những phân tích chuyên sâu hơn bên dưới để cải thiện lưu lượng truy cập để quay lại hướng đúng KPI của bạn.\n3 Diagnostic analytics - Phân tích chẩn đoán Diagnostic analytics (phân tích chẩn đoán) là một loại phân tích dữ liệu trong lĩnh vực quản lý dựa trên dữ liệu (data analytics), nơi mục tiêu chính là tìm hiểu và hiểu rõ nguyên nhân hoặc lý do xảy ra của một sự kiện hoặc tình huống cụ thể. Mục đích chính của diagnostic analytics là giúp tổ chức hoặc người quản lý đối mặt với các vấn đề hoặc sự cố, cung cấp thông tin để làm rõ tại sao chúng xảy ra và giúp đưa ra quyết định hoặc biện pháp sửa chữa.\nCác điểm chính của diagnostic analytics bao gồm:\nTìm hiểu nguyên nhân: Loại phân tích này tập trung vào việc phân tích dữ liệu để tìm ra nguyên nhân gốc rễ của một sự kiện hoặc tình huống cụ thể. Điều này giúp hiểu rõ tại sao điều đó xảy ra và tạo cơ hội để ngăn chặn sự kiện tương tự trong tương lai.\nSử dụng dữ liệu lịch sử: Diagnostic analytics sử dụng dữ liệu lịch sử và thông tin về sự kiện cụ thể để phân tích và phát hiện các mô hình hoặc mối quan hệ giữa các biến.\nHỗ trợ quyết định: Kết quả của diagnostic analytics có thể hỗ trợ quyết định về việc xử lý các vấn đề hoặc sự cố. Dựa vào thông tin này, người quản lý có thể đưa ra các biện pháp cải thiện hoặc điều chỉnh quy trình làm việc để ngăn chặn các vấn đề tương tự.\nVí dụ về ứng dụng của diagnostic analytics bao gồm việc phân tích tại sao sản phẩm có tỷ lệ trả hàng cao, làm rõ tại sao một dự án đã trễ hạn, hoặc tìm hiểu lý do tại sao doanh số bán hàng của một sản phẩm cụ thể đã giảm sút. Khi hiểu được nguyên nhân, tổ chức có thể đưa ra các biện pháp sửa chữa hoặc cải thiện quá trình làm việc để giảm thiểu các vấn đề này trong tương lai.\nCó một số khái niệm cần hiểu trước khi đi sâu vào phân tích chẩn đoán: kiểm tra giả thuyết (hypothesis testing), sự khác nhau giữa mối tương quan và quan hệ nhân quả, phân tích hồi quy chẩn đoán (diagnostic regression analysis).\nTìm hiểu một số khái niệm cơ bản của Diagnostic analytics Hypothesis Testing Kiểm định giả thuyết (Hypothesis Testing) là một phương pháp thống kê cơ bản được sử dụng để đưa ra quyết định về các tham số của quần thể dựa trên dữ liệu mẫu. Đây là một quy trình hệ thống được sử dụng để đánh giá xem có đủ bằng chứng để hỗ trợ hoặc phủ định một giả thuyết cụ thể về quần thể hay không. Dưới đây là các bước và khái niệm chính liên quan đến kiểm định giả thuyết\nXây dựng Giả Thuyết: Giả thuyết không (H0): Đây là giả thuyết mặc định hoặc trạng thái hiện hành. Nó cho rằng không có hiệu ứng, không có sự khác biệt hoặc không có mối quan hệ nào trong quần thể. Thường được ký hiệu là H0.\nGiả thuyết thay thế (Ha hoặc H1): Đây là giả thuyết mà bạn muốn kiểm tra. Nó đại diện cho một khẳng định cụ thể hoặc hiệu ứng mà bạn muốn chứng minh. Thường được ký hiệu là Ha hoặc H1.\nChọn Mức Ý Nghĩa (α): Mức ý nghĩa, ký hiệu là α (alpha), đại diện cho xác suất của lỗi loại I, tức là lỗi sai khi từ chối một giả thuyết không đúng. Mức ý nghĩa thường bao gồm 0,05 (5%) và 0,01 (1%), nhưng sự lựa chọn phụ thuộc vào ngữ cảnh và mức độ tin cậy yêu cầu.\nThu thập và Phân Tích Dữ Liệu: Thu thập một mẫu từ quần thể quan tâm và thực hiện phân tích thống kê cần thiết để tính toán thống kê kiểm định, đo lường mối quan hệ hoặc hiệu ứng đang được nghiên cứu.\nTính Toán Thống Kê Kiểm Định: Thống kê kiểm định phụ thuộc vào loại kiểm định đang thực hiện (ví dụ: kiểm định t-student, kiểm định chi bình phương, kiểm định z). Nó đo lường mức độ mà thống kê mẫu khác biệt so với giá trị kỳ vọng dưới giả thuyết không.\nXác định Vùng Quan Trọng hoặc Giá trị p (P-Value): Vùng Quan Trọng: Trong kiểm định giả thuyết, vùng quan trọng đại diện cho tập hợp các giá trị của thống kê kiểm định mà bạn sẽ từ chối giả thuyết không. Các giá trị này được xác định dựa trên mức ý nghĩa đã chọn và phân phối của thống kê kiểm định.\nGiá trị p (P-Value): Giá trị p là xác suất của việc thu được thống kê kiểm định càng \u0026ldquo;tương đối\u0026rdquo; hoặc \u0026ldquo;tương đối hơn\u0026rdquo; so với thống kê kiểm định quan sát trong mẫu dữ liệu, giả sử rằng giả thuyết không đúng. Giá trị p nhỏ (thường nhỏ hơn α) cho thấy có đủ bằng chứng để phủ định giả thuyết không.\nĐưa Ra Quyết Định: Nếu thống kê kiểm định nằm trong vùng quan trọng (tức là xác suất xảy ra do sự tình cờ thấp), bạn từ chối giả thuyết không để ủng hộ giả thuyết thay thế.\nNếu thống kê kiểm định không nằm trong vùng quan trọng, bạn không từ chối giả thuyết không, tức là không có đủ bằng chứng để ủng hộ giả thuyết thay thế.\nĐưa Ra Kết Luận: Dựa trên quyết định ở bước 6, bạn đưa ra kết luận về tham số của quần thể bạn đang kiểm tra. Bạn có thể kết luận rằng có đủ bằng chứng cho một hiệu ứng, một sự khác biệt, hoặc một mối quan hệ (từ chối giả thuyết không) hoặc rằng không có đủ bằng chứng để làm điều đó (không từ chối giả thuyết không).\nHypothesis testing là quy trình thống kê (the statistical process) để chứng minh hoặc bác bỏ một giả định.\nHypotheses có thể là future-oriented (ví dụ, Nếu chúng ta đổi logo của công ty chúng ta, nhiều ngừi ở Bắc Mỹ sẽ mua sản phẩm của chúng ta), trong predictive analytics hoặc prescriptive analytics.\nTrong diagnostic analytics, hypotheses là historically-oriented (ví dụ, Tôi dự đoán doanh số bán hàng tháng này sụt giảm là do sản phẩm của chúng tôi tăng giá gần đây.). Các giả định định hướng việc phân tích của bạn và được sử dụng như một lời nhắc nhở về điều bạn đang muốn chứng minh hoặc bác bỏ.\nCorrelation vs. Causation Tương quan và nhân quả (Correlation and Causation) là hai khái niệm quan trọng trong thống kê và khoa học dữ liệu. Tuy cùng liên quan đến sự liên kết giữa hai biến, nhưng chúng có ý nghĩa khác nhau:\nTương Quan (Correlation): Tương quan chỉ đơn giản là mô tả mối quan hệ tương đối giữa hai biến. Nó chỉ cho ta biết nếu có sự thay đổi theo cùng hướng hoặc ngược hướng giữa các biến. Khi hai biến tương quan, có thể có sự thay đổi chung nhưng không có liên quan nhân quả. Điều này có thể là do tình cờ hoặc có biến khác ẩn sau mối quan hệ tương quan. Ví dụ: Có một tương quan mạnh giữa việc sử dụng ô tô và lượng dầu tiêu thụ hàng năm. Tuy nhiên, điều này không có nghĩa rằng việc sử dụng ô tô gây ra sự tăng tiêu thụ dầu.\nNhân Quả (Causation): Nhân quả đề cập đến mối quan hệ nguyên nhân và kết quả giữa hai biến, trong đó một biến (biến nguyên nhân) gây ra sự thay đổi trong biến kết quả.\nĐể kết luận về mối quan hệ nhân quả, cần có nhiều bằng chứng hơn so với chỉ tương quan. Thông thường, cần tiến hành thử nghiệm kiểm tra nhân quả hoặc sử dụng thiết kế nghiên cứu để loại trừ các yếu tố khác.\nVí dụ: Nếu bạn thực hiện một thử nghiệm ngẫu nhiên để đo lượng vitamin C được cung cấp cho một nhóm người và xem xét tác động của nó đối với sức khỏe, bạn có thể đưa ra kết luận về mối quan hệ nhân quả giữa vitamin C và sức khỏe.\nTóm lại, tương quan chỉ mô tả mối quan hệ giữa hai biến, trong khi nhân quả đề cập đến mối quan hệ nguyên nhân và kết quả. Việc xác định mối quan hệ nhân quả thường phức tạp hơn và đòi hỏi nhiều nghiên cứu và bằng chứng để có thể chắc chắn rằng một biến gây ra sự thay đổi trong biến khác.\nNếu tổ chức của bạn có đủ tài nguyên, bạn có thể chạy thực nghiệm để tìm ra mối quan hệ nhân quả. Nếu xác định được mối quan hệ nhân quả của 2 biến, mối tương quan vẫn có thể mang lại cái nhìn sâu sắc cần thiết để hiểu dữ liệu của bạn và sử dụng dữ liệu đó để đưa ra các quyết định chính xác hơn.\nDiagnostic Regression Analysis Một số mối quan hệ giữa các biến có thể dễ dàng nhận ra, nhưng một số khác yêu cầu phân tích sâu hơn. Phân tích hồi quy được sử dụng để xác định mối quan hệ giữa hai biến (single linear regression) hoặc ba biến trở lên (multiple regression). Mối quan hệ được thể hiện bằng một phương trình toán học chuyển thành độ dốc của đường phù hợp nhất với mối quan hệ của các biến.\nRegression giúp chúng ta xác định insight về cấu trúc của mối quan hệ trong 2 hay nhiều biến và cung cấp thước đo mức độ phù hợp của dữ liệu với mối quan hệ( của 2 hay nhiều biến) đó\nnostic analytics là việc chúng ta sử dụng phân tích hồi quy để giải thích mối quan hệ giữa các biến trong dữ liệu lịch sử. Sau đó, đường hồi quy có thể được sử dụng để dự đoán cho tương lai ( là ví dụ của nhóm predictive analytics).\nMột số ví dụ sử dụng diagnostic analytics Examining Market Demand Một usecase của diagnostic analytics là xác định lý do đằng sau nhu cầu sản phẩm.\nVí dụ: Công ty HelloFresh - công ty đại chúng quốc tế cung cấp đồ ăn sơ chế sẵn có trụ sở tại Berlin, Đức. Đây là nhà cung cấp đồ ăn sơ chế sẵn lớn nhất ở Hoa Kỳ, và cũng có hoạt động ở Canada, Tây Âu. Công ty thu thập hàng triệu điểm dữ liệu từ người dùng toàn cầu, bao gồm thông tin về vị trí địa lý, dữ liệu nhân khẩu học được tiết lộ, loại bữa ăn, sở thích về hương vị cũng như nhịp và thời gian đặt hàng thông thường.\nNhóm của HelloFresh sử dụng dữ liệu này để xác định mối quan hệ giữa các xu hướng về thuộc tính và hành vi của khách hàng. Như một ví dụ giả định, hãy tưởng tượng nhóm HelloFresh xác định được sự gia tăng đột biến về đơn đặt hàng công thức chế biến từ cá. Sau khi tiến hành phân tích chẩn đoán, họ phát hiện ra rằng các thuộc tính có mối tương quan cao nhất với việc đặt hàng các công thức nấu cá được xác định là nữ và sống ở vùng đông bắc Hoa Kỳ.\nTừ đó, nhóm có thể tiến hành nghiên cứu thị trường với nhóm nhân khẩu học cụ thể đó để tìm hiểu thêm về nhu cầu về công thức nấu cá. Có phải nguyên nhân là do một nghiên cứu khoa học gần đây ca ngợi lợi ích sức khỏe của cá đối với phụ nữ? Có lẽ những người sống ở vùng đông bắc Hoa Kỳ có khẩu vị hải sản tinh tế vì họ sống tương đối gần Đại Tây Dương. Lý luận của họ có thể cung cấp những hiểu biết sâu sắc có tác động cho HelloFresh.\nKhi nghiên cứu các loại phân tích khác, nhóm cũng có thể xem xét liệu xu hướng này có tiếp tục hay không (phân tích dự đoán) và liệu việc tạo ra nhiều công thức nấu ăn từ cá hơn có xứng đáng với công sức và tiền bạc để đáp ứng sở thích của đối tượng này hay không (phân tích theo quy định).\nExplaining Customer Behavior Đối với các công ty, việc thu thập dữ liệu khách hàng, phân tích, chẩn đoán là chìa khóa để hiểu lý do tại sao khách hàng làm như vậy. Những thông tin chi tiết này có thể được sử dụng để cải thiện sản phẩm và trải nghiệm người dùng (UX), định vị lại thông điệp thương hiệu và đảm bảo sản phẩm phù hợp với đối tượng.\nTiếp tục với ví dụ HelloFresh, hãy xem xét giá trị của việc giữ chân khách hàng đối với công ty hoạt động theo mô hình đăng ký. Giữ chân khách hàng sẽ tiết kiệm chi phí hơn so với việc có được khách hàng mới, vì vậy HelloFresh sử dụng phân tích chẩn đoán để xác định lý do khiến khách hàng rời đi chọn hủy đăng ký.\nTrong quá trình hủy, khách hàng rời đi phải cung cấp lý do hủy. Các tùy chọn bao gồm từ “không phù hợp với túi tiền của tôi” đến “không phù hợp với lịch trình hoặc nhu cầu ăn kiêng của tôi” và cũng có tùy chọn để viết câu trả lời. Bằng cách thu thập dữ liệu này, HelloFresh có thể phân tích các lý do mất khách hàng được nêu nhiều nhất ở các khu vực và nhân khẩu học cụ thể, đồng thời sử dụng phân tích chẩn đoán để trả lời câu hỏi “Tại sao mọi người hủy đăng ký?”\nNhững hiểu biết sâu sắc này có thể giúp cải thiện trải nghiệm người dùng và sản phẩm của HelloFresh để tránh mất thêm khách hàng vì những lý do đó.\nIdentifying Technology Issues Một ví dụ về diagnostic analytics trong bài toán này là các tester bị yêu cầu sử dụng chương trình phần mềm và chạy thử nghiệm (test) để xác định nguyên nhân của sự cố, các lỗi. Điều này thường được gọi là \u0026ldquo;chạy chẩn đoán\u0026rdquo; và có thể là điều bạn đã làm trước đây khi gặp sự cố máy tính.\nMột số thuật toán chạy liên tục và hoạt động ở chế độ nền của máy, trong khi những thuật toán khác cần do con người thực hiện. Một loại kiểm tra chẩn đoán mà bạn có thể quen thuộc là chẩn đoán dựa trên giải pháp, phát hiện và gắn cờ các triệu chứng của các vấn đề đã biết và tiến hành quét để xác định nguyên nhân gốc rễ. Điều này có thể cho phép bạn giải quyết vấn đề và báo cáo vấn đề nếu nguyên nhân nghiêm trọng.\nImproving Company Culture Diagnostic analytics cũng có thể được tận dụng để cải thiện văn hóa nội bộ công ty. Bộ phận nhân sự có thể thu thập thông tin về cảm giác an toàn về thể chất và tâm lý của nhân viên, những vấn đề họ quan tâm cũng như những phẩm chất và kỹ năng giúp ai đó thành công và hạnh phúc. Nhiều thông tin chi tiết trong số này đến từ việc thực hiện các cuộc khảo sát nội bộ, ẩn danh và thực hiện các cuộc phỏng vấn thôi việc để xác định các yếu tố góp phần khiến nhân viên muốn ở lại hoặc rời đi.\nThu thập thông tin về suy nghĩ và cảm xúc của nhân viên cho phép bạn phân tích dữ liệu và xác định cách cải thiện các lĩnh vực như văn hóa và lợi ích công ty. Điều này có thể bao gồm bất cứ điều gì từ mong muốn công ty đóng góp nhiều hơn cho trách nhiệm xã hội của doanh nghiệp (CSR) cho đến cảm giác bị phân biệt đối xử tại nơi làm việc. Trong những trường hợp này, dữ liệu trình bày một trường hợp phân bổ nhiều nguồn lực hơn cho CSR và các nỗ lực đa dạng, công bằng, hòa nhập và thuộc về.\nNhững hiểu biết sâu sắc từ các cuộc khảo sát và phỏng vấn cũng có thể cho phép người quản lý tuyển dụng xác định những phẩm chất và kỹ năng nào giúp ai đó thành công tại công ty hoặc trong nhóm cụ thể của bạn, từ đó giúp thu hút và tuyển dụng những ứng viên tốt hơn cho các vai trò còn trống.\nPhân tích chẩn đoán có thể giúp nâng cao mức độ hài lòng, an toàn và giữ chân nhân viên, cũng như giúp quy trình tuyển dụng hiệu quả hơn.\n4 Predictive analytics: Predictive analytics là một phương pháp trong lĩnh vực phân tích dữ liệu (data analytics) sử dụng để dự đoán các sự kiện tương lai hoặc kết quả dựa trên dữ liệu lịch sử và mô hình hóa thống kê. Đây là một công cụ mạnh mẽ trong nhiều lĩnh vực và ngành công nghiệp, như tiếp thị, tài chính, y tế, sản xuất, và nhiều lĩnh vực khác.\nCác bước chính trong quá trình predictive analytics bao gồm:\nThu thập dữ liệu: Bước đầu tiên là thu thập và tổng hợp dữ liệu liên quan đến vấn đề cần dự đoán. Dữ liệu này có thể là dữ liệu lịch sử hoặc dữ liệu thời gian thực.\nTiền xử lý dữ liệu: Dữ liệu thường cần phải được làm sạch và tiền xử lý trước khi sử dụng. Điều này bao gồm việc loại bỏ dữ liệu không hợp lệ hoặc thiếu sót, xử lý giá trị thiếu, và biến đổi dữ liệu nếu cần.\nXây dựng mô hình: Trong bước này, các mô hình thống kê hoặc machine learning được sử dụng để phân tích dữ liệu và dự đoán các sự kiện tương lai. Các mô hình phổ biến bao gồm hồi quy tuyến tính, cây quyết định, mạng nơ-ron, và nhiều mô hình khác.\nĐánh giá và tinh chỉnh mô hình: Mô hình được đánh giá bằng cách sử dụng các phương pháp kiểm tra và đánh giá chất lượng dự đoán. Nếu cần, mô hình được điều chỉnh để cải thiện hiệu suất.\nTriển khai mô hình: Sau khi mô hình được đánh giá và chấp nhận, nó có thể được triển khai để sử dụng trong thực tế. Các dự đoán từ mô hình có thể được tích hợp vào quy trình kinh doanh hoặc hệ thống thông tin.\nPredictive analytics có thể được sử dụng trong nhiều mục đích, chẳng hạn như dự đoán doanh số bán hàng, phân tích rủi ro tín dụng, dự đoán biến động thị trường, quản lý tồn kho, dự đoán thời tiết, và nhiều ứng dụng khác. Đây là công cụ quan trọng giúp doanh nghiệp và tổ chức làm quyết định dựa trên dữ liệu và cải thiện hiệu suất kinh doanh.\n5 Ví dụ của PREDICTIVE ANALYTICS trong thực tế Finance: Forecasting Future Cash Flow Giáo sư V.G. Narayanan của HBS nói: “Các nhà quản lý cần phải nhìn về phía trước để lập kế hoạch cho tình hình hoạt động kinh doanh trong tương lai của họ”. “Bất kể bạn làm việc trong lĩnh vực nào, luôn có rất nhiều điều không chắc chắn liên quan đến quá trình này.”\nMọi doanh nghiệp đều cần lưu giữ hồ sơ tài chính định kỳ và phân tích dự đoán có thể đóng một vai trò lớn trong việc dự báo tình trạng tương lai của công ty. Sử dụng dữ liệu lịch sử từ các báo cáo tài chính trước đó cũng như dữ liệu từ ngành rộng hơn, bạn có thể dự đoán số bán, doanh thu và chi phí để tạo ra bức tranh về tương lai và đưa ra quyết định.\nDự báo dòng tiền trong tương lai là một quy trình phân tích tài chính quan trọng, bao gồm việc ước tính dòng tiền vào và ra mà một doanh nghiệp dự kiến ​​sẽ tạo ra trong một khoảng thời gian cụ thể trong tương lai. Dự báo dòng tiền chính xác là rất quan trọng để lập kế hoạch tài chính hiệu quả, ngân sách và quyết định trong một công ty. Dưới đây là một cái nhìn tổng quan về các bước và yếu tố cần xem xét khi dự báo dòng tiền trong tương lai:\nThu thập Dữ liệu Lịch sử: Bắt đầu bằng việc thu thập và phân tích dữ liệu tài chính lịch sử, bao gồm báo cáo dòng tiền, báo cáo lợi nhuận và bảng cân đối kế toán. Dữ liệu lịch sử cung cấp thông tin quý báu về các mô hình và xu hướng dòng tiền trong quá khứ.\nXác định Các Thành Phần Dòng Tiền: Phân chia dòng tiền thành các thành phần khác nhau, bao gồm dòng tiền từ hoạt động kinh doanh, dòng tiền từ hoạt động đầu tư và dòng tiền từ hoạt động tài chính. Điều này giúp hiểu rõ dòng tiền đến từ đâu và điều gì làm tiêu hao dòng tiền.\nƯớc Tính Doanh Số và Doanh Thu: Ước tính doanh số bán hàng và doanh thu tương lai dựa trên nghiên cứu thị trường, dữ liệu doanh số bán hàng lịch sử và xu hướng trong ngành. Xem xét các yếu tố như mùa vụ, sự phát triển của thị trường và động thái cạnh tranh.\nƯớc Tính Chi Phí và Phí: Dự đoán các chi phí hoạt động, bao gồm chi phí vốn hàng bán, chi phí hoạt động cố định và chi phí biến đổi. Xem xét lạm phát, xu hướng chi phí và các biện pháp tiết kiệm chi phí có thể thực hiện.\nThay Đổi Vốn Làm Việc: Phân tích thay đổi trong vốn làm việc, bao gồm các tài khoản phải thu, tài khoản phải trả và vòng quay tồn kho. Thay đổi trong vốn làm việc có thể ảnh hưởng đáng kể đến dòng tiền.\nLập Kế Hoạch Đầu Tư Cố Định (CapEx): Dự đoán các khoản đầu tư cố định cho việc mua sắm tài sản, thiết bị và cơ sở hạ tầng. CapEx có thể có ảnh hưởng lớn đến dòng tiền, vì vậy cần lập kế hoạch cho những khoản chi này.\nQuản Lý Nợ và Vốn Chủ Sở Hữu: Xem xét bất kỳ khoản trả nợ, vay mới hoặc cấp vốn chủ sở hữu. Dịch vụ nợ, lãi suất và vốn cổ phần ảnh hưởng đến dòng tiền.\nPhân Tích Kịch Bản: Tiến hành phân tích nhạy cảm và lập kế hoạch cho các kịch bản khác nhau để tính đến các kết quả khả dĩ khác nhau. Điều này giúp đánh giá tác động của các kịch bản khác nhau lên dòng tiền.\nTạo Báo Cáo Dòng Tiền: Phát triển báo cáo dự báo dòng tiền bao gồm dòng tiền vào và ra trong khoảng thời gian dự báo. Báo cáo này nên cung cấp cái nhìn rõ ràng về dòng tiền theo từng tháng hoặc quý.\nTheo Dõi và Đánh Giá: Liên tục theo dõi và đánh giá dòng tiền thực tế so với số liệu dự báo. Điều chỉnh dự báo khi cần dựa trên hiệu suất thực tế và thay đổi trong điều kiện thị trường.\nCông Cụ Dự Báo Dòng Tiền: Xem xét việc sử dụng phần mềm mô hình tài chính hoặc công cụ bảng tính để tối ưu hóa quá trình dự báo và thực hiện phân tích nhạy cảm.\nĐánh Giá Rủi Ro: Xác định và đánh giá các rủi ro có thể ảnh hưởng đến dòng tiền, chẳng hạn như suy thoái kinh tế, thay đổi trong hành vi của khách hàng hoặc sự cố trong chuỗi cung ứng.\nDự báo dòng tiền hiệu quả là quan trọng để đảm bảo rằng một công ty có thể đáp ứng các nghĩa vụ tài chính của mình, tận dụng cơ hội tăng trưởng và đối mặt với những thách thức tài chính. Nó cũng giúp tối ưu hóa chiến lược quản lý tiền mặt và đưa ra quyết định đầu tư dựa trên thông tin. Ngoài ra, các dự báo dòng tiền chính xác thường được yêu cầu bởi các nhà cho vay và nhà đầu tư như một phần của quá trình kiểm tra tài chính.\nEntertainment \u0026amp; Hospitality: Determining Staffing Needs Một ví dụ là việc sòng bạc và khách sạn Caesars Entertainment sử dụng phân tích dự đoán để xác định nhu cầu nhân sự của các địa điểm kinh doanh của mình vào những thời điểm cụ thể.\nTrong ngành giải trí và khách sạn, lượng khách hàng đến và đi phụ thuộc vào nhiều yếu tố khác nhau, tất cả đều ảnh hưởng đến số lượng nhân viên mà một địa điểm hoặc khách sạn cần tại một thời điểm nhất định. Việc sử dụng quá nhiều nhân lực sẽ tốn tiền và việc thiếu nhân lực có thể dẫn đến trải nghiệm khách hàng tồi tệ, nhân viên làm việc quá sức và những sai lầm tốn kém.\nĐể dự đoán số lượt nhận phòng khách sạn vào một ngày nhất định, một nhóm đã phát triển mô hình hồi quy bội xem xét một số yếu tố. Mô hình này cho phép Caesars bố trí nhân sự cho các khách sạn và sòng bạc của mình và tránh sử dụng quá nhiều nhân lực ở mức tốt nhất có thể.\nMarketing: Behavioral Targeting Trong tiếp thị, dữ liệu người tiêu dùng rất phong phú và được tận dụng để tạo nội dung, tạo quảng cáo và tạo các chiến lược nhằm tiếp cận khách hàng tiềm năng tốt hơn. Bằng cách kiểm tra dữ liệu hành vi lịch sử và sử dụng dữ liệu đó để dự đoán điều gì sẽ xảy ra trong tương lai, bạn đã sử dụng phân tích dự đoán.\nPhân tích dự đoán có thể được áp dụng trong tiếp thị để dự báo xu hướng bán hàng vào các thời điểm khác nhau trong năm và lên kế hoạch cho các chiến dịch phù hợp.\nManufacturing: Preventing Malfunction Các ví dụ trên sử dụng phân tích dự đoán để thực hiện hành động dựa trên các tình huống có thể xảy ra, nhưng bạn cũng có thể sử dụng phân tích dự đoán để ngăn xảy ra các tình huống không mong muốn hoặc có hại. Ví dụ, trong lĩnh vực sản xuất, các thuật toán có thể được đào tạo bằng cách sử dụng dữ liệu lịch sử để dự đoán chính xác khi nào một bộ phận máy móc có thể gặp trục trặc.\nKhi đáp ứng các tiêu chí cho sự cố sắp xảy ra, thuật toán sẽ được kích hoạt để cảnh báo nhân viên có thể dừng máy và có khả năng tiết kiệm cho công ty hàng nghìn, nếu không muốn nói là hàng triệu đô la chi phí sản phẩm bị hư hỏng và sửa chữa. Phân tích này dự đoán các tình huống trục trặc tại thời điểm này thay vì trước nhiều tháng hoặc nhiều năm.\nMột số thuật toán thậm chí còn đề xuất các bản sửa lỗi và tối ưu hóa để tránh các trục trặc trong tương lai và nâng cao hiệu quả, tiết kiệm thời gian, tiền bạc và công sức. Đây là một ví dụ về phân tích theo quy định; thường xuyên hơn không, một hoặc nhiều loại phân tích được sử dụng song song để giải quyết vấn đề.\nHealth Care: Early Detection of Allergic Reactions Một ví dụ khác về việc sử dụng thuật toán để phân tích dự đoán nhanh chóng nhằm phòng ngừa đến từ ngành chăm sóc sức khỏe. Viện Wyss tại Đại học Harvard hợp tác với Quỹ KeepSmilin4Abbie để phát triển một thiết bị công nghệ có thể đeo được nhằm dự đoán phản ứng dị ứng phản vệ và tự động truyền epinephrine cứu sống.\nCảm biến, được gọi là AbbieSense, phát hiện các dấu hiệu sinh lý sớm của sốc phản vệ như những yếu tố dự báo phản ứng tiếp theo — và nó thực hiện nhanh hơn nhiều so với khả năng của con người. Khi một phản ứng được dự đoán sẽ xảy ra, một phản ứng thuật toán sẽ được kích hoạt. Thuật toán có thể dự đoán mức độ nghiêm trọng của phản ứng, cảnh báo cho cá nhân và người chăm sóc, đồng thời tự động tiêm epinephrine khi cần thiết. Khả năng dự đoán phản ứng của công nghệ này với tốc độ nhanh hơn tốc độ phát hiện thủ công có thể cứu được mạng sống.\n5 Prescriptive analytics: Đưa ra, gợi ý business modelling mới, Data đưa ra lời khuyên để thay đổi mô hình kinh doanh.\nPrescriptive analytics là một loại phân tích dữ liệu trong lĩnh vực quản lý dựa trên dữ liệu (data analytics), nơi mục tiêu chính là cung cấp các hướng dẫn và đề xuất về cách thực hiện một hành động cụ thể để đạt được kết quả tối ưu dựa trên một loạt các biến số và hạn chế. Khái niệm này đặt ra câu hỏi \u0026ldquo;Nên làm gì?\u0026rdquo; và đưa ra các giải pháp hoặc hướng dẫn để đạt được mục tiêu mong muốn.\nPrescriptive analytics thường bao gồm các bước sau:\nThu thập dữ liệu: Bắt đầu bằng việc thu thập và tổng hợp dữ liệu liên quan đến vấn đề hoặc quá trình cần được tối ưu hóa. Điều này có thể bao gồm dữ liệu lịch sử, dữ liệu hiện tại và các thông tin khác liên quan.\nTiền xử lý dữ liệu: Dữ liệu thường cần được làm sạch, biến đổi và chuẩn hóa trước khi sử dụng cho phân tích. Điều này bao gồm việc loại bỏ dữ liệu không hợp lệ, xử lý giá trị thiếu và chuẩn hóa định dạng.\nXây dựng mô hình: Sử dụng các phương pháp phân tích dữ liệu phức tạp như tối ưu hóa, mô hình hóa toán học, mô hình học máy và mô hình lập kế hoạch để tạo ra các kịch bản và giải pháp tối ưu dựa trên dữ liệu và các yếu tố hạn chế.\nĐưa ra giải pháp và quyết định: Dựa trên kết quả của mô hình phân tích, prescriptive analytics đưa ra các giải pháp hoặc quyết định cụ thể để đạt được mục tiêu tối ưu. Điều này có thể bao gồm việc đề xuất kế hoạch sản xuất, quản lý tồn kho, lập lịch giao hàng, hoặc các hành động kinh doanh khác.\nTriển khai và theo dõi: Các giải pháp và quyết định được triển khai trong thực tế và theo dõi để đảm bảo tính hiệu quả và đề xuất điều chỉnh nếu cần.\nPrescriptive analytics thường được sử dụng trong nhiều lĩnh vực, như tài chính, sản xuất, dự án, y tế và quản lý chuỗi cung ứng để giúp tổ chức ra quyết định chiến lược, tối ưu hóa quá trình kinh doanh và tạo ra giá trị tối ưu.\n6 Ví dụ PRESCRIPTIVE ANALYTICS trong thực tế\nVenture Capital: Investment Decisions Các quyết định đầu tư, mặc dù thường dựa trên trực giác, nhưng có thể được củng cố bằng các thuật toán cân nhắc rủi ro và đưa ra khuyến nghị có nên đầu tư hay không.\nMột ví dụ trong lĩnh vực đầu tư mạo hiểm là một thử nghiệm kiểm tra tính hiệu quả của các quyết định của thuật toán về việc nên đầu tư vào công ty khởi nghiệp nào so với quyết định của các nhà đầu tư thiên thần.\nNhững phát hiện này có nhiều sắc thái. Thuật toán hoạt động tốt hơn các nhà đầu tư thiên thần, những người ít kinh nghiệm đầu tư hơn và kém kỹ năng hơn trong việc kiểm soát thành kiến nhận thức của họ; tuy nhiên, các nhà đầu tư thiên thần đã vượt trội hơn thuật toán khi họ có kinh nghiệm đầu tư và có thể kiểm soát những thành kiến ​​nhận thức của mình.\nThử nghiệm này làm sáng tỏ vai trò bổ sung mà phân tích theo quy định phải đóng trong việc đưa ra quyết định và tiềm năng của nó trong việc hỗ trợ việc ra quyết định khi không có kinh nghiệm và cần gắn cờ những thành kiến về nhận thức. Một thuật toán chỉ không thiên vị khi dữ liệu được đào tạo cùng với nó, do đó cần có sự đánh giá của con người cho dù có sử dụng thuật toán hay không.\nSales: Lead Scoring Prescriptive analytics đóng một vai trò nổi bật trong việc bán hàng thông qua việc chấm điểm khách hàng tiềm năng, còn được gọi là xếp hạng khách hàng tiềm năng. Ghi điểm khách hàng tiềm năng là quá trình chỉ định giá trị điểm cho các hành động khác nhau dọc theo kênh bán hàng, cho phép bạn hoặc thuật toán xếp hạng khách hàng tiềm năng dựa trên khả năng họ chuyển đổi thành khách hàng.\nCác hành động bạn có thể gán giá trị bao gồm:\nLượt xem trang\nTương tác qua email\nTìm kiếm trang web\nTương tác nội dung, chẳng hạn như tham dự hội thảo trên web, tải xuống sách điện tử hoặc xem video\nKhi gán cho mỗi hành động một giá trị điểm, hãy chỉ định số điểm cao nhất cho những hành động ngụ ý ý định mua hàng (ví dụ: truy cập trang sản phẩm) và điểm tiêu cực cho những hành động tiết lộ ý định không mua hàng (ví dụ: xem tin tuyển dụng trên trang web của bạn ). Điều này có thể giúp ưu tiên tiếp cận những khách hàng tiềm năng có nhiều khả năng chuyển đổi thành khách hàng nhất, có khả năng tiết kiệm thời gian và tiền bạc cho tổ chức của bạn.\nContent Curation: Algorithmic Recommendations Nếu bạn đã từng sử dụng một mạng xã hội hoặc ứng dụng hẹn hò, bạn có thể đã trực tiếp trải nghiệm các Prescriptive analytics thông qua các đề xuất nội dung thuật toán.\nThuật toán của doanh nghiệp thu thập dữ liệu dựa trên lịch sử tương tác của bạn trên nền tảng của họ (và có thể cả những nền tảng khác). Sự kết hợp các hành vi trước đây của bạn có thể đóng vai trò là tác nhân kích hoạt thuật toán đưa ra đề xuất cụ thể. Ví dụ: nếu bạn thường xuyên xem video đánh giá giày trên YouTube, thuật toán của nền tảng có thể sẽ phân tích dữ liệu đó và khuyên bạn nên xem nhiều hơn cùng loại video hoặc nội dung tương tự mà bạn có thể thấy thú vị.\nTrên mạng xã hội, nguồn cấp dữ liệu “Dành cho bạn” của TikTok là một ví dụ về hoạt động phân tích theo quy định. Trang web của công ty giải thích rằng các tương tác của người dùng trên ứng dụng, giống như việc ghi điểm trong doanh số bán hàng, được tính trọng số dựa trên dấu hiệu quan tâm.\n“Ví dụ”, trang web của TikTok cho biết, “nếu bạn xem hết một video, đó là dấu hiệu mạnh mẽ cho thấy bạn quan tâm. Sau đó, các video được xếp hạng để xác định khả năng bạn quan tâm đến từng video và được gửi tới từng nguồn cấp dữ liệu \u0026lsquo;Dành cho bạn\u0026rsquo; duy nhất.”\nTrường hợp sử dụng phân tích theo quy định này có thể giúp tỷ lệ tương tác của khách hàng cao hơn, mức độ hài lòng của khách hàng tăng lên và khả năng nhắm mục tiêu lại khách hàng bằng quảng cáo dựa trên lịch sử hành vi của họ.\nBanking: Fraud Detection Bài toán prescriptive analytics ở đây là phát hiện và gắng nhãn gian lận ngân hàng.\nVới khối lượng dữ liệu khổng lồ được lưu trữ trong hệ thống của ngân hàng, một người gần như không thể phát hiện thủ công bất kỳ hoạt động đáng ngờ nào trong một tài khoản. Một thuật toán—được đào tạo bằng cách sử dụng dữ liệu giao dịch lịch sử của khách hàng—phân tích và quét dữ liệu giao dịch mới để tìm những điểm bất thường. Ví dụ: có lẽ bạn thường chi 3.000 đô la mỗi tháng, nhưng tháng này, thẻ tín dụng của bạn bị tính phí 30.000 đô la.\nThuật toán phân tích các mẫu trong dữ liệu giao dịch của bạn, cảnh báo ngân hàng và đưa ra hướng hành động được đề xuất. Trong ví dụ này, hành động có thể là hủy thẻ tín dụng vì nó có thể đã bị đánh cắp.\nProduct Management: Development and Improvement Prescriptive analytics cũng có thể cung cấp thông tin cho việc phát triển và cải tiến sản phẩm. Người quản lý sản phẩm có thể thu thập dữ liệu người dùng bằng cách khảo sát khách hàng, chạy thử nghiệm phiên bản beta của sản phẩm, tiến hành nghiên cứu thị trường với những người hiện không phải là người dùng sản phẩm và thu thập dữ liệu hành vi khi người dùng hiện tại tương tác. Tất cả dữ liệu này có thể được phân tích—theo cách thủ công hoặc bằng thuật toán—để xác định xu hướng, khám phá lý do của những xu hướng đó và dự đoán liệu các xu hướng được dự đoán có tái diễn hay không.\nPhân tích theo quy định có thể giúp xác định những tính năng nào nên đưa vào hoặc loại bỏ khỏi sản phẩm và những tính năng nào cần thay đổi để đảm bảo trải nghiệm người dùng tối ưu.\nMarketing: Email Automation Tự động hóa email là một ví dụ rõ ràng về phân tích theo quy định tại nơi làm việc. Các nhà tiếp thị sử dụng tính năng tự động hóa email để sắp xếp khách hàng tiềm năng thành các danh mục dựa trên động lực, suy nghĩ và ý định của họ, đồng thời gửi nội dung email cho họ dựa trên các danh mục đó. Mọi tương tác của khách hàng tiềm năng với email đều có thể xếp họ vào danh mục khác, dẫn đến kích hoạt một nhóm thông báo khác.\nMặc dù đây chỉ là phân tích quy định theo thuật toán thuần túy, nhưng một người nên lập kế hoạch, tạo và giám sát các luồng tự động hóa. Tự động hóa email cho phép các công ty cung cấp tin nhắn được cá nhân hóa trên quy mô lớn và tăng cơ hội chuyển đổi khách hàng tiềm năng thành khách hàng bằng cách sử dụng nội dung phù hợp với động cơ và nhu cầu của họ.\nII. Sáu bước cơ bản bắt đầu một dự án Data Analytics Bước 0 : Prepair - Chuẩn bị Kiến thức ngành\nSản phẩm như thế nào\nKhách hàng là ai (who)\nKhái niệm đặc thù\nBước 1: Define analytics requirement - Tìm ra các câu hỏi cần trả lời Câu hỏi được đưa ra từ các bộ phận trong công ty\nCâu hỏi được đưa ra từ chính bạn trong quá trình các bạn làm việc trong công ty\nBước 2: Collecting data Nắm rõ dữ liệu mình đang có\nNắm rõ data mình có thể lấy được từ\nData Engineer\nData analytics lead\nData sciencetist\nTừ những người có kiến thức liên quan tới data mình cần thu thập\nBước 3: Clearning data Xử lý dữ liệu rỗng\nXử lý outliers\nXử lý giá trị sai lệch\nĐưa các giá trị số về cùng 1 range khi có sự chênh lệch quá lớn\nBước 4: Analyzing data Sử dụng các mô hình\nSegmentation -\u0026gt; clustering\nPredict prices -\u0026gt; time series\nKhách hàng tiềm ẩn -\u0026gt; classification\nĐánh giá campain -\u0026gt; A/B testing\nBước 5: Presenting Report Tạo các Reporting cho ban giám đốc và các đối tượng liên quan\nThu nhận các góp ý\nQuay lại bước 0 để chỉnh sửa\nIII. Các bài toán thông dụng nhất của DA Có 3 bài toàn thông dụng nhất, mình chỉ liệt kê tóm tắt, do mỗi mục là một chủ đề siêu to khổng lồ và có thể được tiếp tục trình bày ở các bài viết tiếp theo\nĐo lường tác động của thay đổi so với hiện tại ( Quan trọng nhất) Sự thay đổi về sản phẩm\nSự thay đổi về sáng kiến\nSử dụng A/B testing\nA lấy 1 tập nhỏ để cải tiến, gọi là A\nB là phần cũ, lúc chưa thay đổi\nDự báo ( Quan trọng) Sử dụng trong tài chính, marketing\nDự báo doanh thu của quý\nDự báo doanh thu của năm\nDự đoán giá cổ phiếu , giá vàng, sức mua \u0026hellip;.\nSử dụng mô hình time series\nPhân tích khách hàng Nhận dạng khách hàng ( Quan trọng nhất) Customer segmentation\nCustomer profiling\nSử dụng thuật toán clustering\nSử dụng mô hình RFM\nCác bạn có thể xem ví dụ trong bài viết này của mình về mô hình RFM\nhttps://www.phamduytung.com/blog/2022-12-03-marketing-with-python/\nCross selling Tìm khách hàng tiềm năng sẽ mua hàng\nCustomer propensity model\nSử dụng logistic regression\nCustomer journey Phân tích phễu khách hàng\ntìm hiểu lộ trình của khách hàng\nFunnel analysis\nBasket analytics Phân tích giỏ hàng\nMột khách hàng sẽ nhiều khả năng mua một loại hàng nào tiếp theo thì sử dụng Basket analytics\nLuật kết hợp\nĐo lường thời hạn tác động của một cải tiến / sale Survival analytics\nCảm ơn các bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\nTài liệu tham khảo\nhttps://online.hbs.edu/blog/post/descriptive-analytics\nhttps://online.hbs.edu/blog/post/diagnostic-analytics\nKhoá học nên học\nhttps://www.coursera.org/specializations/statistical-inference-for-data-science-applications\n","date":"Jul 29, 2023","img":"https://unsplash.it/1920/1080?image=6","permalink":"/blog/2023-09-02-data-analytics-step-by-step/","series":null,"tags":["Data Analytics"],"title":"Data Analytics  - Nghề Mới Thời Thượng"},{"categories":null,"content":" I. Đặt vấn đề II. Cách thức thực hiện III. Cách tiếp cận Store Capacity Clustering Store Attribute Clustering Category Sales Clustering Productivity-based Clustering Price-based Clustering Multi-dimensional clustering IV. Một số ví dụ thực tiễn Tự thiết lập phân khúc giá. Lập kế hoạch phát tờ rơi quảng cáo Bài toán tài xế giao hàng grab Bài viết này là góc nhìn của một ông IT đang tập tành Data Driven Development, viết note lại chơi chơi dựa vào các tài liệu đọc được vào từ khoá store clustering retail, viết bậy viết bạ, các bạn có ý tưởng hay ho có thể góp ý giúp mình có thêm nhiều góc nhìn hơn nha.\nI. Đặt vấn đề Trong mô hình kinh doanh bán lẻ, có hai mô hình dạng chuỗi cửa hàng khác nhau, mỗi mô hình đều có ưu và nhược điểm riêng:\nDạng chuỗi mà tất cả các cửa hàng giống hệt nhau ( ví dụ Starbucks), dạng chuỗi này thì việc vận hành đơn giản, theo quy trình có sẵn, đồng nhất, nhất quán, mang lại trải nghiệm xuyên suốt với khách hàng. Hầu như là quản lý khá nhàn.\nDạng chuỗi thứ 2, các Quản lý được xem như là ông chủ nhỏ của cửa hàng, có quyền tự quyết trong việc phân loại, định giá, và khuyến mãi. Ví dụ: các nhà bán lẻ như Tập đoàn Aeon , ICA, Metro, hoặc bách hoá xanh của Việt Nam trao khá nhiều quyền kiểm soát cho các quản lý cửa hàng.\nDù mô hình kinh doanh là dạng 1 hay dạng 2, thì việc nhóm các siêu thị có tính chất tương đồng thành các nhóm đồng nhất với nhau về các khía cạnh nào đó ( trong machine learning gọi là gom cụm), cũng giúp cho người điều hành có thể scale được các chiến lược marketing, sử dụng cách tiếp cận cookie-cutter (( từ khoá cookie cutter approach in business) đại loại là nếu 1 mô hình marketing thành công cho cửa hàng A, thì cũng có thể scale out và thành công ở các cửa hàng A1.. An có cùng cụm với cửa hàng A.) , cách tiếp cận này cũng có thể áp dụng với mô hình kinh doanh nhượng quyền (franchisee).\nII. Cách thức thực hiện Khái niệm phân loại cửa hàng không mới, nhưng có rất ít nhà bán lẻ ở các thị trường nhỏ sử dụng các phương thức khoa học để tạo mô hình. Và rất rất ít nhà bán lẻ hệ thống hoá quy trình phân loại | gom cụm, có thể là lý do tài chính không cho phép, hoặc việc thuê đơn vị tư vấn độc lập khá tốn kém. Mô hình mình đề cập tới ở đây là: Data Science lập phân tích, Business kiểm chứng, và cần có sự thông suốt về mặt data giữa Data Science và Business.\nCác yếu tố tham chiếu chính:\n% bán hàng trên deal với nhà cung cấp\n% tỷ lệ bán hàng hỗn hợp hợp(category mix) \u0026hellip;\nCác yếu tố tham chiếu bổ sung bên cạnh đó:\nSự hiện diện của đối thủ\nVị trí địa lý\nNhân khẩu học \u0026hellip;\nỞ đây, mình chỉ liệt kê một vài yếu tố, một vài yếu tố chi tiết được liệt kê ở phần tiếp theo.\nCác yếu tố tham chiếu bổ sung sẽ giúp chúng ta phân biệt các cụm rõ ràng hơn và nhìn thấy sự khác nhau giữa các cụm đó.\ntheo solvoyo.com, ngày nay, người ta thường phân loại theo category level thay vì store level. Category level mạng lại sự chính xác cao hơn, nhưng khi thực hiện lại phức tạp hơn\nSau đó, chúng ta sẽ xây dựng model dựa trên các yếu tố trên, và kiểm chứng model. Có một vài mô hình clustering trong machine learning các bạn có thể sử dụng như KNN, SVC, DBScan, kmean \u0026hellip;\nThông qua việc sử dụng phân cụm, các nhà bán lẻ có thể số hoá nhu cầu của từng khu vực và đề ra những phương án vận hành (mới) hiệu quả hơn (tất nhiên, đây mới chỉ là phần lý thuyết, cần thử nghiệm để chứng minh tính đúng đắn xem rằng các cụm chúng ta đề xuất có thật sự liên quan mạnh với nhau hay không, hay còn yếu tố ẩn quan trọng nào đó mà chúng ta đã bỏ lỡ trong quá trình thu thập dữ liệu).\nVí dụ: một chuỗi cửa hàng tiện lợi có thể điều chỉnh cách tiếp cận khách hàng bằng cách bán hàng theo giỏ hàng, chia thành gói giỏ hàng buổi sáng, giỏ hàng buổi trưa, giỏ hàng buổi tối, phù hợp với mục tiêu mua sắm và vị trí của khách hàng.\nVí dụ khác: Cửa hàng cà phê mới, cửa hàng bánh ngọt mới, cửa hàng phở mới, ở gần vị trí địa lý, kích thước gần giống nhau, nhưng triển vọng phát triển có thể sẽ khác nhau, do triển vọng phát triển của thực phẩm mang đi khác hoàn toàn với thực phẩm ăn tại chỗ.\nViệc phân cụm cửa hàng là bước đầu giúp cho các nhà quản lý có một bức tranh ướm chừng khi đưa ra quyết định, và giúp họ đưa ra kết luận một cách định lượng hơn.\nIII. Cách tiếp cận Store Capacity Clustering Net selling space ( đo lượng trên đơn vị mét vuông)\nShelf capacity ( đo lượng trên đơn vị số lượng sản phẩm / đơn vị chuẩn (kệ))\nOption capacity ( số lượng sku trên mỗi cửa hàng có thể được trưng bày tại 1 thời điểm)\nUnit capacity ( Số lượng sản phẩm có thể trưng bày tại 1 thời điểm)\nStore Attribute Clustering Lượt khách trung bình của cửa hàng\nVị trí ( độ thị, nông thôn, khu công nghiệp, khu chế xuất, khu xây dựng mới)\nIncome profiles\nCultural profiles ( xét trên khách hàng trung thành, ví dụ như dân tộc)\nMall type ( đại siêu thị, cửa hàng nhỏ)\n\u0026hellip;\nCategory Sales Clustering Weekly unit sales\nWeekly revenue\nInventory turn (dựa trên yếu tố bao lâu hàng về một lần)\nProductivity-based Clustering Revenue per square meter (or sq. foot)\nGM per square meter\nUnit sales per product (option, or SKU)\nPrice-based Clustering Average unit retail\nPrice profile performance (dựa trên sự đóng góp của doanh số bán hàng từ các phân khúc giá khác nhau như cao, trung bình, thấp hoặc dựa trên sự đóng góp của doanh số giảm giá và bán hàng nguyên giá)\nPrice elasticity (dựa trên sự thay đổi về nhu cầu để đáp ứng với những thay đổi về giá, thường được sử dụng trong các hệ thống lập kế hoạch khuyến mãi và tối ưu hóa giảm giá)\nMulti-dimensional clustering Đây là phương pháp nhóm các cửa hàng dựa trên các metric và các attributes được đề cập ở phía trên.\nVí dụ, với mỗi category của cửa hàng, có thể được phân cụm dựa vào sale performance, capacity, price performance\nIV. Một số ví dụ thực tiễn Tự thiết lập phân khúc giá. Các nơi có tỷ lệ khách hàng mua một lần cao thường sẽ thiết lập một mức giá cao hơn những nơi có tỷ lệ khách hàng trung thành cao. Ví dụ:\nCác website hãng\nSản phẩm được bán ở sân bay so với bán ở các cửa hàng xung quanh nơi bạn sinh sống.\nTất nhiên, sẽ có nhiều bạn sẽ cải lại rằng giá ở sân bay cao do các chi phí thuê nhân viên, chi phí mặt bằng, chi phí từ abc đến xyz cao \u0026hellip;\nViệc phân cụm, định danh các nhóm cửa hàng có đặc điểm riêng biệt như trên, ví dụ như cửa hàng liền kề khu du lịch, sẽ giúp các bạn có góc nhìn khoa học hơn vào quy hoạch phân vùng giá hoặc quy trình bán hàng.\nLập kế hoạch phát tờ rơi quảng cáo Tối ưu hoá chi phí in ấn\nĐa dạng hoá tờ rơi\nDự đoán được tỷ lệ mua hàng , đề ra chiến lược mua bán hợp lý hơn.\nBài toán tài xế giao hàng grab Trong bất kỳ sự thay đổi nào về mặt vận hành và quản lý, chúng ta nên đo lường sự tăng trưởng và kiểm định lại để chứng minh giá trị thay đổi là đúng đắn và hỗ trợ tốt cho việc triển khai ra quy mô lớn.\nCảm ơn các bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\nTài liệu tham khảo\nhttps://analyticsindiamag.com/analytics-driven-store-clustering-sales-and-profits-retail/\nhttps://www.solvoyo.com/whitepapers/approaches-to-retail-store-clustering/\nhttps://www.davinciretail.com/resources/what-is-retail-store-clustering\nhttps://www.forbes.com/sites/forbestechcouncil/2022/04/20/clustering-the-new-world-of-retail-product-segmentation\n","date":"Jul 29, 2023","img":"https://unsplash.it/1920/1080?image=7","permalink":"/blog/2023-07-29-store-clustering/","series":null,"tags":["Retail"],"title":"Phân Cụm Cửa Hàng Để Đưa Ra Quyết Định Thông Minh Hơn - Lý Thuyết"},{"categories":null,"content":" Đặt vấn đề Mở bài Project là gì Product là gì 1. Một số hướng dẫn để chuyển tư duy từ project mindset sang product mindset 1.1 Tạo ra các team sản phẩm nhỏ 1.2 Đón nhận sự thay đổi và chấp nhận sự thích nghi 1.3 Đừng dí dealine 1.4 Đừng cố gắng bấu víu vào ý tưởng đầu tiên 1.5 Cố gắng gắng kết roadmap với mindset 2. Một số kinh nghiệm về product mindset 2.1 Trao giá trị, không trao tính năng 2.2 Phát triển sản phẩm dựa trên dữ liệu (Data Driven development) 2.3 Tập trung vào sản phẩm Phát triển UX 2.4 Minimum Viable Product Các xác định Minimum Viable Product Ví dụ các công ty khởi nghiệp với Minimum Viable Product Airbnb Foursquare Tài liệu tham khảo Đặt vấn đề Có bao giờ bạn gặp một trong các trường hợp sau đây:\nBạn release một sản phẩm ra thị trường và nó đã lỗi thời lúc bạn release\nBạn release một sản phẩm, và người dùng không thèm sử dụng.\nNếu bạn gặp vấn đề trên, hãy học bài viết này, nếu không bạn hãy đọc các bài viết khác trong website của mình www.phamduytung.com\nLưu ý nhỏ: Đây là mindset, không phải là toolset, nên chúng ta cần thực hành nó, cần sự trải nghiệm trong câu \u0026ldquo;kiến thức, kinh nghiệm, trải nghiệm\u0026rdquo; thì mới thấm dần dần được. Tại thời điểm viết bài viết này, mình chỉ có một xíu xíu trải nghiệm như vậy, có thể qua vài năm nữa, trải nghiệm của mình sẽ khác đi, và mình sẽ viết bài viết khác để cập nhật sự trải nghiệm của mình.\nMở bài Từ trước tới nay, chúng ta thường vô tình bị đóng trong cái khung tư duy project, đặc biệt là các bạn trưởng thành từ freelancer. Hầu hết chúng ta là \u0026ldquo;lính đánh thuê\u0026rdquo;, với kiểu làm dự án A trong vòng 3 tháng, xong , lụm tiền. Nhảy vào dự án B, làm, lụm tiền. \u0026hellip; cho đến khi bạn ra ngoài khởi nghiệp với một ý tưởng hay ho nào đó, hoặc bạn vào công ty product làm chỉ đúng 1 product.\nProject là gì Các dự án, theo định nghĩa, là những nỗ lực tạm thời. Một tập hợp các cá nhân và tổ chức đảm nhận các nhiệm vụ cần thiết để đạt được một mục tiêu cụ thể. Có một lịch trình, ngân sách, điểm khởi đầu và điểm cuối. Tiến độ của dự án được đo lường và kết thúc sau khi đạt được các mục tiêu và cột mốc định trước.\nNói tóm lại, một dự án bắt đầu với một kết luận được xác định rõ ràng và được hiểu rõ. Từ đó, nhóm phấn đấu để tránh đi chệch khỏi kế hoạch hoặc lịch trình. Các bên liên quan biết điều gì sẽ xảy ra, khi nào và chi phí bao nhiêu.\nProduct là gì Sản phẩm là mối quan tâm liên tục. Mục tiêu của sản phẩm không phải là khi nào nó kết thúc. Các tổ chức sẽ sử dụng nghiệp vự tối ưu hoá của mình để cực đại hoá lợi nhuận và các KPIs liên quan, bằng cách phân tích mức độ sử dụng (usage), sự tăng trưởng (growth), và sự trung thành của người dùng (loyalty)\nMột sản phẩm mới có thể là kết quả của một dự án hoặc nhiều dự án, nhưng sau khi sản phẩm được giới thiệu, vẫn có nhiều cải tiến và cập nhật lặp đi lặp lại. Công việc của nhóm sản phẩm sẽ không bao giờ kết thúc chừng nào sản phẩm vẫn còn trên thị trường.\nTóm lại, các sản phẩm liên tục phát triển dựa trên các bài học, phản hồi, sự thay đổi của thị trường ° và các nguyên tắc cơ bản về tài chính °° (financial fundamentals). Luôn có chỗ để cải tiến, nâng cao và mở rộng sang các thị trường và trường hợp sử dụng mới.\n° Ví dụ như covid ập tới, chúng ta phải làm gì\n°° Ví dụ như thuế gtgt giảm từ 10% xuống 8%\nĐối với các nhóm khởi nghiệp, sản phẩm hiếm khi nào hoàn thành ngay lập tức ( trừ một số trường hợp đặc biệt). Hầu hết chúng ta thường đưa ra thị trường một bản thăm dò thị trường (có thể có 1 hoặc nhiều phrases, mỗi pharse sẽ hoàn thiện một tính năng nào đó) để đánh giá.\n1. Một số hướng dẫn để chuyển tư duy từ project mindset sang product mindset 1.1 Tạo ra các team sản phẩm nhỏ Nếu team member là coder , thua. Chúng ta đều có chính kiến, đều có niềm tự hào, đều có suy nghĩ , đều có ý tưởng | giải pháp \u0026ldquo;điên rồ\u0026rdquo; cho một vấn đề nào đó.\nCó một tầng nhu cầu trong tháp nhu cầu maslow gắng liền sâu xa với cái này, mình cảm thấy vậy.\n1.2 Đón nhận sự thay đổi và chấp nhận sự thích nghi Tư duy sản phẩm bắt nguồn từ việc tạo ra trải nghiệm độc đáo cho người dùng dựa trên việc học hỏi và phản hồi. Để thúc đẩy môi trường này, nhóm phải luôn cởi mở để thường xuyên điều chỉnh kế hoạch\u0026hellip; và đôi khi loại bỏ chúng hoàn toàn.\n1.3 Đừng dí dealine Áp lực tạo ra kim cương, tạo ra sự đột phá hay áp lực tạo nên tâm lý sợ hãi, cuộc sống bế tắc, chán nản, sợ ngày chủ nhật, sợ ngày X phải release sản phẩm \u0026hellip;.\nTuỳ\n1.4 Đừng cố gắng bấu víu vào ý tưởng đầu tiên Tư duy của product là tư duy mở, đón nhận những sự thay đổi một cách liên tục, cho nên sản phẩm khi hoàn thành đến tay người dùng nó có thể đi xa một vạn tám ngàn dặm so với ý tưởng ban đầu của mình rồi.\n1.5 Cố gắng gắng kết roadmap với mindset Có một thứ gọi là Theme-Based Roadmap được sử dụng trong tư duy product. Nó sẽ được sử dụng để thay thế cho feature roadmap\n2. Một số kinh nghiệm về product mindset 2.1 Trao giá trị, không trao tính năng Hầu hết, người dùng không cần tới những tính năng, họ cần những thứ mà có thể giải quyết được vấn đề của họ. Chúng ta cần thực sự hiểu được giá trị thực sự mà những thứ chúng ta làm ra sẽ mang lại gì cho người dùng, lúc đó, team sẽ thực sự hiểu tại sao cần phải làm nó và đó mới là sản phẩm có ích.\nĐể trao được các giá trị có ích, team member cần biết được vấn đề là gì. BA hoặc PO hoặc PM phải hiểu được việc cần làm mang lại giá trị gì. Dev cũng cần hiểu \u0026ldquo;vấn đề là gì\u0026rdquo; để đưa được các lập trình phù hợp và có thể góp ý ngược lại cho BA.\nNếu dev chỉ hiểu tính năng -\u0026gt; BA cực -\u0026gt; DEV yêu cầu BA mô tả phải thật sự rõ ràng. (dev lúc này chỉ là ông coder)\nNếu phương án của BA không hợp lý -\u0026gt; vấn đề chưa được giải quyết trọn vẹn.\nTam sao thất bản giữ các lần trao đổi , từ đối tác =\u0026gt; Sale =\u0026gt; BA =\u0026gt; Dev -\u0026gt; Dev cần có mặt trong các cuộc thảo luận của BA với khách hàng, với sale (cái này hơi khó, nhất là khi làm dự án với đối tác Nhật)\nThiếu định nghĩa / mô tả rõ ràng về việc hoàn thành tính năng -\u0026gt; bem nhau -\u0026gt; toang.\n2.2 Phát triển sản phẩm dựa trên dữ liệu (Data Driven development) Data Driven là một thuật ngữ kinh doanh đề cập đến việc sử dụng dữ liệu để cung cấp thông tin giúp bạn ra quyết định nhanh hơn. Nói cách khác, quyết định được đưa ra với bằng chứng thực nghiệm thực tế chứ không phải suy đoán hoặc kinh nghiệm cá nhân.\nData driven là thứ bắt buộc phải làm để có thể xây dựng sản phẩm thành công. Mọi thứ cần được đo đạc ở tất cả các khâu, và là nền tảng của việc ra quyết định.\nViệc đánh giá một tính năng có hoàn thành hay không cũng phải dựa vào con số, phải định lượng được, không thể phan bừa phán bậy được. Nếu chúng ta không định lượng được kết quả bằng một con số cụ thể thì việc ta có làm hay không làm sẽ chẳng khác gì nhau cả.\nChúng ta cần chọn thang đo để đánh giá, nếu chọn sai thang đó thì giá trị của dữ liệu rất thấp. Thang đo phải được đánh giá rất cần thận để có được dữ liệu có giá trị. Dữ liệu cần được đặt trong một ngữ cảnh cụ thể thì mới phát huy được hết giá trị của dữ liệu. Còn nếu không có ngữ cảnh, thì dữ liệu đó cũng chỉ là dữ liệu rác mà thôi.\nThang đo này sẽ sử dụng các giá trị chúng ta đã thu thập ở trên, để đưa ra nhận xét.\nVí dụ kinh điển: Khi cần dev một sản phẩm mobile mới mang lại giá trị ABCD \u0026hellip; cho khách hàng, team dev bị vướng vào bài toán, chọn react-native, hay flutter, hay kortin + swift \u0026hellip;\nĐể đánh giá, chúng ta cần phải xây dựng một thang đo dựa trên dữ liệu định lượng. ví dụ công nghệ có ổn định? có nhiều thư viện hỗ trợ?, có cộng đồng mạnh? có group telegram lớn hơn 100 ngàn dev , có tay to ở sau chống lưng, \u0026hellip;.\n2.3 Tập trung vào sản phẩm Cái khác hàng mong muốn là là sản phẩm, là giá trị, khách hàng không quan tâm công sức chúng ta bỏ ra là bao nhiêu, chúng ta đã tối ưu code bằng giải thuật này giải thuật nọ, chúng ta sử dụng công nghệ A, công nghệ B,\u0026hellip; hầm bà lằng.\nNói một cách hơi phũ, công nghệ có tốt tới mức nào mà không giải quyết dược vấn đề của khách hàng thì nó là vô nghĩa. Bản thân mình cũng hay bị chú tâm quá mức vào công nghệ, kéo dữ liệu, kéo source code về để thử tính năng này , chức năng kia, bla, bla, bla, mà không nhận thức rằng nó không phải là mối quan tâm đầu tiên. Ở đây mình cần mở ngoặc một chút là nếu công nghệ đưa ra mà tạo ra giá trị lớn thì lúc nào cũng được welcome.\nPhát triển UX Một sản phẩm tốt mà có UX tồi thì cũng khó có khả năng lôi kéo, giữ chân khách hàng, cái này để lại cho các bạn trải nghiệm để mời ông Designer vào :)\n2.4 Minimum Viable Product Minimum Viable Product MVP, là một sản phẩm có đủ tính năng để thu hút khách hàng chấp nhận sớm và xác nhận ý tưởng sản phẩm sớm trong chu kỳ phát triển sản phẩm. Trong các ngành công nghiệp như phần mềm, MVP có thể giúp nhóm sản phẩm nhận được phản hồi của người dùng càng nhanh càng tốt để lặp lại và cải thiện sản phẩm.\nEric Ries, người đã giới thiệu khái niệm MVP như một phần của phương pháp Lean Startup của mình, mô tả mục đích của MVP theo cách này: Đây là phiên bản của một sản phẩm mới cho phép một nhóm thu thập số lượng tìm hiểu tối đa được xác nhận về khách hàng với ít nỗ lực nhất.\nMột công ty có thể chọn phát triển và phát hành một sản phẩm khả thi tối thiểu vì nhóm sản phẩm của họ muốn:\nPhát hành sản phẩm ra thị trường càng nhanh càng tốt\nThử nghiệm ý tưởng với người dùng thực trước khi cam kết ngân sách lớn cho sự phát triển đầy đủ của sản phẩm\nTìm hiểu những gì cộng hưởng với thị trường mục tiêu của công ty và những gì không\nNgoài việc cho phép công ty của bạn xác nhận ý tưởng cho một sản phẩm mà không cần xây dựng toàn bộ sản phẩm, MVP cũng có thể giúp giảm thiểu thời gian và nguồn lực mà bạn có thể cam kết\nCác xác định Minimum Viable Product Làm thế nào để bạn phát triển MVP và làm thế nào để nhóm của bạn biết khi nào bạn có MVP sẵn sàng ra mắt? Dưới đây là một vài bước chiến lược cần thực hiện.\nĐảm bảo MVP theo kế hoạch phù hợp với mục tiêu kinh doanh Trước khi cân nhắc những tính năng nào cần xây dựng, bước đầu tiên trong việc phát triển MVP là đảm bảo sản phẩm sẽ phù hợp với mục tiêu chiến lược của nhóm hoặc công ty bạn.\nNhững mục tiêu đó là gì? Bạn đang làm việc hướng tới một con số doanh thu trong sáu tháng tới? Bạn có nguồn lực hạn chế? Những câu hỏi này có thể ảnh hưởng đến việc liệu bây giờ có phải là lúc để bắt đầu phát triển MVP mới hay không.\nNgoài ra, hãy hỏi mục đích của sản phẩm MVP này sẽ phục vụ gì? Ví dụ: nó sẽ thu hút người dùng mới trong một thị trường liền kề với thị trường cho các sản phẩm hiện có của bạn? Nếu đó là một trong những mục tiêu kinh doanh hiện tại của bạn, thì kế hoạch MVP này có thể khả thi về mặt chiến lược.\nNhưng nếu ưu tiên hiện tại của công ty bạn là tiếp tục tập trung vào các thị trường cốt lõi của bạn, bạn có thể cần phải gác lại ý tưởng này và thay vào đó, tập trung vào một MVP được thiết kế để cung cấp chức năng mới cho khách hàng hiện tại của bạn.\nBắt đầu xác định các vấn đề cụ thể mà bạn muốn giải quyết hoặc các cải tiến bạn muốn kích hoạt cho tính cách người dùng của mình. Bây giờ bạn đã xác định kế hoạch MVP phù hợp với mục tiêu kinh doanh của mình, bạn có thể bắt đầu suy nghĩ thông qua các giải pháp cụ thể mà bạn muốn sản phẩm của mình cung cấp cho người dùng. Những giải pháp này, mà bạn có thể viết trong câu chuyện của người dùng, không đại diện cho tầm nhìn tổng thể của sản phẩm — chỉ là tập hợp con của tầm nhìn đó. Chúng ta chỉ có thể phát triển một lượng nhỏ chức năng cho MVP của mình.\nBạn sẽ cần phải có chiến lược trong việc quyết định chức năng hạn chế nào sẽ đưa vào MVP của mình. Bạn có thể dựa trên các quyết định này dựa trên một số yếu tố, bao gồm:\nNghiên cứu người dùng\nPhân tích cạnh tranh\nBạn sẽ có thể lặp lại một số loại chức năng nhanh như thế nào khi nhận được phản hồi của người dùng\nChi phí tương đối để thực hiện các câu chuyện người dùng\nChuyển chức năng MVP của bạn thành một kế hoạch hành động phát triển. Bây giờ bạn đã cân nhắc các yếu tố chiến lược ở trên và giải quyết chức năng hạn chế mà bạn muốn cho MVP của mình, đã đến lúc chuyển điều này thành một kế hoạch hành động để phát triển.\nLưu ý: Điều cần thiết là phải ghi nhớ chữ V trong MVP — sản phẩm phải khả thi. Điều đó có nghĩa là nó phải cho phép khách hàng của bạn hoàn thành toàn bộ nhiệm vụ hoặc dự án và cung cấp trải nghiệm người dùng chất lượng cao. MVP không thể là giao diện người dùng với nhiều công cụ và tính năng được xây dựng dở dang. Nó phải là một sản phẩm hoạt động mà công ty của bạn sẽ có thể bán.\nVí dụ các công ty khởi nghiệp với Minimum Viable Product Có hai ví dụ kinh điển thường được nhắc tới, đó là:\nAirbnb Khởi đầu việc kinh doanh với một số vốn ít ỏi, các nhà sáng lập Airbnb đã sử dụng chính những căng hộ của họ để kiểm chứng ý tưởng của họ để tạo ta market offering short-term, peer-to-peer rental housing online (mình để từ tiếng anh ở đây , là từ khoá cho các bạn tham khảo). Họ tạo ra một trang web rất tối giản, quăng lên đó hình và chi tiết về ngôi nhà của họ, và gần như ngay lập tức, đã có khách hàng trả tiền cho dịch vụ của họ.\nGiả sử trong trường hợp vài tháng sau không có ai thuê nhà, các nhà sáng lập Airbnb sẽ làm gì tiếp theo nhỉ? :) :)\nFoursquare Foursquare là một mạng xã hội được sáng lập năm 2009, được ra đời với một MVP duy nhất: \u0026ldquo;offering only check-ins and gamification rewards\u0026rdquo; - check-in -\u0026gt; kiểm điểm -\u0026gt; nhận thưởng.\nNhóm phát triển Foursquare bắt đầu thêm vào các tính năng recommendations, city guides đến khi nọ kiểm chứng dược ý tưởng của mình, thước đo của kinh doanh là sự tăng trưởng của người sử dụng dịch vụ.\nTài liệu tham khảo https://hbr.org/2020/05/approach-your-data-with-a-product-mindset\nhttps://www.productplan.com/learn/product-mindset-vs-project-mindset/\nhttps://www.productplan.com/glossary/minimum-viable-product/\nhttps://www.productplan.com/learn/theme-based-roadmap/\nCảm ơn các bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\n","date":"Jul 23, 2023","img":"https://unsplash.it/1920/1080?image=9","permalink":"/blog/2023-07-23-product-mindset/","series":null,"tags":["mindset"],"title":"Tư Duy Làm Sản Phẩm - The Product Mindset"},{"categories":null,"content":" I. Giới thiệu II. N-gram Tokenizer III. Sử dụng N-gram trong trường hợp tìm kiếm IV. Các phương pháp thay thế n-gram V. Kết luận I. Giới thiệu Chi phí liên quan đến n-gram tokenizer ở ElasticSearch và opensearch thường không đề cập chi tiết trong các tài liệu, do đó, có khi nó sẽ gây ra các hậu quả khá nghiêm trọng về chi phí và hiệu năng. Dẫn đến trường hợp là chúng ta phải \u0026ldquo;lấy thịt đè người\u0026rdquo; bằng cách tăng chi phí phần cứng một cách lãng phí. Trong bài viết này, chúng ta sẽ đề cập đến vài use-case sử dụng n-gram tokenizer, một số phương pháp cải tiến, hoặc một vài phương pháp thay thế nó bằng cách khách hiệu quả hơn.\nNgày nay, Elasticsearch và OpenSearch là hai engines được nhiều công ty sử dụng để làm tìm kiếm văn bản nội bộ, làm bộ máy tìm kiếm chính để sử dụng nội bộ hoặc cung cấp dịch vụ cho khách hàng bên ngoài.\nHầu hết các lập trình viên sẽ sử dụng hàm analyzers và tokenizers mặc định do Elasticsearch và OpenSearch cung cấp sẵn, để chia nhỏ đoạn văn bản thành các token. Ví dụ như câu \u0026ldquo;Đây là năm rất lạ lùng\u0026rdquo; khi sử dụng analyzer mặc định thì sẽ chia thành danh sách các từ [ Đây, là, năm, rất, lạ, lùng], mỗi từ trong danh sách các từ trên đều có thể dễ dàng được search. Đây là cái mà chúng ta thường gọi là \u0026ldquo;full-text-search\u0026rdquo;, là tìm kiếm văn bản bằng dựa trên một hoặc một vài từ có tồn tại trong đoạn văn bản đó.\nTrong một số trường hợp hợp, người ta sẽ sử dụng các analyzer đặc biệt để làm các công việc đặc biệt, không phải là full text search.\nMột trong những thành phần đặc biệt trong elasticsearch và open search là n-gram tokenizer. Hãy điểm qua một vài ứng dụng của nó mà người cấu hình thường hay sử dụng sai\nII. N-gram Tokenizer N-gram Tokenizer tạo ra một nhóm các ký tự, những token nó tạo ra không nhất thiết là những từ giống như analyzer tiêu chuẩn, nó chứa những từ liên tiên tiếp nhau, chiều dài của token phụ thuộc vào N. Ví dụ trong trường hợp N = 2 và từ của cúng ta là \u0026ldquo;lạ lùng\u0026rdquo;, chúng ta có các token là [l, lạ, ạ, \u0026ldquo;ạ \u0026ldquo;, \u0026quot; \u0026ldquo;, \u0026quot; l\u0026rdquo;, l, lù, ù, ùn, n, ng, g]\nBạn có thể thấy rằng, thay vì chỉ tạo ra hai token [lạ, lùng], n-gram sẽ chia dữ liệu thành nhóm các ký tự. Phụ thuộc vào N mà ta có số lượng token khác nhau, Trong ví dụ trên, chúng ta có 17 token với N = 2 , nghĩa là số lượng token đã tăng hơn 6 lần. Trong trường hợp N=3, N=4, hoặc trong trường hợp từ cần index dài hơn, số lượng token còn bị nhân lên gấp nhiều lần nữa\nIII. Sử dụng N-gram trong trường hợp tìm kiếm Có rất nhiều tư vấn trên mạng về cách sử dụng n-gram, và các tư vấn trên thường xoay quanh các chủ đề sau\nPhát hiện lỗi chính tả Việc gõ văn bản sai chính tả là một vấn đề thường gặp, ngay cả các báo chí chính thống cũng gặp trường hợp trên. Ví dụ người dùng có thể gõ sai từ \u0026ldquo;apple\u0026rdquo; thành \u0026ldquo;aple\u0026rdquo; (điện thoại apple). Việc sử dụng n-grams sẽ giúp ta giải phát hiện từ bị gõ sai, trong khi đó, analyzer mặc định sẽ không phát hiện ra.\nTìm kiếm trong lúc gõ Thực hiện việc search trong lúc người dùng gõ trên thanh tìm kiếm. Nó sẽ tìm kiếm trước các kết quả hợp lệ ngay cả khi người dùng chưa hoàn tất việc tìm kiếm. Ví dụ gợi ý từ khoá \u0026ldquo;iphone 14 promax\u0026rdquo; khi người dùng chỉ mới gõ đến từ \u0026ldquo;ipho\u0026rdquo;\nPrefix searches Tìm kiếm văn bản bắt đầu của từ, ví dụ người dùng gõ \u0026ldquo;ip\u0026rdquo; thì sẽ khớp với \u0026ldquo;iphone\u0026rdquo;, khi từ \u0026ldquo;ip\u0026rdquo; được index là token của từ \u0026ldquo;iphone\u0026rdquo;. Prefix search chỉ lấy index, không thực hiện prefix query\nSuffix searches Đối lập với Prefix searches, đôi lúc chúng ta sẽ cần tìm kiếm các ký tự ở cuối, ví dụ như biển số xe (thường người dùng sẽ không nhớ phần ký hiệu và ký số đầu, ví dụ 50A1), số điện thoại ( tìm kiếm 4 ký tự cuối).\nInfix searches Tương tự như trên, nhưng tìm ở giữa.\nTrong những trường hợp trên, lập trình viên hay được tư vấn là xài n-gram. N-gram có thể giải quyết các vấn đề trên, nhưng chúng ta có thể sử dụng nhiều các khác hiệu quả hơn.\nLý do không nên xài n-gram là vì sự bùng nổ token do chính n-gram mang lại, dẫn đến chúng ta cần tiêu tốn nhiều tài nguyên như CPU, RAM để xử lý index, tạo token trong lúc index và search, tốn nhiều ổ cứng để lưu trữ. Cuối cùng, hiệu năng truy vấn sẽ giảm.\nIV. Các phương pháp thay thế n-gram Chúng ta sẽ xem xét từng trường hợp cụ thể\nGõ sai chính tả Thay vì sử dụng n-gram, chúng ta có thể sử dụng term suggester và phrase suggester trong elastic search, link https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#phrase-suggester, https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#term-suggester, đơn giản.\nTìm kiếm trong lúc gõ Cái này thì chúng ta xài n-gram cũng được, nhưng mà elastic search có hỗ trợ cho chúng ta một vài tiện ích đơn giản hơn nhiều, chúng ta không cần phải vắt óc suy nghĩ cấu hình n bằng bao nhiêu. Đó là sử dụng trường dữ liệu search-as-you-type https://www.elastic.co/guide/en/elasticsearch/reference/current/search-as-you-type.html. Hoặc chúng ta có thể sử dụng completion suggester và context suggester, link https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#completion-suggester , https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#context-suggester\nPrefix searches Elastic cũng hỗ trợ sẵn luôn, đó là Prefix queryedit https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html\nSuffix searches Chỗ này chúng ta sẽ sử dụng combo Reverse token filter và Match phrase prefix query https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-reverse-tokenfilter.html, https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase-prefix.html#query-dsl-match-query-phrase-prefix\nVí dụ như chúng ta có số điện thoại 0902987235, chúng ta sẽ Reverse token filter thành 5327892090, 4 số cuối cần tìm là 7235 sẽ bị reverser thành 5327, thực hiện Match phrase prefix query 5327, chúng ta sẽ tìm được 5327892090 , Reverse lại ra chuỗi số điện thoại cần tìm.\nInfix searches Đây là ông tốn nhiều chi phí nhất, với sql engine, chúng ta xài từ khoá like dẫn đến bị mất index, với n-gram, chúng ta phải index hết toàn bộ token, vào. Trong elastic có hỗ trợ chúng ta Word delimiter graph token filter, giải quyết cái này dễ dàng https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-graph-tokenfilter.html\nV. Kết luận Với các công nghệ trên, chúng sẽ giúp chúng ta nhàn hơn khi sử dụng elastic, opensearch. Các bạn nếu có đang bị những vướng mắc trên, hãy thử các cách được đề xuất, biết đâu bất ngờ sẽ xảy ra.\nNguồn: https://blog.bigdataboutique.com/2023/01/dont-use-n-gram-in-elasticsearch-and-opensearch-6f0b48\nCảm ơn các bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\n","date":"Mar 25, 2023","img":"https://unsplash.it/1920/1080?image=10","permalink":"/blog/2023-03-25-elasticsearch-opensearch-ngram/","series":null,"tags":["RFM","Recommendation","Marketing"],"title":"N-Gram Trong Elastic Search Và Opensearch - Khi Nào Không Nên Sử Dụng"},{"categories":null,"content":" Lời mở đầu Symbolic AI là gì Connectionist AI là gì Chúng ta nên chọn cái nào Hướng phát triển tiếp theo của AI Tham khảo Lời mở đầu Dạo gần đây, khi các ứng dụng AI đang hô mưa gọi gió trên toàn cõi, điển hình là hot keywork chatGPT, thì trong cộng đồng nghiên cứu cũng nổ ra cuộc chiến giữa hai phe Symbolic AI và Connectionist AI. Có vẻ như ở nơi nào có chia nhóm, thì sẽ có một nhóm người chọn phe này, và một nhóm khác chọn phe còn lại, một nhóm khác nữa đứng ở cả hai, nhóm khác nữa không chọn nhóm nào cả. Hai nhóm là nhóm đứng cả hai và nhóm không chọn nhóm nào cả thường ít hoặc không làm gì cả, còn nhóm chọn phe này và nhóm chọn phe kia sẽ đối đầu nhau rất gay gắt.\nDưới sự cường điệu của giới truyền thông, cùng với việc nổi như cồn của những ứng dụng được PR một cách mạnh mẽ, thì nhóm Connectionist AI đang bị xem là \u0026hellip; AI.\nSự thật là mỗi nhóm thuật toán đều có chỗ đứng của nó. Không có một thuật toán AI nào toàn năng ở thời điểm hiện tại, giúp giải các bài toán, hay nói cách khác là chúng ta không có \u0026ldquo;viên đạn bạc\u0026rdquo; nào trong AI. Mỗi công cụ đều có điểm mạnh và điểm yếu riêng, và việc chúng ta sử dụng đúng công cụ sẽ là chìa khoá cho sự thành công.\nSymbolic AI là gì Nhóm thuật toán này đôi khi còn được gọi là GOFAI (Good Old Fashioned A.I.), không nên hiểu nó theo nghĩa là công nghệ cũ hay công nghệ lỗi thời. Mà nên hiểu là nó là cách tiếp cận cổ điển của việc biến thông tin tri thức và và các luật của con người thành những dòng code trên máy tính.\nCon người thường xuyên sử dụng các \u0026ldquo;biểu tượng\u0026rdquo; để gán ý nghĩa cho các sự vật và sự kiện trong môi trường của họ. Ví dụ, nếu ai đó nói với bạn của họ rằng họ vừa mua một bó hoa hồng, người nghe tin đó có thể nhanh chóng liên tưởng đến hình ảnh của những bông hoa. Ý tưởng của symbolic AI là những \u0026ldquo;biểu tượng\u0026rdquo; này được xây dựng thành khối nhận thức.\nCác hệ thống thuộc loại này thường liên quan đến lý luận suy diễn, suy luận logic và một số thuật toán tìm kiếm để tìm ra giải pháp trong các ràng buộc của mô hình đã chỉ định. Chúng bao gồm các hệ thống chuyên gia, sử dụng các quy tắc và cây quyết định để suy ra kết luận từ dữ liệu đầu vào, bộ giải ràng buộc, tìm kiếm giải pháp trong một không gian khả năng và hệ thống lập kế hoạch, cố gắng tìm một chuỗi các hành động để đạt được một mục tiêu được xác định rõ ràng từ một số trạng thái ban đầu. Chúng cũng thường có các biến thể có khả năng xử lý sự không chắc chắn và rủi ro.\nCác thuật toán như vậy thường có độ phức tạp thuật toán là NP-hard, khi giải quyết các vấn đề trong thế giới thực, nhóm này phải đối mặt với không gian tìm kiếm siêu lớn. Điều này có nghĩa là các thuật toán thuyền thống trang bị kỹ năng tìm kiếm ngẫu nhiên sẽ không hoạt động, ngoại trừ các trường hợp ngoại lệ, Do khả năng cao là lời giản sẽ vét cạn không gian tìm kiếm.\nCó rất nhiều nhóm thuật toán trong nhóm này:\nBranch and bound algorithms Các thuật toán thuộc nhóm Branch and bound được sử dụng trong các bài toán tối ưu hoá hoặc bài toán ràng buộc có điều kiện. Khi mà chúng ta không thể áp dụng heuristic trong các bài toán đó. Cách thức hoạt động của các bài toán dạng này là phân vấn đề thành các vùng nhỏ sử dụng upper bound và lower bound, và thực hiện tìm kiếm giải pháp trên các vùng đó.\nLocal search Tìm kiếm cục bộ xem xét các biến thể gần đúng và sử dụng các biến thể đó để cố gắng cải thiện nó dần dần. Đôi khi, thực hiện các bước nhảy ngẫu nhiên nhằm thoát khỏi tối ưu cục bộ.\nMeta-heuristics Siêu kinh nghiệm sử dụng các thuật toán tiến hóa, bắt chước các cơ chế cộng tác hoặc phân tán được tìm thấy trong tự nhiên, chẳng hạn như chọn lọc tự nhiên hoặc các hành vi được lấy cảm hứng từ bầy đàn.\nHeuristic search Heuristic search sử dụng hàm lượng giá để xác định mức độ liên quan của điểm dữ liệu và mục tiêu.\nCác thuật toán này thường không phù hợp với dữ liệu đầu vào là nhiễu, hoặc trong các tình huống mô hình không được định nghĩa rõ ràng. Chúng tỏ ra hiệu quả trong ngữ cảnh chúng ta có các hành động rõ ràng và cụ thể, và hệ thống cần cung cấp một kỹ thuật chuẩn để triển khai các hành động trên.\nConnectionist AI là gì Cái tên được lấy từ liên kết mạng mà các thuật toán trong nhóm này sử dụng. Kỹ thuật phổ biến trong nhóm này là sử dụng Artificial Neural Network (ANN). Chúng bao gồm nhiều lớp mạng, mỗi lớp sẽ có nhiều node, chúng sẽ xử lý các tín hiệu đầu vào, kết hợp chúng với các trọng số, và biến đổi chúng trước khi đưa vào lớp tiếp theo. Support Vector Machines (SVMs) cũng thuộc nhóm này.\nANNs có rất nhiều biến thể, ví dụ Convolution Neural Networks (được sử dụng chủ yếu trong xử lý ảnh), Long Short-term Memory Networks ( thường được dùng trong bài toán phân tích time series hoặc các bài toán mà thời gian là đặc trưng quan trọng, bị ẩn đi dưới lăng kính bình thường (văn bản :) ). Deep learning về cơ bản là đồng nghĩa với Artificial Neural Networks.\nGiá trị cốt lõi của loại kỹ thuật này là người dùng không cần chỉ định các quy tắc của miền được mô hình hóa. Mạng tự phát hiện ra các quy tắc từ training data. Người dùng cung cấp dữ liệu đầu vào và dữ liệu đầu ra mẫu (bộ dữ liệu càng lớn và đa dạng càng tốt). Các thuật toán kết nối sau đó áp dụng các mô hình hồi quy thống kê để điều chỉnh các hệ số trọng số của các biến trung gian của chúng, cho đến khi tìm thấy mô hình phù hợp nhất. Các trọng số được điều chỉnh theo hướng giảm thiểu lỗi tích lũy từ tất cả các điểm dữ liệu huấn luyện, sử dụng các kỹ thuật như giảm độ dốc.\nVì các kỹ thuật này sử dụng các thuật toán cực tiểu hoá độ lỗi, nên chúng vốn có khả năng chống nhiễu. Chúng sẽ loại bỏ các giá trị ngoại lệ và đưa ra giải pháp phân loại dữ liệu trong phạm vi sai số nhất định.\nCác thuật toán này không cần một mô hình được cung cấp trước. Nó chỉ cần đủ dữ liệu mẫu và nó sẽ tự suy ra mô hình. Đây là một đặc điểm rất mạnh mẽ, nhưng cũng là một điểm yếu. Các tính năng đầu vào phải được lựa chọn rất cẩn thận. Chúng cũng phải được chuẩn hóa hoặc chia tỷ lệ, để tránh một tính năng áp đảo các tính năng khác và được xử lý trước để có ý nghĩa hơn đối với việc phân loại.\nFeature engineering thường có thể là yếu tố quyết định thành công chính của một dự án máy học. Việc có quá nhiều đặc trưng hoặc không có đủ dữ liệu mẫu đại diện cho toàn bộ vấn đề, có thể dẫn dến overfitting hoặc underfitting. Ngay cả với sự giúp đỡ của nhà khoa học dữ liệu lành nghề nhất, bạn vẫn phải chịu sự phụ thuộc vào chất lượng của dữ liệu mà bạn có. Các kỹ thuật này cũng không tránh khỏi curse of dimensionality, hoặc số lượng input feature tăng, hoặc rủi ro của giải pháp không hợp lệ.\nCác thuật toán mặc nhiên giả định rằng mô hình thế giới mà chúng đang nắm bắt là tương đối ổn định. Điều này làm cho chúng rất hiệu quả đối với các vấn đề mà luật chơi không thay đổi nhiều hoặc thay đổi với tốc độ đủ chậm để cho phép thu thập đủ các mẫu dữ liệu mới để đào tạo lại và thích ứng với thực tế mới. Nhận dạng hình ảnh là câu chuyện thành công, bởi vì con chó năm nay với năm sau thường không thay đổi nhiều.\nChúng ta nên chọn cái nào Việc chọn thuật toán dựa vào vấn đề chúng ta cần giải quyết. Ngày nay, việc chọn sai kỹ thuật đang dần trở nên phổ biến. Nguyên nhân rất nhiều, có thể là do sự cường điệu hoá của kỹ thuật đó, hoặc sự thiếu nhận thức về bối cảnh của thuật toán AI. Khi bạn cầm trong tay một cái búa, một thứ bắt bắt đầu giống một cái đinh.\nKhi AI phát triển mạnh mẽ trong mọi khía cạnh của cuộc sống của chúng ta, các yêu cầu cho AI càng ngày càng trở nên phức tạp hơn, rất có khả năng ứng dụng của chúng ta sẽ cần cả hai kỹ thuật này. Dữ liệu tiếng ồn được thu thập thông qua các cảm biến có thể được xử lý thông qua ANN để suy ra thông tin rời rạc về môi trường, trong khi thuật toán tsymbolic sử dụng thông tin đó để tìm kiếm không gian của các hành động khả thi có thể dẫn đến một số kết luận rõ ràng hơn.\nMột thuật toán học máy có thể rất hiệu quả trong việc suy luận môi trường xung quanh của một phương tiện cá nhân trong một mức xác suất nhất định, nhưng khả năng xảy ra sai sót là không thể chấp nhận được nếu sai sót đó có thể khiến chiếc xe lao xuống vực, nguyên nhân là sai sót đó chưa có trọng training data. Hơn nữa, việc đưa công nghệ học sâu vào các ứng dụng quan trọng đang tỏ ra là một thách thức, đặc biệt là khi một chiếc xe tay ga bị nhầm lẫn với một chiếc dù khi nó bị lật ngửa.\nViệc kết hợp với symbolic AI đảm bảo rằng những gì rõ ràng về mặt logic vẫn được thực thi, ngay cả khi lớp học sâu bên dưới nói khác đi do một số sai lệch thống kê hoặc số đọc cảm biến nhiễu. Điều này ngày càng trở nên quan trọng đối với các ứng dụng có rủi ro cao, như quản lý nhà máy điện, điều động tàu hỏa, hệ thống lái tự động và ứng dụng không gian. Hệ lụy của việc phân loại sai trong các hệ thống như vậy nghiêm trọng hơn nhiều so với việc giới thiệu sai phim.\nMột hệ thống kết hợp sử dụng cả thuật toán connectionist và symbolic sẽ tận dụng điểm mạnh của cả hai trong khi khắc phục điểm yếu của nhau. Các giới hạn của việc sử dụng một kỹ thuật riêng lẻ đã được xác định và nghiên cứu mới nhất đã bắt đầu chỉ ra rằng việc kết hợp cả hai phương pháp có thể dẫn đến một giải pháp thông minh hơn.\nHướng phát triển tiếp theo của AI Theo quy luật tự nhiên, cái gì đạt mức độ cực thịnh thì là thời điểm bắt đầu dẫn tới cực suy, AI cũng không ngoại lệ, do đó, theo phỏng đoán, AI có thể tiến hoá theo các chiều hướng sau:\nTiến hoá của Symbolic AI Symbolic AI sẽ tiến hoá bằng một cách nào đó, sẽ quay lại thống trị\nKết hợp Symbolic AI và Connectionist AI Connectionist AI và Symbolic AI bằng một cách nào đó sẽ kết hợp với nhau\nMột hướng đi mới khác được khai phá ra, và cạnh trang sòng phẳng với Symbolic AI lẫn Connectionist AI Điều này khá khó, nhưng không gì là không thể.\nDù AI có tiến hoá như thế nào, chung quy lại thì chúng được tạo ra để phục vụ nhu cầu và mục đích của con người.\nTham khảo https://towardsdatascience.com/symbolic-vs-connectionist-a-i-8cf6b656927\nhttps://blog.re-work.co/the-difference-between-symbolic-ai-and-connectionist-ai/\nhttps://medium.com/synthetic-intelligence/dialectic-of-ai-connectionism-vs-symbolism-d8b9888d4268\nhttps://www.forbes.com/sites/forbestechcouncil/2020/09/01/symbolism-versus-connectionism-in-ai-is-there-a-third-way/?sh=2f3074fb7549\nCảm ơn các bạn đã theo dõi bài viết, hẹn gặp lại ở các bài viết tiếp theo\n","date":"Feb 18, 2023","img":"https://unsplash.it/1920/1080?image=16","permalink":"/blog/2023-02-18-symbolic-vs-connectionist/","series":null,"tags":["RFM","Recommendation","Marketing"],"title":"Symbolic AI Và Connectionist AI"},{"categories":null,"content":" 1. Giới thiệu RFM 2. Giới thiệu data và tìm hiểu data 2.1 Giới thiệu data 2.2 Thực hiện một số phép thống kê trên dữ liệu 2.3 Phân tích dữ liệu 2.3.1 Tính doanh thu theo tháng 2.3.2 Thống kê tăng trưởng của doanh thu 2.3.3 Phân tích số lượng khách hàng tháng 2.3.4 Số đơn đặt hàng trong tháng 2.3.5 Doanh thu trung bình mỗi đơn hàng 2.3.6 Số lượng khách hàng mới/ cũ theo từng tháng 2.3.7 Tỷ lệ tăng trưởng khách hàng mới 2.3.8 Tỷ lệ giữ chân khách hàng cũ hàng tháng 3. Phân Khúc Khách Hàng 3.1 Clean data 3.1 Tính Recency 3.2 Tính Frequency 3.3 Tính Monetary 3.4 Tạo bảng RFM 3.5 Phân nhóm khách hàng sử dụng RFM Tham khảo 1. Giới thiệu RFM RFM là 3 ký tự đầu tiên của Recency, frequency, monetary. Nó là công cụ phân tích được marketing sử dụng để định danh khách hàng của công ty dựa trên thói quen mua sắm tự nhiên của họ.\nRFM phân tích và đánh giá khách hàng bằng cách tính điểm hành vi mua sắm của họ dựa trên ba tiêu chí:\nRecency: Khoảng thời gian mua hàng gần nhất là bao lâu. Nếu họ đã mua hàng gần đây, xác suất họ sẽ mua thêm một lần nữa rất cao. Tuy nhiên, nếu khách hàng không thực hiện bất kỳ một giao dịch nào trong một khoảng thời gian dài, chúng ta có thể lôi kéo họ bằng một offer đặc biệt, hoặc giới thiệu lại thương hiệu của mình cho họ.\nFrequency: Tần suất mua hàng của khách hàng. Nếu khách hàng có tầng suất mua dày đặc, chúng ta sẽ biết thói quen và sở thích của họ. Nếu họ chỉ mua một lần và chưa bao giờ trở lại, họ có thể là một ứng viên tốt để thực hiện bài khảo sát sự hài lòng của khách hàng.\nMonetary: Số tiền trung bình khách hàng sử dụng trên mỗi giao dịch. Tuy nhiên, đừng quá chú trọng vào con số này. Tất cả các giao dịch mua hàng đều có giá trị. Monetary tác động trực tiếp đến doanh thu của công ty, tác động gián tiếp với 2 chỉ số ở trên kia. Nếu chúng ta gặp một khách hàng thực hiện nhiều lần mua hàng gần đây với mức giá cao, những người đó có thể là khách hàng trung thành của chúng ta.\nRMF có thang điểm từ 1-5 ( 1 là tệ, 5 là tốt) của mỗi khách hàng cho mỗi tiêu chí.\nRFM giúp công ty có khả năng dự đoán những khách hàng nào có khả năng cao sẽ mua lại sản phẩm của họ, doanh thu đến từ khách hàng mới là bao nhiêu, cách biến cơ hội mua hàng thành thói quen.\n2. Giới thiệu data và tìm hiểu data 2.1 Giới thiệu data Dữ liệu ở bài viết này, chúng ta sẽ sử dụng từ nguồn https://www.kaggle.com/datasets/lissetteg/ecommerce-dataset.\nCode load các thư viện cần thiết\n1 2# This Python 3 environment comes with many helpful analytics libraries installed 3# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python 4# For example, here\u0026#39;s several helpful packages to load in 5 6import numpy as np # linear algebra 7import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) 8 9# Input data files are available in the \u0026#34;../input/\u0026#34; directory. 10 11import time, warnings 12import datetime as dt 13 14#visualizations 15import matplotlib.pyplot as plt 16from pandas.plotting import scatter_matrix 17%matplotlib inline 18import seaborn as sns 19 20warnings.filterwarnings(\u0026#34;ignore\u0026#34;) Code load data và in ra 20 dòng đầu tiên\n1 2#load the dataset 3retail_df = pd.read_csv(\u0026#39;../input/data.csv\u0026#39;,encoding=\u0026#34;ISO-8859-1\u0026#34;,dtype={\u0026#39;CustomerID\u0026#39;: str,\u0026#39;InvoiceID\u0026#39;: str}) 4retail_df.head(20) Hình 1: Tổng quan về dữ liệu\n2.2 Thực hiện một số phép thống kê trên dữ liệu Sau khi nhận dữ liệu, chúng ta cần xem xét sơ lược tổng quan về dữ liệu bằng một số hàm thống kê cơ bản\n1 2print(retail_df.describe()) 3print(retail_df.info()) Hình 2: Phân tích xác suất về dữ liệu\nDựa vào kết quả hình 2, chúng ta thấy rằng data có tổng cộng 541909 dòng, 8 cột, trong đó có một số chỗ có giá trị null. Cột CustomerID có giá trị null nhiều nhất.\n2.3 Phân tích dữ liệu 2.3.1 Tính doanh thu theo tháng Doanh thu của một đơn hàng bằng số lượng nhân đơn giá.\nCông việc cần làm:\nTạo một key chung đại diện cho biến tháng trong năm ( ở đây mình đặt tên là MonthKey) Tính doanh thu Group doanh thu theo tháng In ra màn hình doanh thu theo tháng Vẽ chart doanh thu theo tháng Đoạn code mẫu mô tả các bước cần làm ở trên\n1 2#converting the type of Invoice Date Field from string to datetime. 3retail_df[\u0026#39;InvoiceDate\u0026#39;] = pd.to_datetime(retail_df[\u0026#39;InvoiceDate\u0026#39;]) 4 5#creating MonthKey field for reporting and visualization 6retail_df[\u0026#39;MonthKey\u0026#39;] = retail_df[\u0026#39;InvoiceDate\u0026#39;].map(lambda date: 100*date.year + date.month) 7 8#calculate Revenue for each row and create a new dataframe with MonthKey - Revenue columns 9retail_df[\u0026#39;Revenue\u0026#39;] = retail_df[\u0026#39;UnitPrice\u0026#39;] * retail_df[\u0026#39;Quantity\u0026#39;] 10revenue_by_month = retail_df.groupby([\u0026#39;MonthKey\u0026#39;])[\u0026#39;Revenue\u0026#39;].sum().reset_index() 11print(revenue_by_month) 12 13# plot data 14revenue_by_month[\u0026#39;MonthKey\u0026#39;] = revenue_by_month[\u0026#39;MonthKey\u0026#39;].apply(str) 15%matplotlib inline 16plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 17plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;Revenue\u0026#39;, data=revenue_by_month, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 18plt.title(\u0026#34;Revenue by Month\u0026#34;) 19plt.show() Kết quả chúng mình nhận được\n1 2 3\tMonthKey\tRevenue 40\t201012\t748957.020 51\t201101\t560000.260 62\t201102\t498062.650 73\t201103\t683267.080 84\t201104\t493207.121 95\t201105\t723333.510 106\t201106\t691123.120 117\t201107\t681300.111 128\t201108\t682680.510 139\t201109\t1019687.622 1410\t201110\t1070704.670 1511\t201111\t1461756.250 1612\t201112\t433668.010 Hình 3: Doanh thu theo tháng\nNhìn vào hình 3 ở trên, chúng ta thấy rằng, doanh thu bắt đầu tăng từ tháng 8, đạt đỉnh điểm ở tháng 11, tháng 12 doanh thu siêu thấp, ngó qua tháng 12 năm ngoái thì doanh thu khá ổn, nên có thể tạm kết luận là data tháng 12 năm hiện tại có thể chưa đủ tháng. Chúng ta có thể bỏ data tháng 12 ra khỏi dataset để tránh bị nhiễu.\n2.3.2 Thống kê tăng trưởng của doanh thu Để tính tăng trưởng doanh thu, ta lấy doanh thu tháng hiện tại chia cho doanh thu tháng trước -1\nTrong pandas, chúng ta sử dụng hàm pct_change\n1 2revenue_by_month[\u0026#39;MonthlyGrowth\u0026#39;] = revenue_by_month[\u0026#39;Revenue\u0026#39;].pct_change() 3 4 5# Plot data 6 7plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 8plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;MonthlyGrowth\u0026#39;, data=revenue_by_month, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 9plt.plot(range(1,len(revenue_by_month.index)+1),[0 for i in range(len(revenue_by_month.index))], color=\u0026#34;k\u0026#34;, lw=2.5) 10plt.title(\u0026#34;Monthly Revenue Growth Rate\u0026#34;) 11plt.show() Hình 4: Biến động doanh thu theo tháng\nQua sơ đồ hình 4, chúng ta thấy rằng ở tháng 4 có sự sụt giảm mạnh về doanh thu, nhóm kinh doanh cần phải phân tích kỹ hơn các yếu tố ảnh hưởng đến sự sụt giảm nghiêm trọng về mặt doanh thu trong tháng này.\n2.3.3 Phân tích số lượng khách hàng tháng Để tính lượt khách mua hàng hằng tháng, chúng ta đếm không trùng mã khách hàng trong tháng\n1 2customer_by_month = retail_df.groupby([\u0026#39;MonthKey\u0026#39;])[\u0026#39;CustomerID\u0026#39;].nunique().reset_index() 3customer_by_month 4 5 6customer_by_month[\u0026#39;MonthKey\u0026#39;] = customer_by_month[\u0026#39;MonthKey\u0026#39;].apply(str) 7%matplotlib inline 8plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 9plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;CustomerID\u0026#39;, data=customer_by_month, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 10plt.title(\u0026#34;Customer by Month\u0026#34;) 11plt.show() 1 2 3\tMonthKey\tCustomerID 40\t201012\t948 51\t201101\t783 62\t201102\t798 73\t201103\t1020 84\t201104\t899 95\t201105\t1079 106\t201106\t1051 117\t201107\t993 128\t201108\t980 139\t201109\t1302 1410\t201110\t1425 1511\t201111\t1711 1612\t201112\t686 Hình 5: Lượt khách theo tháng\nNhận xét rằng, lượt khách tăng từ tháng 8 trở về sau, từ tháng 5 đến tháng 8 thì lượt khách giảm nhẹ, đều. Tháng 1 và 2 lượt khách rất thấp.\n2.3.4 Số đơn đặt hàng trong tháng Cũng giống như lượt khách, chúng ta sẽ đếm số lượng InvoiceNo trong từng tháng\n1 2order_by_month = retail_df.groupby([\u0026#39;MonthKey\u0026#39;])[\u0026#39;InvoiceNo\u0026#39;].count().reset_index() 3order_by_month 4order_by_month[\u0026#39;MonthKey\u0026#39;] = order_by_month[\u0026#39;MonthKey\u0026#39;].apply(str) 5%matplotlib inline 6plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 7plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;InvoiceNo\u0026#39;, data=order_by_month, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 8plt.title(\u0026#34;Count total order in Month\u0026#34;) 9plt.show() Hình 6: Số lượng đơn đặt hàng trong tháng\nTừ tháng 5 đến tháng 7, số lượng đơn đặt hàng tăng nhẹ, tỷ lệ nghịch với lượt khách\nTừ tháng 8 trở đi, số đơn đặt hàng tăng cao, tỷ lệ thuận với lượt khách\n2.3.5 Doanh thu trung bình mỗi đơn hàng Trong pandas, chúng ta chỉ cần gọi hàm mean để tính trung bình\n1 2avg_order_revenue = retail_df.groupby([\u0026#39;MonthKey\u0026#39;])[\u0026#39;Revenue\u0026#39;].mean().reset_index() 3 4avg_order_revenue[\u0026#39;MonthKey\u0026#39;] = avg_order_revenue[\u0026#39;MonthKey\u0026#39;].apply(str) 5 6# Plot regression line 7 8plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 9plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;Revenue\u0026#39;, data=avg_order_revenue, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 10plt.title(\u0026#34;Average Revenue per Order\u0026#34;) 11plt.show() Hình 7: Doanh thu trung bình mỗi đơn hàng\nNhận xét rằng, tháng 10, 11, lượt khách tăng, số lượng đơn hàng tăng, nhưng trung bình mỗi đơn hàng lại thấp.\n2.3.6 Số lượng khách hàng mới/ cũ theo từng tháng Khách hàng mới và khách hàng cũ đều đóng vai trò rất quan trọng trong chiến lược marketing. Việc giữ chân khách hàng cũ giúp chúng ta hiểu hơn về họ và phục vụ họ tốt hơn. Việc phát triển khách hàng mới giúp chúng ta phát triển lớn lên.\nTrong ngữ cảnh bài này, chúng ta sẽ xét yếu tốt khách hàng mới/cũ như sau\nKhách hàng mới là khách hàng trước đó chưa từng mua hàng. Không quan tâm tháng hiện tại khách đã mua bao nhiêu đơn.\nKhách hàng cũ là khách hàng đã mua ít nhất 1 đơn hàng ở tháng trước đó.\nDo đơn vị chúng ta thống kê tính bằng tháng, cho nên nếu 1 khách hàng có mua 2 hoặc nhiều hơn đơn hàng ở tháng hiện tại, chưa từng mua đơn hàng nào ở các tháng trước đó, khách hàng đó được coi là khách hàng mới.\nKỹ thuật lập trình ở đây như sau:\nLấy ra danh sách khách hàng và ngày mua hàng đầu tiên của họ\nQuy đổi ngày mua hàng đầu tiên thành tháng mua hàng đầu tiên\nTạo thêm cột loại khách hàng cho từng đơn hàng (UserType). Gán mặc định UserType = New. Nếu tháng lên đơn lớn hơn tháng mua đầu tiên -\u0026gt; đơn hàng của khách cũ (UserType=Existing).\n1 2create a dataframe contaning CustomerID and first purchase date 3df_min_date_purchase =retail_df.groupby(\u0026#39;CustomerID\u0026#39;).InvoiceDate.min().reset_index() 4df_min_date_purchase.columns = [\u0026#39;CustomerID\u0026#39;,\u0026#39;MinPurchaseDate\u0026#39;] 5df_min_date_purchase[\u0026#39;MinMonthKey\u0026#39;] = df_min_date_purchase[\u0026#39;MinPurchaseDate\u0026#39;].map(lambda date: 100*date.year + date.month) 6 7#merge first purchase date column to our main dataframe (tx_uk) 8retail_new_df = pd.merge(retail_df, df_min_date_purchase, on=\u0026#39;CustomerID\u0026#39;) 9 10retail_new_df.head() 11 12#create a column called User Type and assign Existing 13#if User\u0026#39;s First Purchase Year Month before the selected Invoice Year Month 14retail_new_df[\u0026#39;UserType\u0026#39;] = \u0026#39;New\u0026#39; 15retail_new_df.loc[retail_new_df[\u0026#39;MonthKey\u0026#39;]\u0026gt;retail_new_df[\u0026#39;MinMonthKey\u0026#39;],\u0026#39;UserType\u0026#39;] = \u0026#39;Existing\u0026#39; 16 17#calculate the Revenue per month for each user type 18revenue_per_month = retail_new_df.groupby([\u0026#39;MonthKey\u0026#39;,\u0026#39;UserType\u0026#39;])[\u0026#39;Revenue\u0026#39;].sum().reset_index() 19 20#filtering the dates and plot the result 21revenue_per_month = revenue_per_month.query(\u0026#34;MonthKey != 201012 and MonthKey != 201112\u0026#34;) 22 23revenue_per_month[\u0026#39;MonthKey\u0026#39;] = revenue_per_month[\u0026#39;MonthKey\u0026#39;].apply(str) 24revenue_per_month.set_index(\u0026#39;MonthKey\u0026#39;,inplace=True) 25# Plot regression line 26 27plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 28fig, ax = plt.subplots() 29for label, grp in revenue_per_month.groupby(\u0026#39;UserType\u0026#39;): 30 grp.plot(x = grp.index, y = \u0026#39;Revenue\u0026#39;,ax = ax, label = label,style=\u0026#39;.-\u0026#39;) 31plt.title(\u0026#34;Old and New user\u0026#34;) 32plt.show() Hình 8: Số lượng khách hàng mới/ cũ theo từng tháng\nNhận xét rằng, doanh thu cho khách cũ tăng dần theo thời gian, còn doanh thu cho khách mới có vẻ giảm. Để chắc chắn, chúng ta thử vẽ ra tỷ lệ tăng trưởng khách hàng mới xem sao\n2.3.7 Tỷ lệ tăng trưởng khách hàng mới Ta tính tỷ lệ khách hàng mới / khách hàng cũ theo từng tháng\nTrong pandas, chúng ta sẽ sử dụng hàm crosstab\n1 2new_user_ratio = retail_new_df.query(\u0026#34;UserType == \u0026#39;New\u0026#39;\u0026#34;).groupby([\u0026#39;MonthKey\u0026#39;])[\u0026#39;CustomerID\u0026#39;].nunique()/retail_new_df.query(\u0026#34;UserType == \u0026#39;Existing\u0026#39;\u0026#34;).groupby([\u0026#39;MonthKey\u0026#39;])[\u0026#39;CustomerID\u0026#39;].nunique() 3new_user_ratio = new_user_ratio.reset_index() 4new_user_ratio = new_user_ratio.dropna() 5new_user_ratio.columns = [\u0026#34;MonthKey\u0026#34;,\u0026#34;NewCustomerRatio\u0026#34;] 6 7new_user_ratio = new_user_ratio.query(\u0026#34;MonthKey != 201012 and MonthKey != 201112\u0026#34;) 8new_user_ratio[\u0026#39;MonthKey\u0026#39;] = new_user_ratio[\u0026#39;MonthKey\u0026#39;].apply(str) 9 10# Plot regression line 11 12plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 13plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;NewCustomerRatio\u0026#39;, data=new_user_ratio, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 14plt.title(\u0026#34;New Customer Ratio\u0026#34;) 15plt.show() Hình 9: Tỷ lệ tăng trưởng khách hàng mới\nNhư hình trên, chúng ta thấy rằng tỷ lệ khách hàng mới / khách hàng cũ giảm dần theo thời gian\n2.3.8 Tỷ lệ giữ chân khách hàng cũ hàng tháng Khách hàng cũ được hiểu theo nghĩa là khách hàng tháng trước có mua, tháng này có mua\nTỷ lệ giữ chân khách hàng là tỷ lệ khách hàng cũ mua hàng / tổng khách hàng trong tháng\nChúng ta sử dụng hàm crosstab trên khách hàng và tháng, để xem thử tháng đó khách có mua hàng hay không.\n1#identify which users are active by looking at their revenue per month 2df_user_purchase = retail_df.groupby([\u0026#39;CustomerID\u0026#39;,\u0026#39;MonthKey\u0026#39;])[\u0026#39;Revenue\u0026#39;].sum().reset_index() 3 4#create retention matrix with crosstab 5df_retention = pd.crosstab(df_user_purchase[\u0026#39;CustomerID\u0026#39;], df_user_purchase[\u0026#39;MonthKey\u0026#39;]).reset_index() 6 7print(df_retention.head()) 8 9#create an array of dictionary which keeps Retained \u0026amp; Total User count for each month 10months = df_retention.columns[2:] 11retention_array = [] 12for i in range(len(months)-1): 13 retention_data = {} 14 selected_month = months[i+1] 15 prev_month = months[i] 16 retention_data[\u0026#39;MonthKey\u0026#39;] = int(selected_month) 17 retention_data[\u0026#39;TotalUserCount\u0026#39;] = df_retention[selected_month].sum() 18 retention_data[\u0026#39;RetainedUserCount\u0026#39;] = df_retention[(df_retention[selected_month]\u0026gt;0) \u0026amp; (df_retention[prev_month]\u0026gt;0)][selected_month].sum() 19 retention_array.append(retention_data) 20 21#convert the array to dataframe and calculate Retention Rate 22df_retention = pd.DataFrame(retention_array) 23df_retention[\u0026#39;RetentionRate\u0026#39;] = df_retention[\u0026#39;RetainedUserCount\u0026#39;]/df_retention[\u0026#39;TotalUserCount\u0026#39;] 24 25df_retention = df_retention.query(\u0026#34;MonthKey != 201012 and MonthKey != 201112\u0026#34;) 26df_retention[\u0026#39;MonthKey\u0026#39;] = df_retention[\u0026#39;MonthKey\u0026#39;].apply(str) 27 28# Plot regression line 29 30plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = [20, 10] 31plt.plot(\u0026#39;MonthKey\u0026#39;, \u0026#39;RetentionRate\u0026#39;, data=df_retention, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;o\u0026#39;) 32plt.title(\u0026#34;Monthly Retention Rate\u0026#34;) 33plt.show() Kết quả\n1# hàm crosstab trả ra ma trận tương quan giữa khách hàng và tháng, ví dụ như khách hàng 12346 tháng 201012 không có mua hàng, nhưng 201101 lại có 2\u0026gt;\u0026gt; print(df_retention.head()) 3MonthKey CustomerID 201012 201101 201102 201103 201104 201105 201106 \\ 40 12346 0 1 0 0 0 0 0 51 12347 1 1 0 0 1 0 1 62 12348 1 1 0 0 1 0 0 73 12349 0 0 0 0 0 0 0 84 12350 0 0 1 0 0 0 0 9 10MonthKey 201107 201108 201109 201110 201111 201112 110 0 0 0 0 0 0 121 0 1 0 1 0 1 132 0 0 1 0 0 0 143 0 0 0 0 1 0 154 0 0 0 0 0 0 Hình 10: Tỷ lệ giữ chân khách hàng cũ\nNhìn hình, ta thấy rằng khách hàng cũ mua lại khá nhiều ở giai đoạn tháng 6 đến tháng 8, sau đó tỷ lệ lại trở lại bình thường.\n3. Phân Khúc Khách Hàng Ở mục trên, chúng ta đã phân tích dữ liệu của công ty bán lẻ trực tuyến và tìm ra yếu tố ảnh hưởng đến doanh thu của công ty. Ở mục này, chúng ta sẽ tiến hành phân loại khách hàng theo nhóm, để phục vụ cho nhu cầu marketing sau này.\nChúng ta phải tiến hành phân loại khách hàng, bởi vì chúng ta không thể đối xử với tất cả khách hàng giống nhau được. Ví dụ là không thể gửi chiến dịch marketing về thịt cho người ăn chay. Hoặc bán lược cho nhà sư (đây là ví dụ minh hoạ của mình nha, còn trên có vài case-study về bán các sản phẩm đó cho đối tượng đó, mình không nói về những trường hợp đó nha).\nỞ phần này, chúng ta sẽ tìm hiểu nhu cầu khách hàng sử dụng mô hình RFB.\nNhư đã đề cập ở mục 1, mô hình RFB bao gồm Recency, Frequency and Monetary\n3.1 Clean data Để mô hình chính xác hơn, chúng ta sẽ clean data, trải qua các bước sau:\nLoại ra các đơn hàng có Quantity \u0026lt;=0\nLoại bỏ những đơn hàng CustomerID NA\nLoại bỏ những đơn hàng bán tháng 12 năm 2010\nLoại bỏ những đơn hàng bán tháng 12 năm 2011, do phân tích ở trên là data tháng 12 không đủ.\n1retail_rfm_df = retail_df.copy() 2#remove canceled orders 3retail_rfm_df = retail_rfm_df[retail_rfm_df[\u0026#39;Quantity\u0026#39;]\u0026gt;0] 4#remove rows where customerID are NA 5retail_rfm_df.dropna(subset=[\u0026#39;CustomerID\u0026#39;],how=\u0026#39;all\u0026#39;,inplace=True) 6retail_rfm_df = retail_rfm_df[retail_rfm_df[\u0026#39;InvoiceDate\u0026#39;]\u0026gt; \u0026#34;2010-12-31\u0026#34;] 7retail_rfm_df = retail_rfm_df[retail_rfm_df[\u0026#39;InvoiceDate\u0026#39;]\u0026lt; \u0026#34;2011-12-01\u0026#34;] 3.1 Tính Recency Để tính thông số này, chúng ta cần tìm ra ngày mua gần nhất của khách hàng, và số ngày không mua hàng, kể từ ngày mua cuối đến hiện tại.\nKỹ thuật lập trình ở đây:\nGán ngày hiện tại là ngày 30 tháng 11 năm 2011\nLấy ra ngày mua hàng cuối cùng của mỗi user\nTính ra khoảng thời gian kể từ lần mua hàng cuối cùng đến thời điểm hiện tại\n1now = dt.date(2011,11,30) 2#create a new column called date which contains the date of invoice only 3retail_rfm_df[\u0026#39;date\u0026#39;] = pd.DatetimeIndex(retail_rfm_df[\u0026#39;InvoiceDate\u0026#39;]).date 4 5#group by customers and check last date of purshace 6recency_df = retail_rfm_df.groupby(by=\u0026#39;CustomerID\u0026#39;, as_index=False)[\u0026#39;date\u0026#39;].max() 7recency_df.columns = [\u0026#39;CustomerID\u0026#39;,\u0026#39;LastPurshaceDate\u0026#39;] 8 9#calculate recency 10recency_df[\u0026#39;Recency\u0026#39;] = recency_df[\u0026#39;LastPurshaceDate\u0026#39;].apply(lambda x: (now - x).days) 11recency_df.drop(\u0026#39;LastPurshaceDate\u0026#39;,axis=1,inplace=True) 12print(recency_df.head()) Kết quả\n1\u0026gt;\u0026gt; recency_df.head() 2 CustomerID Recency 30 12346 316 41 12347 30 52 12348 66 63 12349 9 74 12350 301 8 9\u0026gt;\u0026gt; recency_df.Recency.describe() 10count 4174.000000 11mean 82.557499 12std 88.535941 13min 0.000000 1425% 15.000000 1550% 45.000000 1675% 128.000000 17max 330.000000 Chỉ số thống kê cho thấy, giá trị trung bình của Recency là 82, 50% người dùng lặp lại chu kỳ mua hàng trong vòng 45 ngày\n3.2 Tính Frequency Tần suất giúp chúng ta biết được khách hàng đã mua hàng bao nhiêu lần\nKỹ thuật lập trình ở đây khá đơn giản, gom nhóm đơn hàng theo user và đếm\n1 2# drop duplicates 3retail_rfm_df_copy = retail_rfm_df 4retail_rfm_df_copy.drop_duplicates(subset=[\u0026#39;InvoiceNo\u0026#39;, \u0026#39;CustomerID\u0026#39;], keep=\u0026#34;first\u0026#34;, inplace=True) 5#calculate frequency of purchases 6frequency_df = retail_rfm_df_copy.groupby(by=[\u0026#39;CustomerID\u0026#39;], as_index=False)[\u0026#39;InvoiceNo\u0026#39;].count() 7frequency_df.columns = [\u0026#39;CustomerID\u0026#39;,\u0026#39;Frequency\u0026#39;] 8frequency_df.head() Kết quả\n1\tCustomerID\tFrequency 20\t12346\t1 31\t12347\t5 42\t12348\t3 53\t12349\t1 64\t12350\t1 3.3 Tính Monetary Monetary là tổng tiền khách hàng đã sử dụng\nDo đã tính doanh thu từ trước, nên giờ chúng ta sử dụng lại, chỉ cần gom nhóm theo mã khách hàng là được\n1 2monetary_df = retail_rfm_df.groupby([\u0026#39;CustomerID\u0026#39;])[\u0026#39;Revenue\u0026#39;].sum().reset_index() 3monetary_df.columns = [\u0026#39;CustomerID\u0026#39;,\u0026#39;Monetary\u0026#39;] 4monetary_df.head() Kết quả\n1\tCustomerID\tMonetary 20\t12346\t77183.60 31\t12347\t120.56 42\t12348\t291.76 53\t12349\t15.00 64\t12350\t25.20 3.4 Tạo bảng RFM Cái này thì siêu đơn giản, chúng ta merge 3 bảng trên lại là xong\n1 2#merge recency dataframe with frequency dataframe 3temp_df = recency_df.merge(frequency_df,on=\u0026#39;CustomerID\u0026#39;) 4#merge with monetary dataframe to get a table with the 3 columns 5rfm_df = temp_df.merge(monetary_df,on=\u0026#39;CustomerID\u0026#39;) 6#use CustomerID as index 7rfm_df.set_index(\u0026#39;CustomerID\u0026#39;,inplace=True) 8#check the head 9rfm_df.head() Kết quả\n1 2\tRecency\tFrequency\tMonetary 3CustomerID 412346\t316\t1\t77183.60 512347\t30\t5\t120.56 612348\t66\t3\t291.76 712349\t9\t1\t15.00 812350\t301\t1\t25.20 3.5 Phân nhóm khách hàng sử dụng RFM Cách đơn giản nhất để phân nhóm khách hàng là sử dụng Quartiles. Chúng ta có thể chia tập khách hàng thành 3 hoặc 4 hoặc 5 nhóm gì đó, tuỳ mục đích kinh doanh.\nỞ đây, giả sử mình chia làm 4 nhóm, sử dùng hàm quantile của pandas\nKỹ thuật lập trình như sau:\nChia các giá trị của Recency, Frequency, Monetary thành 4 nhóm, có miền giá trị từ 0 đến 3, được 4 cái phần tư vị cho mỗi nhóm\nGiá trị Recency càng nhỏ càng tốt, trong khi đó, giá trị Frequency và Monetary càng lớn càng tốt, để thống nhất chung, chúng ta sẽ đổi dấu của Recency, để cả 3 cùng thoả tính chất càng lớn càng tốt.\n1 2#RFM Quartiles 3rfm_df[\u0026#39;Recency\u0026#39;] = -rfm_df[\u0026#39;Recency\u0026#39;] 4quantiles = rfm_df.quantile(q=[0.25,0.5,0.75]) 5print(quantiles) 6quantiles.to_dict() 7 8### Creation of RFM Segments 9 10# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict) 11def FMScore(x,p,d): 12 if x \u0026lt;= d[p][0.25]: 13 return 0 14 elif x \u0026lt;= d[p][0.50]: 15 return 1 16 elif x \u0026lt;= d[p][0.75]: 17 return 2 18 else: 19 return 3 20 21#create rfm segmentation table 22rfm_segmentation = rfm_df 23rfm_segmentation[\u0026#39;R_Quartile\u0026#39;] = rfm_segmentation[\u0026#39;Recency\u0026#39;].apply(FMScore, args=(\u0026#39;Recency\u0026#39;,quantiles,)) 24rfm_segmentation[\u0026#39;F_Quartile\u0026#39;] = rfm_segmentation[\u0026#39;Frequency\u0026#39;].apply(FMScore, args=(\u0026#39;Frequency\u0026#39;,quantiles,)) 25rfm_segmentation[\u0026#39;M_Quartile\u0026#39;] = rfm_segmentation[\u0026#39;Monetary\u0026#39;].apply(FMScore, args=(\u0026#39;Monetary\u0026#39;,quantiles,)) 26 27rfm_segmentation.head() 28 29 30rfm_segmentation[\u0026#39;RFMScore\u0026#39;] = rfm_segmentation.R_Quartile.map(str) \\ 31 + rfm_segmentation.F_Quartile.map(str) \\ 32 + rfm_segmentation.M_Quartile.map(str) 33rfm_segmentation.head() 34 35 36#How many customers do we have in each segment? 37 38print(\u0026#34;Best Customers: \u0026#34;,len(rfm_segmentation[rfm_segmentation[\u0026#39;RFMScore\u0026#39;]==\u0026#39;333\u0026#39;])) 39print(\u0026#39;Loyal Customers: \u0026#39;,len(rfm_segmentation[rfm_segmentation[\u0026#39;F_Quartile\u0026#39;]==3])) 40print(\u0026#34;Big Spenders: \u0026#34;,len(rfm_segmentation[rfm_segmentation[\u0026#39;M_Quartile\u0026#39;]==3])) 41print(\u0026#39;Almost Lost: \u0026#39;, len(rfm_segmentation[rfm_segmentation[\u0026#39;RFMScore\u0026#39;]==\u0026#39;133\u0026#39;])) 42print(\u0026#39;Lost Customers: \u0026#39;,len(rfm_segmentation[rfm_segmentation[\u0026#39;RFMScore\u0026#39;]==\u0026#39;033\u0026#39;])) 43print(\u0026#39;Lost Cheap Customers: \u0026#39;,len(rfm_segmentation[rfm_segmentation[\u0026#39;RFMScore\u0026#39;]==\u0026#39;000\u0026#39;])) Kết quả\n1\u0026gt;\u0026gt; print(quantiles) 2\u0026gt;\u0026gt; quantiles.to_dict() 3 4 Recency Frequency Monetary 50.25 15.0 1.0 17.4000 60.50 45.0 2.0 43.5000 70.75 128.0 4.0 119.6625 8{\u0026#39;Frequency\u0026#39;: {0.25: 1.0, 0.5: 2.0, 0.75: 4.0}, 9 \u0026#39;Monetary\u0026#39;: {0.25: 17.399999999999999, 0.5: 43.5, 0.75: 119.66250000000001}, 10 \u0026#39;Recency\u0026#39;: {0.25: 15.0, 0.5: 45.0, 0.75: 128.0}} 11 12\u0026gt;\u0026gt;rfm_segmentation.head() 13 14\tRecency\tFrequency\tMonetary\tR_Quartile\tF_Quartile\tM_Quartile\tRFMScore 15CustomerID 1612346\t316\t1\t77183.60\t3\t0\t3\t303 1712347\t30\t5\t120.56\t1\t3\t3\t133 1812348\t66\t3\t291.76\t2\t2\t3\t223 1912349\t9\t1\t15.00\t0\t0\t0\t000 2012350\t301\t1\t25.20\t3\t0\t1\t301 21 22Best Customers: 10 23Loyal Customers: 980 24Big Spenders: 1044 25Almost Lost: 188 26Lost Customers: 374 27Lost Cheap Customers: 101 Tham khảo https://blog.hubspot.com/service/rfm-analysis\nhttps://www.investopedia.com/terms/r/rfm-recency-frequency-monetary-value.asp\nCảm ơn các bạn đã dành thời gian đọc bài. Nếu có bất kỳ vấn đề gì, hãy để lại comment bên dưới hoặc email cho mình qua địa chỉ alexblack2202@gmail.com. Hẹn gặp lại các bạn ở bài viết tiếp theo.\nSource code mình có để ở https://www.kaggle.com/code/alexblack2202/customer-segmentation-using-rfm-analysis\n","date":"Dec 4, 2022","img":"https://unsplash.it/1920/1080?image=17","permalink":"/blog/2022-12-03-marketing-with-python/","series":null,"tags":["RFM","Recommendation","Marketing"],"title":"Marketing Thực Chiến - Phân Loại Khách Hàng Sử Dụng RFM Analysis"},{"categories":null,"content":"Như các bạn đã biết, tiktok hiện nay là một ứng dụng giải trí phổ biến và đứng top 1 trong số lượt tải xuống từ CH play và AppStore. Thành công của tiktok là do họ đã xây dựng khá thành công thuật toán gợi ý video cho người dùng, làm cho người dùng \u0026ldquo;cuốn\u0026rdquo; vào các video họ đề xuất, mà không biết chán. Ngày 27/09/2022, họ đã công bố bài báo có tự đề Monolith: Real Time Recommendation System With Collisionless Embedding Table tại địa chỉ https://arxiv.org/pdf/2209.07663.pdf. Chủ đề này khá nặng về khả năng xây dựng hệ thống để làm sao đạt được mô hình chất lượng với thời gian near-realtime. Để có thể hiểu sâu bài này, các bạn có nên có kiến thức về Recurrent Neural Networks for Recommendations.\nPhần dẫn nhập - Chào đầu 1. Phần giới thiệu 1.1 Sparsity và Dynamism 1.2 Non-stationary Distribution 2. Design colisionless Hash Table và Online Training 2.1 Xây dựng Hash Table 2.2 Online training 2.2.1 Streaming Engine 2.2 .2 Online Joiner 2.2.3 Parameter Synchronization 2.3 Fault Tolerance 3. Đánh giá - EVALUATION 3.1 Thiết lập thí nghiệm 3.1.1 Xây dựng embedding table 3.1.2 Online training 3.2 Kết quả và phân tích 3.2.1 Hiệu quả của embedding collision 3.2.2 Online Training: Trading-off Reliability For Realtime. Tài liệu tham khảo của paper Tham khảo Phần dẫn nhập - Chào đầu Các doanh nghiệp có nhu cầu xây dựng real-time recommendation để phục vụ khách hàng tốt hơn.\nCác framwork deep-learning được sử dụng trong production thường không đáp ứng được nhu cầu recommend của doanh nghiệp, lý do là:\nTinh chỉnh hệ thống dựa trên các tham số tĩnh và thực hiện nhiều phép tính toán trên feature thưa (sparse) và động (dinamic) làm giảm chất lượng mô hình.\nViệc training và serving tách bạch nhau, không có online training (model không thể retrain ngay lập tức khi có feedback của người dùng)\nVì những nguyên nhân trên, nhóm tác giả của ByteDance đã thiết kế một mô hình online training mới, đặt tên là Monolith.\nMô hình mới có 2 thành tố mới:\nĐề xuất collisionless embedding table với các tối ưu như expirable embeddings và frequency filtering để giảm lượng bộ nhớ tiêu thụ.\nĐề xuất một mô hình kiến trúc production-ready online training với high fault-tolerance.\nCuối cùng, chứng minh rằng độ tin cậy của hệ thống có thể đánh đổi bằng việc học theo thời gian thực.\n1. Phần giới thiệu \u0026ldquo;Hình 1: Monolith Online Training Architecture - Hình ảnh được cắt từ paper\nData của recommendation khác xa data của language modeling hoặc computer vision ở 2 khía cạnh:\nHầu hết các đặc trưng rất thưa, có tính phân loại và thay đổi linh hoạt.\nPhân phối của data là không dừng (non-stationary) , vd Concept Drift. [8]\n1.1 Sparsity và Dynamism Nhắc lại, dữ liệu cho recommendation hầu hết là các đặc trưng category dạng thưa, một vài trong số đó có tầng suất xuất hiện rất thấp. Việc mapping chúng lên không gian đặc trưng cao chiều hơn sẽ gặp các vấn đề:\nKhông giống như các mô hình ngôn ngữ có số lượng từ hạn chế, data user ranking item thường rất rất lớn. Khả năng cao là 1 máy chủ siêu mạnh hiện nay của các doanh nghiệp không chứa nổi Embedding table trên bộ nhớ chính.\nTrường hợp tệ nhất,kích thước của Embedding table sẽ tiếp tục tăng theo thời gian do người dùng và sản phẩm mới liên tục được thêm vào hệ thống. Trong khi đó, một số frameword recommendation sử dụng fixed-size dense variables để biểu diễn embedding table. Ví dụ framework [1,17]\nTrong thực tế, nhiều thuật toán đã dùng vài \u0026ldquo;mẹo\u0026rdquo;, như xài hashing như bài báo [3] và bài báo [6] , để giảm lượng memory tiêu thụ và cho phép tăng ID. Ý tưởng này dựa trên giả định là ID trong Embedding table phân phối đều và việc collisions thì vô hại. Giả định này không đúng trong hiện thực, khi mà một nhóm nhỏ user hoặc item có tầng suất xuất hiện cao hơn. Với sự tăng trưởng tự nhiên của embedding table, xác suất hash key đụng độ sẽ càng cao, dẫn đến giảm chất lượng mô hình.\nDo đó, nhu cầu tự nhiên của một hệ thống gợi ý là có khả năng capture càng nhiều đặc trưng trong chính các tham số của mô hình, và có khả năng điều chỉnh linh hoạt số user và số item mà nó có khả năng lưu giữ.\n1.2 Non-stationary Distribution Các pattern mới về hình ảnh và ngôn ngữ trong bài toán xử lý ảnh và xử lý ngôn ngữ thường không thay đổi nhiều trong hàng thế kỷ. Trong khi đó, sự quan tâm của người dùng về một chủ đề nào đó có thể thay đổi từng phút một. Kết quả là, phân phối của dữ liệu người dùng là không cố định, và hiện tượng này thường được gọi với tên là Concept Drift.\nThông thường, thông tin lịch sử gần nhất thường có đóng góp hiệu quả nhất cho việc dự đoán việc thay đổi hành vi người dùng. Để giảm thiểu tác động của Concept Drift, các mô hình \u0026ldquo;serving\u0026rdquo; cần phải cập nhật thường xuyên từ những phản hồi của người dùng, càng real-time càng tốt, để phản ánh tốt nhất xu hướng quan tâm của người dùng.\nDựa trên các phân tích trên, nhóm tác giả đã xây dựng nên monolith, có khả năng:\nCung cấp đầy đủ năng lực xử lý cho các đặc trưng thưa bằng cách thiết kế một collisionless hash table và cơ chế loại bỏ các dynamic feature.\nĐưa thông tin feedback của người dùng vào training realtime với online training\nDựa vào kiến trúc này, mô hình monolith vượt trội hơn so với các hệ thống sử dụng collisions hash table với dung lượng bộ nhớ sử dụng là tương đương nhau\n2. Design colisionless Hash Table và Online Training Hình 2: Worker-PS Architecture - Hình ảnh được cắt từ paper\nKiến trúc của Monolith sử dụng TensorFlow’s distributed Worker-ParameterServer (Worker-PS) như hình trên. Trong mô hình, các máy được phân công với các nhiệm vụ khác nhau. Worker machine chịu trách nhiệm tính toán theo định nghĩa trước, PS machine lưu trữ các tham số và cập nhật kết quả tham số theo workers.\nTrong mô hình recommendation, các tham số được phân loại làm hai nhóm: nhóm dense và nhóm sparse. Các tham số Dense là các trọng số của mô hình DNN, các tham số sparse tham chiếu tới embedding table tương ứng với các sparse feature. Cả Dense parameter và sparse parameter đều là các phần của TensorFlow Graph, và được lưu trữ trên parameters servers.\n2.1 Xây dựng Hash Table Hình 3: Cuckoo HashMap. - Hình ảnh được cắt từ paper\nNguyên tắc đầu tiên để xây dựng các tham số biểu diễn tính thưa là tránh thu gọn thông tin từ các IDs khác nhau về cùng một fixed-sze embedding.\nViệc xây dựng một embedding table sử dụng TensorFlow Variable sẽ dẫn đến việc đụng độ ID khi số lượng ID mới và table tăng lênh. Do đó, thay vì xây embedding table dựa trên Variable, tác giả đã phát triển một key-value HashTable cho các tham số thưa.\nHashTable này sử dụng Cuckoo Hashmap [16], hỗ trợ việc thêm một key mới mà không đụng độ với key cũ. Cuckoo Hashing trong trường hợp xấu nhất có độ phức tạp O(1) trong việc tìm kiếm và xoá, và O(1) cho việc thêm mới. Như trong hình 3, nó sử dụng hai bảng T0 và T1 với hai hàm hash khác nhau có tên là h0(x) và h1(x), và một phần tử sẽ được lưu trữ trong một trong hai bảng trên. Khi cố gán thêm một phần tử A vào T0, đầu tiên, nó cố gán đặt A vào h0(A). Nếu h0(A) đã hold 1 phần tử B nào đó, nó sẽ xoá B từ T0 và gán B vào T1 với logic tương tự. Quá trình này được lặp đi lặp lại đến khi ổn định.\nViệc giảm bộ nhớ lưu trữ cũng là một yếu tố quang trọng trong thiết kế hệ thống. Một cách tự nhiên, việc mỗi lần thêm 1 phần tử mới vào HashTable sẽ làm cho bộ nhớ nhanh chóng đầy. Có 2 kết luận có thể được rút ra:\nCác ID xuất hiện vài lần có đóng góp rất hạn chế đối với việc cải thiện chất lượng mô hình. Các quan sát quan trọng là các quan sát mà các IDs ở dạng long-tail distributed, khi các ID phổ biến có số lần xuất hiện hàng triệu lần, trong khi đó, các ID không phổ biến xuất hiện không quá mười lần. Các ID tầng suất thấp làm mô hình underfit do thiếu data training và model sẽ không có khả năng đưa ra dự đoán tốt dưa trên những thông tin mà chúng cung cấp. Hơn hết, các thông tin của các ID trên thường ít ảnh hưởng đến kết quả của mô hình, do đó, việc xoá đi các ID có tầng suất thấp không ảnh hưởng nhiều đến chất lượng mô hình.\nLịch sử từ thời napoleon có đóng góp rất thấp vào mô hình hiện tại. Do người dùng ngừng hoạt động, hoặc là video đã lỗi thời. Việc lưu trữ embedding cho các ID này không giúp ích cho mô hình, ngược lại chúng còn góp phần làm tăng chi phí lưu trữ và chi phí tính toán.\nDựa trên những điều trên, nhóm kỹ sư đề xuất thiết kế ID filtering heuristic để giúp tối ưu hoá bộ nhớ lưu trữ:\nCác ID được lọc trước khi được đưa vào trong hệ thống embedding table. Có 2 phương pháp lọc Lọc theo tầng xuất xuất hiện trước khi ID được thêm vào. Giá trị ngưỡng là siêu tham số và được turning. Lọc theo xác xuất, giúp cho giảm bộ nhớ tiêu thụ. ID được gán thời gian và bị expire sau một khoảng thời gian inactive. HashTable được implement dưới dạng Tensorflow resource operation.\n2.2 Online training Việc trainnig được chia làm 2 giai đoạn:\nGiai đoạn Batch training. Giai đoạn này hoạt động như việc training mô hình TF bình thường. Trong mỗi bước training, các worker đọc một mini-batch data training từ storage, lấy tham số từ PS, tính lan truyền xuôi, lan truyền ngược, và cập nhật tham số vào PS. Dataset được train 1 lần duy nhất.\nGiai đoạn training online. Sau khi model được deploy vào online serving, việc traning không có dừng hẵng, mà chuyển qua giai đoạn online training. Thay vì đọc các mini-batch data từ storage, training worker sẽ lấy realtime data để train và cập nhật lại PS. Training PS sẽ định kỳ cập nhật các tham số vào serving PS.\n2.2.1 Streaming Engine Hình 4: Streaming Engine. The information feedback loop from [User → Model Server → Training Worker → Model Server → User] would spend a long time when taking the Batch Training path, while the Online Training will close the loop more instantly - Hình ảnh được cắt từ paper\nHình 4 phía trên mô tả việc chuyển đổi liền mạch giữa batch training và online training.\nMô hình sử dụng Kafka Queue [13] để log lại các hành động của user ( click item, like item, thả tim\u0026hellip;. ) và một Kafka queue khác lưu lại các đặc trưng. Core engine là Flink [4] stream job cho online feature joiner. Online joiner kết hợp các đặc trưng với nhãn từ user action tạo thành training example, sau đó đẩy vào kafka queue. Queue cho training example được consumer bới online training và batch training.\nVới online training, training worker trực tiếp đọc dữ liệu từ kafka Queue.\nVới batch training, data sẽ được đóng gói vào file HDFS (dump job handle việc này). Sau khi data trong HDFS tích luỹ với số lượng đủ dùng, training worker sẽ load data từ HDFS và thực hiện batch training.\n2.2 .2 Online Joiner Hình 5: : Online Joiner - Hình ảnh được cắt từ paper\nTrong ứng dụng thực tế, hành động của user và feature của user được stream vào online joiner mà không đảm bảo thứ tự về thời gian. Do đó, cần phát sinh một unique key cho mỗi request để đảm bảo pair được chúng với nhau.\nViệc user bị lag cũng là một vấn đề cần được xem xét. Ví dụ, một user có khả năng mất vài ngày mới ra quyết định mua một sản phẩm mà họ đã xem vài ngày trước đó. Đây là một thách thức thật sự, bởi vì nếu toàn bộ các đặc trưng được lưu trữ trong bộ nhớ chính, thì chúng ta sẽ không đủ bộ nhớ để lưu trữ. Nhóm tác giả đã sử dụng on-disk key-value storage để lưu trữ các đặc trưng của người dùng ở quá khứ. Khi log của người dùng được đẩy vào hệ thống, trước hết nó sẽ được tìm kiếm trong memory cache, trong trường hợp mising cache, nó sẽ tìm trong key-value storage.\nMột vấn đề nữa là phân bố mẫu âm và mẫu dương trong data không đồng đều. Trong đó, lượng mẫu dương thường cao hơn rất nhiều so với mẫu âm. Để ngăn chặng thằng mẫu dương thống trị, một chiến lược thường hay được sử dụng là sampling mẫu âm. Tất nhiên việc này sẽ làm thay đổi phân bố của mô hình huấn luyện. Và sử dụng log odds corection trong quá trình serving [19]\n2.2.3 Parameter Synchronization Trong suốt quá trình training, dữ liệu sẽ liên tục đổ về online serving module và cập nhật tham số trên PS. Trong môi trường thật, sẽ có một vài thách thức:\nModel trên online serving PS bắt buộc phải hoạt động khi update. Kích thước model khá lớn, và việc update toàn bộ tham số sẽ tốn kha khá thời gian (lưu ý ở đây là không thể thực hiện update từng phần, mà phải update toàn bộ tham số). Nên phải tìm cách thức nào đó để việc update không ảnh hưởng tới việc infer của model.\nViệc tranfer model có kích thước lớn từ training PS tới online server PS sẽ gây áp lực lớn đến băng thông mạng và bộ nhớ trên PS. Một yêu cầu tối thiểu là bộ nhớ phải có kích thước ít nhất là gấp 2 lần kích thước của model (trên RAM)\nĐể scale up model cho phù hợp với nghiệp vụ kinh doanh, nhóm tác giả đã thiết kế riêng một cơ chế đồng bộ hoá, dựa trên các quan sát sau:\nCác tham số thưa thì thường thống trị kích thước của mô hình gợi ý.\nTrong một khoảng thời gian ngắn (short range of time window), chỉ một nhóm nhỏ các ID được training, và chỉ những embedding của những ID đó được cập nhật.\nCác biến Dense tranfer chậm hơn so với spare embeddings. Bởi vì size của Dense variable rất lớn.\nNhận định 1 và 2 ở nhận cho phép chúng ta tránh cập nhật sparse của toàn bộ các đặc trưng của ID. Trong mô hình, các ID chưa được huấn luyện kể từ lần huấn luyện cuối cùng sẽ được đẩy vào touched key. Sau khi training xong, chúng ta sẽ đẩy các sparse parameter trong touched key vào online serving PS với tần suất tính bằng phút. Gói cập nhật này khá nhỏ (so với toàn bộ ), nên chúng ít sẽ sử dụng băng thông mạng rất thấp, và sẽ không tạo mô hình răng cưa cho bộ nhớ RAM trong quá trình đồng bộ.\nVới nhận định (3), chúng ta sẽ giảm I/O mạng và bộ nhớ sử dụng bằng cách đặt lịch đồng bộ hoá dày hơn cho các tham số thưa, trong khi đó sẽ có tầng xuất cập nhật tham số dense ít hơn. Việc này cũng gây ra tình huống là các tham số thưa sẽ mới hơn rất nhiều so với tham số dense, do đó sẽ có mất mát xảy ra. Mất mát này được chấp nhận do nó không quá nghiêm trọng. Trong phần cuối có thí nghiệm về vấn đề này.\n2.3 Fault Tolerance Đối với hệ thống thực, kiến trúc của hệ thống phải đảm bảo khả năng phục hồi trong trường hợp có lỗi xảy ra. Một lựa chọn phổ biến thường được hay dùng là snapshot trạng thái của model định kỳ, và phục hồi dữ liệu từ lần snapshot cuối cùng khi nhận thấy có lỗi. Việc lựa chọn tầng suất snapshot dựa vào hai yếu tố chính:\nChất lượng model. Dĩ nhiên rằng model snapshot ở càng gần phiên bản cuối càng tốt, do đó tầng suất snapshot phải tăng lên.\nChi phí sử dụng hệ thống. Việc snapshot một model có kích thước lớn sẽ tốn kha khá cpu và bộ nhớ để copy data, ngoài ra còng tăng disk I/O\nĐể cân bằng giữa 2 cái trên, Monolith snapshot toàn bộ training PS mỗi ngày. Chúng ta sẽ mất 1 ngày update data khi lỗi xảy ra. Nhưng qua các thử nghiệm của nhóm kỹ sư ByteDance, thì hiệu năng suy giảm vẫn ở mức chấp nhận được.\n3. Đánh giá - EVALUATION Để hiểu hơn về lợi ích và sự đánh đổi của mô hình được đề xuất,chúng ta xây dựng một vài thí nghiệm và A/B testing trên môi trường thực. Mục tiêu là trả lời các câu hỏi sau:\nLợi ích của collisionless hashtable là bao nhiêu?\nMức độ quang trọng của realtime training online?\nLiệu rằng mô hình thiết kế của Monolith với các tham số được đồng bộ như trên đã đủ tốt trong môi trường thực tế?\n3.1 Thiết lập thí nghiệm 3.1.1 Xây dựng embedding table Như mô tả ở mục 2.1, embedding table trong monolith là collisionless hashtable. Để chứng minh sự cần thiết của việc tránh đụng độ trong việc thiết kế embedding table và lợi ích nhận được từ phiên bản collisionless mà mô hình đề xuất, chúng ta thực hiện hai nhóm thí nghiệm trên tập Movielens và trong tập internal production dataset của ByteDance.\nHình 6: DeepFM model architecture - Hình ảnh được cắt từ paper\nMovieLens. Là tập dataset chuẩn , mở, bao gồm 25 triệu đánh giá từ xấp xỉ 162000 user và 62000 bộ phim: Tiền xử lý label. Label gốc của tập có giá trị từ 0.5 đến 5, trong khi đó, mô hình monolith nhận giá trị binary từ user. Chúng ta sẽ chuyển giá trị từ scale label sang binary label bằng việc đặt ngưỡng \u0026gt;=3.5 là positive sample và bé hơn 3.5 là negative sample\nĐánh giá Model và metrics. Chúng ta sẽ implement DeepFM model, một kiến trúc model phổ biến cho bài toán recommend. Nó bao gồm thành phần FM và thành phần dense (xem kỹ hình 6).Sử dụng AUC để đánh giá giá trị predict.\nĐánh giá Embedding collisions. Dataset này có gần 160k user và 60k movie. Để so sánh, chúng ta sẽ sử dụng MD5 làm quân đỏ và mapping vào một nhóm nhỏ ID space, mục đích là làm cho một vài ID sẽ dùng chung embedding với nhau. Bảng bên dưới sẽ hiển thị chi tiết thống kê của user và movie trước và sau hash\nVPB User IDs Movie IDs # Before Hashing 162541 59047 # After Hashing 149970 57361 Collision rate 7.73% 2.86% Bảng 1: Thống kê ID trước và sau khi hash\n3.1.2 Online training Trong quá trình online training, chúng ta sẽ cập nhật tham số từ training PS sang online PS với tần suất theo phút. Chúng ta thiết kế hai nhóm thí nghiệm để đánh giá chất lượng của mô hình và độ tải của hệ thống.\nUpdate frequency. Để đánh giá sự cần thiết của việc update theo phút, chúng ta xây dựng thí nghiệm với tầng xuất update khác nhau và xem sự hiệu quả. Chúng ta sử dụng Criteo Display Ads Challenge dataset (https://www.kaggle.com/competitions/criteo-display-ad-challenge/data), đây là dataset được sử dụng để benchmarking CTR model. Data bao gồm 7 ngày dữ liệu, ghi nhận feature và hành động click của người dùng. Trong thí nghiệm này, chúng ta xài mô hình DeepFM mô tả trong hình 6. Để mô phỏng online training, chúng ta sẽ chia tập dữ liệu thành 2 phần. Phần đầu tiên là 5 ngày, dùng để train, phần thứ 2 là 2 ngày còn lại, dùng cho online training. Trong 2 ngày dữ liệu của phần 2, chúng ta sẽ chia thành N shard. Thí nghiệm với N =10, 50, 100, tương ứng 5h (2 ngày = 48 tiếng / 10 = 4.8 tiếng ~ 5 tiếng), 1h ( 48/50 ) và 30 phút cập nhật dữ liệu một lần.\nLive experiment. Thêm nữa, chúng ta sẽ thực hiện thí nghiệm thực thế với real serving traffice để mô phỏng sự quang trọng của online training trong ứng dụng thực. Thí nghiệm A/B testing này so online training (A) vs batch training (B). 3.2 Kết quả và phân tích 3.2.1 Hiệu quả của embedding collision Hình 7: Effect of Embedding Collision On DeepFM, MovieLens\nCả hai kết quả từ MovieLens dataset và Internal recommedation dataset đều chỉ ra rằng collisions embedding gây tổn hại cho chất lượng của mô hình.\nMô hình với collisionless HashTable cho kết quả tốt hơn, luôn có đồ thị nằm ở ngoài so với mô hình collision. Kết luận này luôn luôn đúng, cho dù: Tăng số lượng training epoch. Như kết quả ở hình 7. Mô hình collisionless embedding table có AUC cao hơn ở epoch đầu tiên, và hội tụ với giá trị cao hơn.\nThay đổi phân phối theo thời gian (Concept Drift). Như hiển thị trong hình 8, mô hình với collisionless embedding table cũng cho kết quả rất tốt trong ngữ cảnh user/items thay đổi.\nTính thưa của data được sinh ra bởi collisionless embedding table sẽ không làm cho mô hình bị overfit. Như kết quả ở hình 7, mô hình không bị overfit sau khi nó đã hội tụ 3.2.2 Online Training: Trading-off Reliability For Realtime. Hình 8: Effect of Embedding Collision On A Recommendation Model In Production.\nChúng ta khám phá ra rằng việc đồng bộ các tham số với tầng suất cao thì luôn luôn cải tiến online serving AUC, và mô hình tốt hơn so với kỳ vọng.\nThe Effect of Parameter Synchronization Frequency. Trong thí nghiệm về online stream training với Criteo Display Ads Challenge dataset, chất lượng model sẽ tốt hơn nếu tăng tầng suất đồng bộ hoá mô hình, chứng minh bằng hai khía cạnh sau: Model có online training sẽ tốt hơn so với mô hình không có online training. Xem hình 9\nModel có tần suất cập nhật cao sẽ tốt hơn so với mô hình có tuần suất cập nhật thấp. Xem hình 10 và bảng 2\nHình 9: : Online training v.s. Batch training on Criteo dataset. Blue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\nSync Interval Average AUC (online) Average AUC (batch) 5 hr 79.66 ± 0.020 79.42 ± 0.026 1 hr 79.78 ± 0.005 79.44 ± 0.030 30 min 79.80 ± 0.008 79.43 ± 0.025 Bảng 2: Average AUC comparison for DeepFM model on Criteo dataset\nHình 10: Comparison of different sync intervals for online training.\nBên cạnh các quan sát này, chúng ta thực hiện đồng bộ sparse parameter vào serving PS với tầng suất càng sớm càng tốt (theo phút) để mở rộng khả năng tính toán và độ tin cậy của hệ thống.\nGiả sử rằng các dense parameter yêu cầu tầng suất cập nhật ít hơn như thảo luận ở mục 2.2.3, chúng ta sẽ cập nhật chúng ở mức ngày, và xác xuất hệ thống bị quá tải sẽ rất thấp. Ví dụ chúng ta có 100k ID được cập nhật trong 1 phút, embedding có kích thước 1024, tổng kích thước của data cần để chuyển 4KB x 100000 = 40000 MB một phút. Với dense parameter, nếu chúng ta thực hiện daily sync, chúng ta sẽ chọn thời điểm sync mà traffice là thấp nhất ( gần sáng chẳng hạn)\nThe effect of PS reliability Với việc đồng bộ hoá các tham số ở mức phút, chúng ta tự nhiên suy nghĩ trong đầu rằng tầng suất snapshot cũng nên phải tương đương như vậy. Tuy nhiên trong thực tế, khi chúng ta sử dụng snapshot với tầng suất 1 ngày delay, chất lượng mô hình cũng không giảm quá nhiều.\nViệc tìm kiếm điểm cân bằng giữa chất lượng mô hình và năng lực tính toán là rất khó trong bành toán personalized ranking. Khi user cực kỳ nhạy cảm với chất lượng recommendation. Theo truyền thống, các hệ thống lớn thường có xu hướng đặt tầng xuất retrain theo hướng hi sinh năng lực tính toán (chạy ì ạch, lâu cũng được) và đánh đổi bởi cực tiểu hoá độ lỗi.\nVí dụ với tỷ lệ lỗi 0.01 của PS machine / day, chúng ta sẽ snapshot lại tham số của ngày hôm trước, Giả sử dúng ta sharding parameter vào 1000PS, chúng snapshot mỗi ngày. Tỷ lệ lỗi 0.01%, mỗi một máy sẽ bị lỗi sau 10 ngày , và chúng ta sẽ mất toàn bộ data của 1 ngày cập nhật. Giả sử DAU của 10 triệu và chúng ta mất 1 ngày dữ liệu của 5k user mỗi 10 ngày. Điều này có thể được chấp nhận bởi vì\na. Với các đặc trưng thưa của user, nó tương đương với tỷ lệ mất 0.01% DAU\nb. Với các đặc trưng Dense, chúng ta cập nhật khá chậm, như thảo luận ở mục 2.2.3, việc mất 1 ngày update của 1000 PS là không đáng kể.\nQua những quan sát và tính toán ở trên, chúng ta có thể kết luận rằng tầng xuất snapshot thấp không ảnh hưởng nhiều đến khả năng chịu lỗi, và giảm khả năng xử lý của hệ thống.\nCảm ơn nhóm kỹ sư ByteDance đã cung cấp rất nhiều thông tin hữu ích trong bài viết. Hi vọng lần sau sẽ đọc được nhiều bài chất lượng hơn thế nữa.\nTài liệu tham khảo của paper [1] \u0026ldquo;Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale machine learning. ArXiv abs/1605.08695 (2016).\u0026rdquo;\n[2] Andrew P. Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–1159.\n[3] Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram\u0002engineering.com/core-modeling-at-instagram-a51e0158aa48\n[4] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi, and Kostas Tzoumas. 2015. Apache Flink™: Stream and Batch Processing in a Single Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\n[5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide \u0026amp; Deep Learning for Recommender Systems. Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (2016).\n[6] Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. Proceedings of the 10th ACM Conference on Recommender Systems (2016).\n[7] Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif\u0002teenth ACM Conference on Recommender Systems (2021).\n[8] João Gama, Indre Žliobait ˙ e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia. 2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46 (2014), 1 – 37.\n[9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI.\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen, David M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Andrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and Xuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based Personalized Recommendation. 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) (2020), 488–501.\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui Huang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, PengSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019 XDL: an industrial deep learning framework for high-dimensional sparse data. Proceedings of the 1st International Workshop on Deep Learning Practice for High\u0002Dimensional Sparse Data (2019).\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying Lin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Haibo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXivabs/2111.05897 (2021).\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom\u0002mender Systems (in Chinese). https://tech.meituan.com/202112/09/meituantensorflow-in-recommender-systems.html\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS.\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler. 2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST) (2010), 1–10.\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative Sampling and Log Odds Correction with Rare Events Data. In Advances in Neural Information Processing Systems.\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen Lin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient Continual Learning for Large-Scale Real-Time Recommendations. SC20: Inter\u0002national Conference for High Performance Computing, Networking, Storage and Analysis (2020), 1–17.\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the 28th ACM International Conference on Information and Knowledge Management (2019).\nTham khảo https://arxiv.org/pdf/2209.07663.pdf\nCảm ơn các bạn đã dành thời gian đọc bài tóm tắt này của mình. Nếu có bất kỳ vấn đề gì, hãy để lại comment bên dưới hoặc email cho mình qua địa chỉ alexblack2202@gmail.com. Hẹn gặp lại các bạn ở bài viết tiếp theo.\n","date":"Dec 1, 2022","img":"https://unsplash.it/1920/1080?image=18","permalink":"/blog/2022-12-01-tiktok-recommendation/","series":null,"tags":["Machine Learning","Tikok","Deep Learning","ByteDance","Recommendation"],"title":"Tiktok Real Time Recommendation"},{"categories":null,"content":"Mục tiêu của chúng ta hôm nay là khám phá tập dataset netflix này, để khám phá và đào xới những thông tin ẩn bên trong, phục vụ cho việc ra quyết định loại phim/ chương trình nào nên sản xuất và chúng ta nên đề xuất loại hình kinh doanh nào cho mỗi quốc gia khác nhau.\nTrực quan hoá dữ liệu - Data Visualization Trực quan hoá dữ liệu - Data Visualization Dataset các bạn có thể download ở đây: https://www.kaggle.com/datasets/shivamb/netflix-shows\nĐầu tiên, chúng ta sẽ import các thư viện cần thiết để phục vụ cho việc phân tích. Các như viện không thể thiếu là pandas, matplotlib, seaborn, numpy.\n1 2import matplotlib.pyplot as plt 3 4import seaborn as sns 5 6import pandas as pd Ngó sơ qua dataset, chúng ta có các cột như sau:\nShow_id: mã định danh duy nhất cho các show / phim\nType: Loại chương trình\nTitle: Tên phim / show\nDirector: Tên đạo diễn\nCast: Diễn viên chính\nCountry: Quốc gia nơi bộ phim được sản xuất\nDate_added: Ngày được đưa lên hệ thống netflix\nRelease_year: Ngày hoàn thành bộ phim\nRating: Điểm yêu thích của bộ phim\nDuration: Thời gian của bộ phim, hoặc số tập .\nListed_in: Thể loại phim\nDescription: Mô tả sơ bộ về bộ phim\nBây giờ, chúng ta load dữ liệu bằng pandas và phân tích thử\n1 2data=pd.read_csv(\u0026#39;netflix.csv\u0026#39;) 3 4print(data.shape) 5 6\u0026gt;\u0026gt;\u0026gt;(8807, 12) Data có 12 cột như đã đề cập ở trên, 8807 dòng. Ngó xem phân bố dữ liệu rỗng như thế nào\n1 2plt.figure(figsize=(8,8)) 3sns.heatmap(data.isna()) 4 5plt.show() Nhìn vào biểu đồ trên, chúng ta thấy rằng cột director bị lủng NA nhiều nhất, tiếp đến là cột cast và cột country. Cột Date_added và và rating lủng chút chút, các cột còn lại khá tốt.\nDo cột director bị lủng nhiều, nên chúng ta sẽ bỏ qua, không phân tích top 10 đạo điễn có số lượng phim nhiều nhất. Tương tự như vậy với cột diễn viên chính (cast), và cột quốc gia sản xuất (country). Ngó đi ngó lại, chỉ còn có liệt kê top 10 thể loại phim (Listed_in) được sản xuất nhiều nhất .\n1 2plt.figure(figsize=(16,16)) 3data[\u0026#34;listed_in\u0026#34;].value_counts()[:10].plot(kind=\u0026#34;barh\u0026#34;, color=\u0026#34;orange\u0026#34;) 4plt.title(\u0026#34;Top 10 thể loại phim trên NETFLIX\u0026#34;,size=20) 5 6plt.show() Phim truyền hình và phim tài liệu có số lượng phim nhiều nhất, tiếp theo là phim hài\nDo Na ở cột country khá nhiều, nên chúng ta sẽ loại bỏ hết các dòng có contry na đi\n1 2df = data.copy() 3df[\u0026#39;country\u0026#39;] = df[\u0026#39;country\u0026#39;].ffill(axis=0) 4df.head(10) Quan sát dữ liệu cột country, chúng ta nhận thấy rằng có nhiều phim được quay ở nhiều địa điểm, nên chúng ta ta cần xử lý lại chỗ này một chút. Đơn giản là mình chỉ lấy quốc gia đầu tiên xuất hiện trong cột country.\n1 2df[\u0026#39;trim_country\u0026#39;] = df[\u0026#39;country\u0026#39;].apply(lambda x: x.split(\u0026#39;,\u0026#39;)[0]) 3df.head(10) Thử show ra biểu đồ rating của top 5 quốc gia nhiều phim nhất\n1 2 3countries = df[\u0026#39;trim_country\u0026#39;].unique() 4 5ratings = df[\u0026#39;rating\u0026#39;].unique() 6 7fig = plt.figure( 8 figsize=(20,30) 9 ) 10 11for i, name in enumerate(countries[:5]): 12 frame = df[df[\u0026#39;trim_country\u0026#39;] == str(name)] 13 ax = fig.add_subplot(len(countries[:5]),1,i+1) 14 topic = name 15 sns.countplot(x=\u0026#39;rating\u0026#39;, data= frame[frame[\u0026#39;rating\u0026#39;].isin(ratings)]) 16 ax.set_title(topic) 17 plt.subplots_adjust(left=0.1, 18 bottom=0.1, 19 right=0.9, 20 top=0.9, 21 wspace=0.4, 22 hspace=0.4) 23 ax.set(ylabel=\u0026#39;Content Produced\u0026#39;) Thử show ra biểu đồ đếm thể loại phim được quay trên top 5 quốc gia có nhiều phim nhất xem như thế nào\n1 2countries = df[\u0026#39;trim_country\u0026#39;].unique() 3 4listing = df[\u0026#39;trim_listed_in\u0026#39;].unique() 5ratings = df[\u0026#39;rating\u0026#39;].unique() 6 7fig = plt.figure( 8 figsize=(40,30) 9 ) 10 11for i, name in enumerate(countries[:5]): 12 frame = df[df[\u0026#39;trim_country\u0026#39;] == str(name)] 13 ax = fig.add_subplot(len(countries[:5]),1,i+1) 14 topic = name 15 sns.countplot(x=\u0026#39;trim_listed_in\u0026#39;, data= frame[frame[\u0026#39;trim_listed_in\u0026#39;].isin(listing)]) 16 ax.set_title(topic) 17 plt.subplots_adjust(left=0.1, 18 bottom=0.1, 19 right=0.9, 20 top=0.9, 21 wspace=0.4, 22 hspace=0.4) 23 ax.set(ylabel=\u0026#39;Content Produced\u0026#39;) Còn nhiều cái để khám phá nữa, ví dụ\nTop 10 diễn viên xuất hiện nhiều nhất trên TV Shows\nTop 10 diễn viên xuất hiện nhiều nhất trên Movies\nTV Shows nhiều mùa nhất\nThời gian công chiếu dài nhất\nTham khảo:\nhttps://www.kaggle.com/code/shivamb/netflix-shows-and-movies-exploratory-analysis/notebook\nhttps://www.analyticsvidhya.com/blog/2021/09/performing-eda-of-netflix-dataset-with-plotly/\nhttps://medium.datadriveninvestor.com/netflix-data-exploration-and-visualization-1d270234c2d4\nhttps://public.tableau.com/views/DataVizProject_16166857387730/dashboard_assignement?%3Aembed=y\u0026%3AshowVizHome=no\u0026%3Adisplay_count=y\u0026%3Adisplay_static_image=y\u0026%3AbootstrapWhenNotified=true\u0026%3Alanguage=en\u0026%3Amobile=true\u0026:embed=y\u0026:showVizHome=n\u0026:apiID=host0\nhttps://jovian.ai/shagunsharma04061998/netflix-data-analysis/v/1?utm_source=embed#C33\n","date":"Aug 2, 2022","img":"https://unsplash.it/1920/1080?image=19","permalink":"/blog/2022-08-02-data-exploration-and-data-visualization/","series":null,"tags":["Machine Learning","Normalization","Deep Learning"],"title":"Data Visualization - Phần 1 - Phân Tích Dữ Liệu Netflix"},{"categories":null,"content":"Photo mình lấy từ unsplash\nPython là ngôn ngữ lập trình nổi tiếng vì tính cực kỳ linh hoạt. Trong bài viết này, mình sẽ liệt kê 7 cách để đọc nội dung file sử dụng ngôn ngữ Python.\nCách 1: Sử dụng hàm open Cách 2: mở file sử dụng context manager Cách 3: Sử dụng thư viện pathlib Cách 4: Sử dụng shell Cách 5: Xây dựng một thư viện đọc file bằng c Cách 1: Sử dụng hàm open Cách đầu tiên, cũng là cách vỡ lòng / giáo khoa/ trường lớp, là sử dụng hàm open, trả về một stream. Sau đó, chúng ta có thể sử dụng hàm read để lấy nội dung từ stream.\nVí dụ, chúng ta sẽ đọc file thegioididong.txt bằng ngôn ngữ python như sau:\n1 2tgdd = open(\u0026#39;thegioididong.txt\u0026#39;,\u0026#39;r\u0026#39;) 3 4lines = tgdd.read() 5print(lines) 6 7tgdd.close() Ưu điểm:\nKhông phải include thêm thư viện\nCode ngắn gọn\nKhuyết điểm:\nPhải close file sau khi sử dụng xong Cách 2: mở file sử dụng context manager Trên stackoverflow thường khuyên chúng ta sử dụng cách này\n1 2with open(\u0026#39;thegioididong.txt\u0026#39;,\u0026#39;r\u0026#39;) as tgdd: 3 4 lines = tgdd.read() 5 print(lines) Ưu điểm:\nKhông phải đóng file sau khi sử dụng xong\nNgăng ngừa memory leaks khi có lỗi trong quá trình xử lý và không gọi đóng file.\nCách 3: Sử dụng thư viện pathlib Cách dùng cũng khá dễ, chỉ cần include thư viện vào là xài thôi\n1 2import pathlib 3tgdd = pathlib.Path(\u0026#34;thegioididong.txt\u0026#34;) 4lines = tgdd.read_text() Cách này cũng hay, không phải đóng mở file, chỉ cần gọi hàm đọc là được. Code thì ngắn gọn, lại không phải thò ra thụt vào như là context manager\nCách 4: Sử dụng shell Chúng ta có thể dùng python, gọi shell script trong linux , và lấy kết quả trả về.\nĐể sử dụng cách này, chúng ta sử dụng thư viện subprocess\n1 2import subprocess 3output = subprocess.run([\u0026#34;cat\u0026#34;, \u0026#34;thegioididong.txt\u0026#34;], capture_output=True) 4lines = output.stdout.decode() Này là một cách có thể dùng được, tuy nhiên, một số file có thể bị lỗi encode. Túm lại là không nên xài cái này\nCách 5: Xây dựng một thư viện đọc file bằng c Để sử dụng cách này, các bạn cần phải cài bản python3-dev vào máy trước (trên ubuntu).\nVí dụ, chúng ta sẽ tạo một file mwg_file.c như sau\n1#define PY_SSIZE_T_CLEAN 2#include \u0026lt;Python.h\u0026gt; 3 4 5PyObject* mwgread(PyObject* self, PyObject* args) { 6 7 8 FILE * pFile; 9 size_t lSize; 10 char * buffer; 11 size_t result; 12 13 // Parse the Python object arguments into C variables 14 char* filename; 15 if (!PyArg_ParseTuple(args, \u0026#34;s\u0026#34;, \u0026amp;filename)) { 16 return NULL; 17 } 18 19 // Try to open the file 20 pFile = fopen(filename, \u0026#34;r\u0026#34;); 21 if (pFile == NULL) { 22 return NULL; 23 } 24 25 // obtain file size: 26 fseek (pFile , 0 , SEEK_END); 27 lSize = ftell (pFile); 28 rewind (pFile); 29 30 // allocate memory to contain the whole file: 31 buffer = (char*) malloc (sizeof(char)*lSize); 32 if (buffer == NULL) {fputs (\u0026#34;Memory error\u0026#34;,stderr); exit (2);} 33 34 // copy the file into the buffer: 35 result = fread (buffer,1,lSize,pFile); 36 if (result != lSize) {fputs (\u0026#34;Reading error\u0026#34;,stderr); exit (3);} 37 38 /* the whole file is now loaded in the memory buffer. */ 39 40 // terminate 41 fclose (pFile); 42 return Py_BuildValue(\u0026#34;s\u0026#34;, buffer); 43} 44 45 46PyMethodDef module_methods[] = { 47 {\u0026#34;read\u0026#34;, mwgread, METH_VARARGS, \u0026#34;Reads a file and returns its contents\u0026#34;}, 48 {NULL} 49}; 50 51struct PyModuleDef file_module = { 52 PyModuleDef_HEAD_INIT, 53 \u0026#34;MWGFile\u0026#34;, 54 NULL, 55 -1, 56 module_methods 57}; 58 59PyMODINIT_FUNC PyInit_MWGFile(void) { 60 return PyModule_Create(\u0026amp;file_module); 61} Một phần code c trên mình lấy từ https://cplusplus.com/reference/cstdio/fread/\nSau đó, chúng ta sẽ tạo file setup.py, file này để chung thư mục với file .c\n1 2from distutils.core import setup, Extension 3 4setup( 5 name=\u0026#39;MWGFile\u0026#39;, 6 ext_modules=[Extension(\u0026#39;MWGFile\u0026#39;, sources=[\u0026#39;mwg_file.c\u0026#39;])] 7) Cuối cùng, chúng ta gọi hàm để biên dịch file c và cài vào thư viện hệ thống\n1 2python3 setup.py build 3python3 setup.py install --user Để chạy thư viện c vừa mới biên dịch, chúng ta sử dụng lệnh sau\n1 2import MWGFile 3contents = MWGFile.read_file(\u0026#34;thegioididong.txt\u0026#34;) 4print(contents) Cách này khá cực, phải reimplement lại những gì cộng đồng đã làm sẵn, nhưng mà cũng nên thử để xây dựng các thư viện nội bộ của riêng mình.\nTài liệu tham khảo\nhttps://www.w3schools.com/python/python_file_open.asp\nhttps://betterprogramming.pub/7-ways-of-reading-a-file-in-python-855340b002dc\n","date":"Jul 31, 2022","img":"https://unsplash.it/1920/1080?image=20","permalink":"/blog/2022-07-31-5-way-open-file/","series":null,"tags":["Machine Learning","Normalization","Deep Learning"],"title":"5 Cách Mở File Trong Python"},{"categories":"python","content":"Nội dung bài viết này sẽ đề cập đến các chủ đề\nHàm trong python Tham số mặc định Arbitrary Arguments Keyword Arguments Arbitrary Keyword Arguments Hàm Lambda trong python Hàm map Hàm filter Hàm reduce Hàm trong python Hàm là một khối lệnh, được thực thi khi được gọi.\nHàm được định nghĩa bằng từ khoá def.\nHàm có thể nhận dữ liệu truyền vào, được gọi là tham số\nHàm có thể trả về dữ liệu\nVí dụ\n1 2def isSoChan(x:int): # khai báo hàm có tên là isSoChan, với tham số truyền vào kiểu int 3 if x \u0026lt;0: 4 return False 5 if x % 2 != 0: 6 return False 7 return True 8 9isSoChan(5) # gọi thực thi hàm isSoChan, với giá trị của tham số x là 5 Tham số mặc định Một số hàm sẽ có tham số mặc định, sử dụng khi ta bỏ trống, không truyền giá trị cho tham số, ví dụ như là tham số start của hàm range có giá trị mặc định là 0.\n1 2def printCountry(contry_name = \u0026#34;Việt Nam\u0026#34;): 3 print(contry_name) 4 5 6printCountry(\u0026#34;USA\u0026#34;) 7printCountry() 8 9#Kết quả 10 11\u0026gt;\u0026gt;\u0026gt; printCountry(\u0026#34;USA\u0026#34;) 12USA 13\u0026gt;\u0026gt;\u0026gt; printCountry() 14Việt Nam Arbitrary Arguments Đôi khi, chúng ta không thể xác định được số lượng tham số truyền vào, python hỗ trợ ta quăng các giá trị truyền dư vào một tham số cấp 1. Tên viết tắt của dạng này là *args\nVí dụ\n1def info(name, *args): 2 print(f\u0026#34;input name: {name}\u0026#34;) 3 for item in args: 4 print(f\u0026#34;other info: {item}\u0026#34;) 5 6info(\u0026#34;alex\u0026#34;,\u0026#34;18\u0026#34;,\u0026#34;staff\u0026#34;,\u0026#34;samsung\u0026#34;,\u0026#34;apple\u0026#34;) 7 8#Kết quả: 9 10input name: alex 11other info: 18 12other info: staff 13other info: samsung 14other info: apple Keyword Arguments Để gọi hàm một cách tường minh, python cho phép truyền tham số bằng cách chỉ rõ tên tham số cần truyền dữ liệu\nVí dụ\n1def info(name, age, position): 2 print(f\u0026#34;name {name} age {age} position {position}\u0026#34;) 3 4info(name=\u0026#34;alex\u0026#34;, age=18, position=\u0026#34;staff\u0026#34;) 5 6 7info(name=\u0026#34;bill\u0026#34;, position=\u0026#34;staff\u0026#34; , age=18) 8 9# Kết quả: 10 11\u0026gt;\u0026gt;\u0026gt; info(name=\u0026#34;alex\u0026#34;, age=18, position=\u0026#34;staff\u0026#34;) 12name alex age 18 position staff 13\u0026gt;\u0026gt;\u0026gt; 14\u0026gt;\u0026gt;\u0026gt; info(name=\u0026#34;bill\u0026#34;, position=\u0026#34;staff\u0026#34; , age=18) 15name bill age 18 position staff Arbitrary Keyword Arguments Trong trường hợp có nhiều tham số quá, chúng ta có thể viết tổng hợp các tham số dưới dạng tham số cấp 2. Tên viết tắt của dạng này là **kwargs\nVí dụ\n1 2def info(**data): 3 print(f\u0026#34;name {data[\u0026#39;name\u0026#39;]} age {data[\u0026#39;age\u0026#39;]} position {data[\u0026#39;position\u0026#39;]}\u0026#34;) 4 5info(name=\u0026#34;alex\u0026#34;, age=18, position=\u0026#34;staff\u0026#34;) 6 7 8info(name=\u0026#34;bill\u0026#34;, position=\u0026#34;staff\u0026#34; , age=18) 9 10 11# Kết quả 12 13\u0026gt;\u0026gt;\u0026gt; info(name=\u0026#34;alex\u0026#34;, age=18, position=\u0026#34;staff\u0026#34;) 14name alex age 18 position staff 15\u0026gt;\u0026gt;\u0026gt; 16\u0026gt;\u0026gt;\u0026gt; info(name=\u0026#34;bill\u0026#34;, position=\u0026#34;staff\u0026#34; , age=18) 17name bill age 18 position staff Hàm Lambda trong python Hàm Lambda là hàm chỉ có một biểu thức\nHàm Lambda có thể nhận nhiều tham số\nCú pháp\n1 2lambda arguments : expression Ví dụ:\n1 2info = lambda name, age, position : f\u0026#34;name {name} age {age} position {position}\u0026#34; 3 4info(name=\u0026#34;alex\u0026#34;, age=18, position=\u0026#34;staff\u0026#34;) 5 6# Kết quả 7\u0026gt;\u0026gt;\u0026gt; info(name=\u0026#34;alex\u0026#34;, age=18, position=\u0026#34;staff\u0026#34;) 8\u0026#39;name alex age 18 position staff\u0026#39; Sức mạnh của lambda được khai thác tối đa, khi lamda là tham số của một hàm khác.\nHàm map Cú pháp\n1 2map(function, iterable) Do input của map là function, nên nó có thể là một hàm tường minh, hoặc là một lambda function\nVí dụ:\nHãy nhân đôi tất cả các giá trị trong list\n1 2aList = [1,2,3,4,5] 3 4# Cách viết thông thường 5 6def square(x:int): 7 return x**2 8 9newList = [] 10for x in aList: 11 newList.append(square(x)) 12 13print(newList) 14 15# Cách viết sử dụng list comprehension 16 17newList = [x**2 for x in newList] 18print(newList) 19 20# Cách viết sử dụng map kết hợp lambda 21 22newList = list(map(lambda x:x**2,aList)) 23 24print(newList) 25 26\u0026gt;\u0026gt;\u0026gt; print(newList) 27[1, 4, 9, 16, 25] Hàm filter Cú pháp\n1 2filter(function, iterable) Hàm này tựa tựa như list comprehension với if contion\nVí dụ:\nHãy lọc ra các phần tử là số chẵn\n1 2aList = [1,2,3,4,5] 3 4# Cách viết thông thường 5 6def isEven(x:int): 7 return x%2==0 8 9newList = [] 10for x in aList: 11 newList.append(isEven(x)) 12 13print(newList) 14 15# Cách viết sử dụng list comprehension 16 17newList = [x for x in newList if x %2 ==0] 18print(newList) 19 20# Cách viết sử dụng filter kết hợp lambda 21 22newList = list(filter(lambda x:x%2==0,aList)) 23 24print(newList) 25 26\u0026gt;\u0026gt;\u0026gt; print(newList) 27[2, 4] Hàm reduce Hàm có nhiệm vụ tích luỹ tất cả các phần tử và trả về một giá trị duy nhất\nCú pháp\n1 2reduce(function, iterable, [, initializer]) Ví dụ:\nTính tổng các phần tử trong list sử dụng reduce\n1 2from functools import reduce 3 4aList = [1,2,3,4,5] 5 6print(reduce(lambda x,y: x+y,aList)) 7 8Kết quả: 9\u0026gt;\u0026gt;\u0026gt; print(reduce(lambda x,y: x+y,aList)) 1015 Đếm số lần xuất hiện của số chẵn trong list\n1 2from functools import reduce 3 4aList = [1,2,3,5,9] 5 6print(reduce(lambda acc,x: acc+1 if x%2 == 0 else acc,aList,0)) 7 8Kết quả: 9\u0026gt;\u0026gt;\u0026gt; print(reduce(lambda x,y: x+y,aList,0)) 1015 Cảm ơn các bạn đã theo dõi bài viết.\n","date":"Jul 16, 2022","img":"","permalink":"/courses/python/5_python_function/","series":["Khóa học python căn bản"],"tags":["python"],"title":"Bài 4: Hàm Trong Python"},{"categories":"python","content":"Trong bài viết này, chúng ta sẽ đề cập tới các câu lệnh điều khiển trong python. Các câu lệnh điều khiển bao gồm if, if-else, for, while\nCâu lệnh điều khiển if Câu lệnh điều khiển for Hàm range Kết hợp câu lệnh for với if từ khoá break, từ khoá continue, từ khoá pass Vòng lặp while Kỹ thuật duyệt container trong python Duyệt container sử dụng hàm enumerate Duyệt container sử dụng hàm zip Duyệt dic sử dụng hàm items Duyệt container sử dụng hàm sorted Duyệt container sử dụng hàm reversed List Comprehension Câu lệnh điều khiển if Câu lệnh if là câu lệnh căn bản và quan trong nhất. Câu lệnh được sử dụng để quyết định xem một khối lệnh có được thực hiện hay không. Về cơ bản, chúng ta có thể phân loại thành 3 nhóm câu lệnh if như sau.\nNhóm if loại 1. Câu lệnh if bình thường\n1 2if \u0026lt;điều kiện\u0026gt;: 3 # trường hợp \u0026lt;điều kiện\u0026gt; là đúng 4 câu lệnh 1 5 ... 6 câu lệnh n 7câu lệnh n+1 Nhóm if loại 2. Câu lệnh if có else\n1 2if \u0026lt;điều kiện\u0026gt;: 3 # trường hợp \u0026lt;điều kiện\u0026gt; là đúng 4 câu lệnh 1 5 ... 6 câu lệnh n 7else: 8 # trường hợp \u0026lt;điều kiện\u0026gt; là sai 9 câu lệnh 1 10 ... 11 câu lệnh n 12 13câu lệnh n+1 Nhóm if loại 3. Câu lệnh if else lồng nhau\n1 2if \u0026lt;điều kiện 1\u0026gt;: 3 # trường hợp \u0026lt;điều kiện\u0026gt; là đúng 4 câu lệnh 1 5 ... 6 câu lệnh n 7elif \u0026lt;điều kiện 2\u0026gt;: 8 # trường hợp \u0026lt;điều kiện 1\u0026gt; là sai, \u0026lt;điều kiện 2\u0026gt; là đúng 9 câu lệnh 1 10 ... 11 câu lệnh n 12... 13elif \u0026lt;điều kiện n\u0026gt;: 14 15 # trường hợp \u0026lt;điều kiện 1\u0026gt; là sai, \u0026lt;điều kiện 2\u0026gt; là sai, ... \u0026lt;điều kiện n-1\u0026gt; là sai, \u0026lt;điều kiện n\u0026gt; là đúng 16 câu lệnh 1 17 ... 18 câu lệnh n 19else: 20 # trường hợp \u0026lt;điều kiện 1\u0026gt; là sai, ..., \u0026lt;điều kiện 2\u0026gt; là sai 21 câu lệnh 1 22 ... 23 câu lệnh n 24 25 26câu lệnh n+1 Ví dụ:\nMẹ bé Thu trước khi đi làm nói với bé Thu rằng: \u0026ldquo;Nếu trời sắp mưa, con hãy rút quần áo ở dây phơi đồ, hốt lúa cất vào bồ, bế em vào nhà, gài then đóng cửa thật chặt, gài then đóng cửa thật chặt. Con ngoan ở nhà, chiều mẹ về mua kẹo cho con ăn\u0026rdquo;. Chúng ta sẽ biến đổi lời căn dặn của mẹ bé Thu thành câu lệnh if như sau:\n1 2thoi_tiet = \u0026#39;sap_mua\u0026#39; 3is_be_thu_ngoan = True 4if thoi_tiet == \u0026#39;sap_mua\u0026#39;: 5 print(\u0026#39;rút quần áo ở dây phơi đồ\u0026#39;) 6 print(\u0026#39;hốt lúa cất vào bồ\u0026#39;) 7 print(\u0026#39;bế em vào nhà\u0026#39;) 8 print(\u0026#39;gài then đóng cửa thật chặt\u0026#39;) 9 print(\u0026#39;rút quần áo ở dây phơi đồ\u0026#39;) 10 11if is_be_thu_ngoan: 12 print(\u0026#39;Mẹ bé Thu mua kẹo\u0026#39;) 13 print(\u0026#39;Mẹ bé Thu cho bé Thu ăn kẹo\u0026#39;) 14 15Kết quả 16 17rút quần áo ở dây phơi đồ 18hốt lúa cất vào bồ 19bế em vào nhà 20gài then đóng cửa thật chặt 21rút quần áo ở dây phơi đồ 22 23 24Mẹ bé Thu mua kẹo 25Mẹ bé Thu cho bé Thu ăn kẹo Chúng ta có câu tục ngữ: Chuồn chuồn bay thấp thì mưa, bay cao thì nắng, bay vừa thì râm.\nCâu lệnh if else của câu tục ngữ trên là:\n1 2vi_tri_chuon_chuon = \u0026#39;bay_vua\u0026#39; 3 4if vi_tri_chuon_chuon == \u0026#39;bay_thap\u0026#39;: 5 print(\u0026#39;trời sắp mưa\u0026#39;) 6elif vi_tri_chuon_chuon == \u0026#39;bay_cao\u0026#39;: 7 print(\u0026#39;trời nắng\u0026#39;) 8elif vi_tri_chuon_chuon == \u0026#39;bay_vua\u0026#39;: 9 print(\u0026#39;trời râm\u0026#39;) 10else: 11 print(\u0026#39;không xác định\u0026#39;) 12 13# Kết quả 14 15trời râm Câu lệnh điều khiển for Câu lệnh for được sử dụng để duyệt các phần tử trong các container như String, Tuple, List, Set hoặc Dictionary, Array.\nfor trong python tương đương với foreach trong các ngôn ngữ thuộc họ c. Python không có câu lệnh for giống for trong c/c++, c#, java \u0026hellip;\nCú pháp câu lệnh for\n1 2for item in container: 3 #statement Ví dụ\n1 2brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 3 4for item in brands: 5 print(item) 6 7#Kết quả 8 9iphone 10samsung 11xiaomi 12nokia Hàm range cú pháp\n1 2range(start,stop,steep) Hàm range được sử dụng để trả về một chuỗi các số từ start (mặc định là 0) đến stop, với bước nhảy là steep (mặc định là 1)\nVí dụ:\nTạo một chuỗi các số từ 5 đến 9, in ra các số trên\n1 2itemRange = range(5,10) 3 4for item in itemRange: 5 print(item) 6 7# Kết quả 8 95 106 117 128 139 Hàm range thường được sử dụng với hàm len, để duyệt index của list\n1 2brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;] 3 4for x in range(len(brands)): 5 print(f\u0026#34;element at {x} in list is {brands[x]} \u0026#34;) 6 7# Kết quả 8 9element at 0 in list is iphone 10element at 1 in list is samsung 11element at 2 in list is xiaomi Ngoài ra, còn tuỳ vào bài toán, chúng ta sử dụng hàm range một cách thông minh để code được trong sáng và sạch đẹp hơn.\nKết hợp câu lệnh for với if Hoàng đế Julius Caesar là một nhà quân sự tài ba. Trong lúc ông lãnh đạo quân đội La Mã, để tránh bị rò rỉ nội dung thư tín khi truyền tải cho các tướng sĩ, ông đã thiết lập một bộ mật mã là dịch từng chữ trong thông tin qua 3 chữ cái trong bảng mã ascii. Nghĩa là, thay vì viết chữ a, ông lại viết thành chữ d, thay vì viết chữ b, ông lại viết chữ e, \u0026hellip;., cho đến thay z thành c. Khi tướng sĩ của ông nhận được thư tín, chỉ cần dịch ngược lại với quy luật trên là có được nội dung bức thư.\nVí dụ nội dung bức thư ông gửi.\ngdqk Jdoold qjdb pxrl ed\nKhi tướng sĩ nhận được đoạn lệnh trên, họ tiến hành dịch ngược lại. g tương ứng với chữ d (d+3 =g) \u0026hellip;, và giải mã bức mật thư của hoàng đế gửi là:\ndanh Gallia ngay muoi ba\nChúng ta viết chương trình nhỏ với for và if để mã hoá nội dung thông tin giúp Julius Caesar nhé.\n1 2input = \u0026#39;danh Gallia ngay muoi ba\u0026#39; 3 4for c in input: 5 if c == \u0026#39;a\u0026#39;: 6 print(\u0026#39;d\u0026#39;,end=\u0026#39;\u0026#39;) 7 elif c == \u0026#39;b\u0026#39;: 8 print(\u0026#39;e\u0026#39;,end=\u0026#39;\u0026#39;) 9 elif c == \u0026#39;c\u0026#39;: 10 print(\u0026#39;f\u0026#39;,end=\u0026#39;\u0026#39;) 11 elif c == \u0026#39;d\u0026#39;: 12 print(\u0026#39;g\u0026#39;,end=\u0026#39;\u0026#39;) 13 elif c == \u0026#39;e\u0026#39;: 14 print(\u0026#39;h\u0026#39;,end=\u0026#39;\u0026#39;) 15 elif c == \u0026#39;f\u0026#39;: 16 print(\u0026#39;i\u0026#39;,end=\u0026#39;\u0026#39;) 17 elif c == \u0026#39;g\u0026#39;: 18 print(\u0026#39;j\u0026#39;,end=\u0026#39;\u0026#39;) 19 elif c == \u0026#39;h\u0026#39;: 20 print(\u0026#39;k\u0026#39;,end=\u0026#39;\u0026#39;) 21 elif c == \u0026#39;i\u0026#39;: 22 print(\u0026#39;l\u0026#39;,end=\u0026#39;\u0026#39;) 23 elif c == \u0026#39;j\u0026#39;: 24 print(\u0026#39;m\u0026#39;,end=\u0026#39;\u0026#39;) 25 elif c == \u0026#39;k\u0026#39;: 26 print(\u0026#39;n\u0026#39;,end=\u0026#39;\u0026#39;) 27 elif c == \u0026#39;l\u0026#39;: 28 print(\u0026#39;o\u0026#39;,end=\u0026#39;\u0026#39;) 29 elif c == \u0026#39;m\u0026#39;: 30 print(\u0026#39;p\u0026#39;,end=\u0026#39;\u0026#39;) 31 elif c == \u0026#39;n\u0026#39;: 32 print(\u0026#39;q\u0026#39;,end=\u0026#39;\u0026#39;) 33 elif c == \u0026#39;o\u0026#39;: 34 print(\u0026#39;r\u0026#39;,end=\u0026#39;\u0026#39;) 35 elif c == \u0026#39;p\u0026#39;: 36 print(\u0026#39;s\u0026#39;,end=\u0026#39;\u0026#39;) 37 elif c == \u0026#39;q\u0026#39;: 38 print(\u0026#39;t\u0026#39;,end=\u0026#39;\u0026#39;) 39 elif c == \u0026#39;r\u0026#39;: 40 print(\u0026#39;u\u0026#39;,end=\u0026#39;\u0026#39;) 41 elif c == \u0026#39;s\u0026#39;: 42 print(\u0026#39;v\u0026#39;,end=\u0026#39;\u0026#39;) 43 elif c == \u0026#39;t\u0026#39;: 44 print(\u0026#39;w\u0026#39;,end=\u0026#39;\u0026#39;) 45 elif c == \u0026#39;u\u0026#39;: 46 print(\u0026#39;x\u0026#39;,end=\u0026#39;\u0026#39;) 47 elif c == \u0026#39;v\u0026#39;: 48 print(\u0026#39;y\u0026#39;,end=\u0026#39;\u0026#39;) 49 elif c == \u0026#39;w\u0026#39;: 50 print(\u0026#39;z\u0026#39;,end=\u0026#39;\u0026#39;) 51 elif c == \u0026#39;x\u0026#39;: 52 print(\u0026#39;a\u0026#39;,end=\u0026#39;\u0026#39;) 53 elif c == \u0026#39;y\u0026#39;: 54 print(\u0026#39;b\u0026#39;,end=\u0026#39;\u0026#39;) 55 elif c == \u0026#39;z\u0026#39;: 56 print(\u0026#39;c\u0026#39;,end=\u0026#39;\u0026#39;) 57 else: 58 print(c,end=\u0026#39;\u0026#39;) 59print() 60 61# Kết quả 62 63gdqk Gdoold qjdb pxrl ed Cách viết trên khá cơ bắp, tay to, dài dòng, chúng ta hãy viết đoạn code trên ngắn gọn hơn bằng cách.\nTạo ra 2 chuỗi, một chuỗi chứa các ký tự alphabet, một chuỗi chứa bảng mã hoá.\nTìm vị trí của từ cần mã hoá trong chuỗi alphabet\nIn ra từ cần lấy trong bảng mã hoá\n1 2input = \u0026#39;danh Gallia ngay muoi ba\u0026#39; 3 4alphabet = \u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; 5caesar_cipher = \u0026#39;defghijklmnopqrstuvwxyzabc\u0026#39; 6 7for c in input: 8 index = alphabet.find(c) 9 if index \u0026gt;-1: 10 print(caesar_cipher[index],end=\u0026#39;\u0026#39;) 11 else: 12 print(c,end=\u0026#39;\u0026#39;) 13 14print(\u0026#39;\u0026#39;) từ khoá break, từ khoá continue, từ khoá pass Để thoát khỏi vòng lặp for, chúng ta sử dụng từ khoá break\nĐể bỏ qua khối lệnh bên dưới, tiếp tục lệnh for, chúng ta sử dụng từ khoá continue.\nĐể giữ chỗ cho tính năng tương lai sẽ phát triển, chúng ta sử dụng từ khoá pass để đánh dấu, và cũng để cho chương trình có thể hoạt động được.\nVí dụ về break\nTìm là in ra 5 số lẻ nguyên dương đầu tiên bé hơn 100\n1 2count = 0 3 4for x in range(100): 5 if x % 2 != 0: 6 print(x) 7 count = count + 1 8 if count \u0026gt;=5: 9 break 10 11# Kết quả 12 131 143 155 167 179 Ví dụ về continue\nIn ra các số lẻ bé hơn 10\n1 2for x in range(10): 3 if x % 2 == 0: 4 continue 5 print(x) 6 7# Kết quả 8 91 103 115 127 139 Ví dụ về pass\nViết một vòng lặp for lặp 10 lần, để giành đó mai mốt code tiếp\n1 2for x in range(10): 3 pass 4 5# Kết quả Vòng lặp while Ý nghĩa: Trong khi điều kiện còn đúng, thì thực hiện câu lệnh.\nKết thúc vòng lặp khi điều kiện sai\nCú pháp\n1 2while \u0026lt;condition\u0026gt;: 3 # statement Ví dụ:\nIn ra các số nguyên bé hơn 10\n1 2 3i = 1 4while i \u0026lt; 10: 5 print(i) 6 i += 1 vòng lặp while có thể sử dụng các từ khoá pass, continue, break giống như for\nKỹ thuật duyệt container trong python Python hỗ trợ nhiều hàm dựng sẵn, giúp chúng ta có thể duyệt các container một cách dễ dàng.\nViệc sử dụng các hàm duyệt bên dưới, giúp cho coder:\nSử dụng nhanh chóng, giảm thời gian coding.\nTên hàm chính là từ khoá, mô tả chính xác mục đích sử dụng hàm. Giúp giảm thời gian đọc code, khi so với việc sử dụng for/while.\nCode ngắng gọn hơn, rõ ràng hơn, so với for \u0026amp; while.\nDuyệt container sử dụng hàm enumerate Hàm enumerate hỗ trợ trả về index và value của container\nVí dụ:\n1 2 3brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 4 5for index,item in enumerate(brands): 6 print(f\u0026#34;element at index {index} in list is {item} \u0026#34;) 7 8 9# Kết quả 10 11element at index 0 in list is iphone 12element at index 1 in list is samsung 13element at index 2 in list is xiaomi 14element at index 3 in list is nokia Duyệt container sử dụng hàm zip Hàm dựng sẵn zip hỗ trợ chúng ta kết hợp 2 container cùng loại (list với list, dict với dict, string với string) với nhau\nVí dụ:\n1 2alphabet = \u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; 3caesar_cipher = \u0026#39;defghijklmnopqrstuvwxyzabc\u0026#39; 4 5 6for decode, encode in zip(alphabet,caesar_cipher): 7 print(f\u0026#34;Caesar send {encode}, we have {decode}\u0026#34;) 8 9# Kết quả 10 11Caesar send d, we have a 12Caesar send e, we have b 13Caesar send f, we have c 14Caesar send g, we have d 15Caesar send h, we have e 16Caesar send i, we have f 17Caesar send j, we have g 18Caesar send k, we have h 19Caesar send l, we have i 20Caesar send m, we have j 21Caesar send n, we have k 22Caesar send o, we have l 23Caesar send p, we have m 24Caesar send q, we have n 25Caesar send r, we have o 26Caesar send s, we have p 27Caesar send t, we have q 28Caesar send u, we have r 29Caesar send v, we have s 30Caesar send w, we have t 31Caesar send x, we have u 32Caesar send y, we have v 33Caesar send z, we have w 34Caesar send a, we have x 35Caesar send b, we have y 36Caesar send c, we have z Duyệt dic sử dụng hàm items Ví dụ:\n1 2profile = {\u0026#39;name\u0026#39;:\u0026#39;alex\u0026#39;,\u0026#39;age\u0026#39;:18,\u0026#39;location\u0026#39;:\u0026#39;vietnam\u0026#39;} 3 4for key, value in profile.items(): 5 print(key,value) 6 7# Kết quả 8 9name alex 10age 18 11location vietnam Duyệt container sử dụng hàm sorted Hàm sorted sẽ xắp xếp lại phần tử trong container theo thứ tự (với số thì từ nhỏ đến lớn, với chữ thì theo thứ tự từ điển), và trả về từng phần tử trong container đã được sort.\nVí dụ:\n1 2 3brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 4 5for item in sorted(brands): 6 print(item) 7 8# Kết quả 9 10iphone 11nokia 12samsung 13xiaomi Duyệt container sử dụng hàm reversed Hàm reversed sẽ duyệt ngược phần tử trong container. Hàm này không làm ảnh hưởng thứ tự của các phần tử trong container\nVí dụ:\n1 2 3brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 4 5for item in reversed(brands): 6 print(item) 7 8# Kết quả 9 10iphone 11nokia 12samsung 13xiaomi List Comprehension List comprehension cung cấp cho chúng ta một cú pháp ngắn gọn, súc tích, giúp chúng ta tạo một list, là tập con từ một list lớn.\nCú pháp chung của list comprehension là:\n1 2newlist = [expression for item in iterable if condition == True] Với condition là điều kiện lọc để giảm số lượng phần tử trả về.\nexpression: có thể là một biểu thức if\nVí dụ:\n1 2brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 3filter_brands = [] 4 5for item in brands: 6 if \u0026#39;i\u0026#39; in item: 7 filter_brands.append(item) 8 9print(filter_brands) 10 11# Kết quả 12\u0026gt;\u0026gt;\u0026gt; print(filter_brands) 13[\u0026#39;iphone\u0026#39;, \u0026#39;xiaomi\u0026#39;, \u0026#39;nokia\u0026#39;] Đoạn mã trên thực hiện việc in ra các hãng có chứa ký tự i trong tên. Chúng ta tốn 3 dòng code (1 vòng for, 1 vòng if, 1 vòng append). Giờ chúng ta sẽ viết lại bằng list comprehension\n1 2 3brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 4 5filter_brands = [item for item in brands if \u0026#39;i\u0026#39; in item] 6 7print(filter_brands) 8 9#Kết quả 10 11\u0026gt;\u0026gt;\u0026gt; print(filter_brands) 12[\u0026#39;iphone\u0026#39;, \u0026#39;xiaomi\u0026#39;, \u0026#39;nokia\u0026#39;] Ví dụ 2\nViết hoa toàn bộ tên hãng\n1 2# Mẫu vòng lặp for 3 4brands = [\u0026#39;iphone\u0026#39;,\u0026#39;samsung\u0026#39;,\u0026#39;xiaomi\u0026#39;,\u0026#39;nokia\u0026#39;] 5 6old_upper_brands = [] 7for brand in brands: 8 old_upper_brands.append(brand.upper()) 9 10# Mẫu list comprehension 11 12new_upper_brands = [brand.upper() for brand in brands] Ví dụ 3:\nCho một list có 10 phần tử, lấy ra các phần tử \u0026gt; 5, thay các phần tử lớn hơn 10 bằng 0\n1 2# Mẫu cũ 3items = [100,5,8,2,9,7,1,20,89,99] 4old_items = [] 5for item in items: 6 if item \u0026gt; 5: # chỉ xét những phần tử \u0026gt;5 7 if item\u0026gt;10: # thay những phần tử \u0026gt; 10 thành 0 8 old_items.append(0) 9 else: 10 old_items.append(item) 11 12print(old_items) 13 14new_items = [item if item \u0026lt;=10 else 0 for item in items if item\u0026gt;5] 15 16print(new_items) 17 18# Kết quả 19 20\u0026gt;\u0026gt;\u0026gt; print(old_items) 21[0, 8, 9, 7, 0, 0, 0] 22 23\u0026gt;\u0026gt;\u0026gt; print(new_items) 24[0, 8, 9, 7, 0, 0, 0] Cảm ơn các bạn đã theo dõi bài viết.\n","date":"Jul 16, 2022","img":"","permalink":"/courses/python/4_python_conditional_loop/","series":["Khóa học python căn bản"],"tags":["python"],"title":"Bài 3: Câu Lệnh Điều Khiển Trong Python"},{"categories":"python","content":"Trong bài viết này, chúng ta sẽ tìm hiểu các kiểu dữ liệu dạng container trong python\nKiểu dữ liệu string Một vài phương thức cơ bản của string Phương thức isdecimal, isdigit, isnumeric Phương thức isascii, isalpha, isalnum, isspace, isupper Phương thức lstrip, rstrip, strip Phương thức find, index Phương thức format f string Kiểu dữ liệu tuple Packing và Unpacking So sánh các biến có kiểu dữ liệu tuple Slicing trong Tuple Các hàm dựng sẵn của Tuple Kiểu dữ liệu từ điển - dictionary Thuộc tính của keys trong từ điển. Một vài phương thức của dictionary copy update del item len Merge Tổng kết: Kiểu dữ liệu list Truy xuất dữ liệu trong list slicing Các phương thức được hỗ trợ append pop remove reverse Các hàm được hỗ trợ len max min Kiểu dữ liệu set Một vài phương thức cơ bản của Set Phương thức Add Phương thức Remove, Discard Phương thức Pop Phương thức Clear Kiểu dữ liệu array Một vài phương thức cơ bản của array Phương thức insert, phương thức append Phương thức truy xuất phần tử theo index Phương thức remove, phương thức pop Phương thức index Kiểu dữ liệu string string là tập hợp các bytes được biểu diễn dưới dạng ký tự unicode\nVí dụ\n1 2hello = \u0026#34;hi, i am alex\u0026#34; 3 4print(hello) 5 6greating = \u0026#34;i am from việt Nam\u0026#34; 7 8print(greating) 9 10 11#Kết quả 12 13\u0026gt;\u0026gt;\u0026gt; print(hello) 14hi, i am alex 15 16\u0026gt;\u0026gt;\u0026gt; print(greating) 17i am from việt Nam Một vài phương thức cơ bản của string Phương thức isdecimal, isdigit, isnumeric isdecimal: Trả về true nếu toàn bộ các ký tự là decimal (0-9)\nisdigit: Trả về true nếu toàn bộ ký tự là digit. Bao gồm các số (0-9), số mũ trên (ví dụ: x2), số mũ dưới (ví dụ: x2).\nisnumeric: Trả về true nếu toàn bộ ký tự là numeric. Bao gồm các số (0-9), số mũ trên (ví dụ: x2), số mũ dưới (ví dụ: x2) , phân số ( ví dụ: 1⁄2)\nVí dụ:\n1 2# số 18 là: 3print(\u0026#39;18\u0026#39;.isdecimal()) 4print(\u0026#39;18\u0026#39;.isdigit()) 5print(\u0026#39;18\u0026#39;.isnumeric()) 6 7# Kết quả 8 9\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;18\u0026#39;.isdecimal()) 10True 11\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;18\u0026#39;.isdigit()) 12True 13\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;18\u0026#39;.isnumeric()) 14True 1 2# số 2 mũ 3 là: 3print(\u0026#39;2\\u00b3\u0026#39;) 4print(\u0026#39;2\\u00b3\u0026#39;.isdecimal()) 5print(\u0026#39;2\\u00b3\u0026#39;.isdigit()) 6print(\u0026#39;2\\u00b3\u0026#39;.isnumeric()) 7 8# Kết quả 9 10\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;2\\u00b3\u0026#39;) 112³ 12\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;2\\u00b3\u0026#39;.isdecimal()) 13False 14\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;2\\u00b3\u0026#39;.isdigit()) 15True 16\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;2\\u00b3\u0026#39;.isnumeric()) 17True 1 2# số ⅓ là: 3print(\u0026#39;\\u2153\u0026#39;) 4print(\u0026#39;\\u2153\u0026#39;.isdecimal()) 5print(\u0026#39;\\u2153\u0026#39;.isdigit()) 6print(\u0026#39;\\u2153\u0026#39;.isnumeric()) 7 8# Kết quả 9 10\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;\\u2153\u0026#39;.isdecimal()) 11False 12\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;\\u2153\u0026#39;.isdigit()) 13False 14\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;\\u2153\u0026#39;.isnumeric()) 15True Phương thức isascii, isalpha, isalnum, isspace, isupper isalpha: Trả về true nếu toàn bộ ký tự trong bảng alphabet(a-z). Không chứa ký tự khoảng trắng, # @ $ \u0026hellip;\nisalnum: Trả về true nếu toàn bộ ký tự là alphanumeric (a-z,0-9)\nisascii: Trả về true nếu toàn bộ là ký tự ascii (a-z)\nisspace: Trả về true nếu toàn bộ ký tự là khoảng trắng\nisupper: Trả về true nếu toàn bộ ký tự đều in hoa.\nVí dụ:\n1 2\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex\u0026#34;.isalpha()) 3True 4 5\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex100\u0026#34;.isalpha()) 6False 7 8\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex100\u0026#34;.isalnum()) 9True 10 11\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex100 $%\u0026#34;.isalnum()) 12False 13 14\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex100 $%\u0026#34;.isascii()) 15True 16 17 18\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex100\u0026#34;.isspace()) 19False 20\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;\u0026#34;.isspace()) 21False 22\u0026gt;\u0026gt;\u0026gt; print(\u0026#34; \u0026#34;.isspace()) 23True 24 25 26\u0026gt;\u0026gt;\u0026gt; print(\u0026#34; \u0026#34;.isupper()) 27False 28\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;abc\u0026#34;.isupper()) 29False 30\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;AA\u0026#34;.isupper()) 31True Phương thức lstrip, rstrip, strip Phương thức lstrip: Xoá chuỗi dư thừa ở bên trái, mặc định chuỗi dư thừa là khoảng trắng Phương thức rstrip: Xoá chuỗi dư thừa ở bên phải, mặc định chuỗi dư thừa là khoảng trắng Phương thức strip: Xoá chuỗi dư thừa ở hai bên, mặc định chuỗi dư thừa là khoảng trắng Ví dụ\n1 2\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;.... alex black ..\u0026#34;.lstrip(\u0026#34;.\u0026#34;)) # chỉ xoá . bên trái 3 alex black .. 4 5\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;.... alex black ..\u0026#34;.rstrip(\u0026#34;.\u0026#34;)) # chỉ xoá . bên phải 6.... alex black 7 8\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;....alex black..\u0026#34;.strip(\u0026#34;.\u0026#34;)) # xoá . ở hai bên 9alex black 10 11 12\u0026gt;\u0026gt;\u0026gt; print(\u0026#34; ....alex black..\u0026#34;.strip()) # xoá khoảng trắng ở hai bên 13....alex black.. Phương thức find, index Cả hai phương thức find và index được sử dụng để tìm vị trí đầu tiên của phần tử cần tìm\nPhương thức find: Trả về -1 nếu phần tử không tìm thấy\nPhương thức index: Trả về lỗi nếu phần tử không tìm thấy\nVí dụ:\n1 2\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex black 18\u0026#34;.find(\u0026#34;b\u0026#34;)) 35 4 5\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex black 18\u0026#34;.index(\u0026#34;b\u0026#34;)) 65 7 8\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex black 18\u0026#34;.find(\u0026#34;a\u0026#34;,5)) 97 10 11\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex black 18\u0026#34;.index(\u0026#34;a\u0026#34;,5)) 127 13 14\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex black 18\u0026#34;.find(\u0026#34;z\u0026#34;)) 15-1 16 17\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;alex black 18\u0026#34;.index(\u0026#34;z\u0026#34;)) 18Traceback (most recent call last): 19 File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; 20ValueError: substring not found Phương thức format Phương thức format được sử dụng để định dạng chuỗi.\nVí dụ:\n1 2greetings = \u0026#34;Hello everyone, my name {name}, i am {age} year old. I come from {location}\u0026#34; 3 4name = \u0026#34;alex Black\u0026#34; 5age = 18 6location = \u0026#34;the moon\u0026#34; 7print(greetings.format(name=name, age=age, location=location)) 8 9#Kết quả 10 11\u0026gt;\u0026gt;\u0026gt; print(greetings.format(name=name, age=age, location=location)) 12Hello everyone, my name alex Black, i am 18 year old. I come from the moon Cách viết này khá dài dòng lê thê, một cách khác là chúng ta có thể sử dụng f string.\nf string python 3 hỗ trợ f string, giúp format chuỗi, trông đẹp hơn so với phương thức format ở trên.\n1 2\u0026gt;\u0026gt;\u0026gt; age =18 3\u0026gt;\u0026gt;\u0026gt; name= \u0026#39;Alex Black\u0026#39; 4\u0026gt;\u0026gt;\u0026gt; location=\u0026#39;the moon\u0026#39; 5 6\u0026gt;\u0026gt;\u0026gt; print(f\u0026#34;hello, my name {name}. I am {age} years old. I come from {location}\u0026#34;) 7hello, my name Alex Black. I am 18 years old. I come from the moon Kiểu dữ liệu tuple Tuple là tập cho phép chúng ta gán nhiều biến vào một biến. Ví dụ:\n1tupinfo = (\u0026#39;Alex\u0026#39;, \u0026#39;Black\u0026#39;,\u0026#39;1978\u0026#39;,\u0026#39;Emprise\u0026#39;, \u0026#39;Engineer\u0026#39;,\u0026#39;Ho Chi Minh\u0026#39;); 2tupinfo = (1,3,5,7,9,9); 3print(tupinfo[0]) 4print(tupinfo[1:4]) 5 6#kết quả 7\u0026gt;\u0026gt;\u0026gt; print(tupinfo[0]) 81 9\u0026gt;\u0026gt;\u0026gt; print(tupinfo[1:4]) 10(3, 5, 7) 11\u0026gt;\u0026gt;\u0026gt; Packing và Unpacking Thuật ngữ packing ám chỉ việc ta thêm giá trị vào tuple.\nThuật ngữ unpacking ám chỉ việc ta phân giải các giá trị của tuple ra nhiều biến.\nChúng ta cùng xem ví dụ:\n1 2a = (\u0026#34;alex\u0026#34; , 18, \u0026#34;Staff\u0026#34;) # tuple packing 3 4(name, age, position) = a # unpacking tuple 5 6print(name) 7print(age) 8print(position) 9 10# Kết quả 11 12\u0026gt;\u0026gt;\u0026gt; print(name) 13alex 14\u0026gt;\u0026gt;\u0026gt; print(age) 1518 16\u0026gt;\u0026gt;\u0026gt; print(position) 17Staff So sánh các biến có kiểu dữ liệu tuple Python cho phép so sánh các biến thuộc kiểu dữ liệu tuple với nhau. Chúng ta có thể thực hiện các phép so sánh bằng, so sánh lớn hơn, so sánh bé hơn. Việc so sánh được thực hiện lần lượt bằng cách so sánh giá trị của từng phần tử với nhau theo thứ tự. Phần tử thứ nhất sẽ so sánh với phần tử thứ nhất, phần tử thứ hai sẽ so sánh với phần tử thứ hai\u0026hellip;.\nVí dụ:\n1 2num1 = (3,5,7) 3 4num2 = (3,6,4) 5 6print(num1\u0026gt;num2) 7print(num1==num2) 8print(num1\u0026lt;num2) 9 10Kết quả: 11 12\u0026gt;\u0026gt;\u0026gt; print(num1\u0026gt;num2) 13False 14\u0026gt;\u0026gt;\u0026gt; print(num1==num2) 15False 16\u0026gt;\u0026gt;\u0026gt; print(num1\u0026lt;num2) # do 6 lớn hơn 5, nên num2 lớn hơn num1 17True Một lưu ý nhỏ là ở python, phép so sánh bằng sẽ là hai dấu bằng (==), không phải một dấu =. Dấu = đại diện cho phép gán giá trị cho biến.\nSlicing trong Tuple Để lấy ra một nhóm các phần tử liền kề nhau trong tuple, chúng ta sử dụng một hàm có tên là slicing. Slicing có thể áp dụng cho tuple, array, list.\nVí dụ:\n1 2ages = (18,16,15,18,15,17,19,18,17) 3print(ages[2:4]) 4\u0026gt;\u0026gt;\u0026gt; print(ages[2:4]) 5(15, 18) Các hàm dựng sẵn của Tuple Để thực hiện các công việc khác nhau, kiểu dữ liệu tuple có xây dựng một số hàm để chúng ta sử dụng, như là all(), any(), enumerate(), max(), min(), sorted(), len(), tuple(), etc.\nKiểu dữ liệu từ điển - dictionary Trong python, kiểu từ điển là tập hợp các dữ liệu có dạng key-value. Trong đó, Key là duy nhất trong từ điển. Value có thể là list, tuple, dictionary, số, chuỗi, túm lại là value không bị giới hạn về kiểu dữ liệu, thích lưu kiểu gì cũng được. Có hai cách để tạo biến có kiểu dữ liệu từ điển, một là dùng từ khoá dict(), hai là dùng dấu đóng mở ngoặc nhọn {}.\nVí dụ\n1 2info = {\u0026#39;name\u0026#39;: \u0026#34;alex\u0026#34;, age:18, \u0026#39;position\u0026#39;: \u0026#34;Staff\u0026#34; } 3print(info) 4 5Kết quả: 6 7\u0026gt;\u0026gt;\u0026gt; print(info) 8{\u0026#39;alex\u0026#39;: \u0026#39;alex\u0026#39;, 18: 18, \u0026#39;position\u0026#39;: \u0026#39;Staff\u0026#39;} Qua 10 triệu lần test trên con máy apple m1 của mình, mình thấy rằng khai báo biến dictionary bằng dấu {} sẽ chạy nhanh hơn so với khai báo sử dụng dict()\nThuộc tính của keys trong từ điển. Có ba điểm quan trọng về key của dictionary chúng ta cần phải nhớ:\nMột là key không cho phép trùng nhau.\nKey phải là thuộc nhóm bất biến - immutable, như number, tuple , string.\nKey có phân biệt hoa thường.\nVí dụ:\n1 2 3item = {\u0026#34;name\u0026#34;:\u0026#34; iPhone 13 Pro Max 512GB\u0026#34;,\u0026#34;Price\u0026#34;:\u0026#34;34.690.000\u0026#34;,\u0026#34;Brand\u0026#34;:\u0026#34;Apple\u0026#34;,\u0026#34;BRAND\u0026#34;:\u0026#34;Apple\u0026#34;} 4 5print(item[\u0026#34;Brand\u0026#34;]) 6 7\u0026gt;\u0026gt;\u0026gt; print(item[\u0026#34;Brand\u0026#34;]) 8Apple Một vài phương thức của dictionary copy Phương thức này được sử dụng để copy phần tử của biến này sang biến khác.\nVí dụ:\n1 2item = {\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34; :\u0026#34;34.690.000\u0026#34;,\u0026#34;Điện thoại iPhone 13 Pro Max 1TB\u0026#34;:\u0026#34;40.990.000\u0026#34;} 3 4item_new = item.copy() 5 6print(item_new) 7 8\u0026gt;\u0026gt;\u0026gt; print(item_new) 9{\u0026#39;Điện thoại iPhone 13 Pro Max 512GB\u0026#39;: \u0026#39;34.690.000\u0026#39;, \u0026#39;Điện thoại iPhone 13 Pro Max 1TB\u0026#39;: \u0026#39;40.990.000\u0026#39;} update Phương thức update được sử dụng để cập nhật dữ liệu nếu key đã có, nếu key chưa có thì thêm cặp key-value vào từ điển.\nVí dụ:\n1 2item = {\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34; :\u0026#34;34.790.000\u0026#34;,\u0026#34;Điện thoại iPhone 13 Pro Max 1TB\u0026#34;:\u0026#34;40.990.000\u0026#34;} 3 4item_new = item.copy() 5item_new.update({\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34; :\u0026#34;34.690.000\u0026#34;}) # cập nhật giá trị, vì key đã tồn tại 6 7item_new.update({\u0026#34;Điện thoại iPhone 13 Pro Max 128GB\u0026#34; :\u0026#34;28.390.000\u0026#34;}) # thêm cặp key-value vào biến item_new 8 9print(item) 10print(item_new) 11 12#Kết quả 13 14\u0026gt;\u0026gt;\u0026gt; print(item) 15{\u0026#39;Điện thoại iPhone 13 Pro Max 512GB\u0026#39;: \u0026#39;34.790.000\u0026#39;, \u0026#39;Điện thoại iPhone 13 Pro Max 1TB\u0026#39;: \u0026#39;40.990.000\u0026#39;} 16\u0026gt;\u0026gt;\u0026gt; print(item_new) 17{\u0026#39;Điện thoại iPhone 13 Pro Max 512GB\u0026#39;: \u0026#39;34.690.000\u0026#39;, \u0026#39;Điện thoại iPhone 13 Pro Max 1TB\u0026#39;: \u0026#39;40.990.000\u0026#39;, \u0026#39;Điện thoại iPhone 13 Pro Max 128GB\u0026#39;: \u0026#39;28.390.000\u0026#39;} del Để xoá một key ra khỏi từ điển, chúng ta dùng từ khoá del\n1 2item = {\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34; :\u0026#34;34.790.000\u0026#34;,\u0026#34;Điện thoại iPhone 13 Pro Max 1TB\u0026#34;:\u0026#34;40.990.000\u0026#34;} 3 4del item[\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34;] 5 6print(item) 7 8#Kết quả 9 10\u0026gt;\u0026gt;\u0026gt; print(item) 11{\u0026#39;Điện thoại iPhone 13 Pro Max 1TB\u0026#39;: \u0026#39;40.990.000\u0026#39;} item Phương thức items trả về giá trị của từ điển dưới dạng list tuple (key,value)\n1 2info = {\u0026#34;name\u0026#34;:\u0026#34;Alex\u0026#34;,\u0026#34;age\u0026#34;:18,\u0026#34;position\u0026#34;:\u0026#34;staff\u0026#34;} 3print(info.items()) 4 5# Kết quả 6 7\u0026gt;\u0026gt;\u0026gt; print(info.items()) 8dict_items([(\u0026#39;name\u0026#39;, \u0026#39;Alex\u0026#39;), (\u0026#39;age\u0026#39;, 18), (\u0026#39;position\u0026#39;, \u0026#39;staff\u0026#39;)]) len Phương thức len trả về số lượng phần tử trong từ điển\n1 2item = {\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34; :\u0026#34;34.790.000\u0026#34;,\u0026#34;Điện thoại iPhone 13 Pro Max 1TB\u0026#34;:\u0026#34;40.990.000\u0026#34;} 3 4print(len(item)) 5 6#Kết quả 7 8\u0026gt;\u0026gt;\u0026gt; print(len(item)) 92 Merge Để nối hai hay nhiều từ điển vào làm một, có một số cách sau:\nSử dụng hàm update, hàm này đã được mình nói rõ ở trên, mình không nhắc lại nữa\nSử dụng Kwargs **.\nTừ phiên bản 3.5 trở lên, python hỗ trợ \u0026ldquo;đối số từ khóa\u0026rdquo; - Kwargs - keyword arguments là **, và lúc này, chúng ta có thể sử dụng ** ở trước tên biến.\nVí dụ\n1 2itemApple = {\u0026#34;Điện thoại iPhone 13 Pro Max 512GB\u0026#34; :\u0026#34;34.790.000\u0026#34;,\u0026#34;Điện thoại iPhone 13 Pro Max 1TB\u0026#34;:\u0026#34;40.990.000\u0026#34;} 3 4 5itemSamsung = {\u0026#34;Điện thoại Samsung Galaxy S22 Ultra 5G 128GB \u0026#34;:\u0026#34;27.990.000\u0026#34;,\u0026#34;Điện thoại Samsung Galaxy S22 Ultra 5G 512GB\u0026#34;:\u0026#34;33.990.000\u0026#34;} 6 7itemPhone = {**itemApple,**itemSamsung} 8 9print(itemPhone) 10 11 12\u0026gt;\u0026gt;\u0026gt; print(itemPhone) 13{\u0026#39;Điện thoại iPhone 13 Pro Max 512GB\u0026#39;: \u0026#39;34.790.000\u0026#39;, \u0026#39;Điện thoại iPhone 13 Pro Max 1TB\u0026#39;: \u0026#39;40.990.000\u0026#39;, \u0026#39;Điện thoại Samsung Galaxy S22 Ultra 5G 128GB \u0026#39;: \u0026#39;27.990.000\u0026#39;, \u0026#39;Điện thoại Samsung Galaxy S22 Ultra 5G 512GB\u0026#39;: \u0026#39;33.990.000\u0026#39;} Tổng kết: Kiểu dữ liệu từ điển lưu dữ liệu dưới dạng key-value\nKey-value được ngăn cách với nhau bởi dấu hai chấm (:)\nCặp key-value được ngăn cách với cặp khác bởi dấu phẩy\nKey trong kiểu dữ liệu từ điển là duy nhất\nKiểu từ điển không lưu thông tin theo một thứ tự cụ thể, thông tin khi lấy ra có thể khác thứ tự với thông tin khi nhập vào. Tuy nhiên, từ phiên bản python3.7 trở đi, kiểu từ điển đã được sắp xếp theo thứ tự của key\nKiểu dữ liệu list List là cái thùng chứa, để chứa tập các dữ liệu. Để khai báo kiểu dữ liệu list, ta có thể dụng dấu đóng mở ngoặc vuông ([]), hoặc dùng từ khoá list()\n1 2lsta = [1,2,3,4,5] # đây là khai báo list chính thống của python 3 4lstb = list((1,2,3,4,5)) # đây là sử dụng hàm để tạo list 5 6print(lsta) 7 8print(lstb) Truy xuất dữ liệu trong list Dữ liệu trong list có thể được truy xuất thông qua index. Index là vị trí đứng của phần tử trong list.\nVí dụ, nếu ta muốn lấy ra giá trị ở vị trí 0 của list có tên là lsta, ta thực hiện như sau: lst[0]\n1 2lsta = [5,3,6,9] 3 4print(lsta[0]) 5print(lsta[2]) 6 7\u0026gt;\u0026gt;\u0026gt; print(lsta[0]) 85 9\u0026gt;\u0026gt;\u0026gt; print(lsta[2]) 106 slicing slicing là lấy một nhóm các phần tử trong list ra, cách thực hiện giống như tuple\n1lsta = [5,3,6,9,3,5,1,2,9,6] 2 3print(lsta[1:5]) 4 5\u0026gt;\u0026gt;\u0026gt; print(lsta[1:5]) 6[3, 6, 9, 3] Tuy nhiên, không giống như tuple, giá trị của tuple không thể thay đổi được, giá trị của list có thể thay đổi được, nên chúng ta có thể cập nhật giá trị cho list sử dụng slicing\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4print(lsta) 5 6print(lsta[2:4]) 7 8lsta[2:4] = [8,8] 9 10print(lsta) 11 12print(lsta[2:4]) 13 14 15#Kết quả 16 17\u0026gt;\u0026gt;\u0026gt; print(lsta[2:4]) 18[6, 9] 19\u0026gt;\u0026gt;\u0026gt; 20\u0026gt;\u0026gt;\u0026gt; lsta[2:4] = [8,8] 21\u0026gt;\u0026gt;\u0026gt; 22\u0026gt;\u0026gt;\u0026gt; print(lsta) 23[5, 3, 8, 8, 3, 5, 1, 2, 9, 6] 24\u0026gt;\u0026gt;\u0026gt; 25\u0026gt;\u0026gt;\u0026gt; print(lsta[2:4]) 26[8, 8] Một điều khá thú vị, là list hỗ trợ index ngược, ví dụ, nếu chúng ta muốn lấy phần tử cuối cùng, ta có thể sử dụng index [-1], dùng [-2] nếu muốn lấy phần tử kế cuối .\nví dụ\n1 2 3lsta = [5,3,6,9,3,5,1,2,9,6] 4 5print(lsta[-1]) 6 7print(lsta[-2]) 8 9#Kết quả 10 11\u0026gt;\u0026gt;\u0026gt; print(lsta[-1]) 126 13\u0026gt;\u0026gt;\u0026gt; 14\u0026gt;\u0026gt;\u0026gt; print(lsta[-2]) 159 Hệ quả của index ngược, là chúng ta có slicing với số âm\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4print(lsta[5:-1]) 5 6print(lsta[5:-2]) 7 8#Kết quả 9 10\u0026gt;\u0026gt;\u0026gt; print(lsta[5:-1]) 11[5, 1, 2, 9] 12\u0026gt;\u0026gt;\u0026gt; 13\u0026gt;\u0026gt;\u0026gt; print(lsta[5:-2]) 14[5, 1, 2] Các phương thức được hỗ trợ append Phương thức append dùng để thêm phần tử vào list\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4lsta.append(1) 5 6lsta.append([11,12]) 7 8print(lsta) 9 10 11# Kết quả 12 13\u0026gt;\u0026gt;\u0026gt; print(lsta) 14[5, 3, 6, 9, 3, 5, 1, 2, 9, 6, 1, [11, 12]] pop Phương thức pop dùng để xoá phần tử ở vị trí index ra khỏi list\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4lsta.pop(0) 5 6print(lsta) 7 8# Kết quả 9 10\u0026gt;\u0026gt;\u0026gt; print(lsta) 11[3, 6, 9, 3, 5, 1, 2, 9, 6] remove Phương thức remove dùng để xoá phần tử ra khỏi list, nếu có nhiều phần tử có cùng giá trị với phần tử cần xoá, thì chỉ xoá thằng đầu tiên\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4lsta.remove(9) 5 6print(lsta) 7 8 9# Kết quả 10 11\u0026gt;\u0026gt;\u0026gt; print(lsta) 12[5, 3, 6, 3, 5, 1, 2, 9, 6] reverse Phương thức reverse được dùng để đảo ngược list\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4lsta.reverse() 5 6print(lsta) 7 8 9# Kết quả 10 11\u0026gt;\u0026gt;\u0026gt; print(lsta) 12[6, 9, 2, 1, 5, 3, 9, 6, 3, 5] Các hàm được hỗ trợ len Hàm len trả về số lượng phần tử trong list\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4print(len(lsta)) 5 6 7# Kết quả 8 9\u0026gt;\u0026gt;\u0026gt; print(len(lsta)) 1010 max Hàm max trả về phần tử có giá trị lớn nhất trong list\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4print(max(lsta)) 5 6 7# Kết quả 8 9\u0026gt;\u0026gt;\u0026gt; print(max(lsta)) 109 min Hàm min trả về phần tử có giá trị nhỏ nhất trong list\n1 2lsta = [5,3,6,9,3,5,1,2,9,6] 3 4print(min(lsta)) 5 6 7# Kết quả 8 9\u0026gt;\u0026gt;\u0026gt; print(min(lsta)) 101 Kiểu dữ liệu set Trong python, set là tập hợp không có thứ tự các dữ liệu. Dữ liệu trong set là duy nhất.\nĐể tạo một set, chúng ta sử dụng hàm set(), hoặc dùng dấu đóng mở ngoặc nhọn {}\nVí dụ:\n1 2item_samsung = set([\u0026#34;Samsung Galaxy S22 Ultra 5G\u0026#34;,\u0026#34;Samsung Galaxy A13\u0026#34;]) 3 4item_iphone = {\u0026#34;Iphone 12\u0026#34;,\u0026#34;Iphone 13\u0026#34;} 5 6print(item_samsung) 7 8print(item_iphone) 9 10#Kết quả 11 12\u0026gt;\u0026gt;\u0026gt; print(item_samsung) 13{\u0026#39;Samsung Galaxy S22 Ultra 5G\u0026#39;, \u0026#39;Samsung Galaxy A13\u0026#39;} 14\u0026gt;\u0026gt;\u0026gt; 15\u0026gt;\u0026gt;\u0026gt; print(item_iphone) 16{\u0026#39;Iphone 13\u0026#39;, \u0026#39;Iphone 12\u0026#39;} Một vài phương thức cơ bản của Set Phương thức Add Phương thức này có nhiệm vụ thêm phần tử và Set\nVí dụ:\n1 2item = set() 3 4item.add(\u0026#34;Iphone\u0026#34;) 5 6item.add(\u0026#34;Samsung\u0026#34;) 7 8print(item) 9 10 11# Kết quả 12 13\u0026gt;\u0026gt;\u0026gt; print(item) 14{\u0026#39;Samsung\u0026#39;, \u0026#39;Iphone\u0026#39;} Phương thức Remove, Discard Phương thức này dùng để xoá phần tử ra khỏi set. Điểm khác nhau của hai phương thức này là:\nPhương thức remove: Xoá phần tử ra khỏi set, nếu không tồn tại phần tử cần xoá trong set, chương trình sẽ trả về lỗi KeyError\nPhương thức discard: Xoá phần tử ra khỏi set, nếu không tồn tại phần tử cần xoá trong set, chương trình vẫn hoạt động bình thường.\nVí dụ:\n1 2 3item = set() 4 5item.add(\u0026#34;Iphone\u0026#34;) 6 7item.add(\u0026#34;Samsung\u0026#34;) 8 9print(item) 10 11item.discard(\u0026#34;Samsung\u0026#34;) 12 13item.discard(\u0026#34;Xiaomi\u0026#34;) 14 15item.remove(\u0026#34;Oppo\u0026#34;) 16 17# Kết quả 18 19\u0026gt;\u0026gt;\u0026gt; print(item) 20{\u0026#39;Samsung\u0026#39;, \u0026#39;Iphone\u0026#39;} 21\u0026gt;\u0026gt;\u0026gt; 22\u0026gt;\u0026gt;\u0026gt; item.discard(\u0026#34;Samsung\u0026#34;) # xoá bình thường 23\u0026gt;\u0026gt;\u0026gt; 24\u0026gt;\u0026gt;\u0026gt; item.discard(\u0026#34;Xiaomi\u0026#34;) # Xiaomi không tồn tại trong set item, chương trình vẫn không búng ra lỗi 25\u0026gt;\u0026gt;\u0026gt; 26\u0026gt;\u0026gt;\u0026gt; item.remove(\u0026#34;Oppo\u0026#34;) # Oppo không tồn tại trong set item, chương trình báo lỗi KeyError 27Traceback (most recent call last): 28 File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; 29KeyError: \u0026#39;Oppo\u0026#39; Phương thức Pop Phương thức pop được sử dụng để lấy phần tử đầu tiên của set ra, và loại phần tử cuối đó ra khỏi set.\nVí dụ:\n1 2 3item = {1,5,6,4,3,8,10} 4 5print(item) 6 7print(item.pop()) 8 9print(item) 10 11 12# Kết quả 13 14\u0026gt;\u0026gt;\u0026gt; item = {1,5,6,4,3,8,10} 15\u0026gt;\u0026gt;\u0026gt; 16\u0026gt;\u0026gt;\u0026gt; print(item) 17{1, 3, 4, 5, 6, 8, 10} 18\u0026gt;\u0026gt;\u0026gt; 19\u0026gt;\u0026gt;\u0026gt; print(item.pop()) 201 21\u0026gt;\u0026gt;\u0026gt; 22\u0026gt;\u0026gt;\u0026gt; print(item) 23{3, 4, 5, 6, 8, 10} Phương thức Clear Phương thức clear được sử dụng để xoá mọi phần tử trong set. Kết quả là chúng ta được một set rỗng\n1 2 3item_iphone = {\u0026#34;Iphone 12\u0026#34;,\u0026#34;Iphone 13\u0026#34;} 4 5item_iphone.clear() 6 7print(item_iphone) 8 9 10# Kết quả 11 12\u0026gt;\u0026gt;\u0026gt; print(item_iphone) 13set() Kiểu dữ liệu array Array là tập các phần tử được lưu trữ có thứ tự trong bộ nhớ. Các phần tử trong array phải có cùng kiểu dữ liệu. Kiểu dữ liệu array giống kiểu array trong c++.\nVí dụ:\n1 2import array as arr 3 4item_number = arr.array(\u0026#39;i\u0026#39;,[1,2,3,5]) 5 6print(item_number) 7 8 9item_number_decimal = arr.array(\u0026#39;d\u0026#39;,[5.391,6.626,1.054,1.616]) 10 11 12print(item_number_decimal) 13 14 15 16#Kết quả 17 18\u0026gt;\u0026gt;\u0026gt; print(item_number) 19array(\u0026#39;i\u0026#39;, [1, 2, 3, 5]) 20 21\u0026gt;\u0026gt;\u0026gt; print(item_number_decimal) 22array(\u0026#39;d\u0026#39;, [5.391, 6.626, 1.054, 1.616]) Một vài phương thức cơ bản của array Phương thức insert, phương thức append Phương thức insert và append được sử dụng để thêm phần tử vào array, điểm khác biệt của hai phương thức là:\nPhương thức insert: được sử dụng để chèn phần tử vào vị trí tuỳ ý.\nPhương thức append: Chèn vào cuối array.\nVí dụ:\n1 2import array as arr 3 4item_number = arr.array(\u0026#39;i\u0026#39;,[1,2,3,5]) 5 6item_number.insert(2,4) 7 8print(item_number) 9 10item_number.append(1) 11 12print(item_number) 13 14#Kết quả 15\u0026gt;\u0026gt;\u0026gt; item_number.insert(2,4) 16\u0026gt;\u0026gt;\u0026gt; print(item_number) 17array(\u0026#39;i\u0026#39;, [1, 2, 4, 3, 5]) 18 19\u0026gt;\u0026gt;\u0026gt; item_number.append(1) 20\u0026gt;\u0026gt;\u0026gt; print(item_number) 21array(\u0026#39;i\u0026#39;, [1, 2, 4, 3, 5, 1]) Phương thức truy xuất phần tử theo index Để truy xuất phần tử theo index, chúng ta sử dụng dấu ngoặc vuông [], kèm theo vị trí của phần tử cần truy xuất.\n1 2 3 4import array as arr 5 6item_number = arr.array(\u0026#39;i\u0026#39;,[1,2,3,5]) 7 8print(item_number[2]) 9 10 11#Kết quả 12 13\u0026gt;\u0026gt;\u0026gt; print(item_number[2]) 143 Phương thức remove, phương thức pop Phương thức remove và pop được sử dụng để xoá phần tử ra khỏi array.\nPhương thức remove: Xoá phần tử ra khỏi mảng, nếu mảng có nhiều phần tử trùng với phần tử cần xoá thì chỉ xoá phần tử xuất hiện đầu tiên. Nếu phần tử không có trong mảng, chương trình sẽ búng ra lỗi.\nPhương thức pop: Xoá phần tử ở vị trí index ra khỏi mảng, trả về là giá trị của phần tử bị remove. Nếu không truyền vào vị trí cần xoá, thì sẽ xoá phần tử cuối cùng trong mảng.\n1 2 3import array as arr 4 5item_number = arr.array(\u0026#39;i\u0026#39;,[1,2,3,5,2]) 6 7print(item_number) 8 9item_number.remove(2) 10 11print(item_number) 12 13print(item_number.pop(2)) 14 15print(item_number) 16 17 18#Kết quả 19 20\u0026gt;\u0026gt;\u0026gt; print(item_number) 21array(\u0026#39;i\u0026#39;, [1, 2, 3, 5, 2]) 22\u0026gt;\u0026gt;\u0026gt; item_number.remove(2) # xoá số 2 đi 23\u0026gt;\u0026gt;\u0026gt; 24\u0026gt;\u0026gt;\u0026gt; print(item_number) 25array(\u0026#39;i\u0026#39;, [1, 3, 5, 2]) # Chỉ số 2 đầu tiên bị xoá 26\u0026gt;\u0026gt;\u0026gt; 27\u0026gt;\u0026gt;\u0026gt; print(item_number.pop(2)) # xoá phần tử ở vị trí số 2 285 # Phần tử ở vị trí số 2 là 5, phần tử 5 đã bị xoá 29\u0026gt;\u0026gt;\u0026gt; 30\u0026gt;\u0026gt;\u0026gt; print(item_number) 31array(\u0026#39;i\u0026#39;, [1, 3, 2]) Phương thức index Phương thức này được sử dụng để tìm vị trí của phần tử trong array\n1 2import array as arr 3 4item_number = arr.array(\u0026#39;i\u0026#39;,[1,2,3,5,2]) 5 6print(item_number.index(2)) 7 8 9print(item_number.index(2,2)) 10 11 12 13#Kết quả 14 15\u0026gt;\u0026gt;\u0026gt; print(item_number.index(2)) #Tìm vị trí của số 2 161 17\u0026gt;\u0026gt;\u0026gt; 18\u0026gt;\u0026gt;\u0026gt; print(item_number.index(2,2))# Tìm vị trí của số 2, bắt đầu từ vị trí 2 194 Chúc các bạn học thật tốt.\n","date":"Jul 10, 2022","img":"","permalink":"/courses/python/3_python_data_struct/","series":["Khóa học python căn bản"],"tags":["python"],"title":"Bài 2: Kiểu Dữ Liệu Trong Python"},{"categories":"python","content":"Trong bài viết này, chúng ta sẽ tìm hiểu các mục sau\nCài đặt python Cài đặt phần mềm để lập trình python (IDE) Chương trình python đầu tiên - Hello word Biến trong python Kiểu dữ liệu Khai báo và sử dụng biến trong python Ép kiểu dữ liệu Xem kiểu dữ liệu Cài đặt python Để cài đặt python, các bạn truy cập vào đường dẫn https://www.python.org/downloads/ và download phiên bản python mới nhất, phù hợp với hệ điều hành của bạn. Tại thời điểm mình viết bài viết này, phiên bản python mới nhất là 3.10.4. Nếu các bạn sử dụng hệ điều hành window, công việc sẽ hết sức đơn giản, các bạn chỉ cần download file cài đặt python về, ấn next -\u0026gt; next -\u0026gt; next \u0026hellip; finish. Xong\nĐối với hệ điều hành macos hoặc linux, thông thường thì đã được cài đặt sẵn python, nên các bạn có thể bỏ qua bước này.\nHãy liên hệ mình qua chat message nếu các bạn gặp bất kỳ khó khăn hoặc lỗi gì khi cài đặt python nhé.\nCài đặt phần mềm để lập trình python (IDE) Ở đây, chúng ta sẽ sử dụng Visual studio code, một IDE nhẹ nhàng, hỗ trợ nhiều tính năng, hỗ trợ nhiều môi trường. Các bạn hãy truy cập vào đường dẫn https://code.visualstudio.com/download và download file cài đặt về. Với hệ điều hành window thì chúng ta chỉ cần next -\u0026gt; next \u0026hellip; finish.\nChương trình python đầu tiên - Hello word Chúng ta hãy mở chương trình visual studio code lên, chọn File -\u0026gt; New File -\u0026gt; đặt tên file là hello_word.py\nGõ vào dòng lệnh\n1print (\u0026#34;Hello World!\u0026#34;) Chọn File -\u0026gt; Save hoặc ấn tổ hợp phím ctr + s (window - ubuntu) hoặc command + s (macos)\nChọn Terminal -\u0026gt; New Terminal\nGõ vào trong terminal dòng lệnh\n1python3 hello_word.py Cửa sổ terminal sẽ hiện ra như sau:\n1python3 hello_word.py 2\u0026gt;\u0026gt;\u0026gt;Hello World! Biến trong python Biến là nơi lưu trữ các giá trị. Giá trị được gán vào biến thông qua dấu =\nVí dụ:\n1 2name = \u0026#39;alex\u0026#39; # name là tên biến, giá trị của name là alex 3 4age = 18 # age là tên biến, giá trị của age là 18 Kiểu dữ liệu Mỗi giá trị trong python đều thuộc một kiểu dữ liệu nào đó. Các kiểu dữ liệu được định nghĩa sẵn trong python là string, number, list, dictionary, set. Ngoài ra, chúng ta có thể tự định nghĩa các kiểu dữ liệu để đáp ứng nhu cầu trong bài toán của mình. Ví dụ kiểu dữ liệu con mèo, con chó, động vật, cây, xe đạp, xe hơi \u0026hellip;\nKhai báo và sử dụng biến trong python Python không có câu lệnh khai báo biến. Biến được tạo ra tại thời điểm chúng được gán giá trị.\nVí dụ:\n1x = 5 # biến x được tạo ra, có giá trị là 5, kiểu dữ liệu là int 2 3x = \u0026#34;Alex\u0026#34; # biến x được gán giá trị là Alex, kiểu dữ liệu là string Ép kiểu dữ liệu Ép kiểu, nghĩa là biến đang có kiểu dữ liệu này, chúng ta muốn biến nó thành kiểu dữ liệu nọ. Ví dụ, một biến đang có kiểu dữ liệu là int, bài toán yêu cầu chuyển sang kiểu dữ liệu float rồi tính toán\n1 2 3x = 5 # kiểu dữ liệu của x là int 4 5x = float(x) # kiểu dữ liệu của x là float 6 7x = str(x) # kiểu dữ liệu của x giờ là string Xem kiểu dữ liệu Để xem kiểu dữ liệu của một biến, chúng ta sử dụng hàm type\n1 2x = 9 3 4y = \u0026#39;alex\u0026#39; 5 6 7print(type(x)) 8 9\u0026gt;\u0026gt;\u0026gt; \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; 10 11print(type(y)) 12 13\u0026gt;\u0026gt;\u0026gt; \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; ","date":"Jun 5, 2022","img":"","permalink":"/courses/python/2_python_basic/","series":["Khóa học python căn bản"],"tags":["python"],"title":"Bài 1: Căn Bản Về Python"},{"categories":null,"content":" Chỉ số MACD, là tên gọi tắt của moving average convergence/divergence, là một chỉ số giao dịch được sử dụng trong phân tích kỹ thuật trên giá cổ phiếu. Chỉ số được phát triển bởi Gerald Appel vào những năm cuối thập niên 70 của thế kỷ 20.\nChỉ số MACD giúp chúng ta xác định sự thay đổi của sức mạnh, hướng, động lượng ( momentum), và khoảng thời gian của xu hướng giá cổ phiếu. Để làm được điều này, MACD sử dụng 3 chuỗi thời gian EMA khác nhau, theo sách vở kinh điển là EMA12, EMA26 và EMA9. Chỉ số MACD dùng 3 chuỗi EMA trên, được ký hiệu là MACD(12,26,9). Do chỉ sử dụng lịch sử giá của quá khứ, nên chỉ số MACD được xếp vào nhóm chỉ số dự báo muộn.\nMô hình MACD Đường MACD $$ MACD = EMA_{12} - EMA_{26} $$\nMACD được cấu tạo bằng cách lấy chu kỳ ngắn hạn trừ chu kỳ dài hạn (EMA12 - EMA26). Thị trường tăng giá là tại thời điểm MACD chuyển trạng thái giá trị từ âm sang dương. Ngược lại, thị trường giảm giá là tại thời điểm MACD chuyển trạng thái từ dương sang âm\n$$ signal = EMA_9 $$\nĐường tín hiệu đi song song với đường MACD, và xảy ra các trường hợp sau\nXu hướng tăng giá: Đường MACD cắt đường tín hiệu, MACD đi từ dưới lên\nXu hướng giảm giá: Đường MACD cắt đường tín hiệu, MADC đi từ trên xuống\n$$ histogram = MACD - signal $$\nĐường histogram: đo khoảng cách chên lệch giữa MACD và signal. Như hình 1 phía trên, giá trị của histotram được biểu diễn là các đường trụ hình vuông, có giá trị dài ngắn khác nhau. Giá trị histogram phản ánh giá trị động lượng (momemtum) về giá. Nếu MACD lớn hơn đường tín hiệu thì chúng ta sẽ có vùng đồi dương.\nNếu để ý kỹ, chúng ta có thể thấy rằng, nếu lực mua vẫn dương, đồi vẫn dương, nhưng giá trị histogram ngắn lại, gần 0, đó có thể là tín hiệu của việc giá có thể giảm.\nViết bot mua chứng khoán mã MWG, tự động, dựa trên MACD, kiểm thử lợi nhuận Dưới tư cách là lập trình viên, mình sẽ coding một con bot nhỏ, chỉ sử dụng MACD, và xem thử lợi nhuận như thế nào.\nBài toán giả định:\nCho dư 100 triệu VND trong tay\nĐầu tư cổ phiếu của tập đoàn thế giới Di động, mã cổ phiếu MWG\nThời gian: Từ ngày 1/1/2021 đến 31/12/2021\nĐầu tư toàn bộ trong một lần, không DCA, không xét yếu tố T+3, giá mua và giá chốt lời là giá tại thời điểm close.\nLấy dữ liệu: Mình sử dụng thư viện vnquant của anh Phạm Khánh Đình. Mã nguồn của thư viện ở địa chỉ https://github.com/phamdinhkhanh/vnquant. DataSource được dùng là của vndirect. Thư viện của anh Khánh tại thời điểm mình sử dụng bị lỗi khi load data vndirect, mình có rewrite lại.\nĐể tính EMA, mình xài hàm ewm có sẵn của pandas.\nKết quả: Như hình 1 mình đã show phía trên, hi hi.\n1Profit gained from the MACD strategy by investing $100M in MWG : 15619726.64 2Profit percentage of the MACD strategy : 15% Nếu mình đầu tư 100 triệu ở đầu năm 2021, sử dụng MACD với các tham số (12, 26, 9), đến cuối năm mình thu được thêm 15 triệu 6 trăm 19 ngàn 7 trăm 26 đồng. Lợi nhận hơn 15%, cao hơn tiền lời gửi ngân hàng.\nTham số 12,16,9 theo sách vở, mình không ưng lắm, thế là mình đã cho chạy grid search tìm tham số tốt nhất, kết quả ở hình 2. Cuối cùng mình đã tìm được vài tổ hợp như (9,25,6) hoặc (10,26,5) cho ra lợi nhuận đạt 40%.\nHình 2: MWG grid search\nCho dù các tổ hợp ở trên khá cao, nhưng mình quyết định chọn cặp tổ hợp (11,31,5) cho các bài toán trong tương lai. Lý do là lợi nhuận của tập tổ hợp cũng khá cao, đạt 39%, thứ hai là tập tổ hợp trên chứa toàn số nguyên tố.\nBOT mua chứng khoán, rỗ VN30 Tương tự như mua cổ phiếu MWG. Mình sẽ thực hiện test mua cổ phiếu trong rỗ VN30, gồm các mã cổ phiếu VPB, HPG, MBB, POW, STB, TCB, SSI, CTG, VRE, TCH, VHM, NVL, PDR, BID, FPT, HDB, TPB, SBT, MWG, VIC, BVH, VNM, PLX, MSN, PNJ, VCB, VJC, KDH, REE, GAS như sau:\nMỗi loại cổ phiếu được cấp vốn 100 triệu, tổng cộng 30 mã cổ phiếu, có 3 tỷ\nSử dụng tổ hợp MACD(11,31,5)\nThống kê lợi nhuận cuối năm\nKết quả\nStock code Profit Profit Percent VPB 59531568.83 59 HPG 47797426.41 47 MBB 43655931.7 43 POW 11334630.9 11 STB 68945860.5 68 TCB 30909090.6 30 SSI 88181694.47 88 CTG 16023538.94 16 VRE 13112161.55 13 TCH 61063208.12 61 VHM 5633657.62 5 NVL 83612690.71 83 PDR 73030645.91 73 BID -10671169.9 -11 FPT 16548330.46 16 HDB 42686713.83 42 TPB 30960123.75 30 SBT 25536989.15 25 MWG 39039335.94 39 VIC 30392688.88 30 BVH -3770737.5 -4 VNM -13891787.01 -14 PLX 5485716.75 5 MSN 8503902.24 8 PNJ -6325266.73 -7 VCB -7289353.31 -8 VJC -2464227.2 -3 KDH 44801727.18 44 REE 26242537.2 26 GAS 41354753.12 41 Có ông lời, có ông lỗ. Tổng lời là 869.972.383 triệu, trên tỷ lệ đầu tư là 3 tỷ. Đạt tỷ lệ lợi nhuận 28%.\nỞ trên, mình chỉ sử dụng đường xu hướng để quyết định mua / bán, chưa hề sử dụng giá trị động lượng của histogram để xét việc chốt lời hiệu quả. Nên hiệu quả đầu tư vẫn còn nhỏ.\nMình điều chỉnh lại một chút, nếu giá trị histogram giảm bé hơn 10% so với thời điểm cao nhất ở thời điểm đồi dương, mình sẽ chốt lời ngay. Ngoài ra, chúng ta sẽ kết hợp với mây Ichimoku để lọc lại các thời điểm mua cho hợp lý hơn.\nCảm ơn các bạn đã theo dõi bài viết, hẹn gặp lại ở bài tiếp theo.\n","date":"Apr 11, 2022","img":"","permalink":"/courses/stocks/1_macd/","series":["Chứng khoán căn bản"],"tags":["stock"],"title":"Chỉ Số Dự Báo MACD"},{"categories":null,"content":" Tại sao chúng ta cần chuẩn hóa layer Batch Normalization Batch Normalization hoạt động như thế nào Khuyết điểm của Batch Normalization Weight Normalization Layer Normalization Instance Normalization Group Normalization Nguồn tham khảo Tại sao chúng ta cần chuẩn hóa layer Mình nghĩ, câu trả lời thỏa đáng nhất là bởi vì nó làm tăng độ chính xác của mô hình. Trong quá trình thực nghiệm, các nhà nghiên cứu nhận thấy rằng việc thêm Layer Normalization cho kết quả test tốt hơn/ chạy nhanh hơn, hội tụ sớm hơn \u0026hellip; Và từ đó, các nhà nghiên cứu đổ hết tâm sức khai phá, đào bới nó ra thử sai , cải tiến, đề xuất các mô hình chuẩn hóa liên lục, tạo nên các mô hình mà mình sẽ liệt kê ở dưới.\nThật ra, một ý tưởng nào hay thì cũng có nhiều nhà nghiên cứu đổ hết tâm huyết vào nghiên cứu, đào sâu tận cùng nó ra, để cống hiến cho nhân loại.\nBatch Normalization Đây là một trong các phương pháp chuẩn hóa lâu đời và được sử dụng rộng rãi nhất. Ngay cả mình khi test các data mới cũng xài nó vì sự tiện lợi và nhanh chóng của nó. Các bạn có thể tìm đọc paper có tựa đề Batch normalization: Accelerating deep network training by reducing internal covariate shift. Những phần bên dưới, mình sẽ thay chữ Batch Normalization thành BN để cho câu chữ được ngắng gọn và tập trung vào ý chính hơn.\nBatch Normalization (BN) đề cập đến việc chuẩn hóa giá trị input của layer bất kỳ. Chuẩn hóa có nghĩa là đưa phân phối của layer về xấp xỉ phân phối chuẩn với trung bình xấp xỉ 0 và phương sai xấp xỉ 1. Về mặc toán học, Batch Normalization (BN) thực hiện như sau: với mỗi layer, BN tính giá trị trung bình và phương sai của nó. Sau đó sẽ lấy giá trị đặc trưng trừ giá trị trung bình , sau đó chia cho độ lệch chuẩn. Thực tế, chúng ta hay chia tập train thành từng batch với kích thước là 16,32,64 ,128 \u0026hellip; hình, hay còn gọi là 1 mini-batch size 16,32,64,128 \u0026hellip;. BN được tính toán trên các mini-batch đó.\nCông thức tính trung bình của mini-batch\n$$ \\mu_B \\leftarrow \\frac{1}{m}\\sum^{m}_{i=1}x_i $$\nCông thức tính phương sai của mini-batch\n$$ \\sigma^2_\\beta \\leftarrow \\frac{1}{m}\\sum^{m}_{i=1}(x_i-\\mu_B)^2 $$\nChuẩn hóa\n$$ \\hat{x}_i \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} $$\nPhía trên mà mô tả toán học phép biến đổi Batch Normalizing , sử dụng cho hàm kích hoạt x trên mini-batch.\nThực tế, đôi khi mô hình lại hoạt động hiệu quả với một giá trị trung bình và phương sai khác, nên tác giả thêm 2 siêu tham số là gamma - scale và beta - shift để có tính tổng quát.\n$$ y_i \\leftarrow \\gamma\\hat{x}_i + \\beta $$\nBatch Normalization hoạt động như thế nào Về mặc trực quan, chúng ta biết rằng, trong gradient descent, mạng NN tính giá trị đạo hàm và giảm trọng số của nó dựa vào hướng đi của đạo hàm. Nhưng do các layer được xếp chồng lên nhau, phân phối của dữ liệu đầu vào sẽ bị thay đổi dần do việc cập nhật trọng số của các layer trước đó, làm cho phân phối của đầu vào của các layer phía sau sẽ khác xa so với phân phối của data input. BN giúp cố định phân phối của dữ liệu về phân phối chuẩn, qua tất cả các lớp, dẫn tới tính chất phân phối của dữ liệu không thay đổi qua các lớp.\nKhuyết điểm của Batch Normalization BN thực hiện lại các phép tính trình bày phía trên qua các lần lặp, cho nên, về lý thuyết, chúng ta cần batch size đủ lớn để phân phối của mini-batch xấp xỉ phân phối của dữ liệu. Điều này gây khó khăn cho các mô hình đòi hỏi ảnh đầu vào có chất lượng cao (1920x1080) như object detection, semantic segmentation, \u0026hellip; Việc huấn luyện với batch size lớn làm mô hình phải tính toán nhiều và chậm.\nVới Batch size = 1, giá trị phương sai sẽ là 0. Do đó BN sẽ không hoạt động hiệu quả.\nBN không hoạt động tốt với RNN. Lý do là RNN có các kết nối lặp lại với các timestamps trước đó, và yêu cầu các giá trị beta và gamma khác nhau cho mỗi timestep, dẫn đến độ phức tạp tăng lên gấp nhiều lần, và gây khó khăn cho việc sử dụng BN trong RNN.\nTrong quá trình test, BN không tính toán lại giá trị trung bình và phương sai của tập test. Mà sử dụng giá trị trung bình và phương sai được tính toán từ tập train. Điều này làm cho việc tính toán tăng thêm. Ỏ pytorch, hàm model.eval() giúp chúng ta thiết lập mô hình ở chế độ evaluation. Ở chế độ này, BN layer sẽ sử dụng các giá trị trung bình và phương sai được tính toán từ trước trong dữ liệu huấn luyện. Giúp cho chúng ta không phải tính đi tính lại giá trị này.\nWeight Normalization Tham khảo https://arxiv.org/pdf/1602.07868.pdf Do các bất lợi của BN, T. Saliman và P. Kingma đề xuất cách tính khác, và đặt tên là Weight Normalization. Ý tưởng của tác giả là tách trọng số thành 2 thành phần là giá trị của trọng số và hướng của trọng số. Nhằm mục đích tăng tốc tốc độ train.\nTác giả đề xuất sử dụng hai giá trị g( cho giá trị trọng số ) và v cho hướng của trọng số thay vì sử dụng 1 giá trị w nguyên thủy.\n$$ w = \\frac{g}{||v||}v $$\nVới g là giá trị scala, v là vector. Công thức này nhanh do chúng ta đã fixed được giá trị chuẩn của w. Do chuẩn của w lúc này bằng g.\nKhông giống như BN, WN hoạt động được trong mô hình RNN. Tuy nhiên, về thực nghiệm cho thấy mô hình với WN thường không ổn định, nên ít khi được sử dụng trong thực tế\nLayer Normalization Tham khảo https://arxiv.org/pdf/1607.06450.pdf\nLấy cảm hứng từ BN, Geoffrey Hinton và các đồng sự đã đề xuất Layer Normalization. Phép chuẩn hóa được sử dụng trên từng layer như sau\n$$ \\mu^l =\\frac{1}{H}\\sum^{H}_{i=1}\\alpha^l_i $$\n$$ \\sigma^l = \\sqrt{\\frac{1}{H}\\sum^{H}_{i=1}(\\alpha^l_i-\\mu^l)^2} $$\nVới H là số lượng phần tử trong một hidden layer.\nCái khác nhau chính giữa BN và LN là LN sử dụng chung một giá trị trung bình và phương sai trong 1 hidden layer. LN không phụ thuộc vào mini-batch, nên có thể train được với batch-size = 1 mà không gặp vấn đề gì cả.\nNgoài ra LN cũng có thể được sử dụng trong RNN mà không gặp trở ngại nào như BN.\nInstance Normalization Instance Normalization còn có tên gọi khác là contrast normalization\nÝ tưởng ở đây là chúng ta sẽ chuẩn hoá trên từng channel của từng batch.\nGroup Normalization Tham khảo https://arxiv.org/pdf/1803.08494.pdf\nĐược đề xuất bởi Kaiming He và cộng sự , Group Normalization có cách thức hoạt động tương tự LN, chỉ một khác biệt duy nhất là thuật toán sẽ chia các layer thành từng nhóm và thực hiện chuẩn hóa trên các nhóm đó. Chúng ta phải turning tham số num_groups để tìm số lượng nhóm cho kết quả tốt nhất.\nHai cái chuẩn hoá cuối khá đơn giản, mình không đề cập chi tiết nhiều. Nếu có bạn nào quan tâm thì vui lòng để lại lời nhắn, mình sẽ update thông tin các bạn cần.\nNguồn ảnh : https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\nJournalist: Tony Peng| Editor: Michael Sarazen\nNguồn tham khảo @inproceedings{ioffe2015batch, title={Batch normalization: Accelerating deep network training by reducing internal covariate shift}, author={Ioffe, Sergey and Szegedy, Christian}, booktitle={International conference on machine learning}, pages={448\u0026ndash;456}, year={2015}, organization={PMLR} }\nhttps://analyticsindiamag.com/understanding-normalization-methods-in-deep-learning/\nhttps://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8\nhttps://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6\nhttps://arxiv.org/pdf/1602.07868.pdf\nhttps://arxiv.org/pdf/1607.06450.pdf\nhttps://arxiv.org/pdf/1803.08494.pdf\nhttps://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n","date":"Feb 25, 2022","img":"https://unsplash.it/1920/1080?image=30","permalink":"/blog/2022-02-25-normalization/","series":null,"tags":["Machine Learning","Normalization","Deep Learning"],"title":"Các Kỹ Thuật Chuẩn Hóa Trong Deep Learning"},{"categories":"dataset","content":"1. CASIA-WebFace Dataset có kích thước tầm 4.1G, bao gồm 494,414 hình khuôn mặt của 10,575 người thật được thu thập trên web và đã gán nhãn đầy đủ. Dataset này phục vụ cho bài toán face verification và face identification .\nhttps://archive.org/download/NudeNet_classifier_dataset_v1/NudeNet_Classifier_train_data_x320.zip\nĐối với các bạn muốn mì ăn liền, thì có thể tải pretrain model NudeNet trên pip về rồi thử.\n2. MS-Celeb-1M Tập dataset khuôn mặt gốc được microsoft công bố năm 2016 phục vụ cho bài toán nhận diện khuôn mặt. Tập này chứa tầm 10 triệu ảnh của 100,000 cá nhân khác nhau, đa số là các diễn viên Hollywood (nên có thêm từ Celeb - viết tắt của celebrity).\nNguồn microsoft.com\nHiện nay dataset này đã bị xóa bỏ khỏi website gốc msceleb.org và dự án này của microsoft đã bị kết thúc vì một lý do nào đó.\nLink download: https://academictorrents.com/details/9e67eb7cc23c9417f39778a8e06cca5e26196a97\nCác bạn cân nhắc kỹ trước khi download. Do không phải là link chính chủ\nMã lệnh convert tsv file sang hình ảnh\n1import argparse 2import base64 3import csv 4import os 5# import magic # Detect image type from buffer contents (disabled, all are jpg) 6 7parser = argparse.ArgumentParser() 8parser.add_argument(\u0026#39;--croppedTSV\u0026#39;, type=str) 9parser.add_argument(\u0026#39;--outputDir\u0026#39;, type=str, default=\u0026#39;raw\u0026#39;) 10args = parser.parse_args() 11 12with open(args.croppedTSV, \u0026#39;r\u0026#39;) as tsvF: 13 reader = csv.reader(tsvF, delimiter=\u0026#39;\\t\u0026#39;) 14 i = 0 15 for row in reader: 16 MID, imgSearchRank, faceID, data = row[0], row[1], row[4], base64.b64decode(row[-1]) 17 18 saveDir = os.path.join(args.outputDir, MID) 19 savePath = os.path.join(saveDir, \u0026#34;{}-{}.jpg\u0026#34;.format(imgSearchRank, faceID)) 20 21 # assert(magic.from_buffer(data) == \u0026#39;JPEG image data, JFIF standard 1.01\u0026#39;) 22 23 os.makedirs(saveDir, exist_ok=True) 24 with open(savePath, \u0026#39;wb\u0026#39;) as f: 25 f.write(data) 26 27 i += 1 28 29 if i % 1000 == 0: 30 print(\u0026#34;Extracted {} images.\u0026#34;.format(i)) 31 32# Nguồn https://github.com/EB-Dodo/C-MS-Celeb/issues/1#issuecomment-844894295 Dữ liệu gốc của MS-Celeb-1M có nhiều hình ảnh trùng, gán sai. Có nhiều task đã được implement để làm sạch dataset trên. Một trong những task mình thấy khá ổn là\nhttps://github.com/EB-Dodo/C-MS-Celeb\nTác giả đã xử lý, rút trích, giữ lại tầm 6.5 triệu hình của 94,682 người nổi tiếng\n3. VGG Face và VGG Face2 Dataset bao gồm 494,414 hình khuôn mặt của 10,575 người. Các bạn có thể download tại link chính chủ\nhttps://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\ntập VGG Face2 đã bị xóa trên trang chủ do vi phạm bản quyền. Nên hiện thời không có link chính chủ\n","date":"Feb 25, 2022","img":"","permalink":"/courses/ml_dataset/web_face/","series":["Machine learning dataset"],"tags":["dataset"],"title":"Dataset Nhận Dạng Khuông Mặt"},{"categories":"dataset","content":"Có đôi khi, mình muốn test một model nào đó, nhưng mà mình lại tốn rất nhiều thời gian để tìm kiếm test data. Vì vậy, mình tạo cái tut này để lưu lại những data mình lượm lặt được, phục vụ cho việc tìm kiếm sau này dễ dàng hơn.\nDataset này cung cấp tầm 19G hình ảnh nhạy cảm. Phục vụ cho các bài toán phân loại, nhận dạng và kiểm duyệt nội dung hình ảnh/ video.\nhttps://archive.org/download/NudeNet_classifier_dataset_v1/NudeNet_Classifier_train_data_x320.zip\nĐối với các bạn muốn mì ăn liền, thì có thể tải pretrain model NudeNet trên pip về rồi thử.\n","date":"Feb 25, 2022","img":"","permalink":"/courses/ml_dataset/nunet/","series":["Machine learning dataset"],"tags":["dataset"],"title":"NudeNet Dataset [Dataset Nhạy Cảm, 18+ Only]"},{"categories":"c++","content":"Lịch sử hình thành, phát triển ngôn ngữ c++ C++ là ngôn ngữ lập trình \u0026ldquo;bậc trung\u0026rdquo; được phát triển bởi Bjarne Stroustrup vào năm 1979 tại phòng thí nghiệm Bell Labs. C++ hoạt động được trên nhiều nền tảng khác nhau như Windows, Mac OS, và các phiên bản của UNIX. Series bài học này hướng tới một khóa học đơn giản, với đầy đủ các kiến thức nền tảng của C++ cho người bắt đầu học.\nLý do nên học ngôn ngữ C++ C++ là một trong những ngôn ngữ lập trình phổ biến trên thế giới.\nC++ được sử dụng để phát triển nhiều ứng dụng khác nhau, ví dụ như lập trình game, lập trình hệ điều hành, phát triển các ứng dụng nhúng, làm website \u0026hellip;\nC++ phát triển phần mềm chạy trên nhiều nền tảng.\nC++ có cộng đồng developer mạnh mẽ\nC++ chạy nhanh\nHàng tỷ lý do khác nữa, mình liệt kê không nổi.\nC ++ là một ngôn ngữ lập trình tuyệt vời và nó giải quyết được nhiều nhu cầu cụ thể. Ngôn ngữ lập trình này đã tồn tại được gần 40 năm, nên hầu hết các vấn đề trong việc phát triển phần mềm có thể được giải quyết bằng các thư viện open-source và các frameworks. Hiện nay, điểm nổi bật của C ++ là nó được tạo ra để có tốc độ cực nhanh, nhưng nó cũng phụ thuộc vào tốc độ chạy của bộ xử lý. Một trong những điểm nổi bật khác là C ++ là một ngôn ngữ biên dịch, cho phép nó được thực thi một cách hiệu quả. Điều này là do ngôn ngữ biên dịch được thực thi trực tiếp, hoàn toàn ngược lại với một ngôn ngữ thông dịch. C ++ dịch từ một nguồn sang mã máy, trong khi một ngôn ngữ thông dịch như JavaScript hoặc Python được dịch khi trình thông dịch xử lý mã nguồn.\nC ++ cung cấp các cơ chế trừu tượng, cho phép các thuật toán công nghiệp phức tạp được đóng gói trong các thư viện bổ sung, tốn ít chi phí hơn so với việc phát triển từ đầu. Có hàng ngàn thư viện như này đã được xuất bản trong nhiều năm và các ứng dụng thường có thể nhanh chóng triển khai các thuật toán điều chỉnh này để đạt được các hiệu quả mong muốn với hiệu suất máy gần như tối ưu. Đây là một yếu tố phát huy tác dụng làm cho việc phát triển phần mềm trên C ++ trở nên nhanh chóng.\nTốc độ của C ++ cũng khiến nó trở thành sự lựa chọn tuyệt vời cho các hệ thống nhúng như NASA, robot và thậm chí là các trò chơi quy mô lớn được xếp hạng hàng đầu như bạn có, chẳng hạn như Assassin\u0026rsquo;s Creed, Battlefield, Call of Duty và Doom. Và nếu bạn nghĩ về điều đó, các trò chơi này cần phải vắt kiệt từng phần hiệu suất và thực hiện các phép tính nhanh và tính toán lại nhanh chóng, điều mà C ++ đã làm cho điều đó xảy ra.\nLý do không nên học C++ Mặt khác, C ++ là một ngôn ngữ rất nghiêm ngặt, rất mạnh và rất phức tạp. Và điều này làm cho C ++ trở nên cực kỳ khó học, ngay cả đối với các nhà phát triển dày dạn kinh nghiệm. Nếu bạn thực hiện tìm kiếm trên Google cho “ngôn ngữ lập trình khó nhất”, bạn sẽ nhanh chóng thấy rằng C ++ được liệt kê là ứng cử viên hàng đầu.\nTrên hết, C ++ không phải là lựa chọn phù hợp cho nhiều dự án và ứng dụng. Nếu bạn đang xem xét C ++ để xây dựng các API web, ứng dụng máy tính để bàn, ứng dụng iPhone, v.v., thì C ++ không nên là lựa chọn của bạn trừ khi bạn có kế hoạch cho các ứng dụng của mình nhận được hàng trăm nghìn lượt truy cập mỗi giây. Hầu hết các ứng dụng không cần những mức tăng hiệu suất này. Mặc dù, trong phần trên, tôi cũng đã nói về việc C ++ là một lựa chọn tuyệt vời cho các hệ thống nhúng, một khía cạnh khác để phát triển nhúng là tăng hiệu suất bộ xử lý, dung lượng bộ nhớ khả dụng và tiêu chuẩn hóa trên nền tảng 32 và 64-bit. Và điều này cho phép các ngôn ngữ như Java, Lua và Python được sử dụng trong các hệ thống nhúng sâu và đây là những ngôn ngữ dễ sử dụng hơn.\nNgay cả các hệ thống trò chơi điện tử cũng phát triển nhanh đến mức những trò chơi quy mô lớn này hiện đang sử dụng Unity hoặc C #. Vì vậy, mọi người đang chọn những ngôn ngữ này vì chúng cung cấp khả năng tương thích đa nền tảng giống như C ++, nhưng chúng dễ làm việc hơn nhiều. Bạn có thể vào các trang tìm việc ở Việt Nam như ItViec, hoặc các trang freelacer như Upwork để tìm hiểu số lượng việc làm C++ so với python , javascrip , C# để kiểm chứng. Tại thời điểm viết bài viết này, mình search trên trang itviec và với từ khóa .NET, mình tìm thấy 238 jobs , c++ là 78 jobs, python là 264 jobs, javascrip là 484 jobs. Các bạn có thể tự đưa ra kết luận cho riêng mình dựa vào các con số trên.\nKết luận Mình hi vọng có thể cung cấp đủ thông tin để giúp bạn quyết định xem việc học C ++ có xứng đáng với bạn hay không. C ++ là một trong những ngôn ngữ lập trình hàng đầu, vì vậy hãy yên tâm rằng ngôn ngữ lập trình này sẽ không biến mất khỏi ngành công nghệ. Nhưng bạn chỉ nên học C ++ nếu nó được yêu cầu trong vai trò công việc của bạn hoặc trong lĩnh vực mà nó được sử dụng rộng rãi. Ngược lại, bạn hãy quay xe đúng lúc. Mình sẽ biên soạn thêm nhiều bộ giáo trình học các ngôn ngữ lập trình khác nữa. See Ya\n","date":"Feb 20, 2022","img":"","permalink":"/courses/cplusplus/1_introduction/","series":["Khóa học c++ căn bản"],"tags":["c++"],"title":"Bài 1: Giới Thiệu Về C++"},{"categories":"c++","content":"Lời giới thiệu Chúng ta cần một phần mềm có chất lượng tốt tốt một chút để hỗ trợ coding nhanh, gọn, lẹ. Các bạn có thể sử dụng các phần mềm miễn phí như Eclipse, NetBean, CodeBlock, Notepad++ \u0026hellip;.\nTrong bài viết này, mình đề nghị các bạn cài visual studio hoặc visual studio code. Visual stuido là một phần khá bá đạo, hỗ trợ mạnh mẽ, giúp các bạn lập trình viên coding một cách thoải mái mà không phải vướng bận các vấn đề cấu hình bên ngoài. Nếu có điều kiện, các bạn nên sử dụng phiên bản visual studio enterpise, được mở khóa tất cả các tính năng giúp chúng ta chỉ cần tập trung vào coding.\nCài đặt trên Windows Tại thời điểm mình viết bài viết này, Visual Studio 2022 là phiên bản mới nhất. Các bạn có thể cài phiên bản Visual Studio 2019 vẫn được. Hãy download bộ cài visual studio tại link https://visualstudio.microsoft.com/downloads/ và cài đặt bình thường.\nTrên MacOS Cài Visual studio code bản mới nhất từ trang chủ microsoft\nCài extentsion c/c++\nCài Clang\nCâu lệnh để kiểm tra clang đã được cài hay chưa\n1clang --version Nếu chưa , mở terminal lên và paste đoạn lệnh này vào để cài\n1xcode-select --install Online Compilers Thử tưởng tượng bạn đang ngồi trên xe buýt, trên tay có 1 chiếc điện thoại trang bị 4G đầy đủ, bạn có 1 ý tưởng lóe lên về một hàm nào đấy. Bạn phải làm sao ???\nCác đơn giản nhất là truy cập vào một website compiler c++ online, dev ngay cái ý tưởng của bạn và chạy thử xem như thế nào. Hiện nay, có rất nhiều trang web hỗ trợ chúng ta biên dịch mã nguồn c++ online và xem kết quả tức thì. Trong bài viết này, mình giới thiệu các bạn trang http://cpp.sh/. Lý do là trang này không có chứa quảng cáo, những trang khác ít nhiều có chèn quảng cáo, mình không thích.\ntrang web cpp.sh\nTrang này hỗ trợ 3 trình biên dịch là c++98, c++11 và c++ 14. Ngoài ra, trang web còn hỗ trợ chúng ta sinh ra shot link để gửi mã nguồn ý tưởng của chúng ta cho bạn bè, khá tiện lợi.\n","date":"Feb 20, 2022","img":"","permalink":"/courses/cplusplus/2_ide/","series":["Khóa học c++ căn bản"],"tags":["c++"],"title":"Bài 2: Cài Đặt Công Cụ Hỗ Trợ"},{"categories":"c++","content":"Chương trình đầu tiên, Hello World Mã nguồn\n1#include \u0026lt;iostream\u0026gt; 2using namespace std; 3 4// main() is where program execution begins. 5int main() { 6 cout \u0026lt;\u0026lt; \u0026#34;Hello World. My name AlexBlack.\u0026#34;; // prints Hello World. My name AlexBlack. 7 return 0; 8} Các bạn hãy thực hiện các bước mình mô tả kỹ ở bên dưới\nMở text editor bất kỳ, ví dụ visual studio code.\nTạo 1 file text, đặt tên là main.cpp\nCopy đoạn mã lệnh bên dưới, quăng vào file main.cpp vừa tạo\nMở terminal (cmd trên windown), cd vào thư mục chưa file main.cpp bạn vừa tạo.\nGõ \u0026lsquo;g++ hello.cpp\u0026rsquo; và ấn nút enter. Nếu không có bất kỳ lỗi nào xảy ra, sau khi thực thi xong đoạn lệnh trên, chương trình sẽ sinh ra 1 file có tên là a.out\ngõ \u0026lsquo;a.out\u0026rsquo; để chạy chương trình\nBạn sẽ nhìn thấy dòng chữ \u0026ldquo;Hello World. My name AlexBlack.\u0026rdquo; trên màn hình terminal của bạn\n1g++ main.cpp 2./a.out 3Hello World. My name AlexBlack. Giải thích:\nMột chương trình c++ là một tổ hợp bao gồm nhiều câu lệnh, mỗi câu lệnh có nhiệm vụ và chức năng khác nhau, với đoạn code helloworld phía trên, chương trình có chứa các thành phần.\nDòng đầu tiên, khai báo header thư viện mà chúng ta sử dụng. Ở đây, chúng ta sử dụng thư viện iostream. Đây là thư viện cơ bản, nằm trong bộ thư viện chuẩn của c++.\nDòng tiếp theo using namespace std; báo cho trình biên dịch biết sử dụng namespace std. Khái niệm namespace mình sẽ đề cập ở các chương tiếp theo, đến chương đó, các bạn sẽ hiểu lý do dùng nó, có nên xóa nó đi hay không. Ở bước cơ bản này, các bạn chỉ việc copy đoạn lệnh này rồi quăng vào xài, đừng thắc mắc, phân tâm.\nDòng tiếp theo // main() is where program execution begins. Đây là đoạn comment 1 dòng trong c++. Trình biên dịch gặp // thì sẽ bỏ qua, không biên dịch nội dung ở sau đoạn //. Comment 1 dòng được bắt đầu bởi dấu //, và kết thúc bởi ký tự xuống dòng.\nDòng *int main() * là tên hàm chính. Bất kỳ một chương trình c++ nào, đều có hàm bắt đầu là main. Trình biên dịch sẽ tìm hàm main để bắt đầu chạy thực thi.\nDòng cout \u0026laquo; \u0026ldquo;Hello World. My name AlexBlack.\u0026rdquo;; // prints Hello World. My name AlexBlack. in ra dòng chữ Hello World. My name AlexBlack. lên màn hình\nDòng *return 0; * kết thúc chương trình, trả về giá trị 0 cho chương trình cha gọi chương trình của mình đang viết.\nỞ tiếng việt, chúng ta kết thúc câu bởi dấu \u0026lsquo;chấm(.)\u0026rsquo;. Ngôn ngữ C/C++ kết thúc câu bởi dấu chấm phẩy \u0026lsquo;;\u0026rsquo;. Ở ví dụ trên, câu \u0026lsquo;cout \u0026laquo; \u0026ldquo;Hello World. My name AlexBlack.\u0026rdquo;;\u0026rsquo; được kết thúc bởi dấu chấm phẩy. Nếu thiếu dấu chấm phẩy, trình biên dịch sẽ báo lỗi cú pháp (có đề cập ở mục lỗi, bên dưới)\nComment trong c++ C++ hỗ trợ hai loại comment. Comment một dòng và comment nhiều dòng\nComment một dòng, bắt đầu bằng dấu // kết thúc bằng ký tự xuống dòng. Ví dụ như đoạn mã lệnh hello world ở trên, có 2 cái comment 1 dòng.\nVí dụ:\n1trời nắng, đường vắng Comment nhiều dòng, bắt đầu bằng dấu /*, kết thúc bằng đấu */. Khi bạn muốn viết 1 đoạn chú thích dài, nêu nổi bật vấn đề đang gặp phải, hoặc cách xử lý hay của bạn, hoặc bất kỳ vấn đề gì mà bạn muốn note lại để sau này đọc rõ hơn.\n1/* hôm nay trời nắng chang chang 2mèo con đi học chẳng mang thứ gì */ Một câu hỏi thường được đặt ra là có nên comment hay không. Theo ý kiến riêng của mình là nên. Comment càng nhiều càng tốt, càng chi tiết càng tốt. Tất nhiên là chúng ta phải comment trọng tâm của vấn đề, tránh comment lang mang.\nThử đặt trường hợp, bạn viết 1 hàm tìm điểm rơi của viên bi sắt có khối lượng x khi ném bằng tay phải với góc ném y và lực ném z. 1 tuần sau bạn đọc lại đoạn mã nguồn đó, bạn tự tin rằng mình sẽ hiểu bao nhiêu phần?\nErrors and Warnings Lỗi là một hoạt động bất hợp pháp được thực hiện bởi người dùng dẫn đến hoạt động bất thường của chương trình.\nCác lỗi lập trình thường không bị phát hiện cho đến khi chương trình được biên dịch hoặc thực thi. Một số lỗi ngăn cản chương trình được biên dịch hoặc thực thi. Vì vậy, các lỗi cần được loại bỏ trước khi biên dịch và thực thi.\nCác lỗi phổ biến nhất có thể được phân loại như sau.\nLỗi trong C++ Syntax errors Run-time Errors Linker Errors Logical Errors 1. Syntax errors (lỗi cú pháp) Là lỗi khi mình vi phạm các luật của việc viết code c++. Các lỗi dạng này thường được phát hiện bởi trình biên dịch, nên nó còn có tên gọi khác là compile-time errors. Khi gặp lỗi này, chúng ta sẽ không biên dịch thành công mã nguồn của chương trình.\nMột số lỗi cú pháp phổ biến:\nViết thiếu dấu ;\nViết thiếu dấu đóng ngoặc/mở ngoặc.\nSử dụng biến chưa được khai báo.\n1 // C++ program to illustrate syntax error 2 3#include \u0026lt;iostream\u0026gt; 4 5int main() 6{ 7 int x = 10; 8 int y = 15; 9 10 std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34;\u0026lt;\u0026lt; x\u0026lt;\u0026lt; z \u0026lt;\u0026lt;std::endl // semicolon missed 11 12 return 0; 13} 14 15//Mã nguồn này được viết và chia sẻ bởi Phạm Duy Tùng. Khi chạy dòng code lên, ta sẽ gặp thông báo lỗi:\n1main.cpp:11:41: error: expected \u0026#39;;\u0026#39; after expression 2 std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34;\u0026lt;\u0026lt; x\u0026lt;\u0026lt; z \u0026lt;\u0026lt;std::endl // semicolon missed 3 ^ 4 ; 5main.cpp:11:28: error: use of undeclared identifier \u0026#39;z\u0026#39; 6 std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34;\u0026lt;\u0026lt; x\u0026lt;\u0026lt; z \u0026lt;\u0026lt;std::endl // semicolon missed 7 ^ 82 errors generated. Đoạn báo lỗi trên nhắc là chúng ta thiếu đấu ; sau biểu thước ở dòng 11 cột 41. Và sử dụng biến z chưa được khai báo.\n2. Run-time Errors Lỗi xảy ra trong quá trình chạy chương trình, khi chương trình đã build thành công. Một lỗi phổ biến trong nhóm lỗi này là lỗi chia cho 0.\n1// C++ program to illustrate Run-time error 2 3#include \u0026lt;iostream\u0026gt; 4 5int main() 6{ 7 int x = 10; 8 int x = 0; 9 std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34;\u0026lt;\u0026lt; x/ z \u0026lt;\u0026lt;std::endl; // run-time error 10 11 return 0; 12} 13 14//Mã nguồn này được viết và chia sẻ bởi Phạm Duy Tùng. 3. Linker Errors Lỗi này xảy ra khi ta viết chương trình có sử dụng thêm thư viện ngoài, hoặc thư viện do chính chúng ta viết. Trong quá trình biên dịch, trình biên dịch đã biên dịch thành công các file, nhưng không thể liên kết các file lại với nhau.\n1// C++ program to illustrate Linker error 2#include \u0026lt;iostream\u0026gt; 3int Main() 4{ 5 return 0; 6} 7 8//Mã nguồn này được viết và chia sẻ bởi Phạm Duy Tùng. Trong C/C++ quy định hàm main phải là chữ main (viết thường), ở đây chương trình không tìm được hàm main để bắt đầu thực thi, nên báo lỗi như bên dưới.\n1Undefined symbols for architecture arm64: 2 \u0026#34;_main\u0026#34;, referenced from: 3 implicit entry/start for main executable 4ld: symbol(s) not found for architecture arm64 5clang: error: linker command failed with exit code 1 (use -v to see invocation) 4. Logical Errors Lỗi này xảy ra khi chúng ta nhỡ tay gõ một cái gì đó sai trái làm cho đoạn chương trình không làm đúng theo logic đã được thiết kế từ trước.\n1// C++ program to illustrate Logical error 2#include\u0026lt;iostream\u0026gt; 3using namespace std; 4 5int main(){ 6 // logical error : a semicolon after loop 7 int i=1; 8 while (true); 9 { 10 i++; 11 if(i\u0026gt;10)return i; 12 } 13 14 return 0; 15} 16//Mã nguồn này được viết và chia sẻ bởi Phạm Duy Tùng. Ví dụ trên, mình đã nhỡ tay gõ thêm ký tự ; sau vòng lặp while, làm chương trình lặp vô tận và không có lối thoát.\n1main.cpp:7:17: warning: while loop has empty body [-Wempty-body] 2 while (true); 3 ^ 4main.cpp:7:17: note: put the semicolon on a separate line to silence this warning 51 warning generated. Trong trường hợp mình mắc các lỗi phổ biến, trình biên dịch có thể đưa ra cảnh báo và đưa ra nhắc nhở cho chúng ta.\nCâu lệnh và hàm Câu lệnh Câu lệnh là một phần chương trình c/c++, được thực thi một cách tuần tự.\nVí dụ\n1// C++ program to illustrate statement 2#include\u0026lt;iostream\u0026gt; 3using namespace std; 4 5int main(){ 6 // logical error : a semicolon after loop 7 int i=1; // câu lệnh khai báo declaration statement 8 while (true) //Câu lệnh điều kiện 9 { 10 i++; //Câu lệnh biểu thức 11 if(i\u0026gt;10) //Câu lệnh điều kiện 12 return i; //Câu lệnh return 13 } 14 15 return 0; //Câu lệnh return. 16} 17//Mã nguồn này được viết và chia sẻ bởi Phạm Duy Tùng. Hàm Hàm là nhóm các câu lệnh lại với nhau, để thực hiện một nhiệm vụ. Một chương trình c++ có ít nhất 1 hàm main.\nBạn có thể tùy ý tách các đoạn code nhỏ ra thành nhiều hàm khác nhau. Phụ thuộc vào phong cách code của bạn. Không ai quy định phải tách hàm như thế nào cả. Thông thường, các lập trình viên sẽ tách hàm theo chức năng, công dụng của hàm.\nCấu trúc một hàm bao gồm :\n1return_type function_name( parameter list ){ 2 body ; 3} Trong thư viện c++ chuẩn có cung cấp cho chúng ta kha khá các hàm được xây dựng sẵn, ví dụ hàm làm tròn lên ceil, hàm làm tròn xuống floor. Các bạn có tham khảo trong https://en.cppreference.com/w/cpp/header.\nNhập, xuất dữ liệu Thư viện C++ hỗ trợ chúng ta nhiều thư viện nhập xuất. Trong C++, dữ liệu được thực hiện là một chuỗi tuần tự các byte. Từ chuyên ngành là streams. Vì vây, nên chia làm 2 dạng.\nInput stream: Chuỗi các byte được đưa từ bên ngoài (bàn phím, mạng lan, file \u0026hellip;) vào trong bộ nhớ -\u0026gt; gọi là chuỗi dữ liệu nhập , hay gọi là nhập liệu.\nOutput stream: chuỗi các byte từ bộ nhớ chính đi ra (hiển thị lên màn hình, , qua mạng lan, ra đèn led \u0026hellip; ) -\u0026gt; gọi là chuỗi dữ liệu xuất.\nỞ đây, mình sẽ sử dụng iostream có trong thư viện cơ sở của C++ làm ví dụ minh họa, ngoài ra, c++ còn có iomanip và fstream, các bạn có thể tìm hiểu thêm\n1 2// C++ program to illustrate data stream 3#include\u0026lt;iostream\u0026gt; 4using namespace std; 5 6int main(){ 7 int age; 8 9 cout \u0026lt;\u0026lt; \u0026#34;nhap vao so tuoi cua ban: \u0026#34;; 10 cin \u0026gt;\u0026gt; age; 11 cout \u0026lt;\u0026lt; endl\u0026lt;\u0026lt;\u0026#34;Tuoi cua ban la: \u0026#34; \u0026lt;\u0026lt; age\u0026lt;\u0026lt;endl; 12 13 return 0; 14} 15 16//Mã nguồn này được viết và chia sẻ bởi Phạm Duy Tùng. Trong ví dụ trên, mình sử dụng hàm nhập liệu là cin ( đọc là xi in), có sẵn trong iostream. Hàm sẽ nhận các ký tự mình gõ trên bàn phím, kết thúc bởi dấu enter ( giả sử mình nhập số 5 rồi ấn enter). Bản chất bên trong là các ký tự mình gõ trên bàn phím sẽ biến thành mỗi chuỗi tuần tự các byte (stream) và đẩy vào trong bộ nhớ ram.\nĐể hiển thị lên màn hình, chúng ta dùng hàm cout ( đọc là xi ao). Bản chất bên trong là dữ liệu chúng ta muốn hiển thị lên màn hình sẽ mã hóa thành chuỗi tuần tự byte và đẩy ra các thiết bị ngoại vi.\n1nhap vao so tuoi cua ban: 5 2 3Tuoi cua ban la: 5 Phân biệt C++ Standard library và STL STL và C++ Standard library là 2 ông khác biệt hoàn toàn, phân biệt như sau.\nC++ Standard library C++ Standard library là tập các thư viện chuẩn của C Standard Library, được viết lại dưới dạng tên khác, thông thường là bị xóa .h đi và thêm chữ c ở đầu. Ví dụ thư viện time.h trong c sẽ được xào nấu thành ctime\nSTL STL là từ viết tắt của Standard Template Library , là thư viện bao gồm 4 thành phần chính là algorithms, containers, Numeric, và iterators. Giúp tăng sự linh hoạt và mềm dẻo của C++. Cụ thể\nContainer Containers - tiếng việt dịch ra là thùng chứa, là đối tượng dùng để chứa các đối tượng khác. Container lưu trữ và quản lý các đối tượng, cung cấp các hàm để truy xuất đến các đối tượng.\nContainer được phân loại như sau:\nSequence containers\nvector deque list Associative containers\nset multiset map multimap hash_set hash_map hash_multiset hash_multimap Containers adpators\nStack Queue Priority_queue Phụ thuộc vào bài toán mà chúng ta sẽ lựa chọn container phù hợp để đáp ứng độ phức tạp và thời gian thực thi. Không nên xài đại 1 loại container nào đó.\nIterators Iterators là đối tượng giúp lập trình viên duyệt containers. Chỉ có 2 loại nhóm container là Sequence container và Associative container mới có iterator\nCó 5 loại iterators được hỗ trợ trong c++ là:\ninput (dùng để đọc chuỗi giá trị) output (dùng để ghi chuỗi giá trị) forward ( đọc, ghi, di chuyển lên đến 1 vùng khác) bidirectional (đọc, ghi , di chuyển lên, di chuyển xuống) random access (nhảy tự do đến 1 bước khác) Algorithms Algorithms chứa tập các hàm giúp xử lý nhiều phần tử. Ví dụ sort dùng để xắp xếp các phần tử theo thứ tự. binary_search giúp tìm kiếm dữ liệu dạng nhị phân, cho tốc độ tìm kiếm cao hơn\u0026hellip;.\nNumeric Là tập các thư viện hỗ trợ lập trình viên thực hiện các phép toán trên số. Ví dụ complex hỗ trợ các template và các hàm tính toán số phức.\n","date":"Feb 20, 2022","img":"","permalink":"/courses/cplusplus/3_steep/","series":["Khóa học c++ căn bản"],"tags":["c++"],"title":"Bài 3: Làm Quen Với C++"},{"categories":null,"content":"Tools sinh mật khẩu thông minh, tự động, tránh làm lộ mật khẩu\nChiều dài: Generate Password Ký tự thường: abcd Ký tự hoa: ABCD Ký số: 1234 Ký tự đặc biệt @#$! Pass của bạn: copy \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; Một số lưu ý khi đặt mật khẩu Để tránh cho mật khẩu bị tấn công bởi yếu hacker bằng kỹ thuật tấn công từ điển, tấn công từ điển, tấn công bằng social engineering, và giữ cho tài khoảng online của bạn được an toàn, bạn nên thực hiện những điều sau:\nKhông nên sử dụng chung mật khẩu, câu hỏi bảo mật, câu trả lời bảo nhật cho cùng các tài khoảng quang trọng như ngân hàng, email, facebook\u0026hellip; Sử dụng mật khẩu có chiều dài ít nhất là 16 ký tự, trong đó ít nhất phải chứa 1 ký số, 1 ký tự viết hoa và 1 ký tự đặc biệt. Không nên sử dụng họ/tên của mình hoặc những người trong gia đình, tên thú cưng, ngày tháng năm sinh của mình/gia đình mình để đặt mật khẩu Không nên sử dụng mã bưu chính, số nhà, tên đường, số điện thoại, ngày sinh nhật, số chứng minh nhân dân / căn cước công dân, số bảo hiểm xã hội, số bảo hiểm y tế, bất kỳ số gì mà có thể định danh là bạn làm mật khẩu. Không nên sử dụng những mật khẩu đã bị công bố tên internet làm mật khẩu. Ví dụ như 123456, iloveyou, qwerty\u0026hellip; Không nên sử dụng mật khẩu có đoạn ký tự trùng nhau, ví dụ iloveyoupacpac, cualolocua, \u0026hellip; Không nên sử dụng những thứ có thể bị copy (mà bạn không thể thay đổi) làm mật khẩu. ví dụ như là xác thực bằng vân tay, khuôn mặt (Trong điều kiện bạn muốn an toàn tuyệt đối, thì không nên bật xác thực vân tay và xác thực khuôn mặt trên iphone , hi hi). Không nên cho phép trình duyệt lưu toàn bộ mật khẩu (các trình duyệt hỗ trợ lưu mật khẩu như FireFox, Chrome, Safari, Opera, IE, Microsoft Edge ). bởi vì chúng ta có thể dễ dàng lấy lại mật khẩu từ trình duyệt. Không nên đăng nhập vào tài khoảng quang trọng trên máy người lạ. Không nên đăng nhập vào tài khoảng quang trọng khi sử dụng wifi công cộng, free VPN, free web proxy, tor\u0026hellip; Không nên gửi các thông tin quang trọng qua các giao thức chưa được mã hóa( ví dụ HTTP, FTP ), bởi vì các thông tin đó có thể được đánh cắp một cách dễ dàng qua kỹ thuật sniffed. Bạn nên sử dụng các giao thức đã được mã hóa như là HTTPS, SFTP, FTPS, SMTPS, IPSec bất cứ khi nào có thể. Khi đi du lịch, và sử dụng mạng internet / wifi miễn phí, nếu có thể, hãy mã hóa thông tin của bạn trước khi gửi đi. Ví dụ, bạn có thể sử dụng phần mềm hỗ trợ tạo VPN cá nhân hỗ trợ giao thức WireGuard( hoặc IKEv2, OpenVPN, SSTP, L2TP over IPSec ) vào server cá nhân của bạn( máy tính ở nhà, server bạn dựng trên AWS, VPS\u0026hellip;). Hoặc bạn có thể thiết lập mã hóa kết nối SSH giữa máy bạn và server của bạn, và sau đó cấu hình cho Chrome hoặc FireFox sử dụng socks proxy của bạn. Do đó, ngay cả khi người xấu đã sniff được mảnh dữ liệu thông tin của bạn, họ cũng không thể xem được nó, bởi vì dữ liệu đã được mã hóa. Thông thường, người dùng sẽ rất tự tin rằng mật khẩu họ đặt rất mạnh và khó bị hack. Nhưng trong thực tế, không có gì có thể bảo đảm được điều đó. Một trong những cách có thể kiểm tra là bạn sử dụng một chương trình hash md5 mật khẩu của bạn lại, kiểm tra trên các trạng MD5 decryption website, và kiểm tra xem đoạn md5 của bạn sẽ bị crack trong vòng bao lâu. Khuyến cáo là nên đổi mật khẩu mỗi 10 tuần 1 lần, đối với các tài khoảng quang trọng. Một số tổ chức ngân hàng như techcombank ở Việt Nam có thực hiện theo khuyến nghị này, nhưng với số tuần dài hơn. Thông thường, chúng ta không thể nhớ hết tất cả toàn bộ mật khẩu của chúng ta dã đặt ra. Vì vậy, có một cách khác để thực hiện là chỉ nhớ mật khẩu của những tài khoảng quang trọng. Đối với những tài khoảng ít quang trọng hơn, chúng ta có thể lưu dưới dạng text file và mã hóa file text này bằng các phần mềm như 7-zip, GPG hoặc BitLocker. Nên sao lưu bản mã hóa file text mật khẩu ở một vài nơi, ví dụ đẩy lên email, lưu ở ổ phụ. Để nhỡ xui rủi, vì một lý do nào đó, bạn không thể truy xuất vào máy tính của bạn và lấy lại mật khẩu. Thì bạn có thể dễ dàng xem và lấy lại mật khẩu của những tài khoảng khác. Bật xác minh hai bước bất cứ khi nào có thể. Không nên lưu mật khẩu quang trọng trên mây Truy cập những trang web quan trọng như paypal, ngân hàng, mail\u0026hellip; từ bookmark. Nếu truy cập từ link lại, nên check kỹ domain xem đã chính xác hay chưa. Một mẹo nhỏ là chúng ta có thể kiểm tra độ phổ biến của website bằng công cụ Alexa toolbar để chắc chắn rằng domail bạn đang truy cập không phải là hàng giả Bảo vệ máy tính bạn bằng tường lửa và chương trình diệt virus. Chặn tất cả các kết nối vào và tất cả các kết nối ra không cần thiết bằng tường lửa. Tải các phần mềm từ các site chính thống. Check mã MD5 / SHA1 / SHA256 checksum hoặc GPG signature của phần mềm nếu có thể. Cập nhật hệ điều hành, các phần mềm duyệt web trong máy tính của bạn lên phiên bản mới nhất để fix các lỗi bảo mật Nếu trong máy của bạn có lưu những file cực kỳ quang trọng, ví dụ như bảng lương của công ty, tin nhắn private với trà xanh \u0026hellip; , thì nãy kiểm tra kỹ xem trong máy có chứa phần mềm keyloggers, hoặc phần cứng keyloggers( vd wireless keyboard sniffer ), hoặc camera ẩn. Những thứ ví dụ ở trên là một mối nguy hại rất lớn. Bật tính năng khóa màn hình máy tính / máy tính bảng/ điện thoại ngay khi bạn không sử dụng chúng Mã hóa toàn bộ ổ đĩa cứng bằng các phần mềm mã hóa như VeraCrypt, FileVault, LUKS, \u0026hellip; để bảo vệ các file quan trọng trong máy. Hãy hủy vật lý ổ cứng cũ có chứa thông tin quan trọng của bạn (thay vì ném vào sọt rác, bán ve chai) Nếu được, hãy dùng ít nhất 3 email để nhận mail. Một mail để nhận các thông tin quang trọng từ ngân hàng, paypal hoặc những gì có yếu tố ảnh hưởng đến túi tiền của bạn. Mail thứ hai dùng để xác thực/ nhận mail từ những site không quan trọng. Mail thứ 3 nhận pasword - reset mail khi mail số 1 bị hack. Một lưu ý là mail thứ 3 nên dùng một nền tảng mail khác mail 1. Ví dụ mail 1 dùng gmail thì mail 3 dùng outlook. Nếu được, hãy sử dụng ít nhất 2 số điện thoại. Số điện thoại đầu tiên để xác thực 2 bước với các site quang trọng như ngân hàng. Không cho gia đình, người thân, bạn bè, đồng nghiệp biết số điện thoại số 1. Số điện thoại 2 là số điện thoại public, cho bạn bè người thân liên lạc, đăng ký các app/ web ít quang trọng hơn, vd là số điện thoại đặt hàng tiki, shopee, bách hóa xanh, điện máy xanh, mua trà sửa, tán anh hàng xóm\u0026hellip;. Đừng click vào link trong email/SMS, hãy kiểm tra đường dẫn thật kỹ, và copy paste vào new tab nếu link chính xác, không phải giả mạo. Nếu bạn xác định nó giả mạo, nãy xóa nó đi, hoặc đánh dấu spam rồi xóa nó đi. Không gửi mật khẩu của bạn cho người khác qua email. Cẩn thận với các chương trình online paste tools và chương trình chụp ảnh màn hình download ở trên mạng. Nếu bạn dev web, hãy làm có tâm một chút, đừng lưu mật khẩu của người dùng dạng plain text vào database. Bạn nên lưu bản mã hóa SHA1, SHA256 hoặc SHA512, kèm thêm một chút muối, tiêu, để bảo vệ người dùng. Một ý tưởng hay là lưu thêm mã hash định danh thiết bị người dùng đang sử dụng (vd hệ điều hành, kích thước màn hình, số cpu/ram \u0026hellip;). Khi người dùng đăng nhập sai mật khẩu, và thiết bị người dùng sử dụng không giống với thiết bị đã sử dụng trước đó. Thì hãy bật xác thực 2 bước qua số điện thoại/ email sau khi người dùng đã đăng nhập được. Nếu bạn là nhà phát triển phần mềm, hãy cố gắng cung cấp mã private key sử dụng GnuPG hoặc SHA-256 file ứng dụng của bạn để người dùng có thể kiểm tra xem file đã bị chỉnh sửa hay chưa. Nếu bạn kinh doanh online, hãy đăng ký một domain, thiết lập tài khoảng email tương ứng với domain. Điều này là cần thiết bởi vì bạn sẽ không đánh mất các thông tin liên lạc, và tài khoảng email của bạn sẽ không bị nhà cung cấp nào có thể xóa đi cả. ","date":"Feb 20, 2022","img":"","permalink":"/utils/gen_paswords/","series":null,"tags":["password","random password","password generator"],"title":"Chương Trình Sinh Pasword Ngẫu Nhiên"},{"categories":"python","content":"Nội dung khóa học Bài 1: Giới thiệu về C++\nTổng quan ngôn ngữ C++ Tại sao nên học ngôn ngữ C++ Bài 2: Cài đặt môi trường phát triển (IDE) Visual studio 2015\nGiới thiệu Microsoft Visual Studio Hướng dẫn download và cài đặt visual studio Bài 3: Xây dựng chương trình C++ đầu tiên với Visual Studio 2015\nMột số kiến thức cần lưu ý Cách tạo và biên dịch chương trình C++ đầu tiên trên Visual Studio Một số vấn đề thường gặp đối với lập trình viên mới Bài 4: Cấu trúc một chương trình C++ (Structure of a program)\nCấu trúc của một chương trình C++ Cú pháp và lỗi cú pháp trong C++ (Syntax and syntax errors) Bài 5: Ghi chú trong C++ (Comments in C++)\nCú pháp comment trong C++ Một số kinh nghiệm khi comment trong lập trình Bài 6: Biến trong C++ (Variables in C++)\nBiến trong C++ Khởi tạo biến trong C++ (Defining a variable) Định nghĩa biến ở đâu (Where to define variables) Bài 7: Số tự nhiên và Số chấm động trong C++ (Integer, Floating point)\nTổng quan về kiểu dữ liệu cơ bản trong C++ Kiểu số nguyên (Integer) Số chấm động (Floating point numbers) Bài 8: Kiểu ký tự trong C++ (Character)\nTổng quan về kiểu ký tự (Character) Khai báo, khởi tạo và gán giá trị một biến ký tự In ký tự ra màn hình In ký tự từ số nguyên và ngược lại (Casting) Escape sequences Newline ‘\\n’ và std::endl Dấu nháy đơn ‘K’ và dấu nháy kép “Kteam” Bài 9: Kiểu luận lý và cơ bản về Câu điều kiện If (Boolean and If statements)\nTổng quan về kiểu luận lý (Boolean) Cơ bản về câu điều kiện If và Boolean Bài 10: Nhập, Xuất và Định dạng dữ liệu trong C++ (Input and Output)\nXuất dữ liệu với std::cout trong C++ Nhập dữ liệu với std::cin trong C++ Định dạng dữ liệu nhập xuất trong C++ Bài 11: Hằng số trong C++ (Constants)\nTổng quan hằng số (Constants) Hằng số với từ khóa const Hằng số với chỉ thị tiền xử lý #define Nên định nghĩa hằng số ở đâu Bài 12: Toán tử số học, toán tử tăng giảm, toán tử gán số học trong C++ (Operators)\nTổng quan về toán tử Toán tử số học trong C++ (Arithmetic operators) Toán tử gán số học trong C++ (Arithmetic assignment operators) Bài 13: Toán tử quan hệ, logic, bitwise, misc và độ ưu tiên toán tử trong C++\nToán tử quan hệ trong C++ (Relational operators) Toán tử logic trong C++ (Logical operators) Toán tử trên bit trong C++ (Bitwise operators) Các toán tử hỗn hợp trong C++ (Misc Operators) Độ ưu tiên và quy tắc kết hợp toán tử trong C++ Bài 14: Cơ bản về chuỗi ký tự trong C++ (An introduction to std::string)\nTổng quan về chuỗi ký tự (std::string) Khai báo, khởi tạo và gán giá trị một chuỗi ký tự Xuất một chuỗi ký tự (string output): Nhập một chuỗi ký tự (string input) Một số thao tác cơ bản với chuỗi ký tự Bài 15: Biến cục bộ trong C++ (Local variables in C++)\nTổng quan về tầm vực của biến Biến cục bộ (Local variables) Bài 16: Biến toàn cục trong C++ (Global variables in C++)\nTổng quan về tầm vực của biến Biến toàn cục (Global variables) Sử dụng biến toàn cục là nguy hiểm Khi nào cần sử dụng biến toàn cục (non-const) Bài 17: Biến tĩnh trong C++ (Static variables in C++)\nTổng quan về biến tĩnh (static variables) Khi nào nên sử dụng biến tĩnh Bài 18: Ép kiểu ngầm định trong C++ (Implicit type conversion in C++)\nTổng quan về ép kiểu dữ liệu Ép kiểu ngầm định trong C++ (Implicit type conversion) Bài 19: Ép kiểu tường minh trong C++ (Explicit type conversion in C++)\nÉp kiểu tường minh trong C++ (Explicit type conversion) Bài 20: Cơ bản về Hàm và Giá trị trả về (Basic of functions and return values)\nTổng quan về hàm (functions overview) Giá trị trả về (return values) Giá trị trả về của kiểu void (return values of type void) Bài 21: Truyền Giá Trị cho Hàm (Passing Arguments by Value)\nTham số và đối số của hàm (Function parameters and arguments) Truyền giá trị cho hàm (Passing arguments by value) Tổng kết về phương pháp truyền giá trị cho hàm (Passing argument by value) Bài 22: Truyền Tham Chiếu cho Hàm (Passing Arguments by Reference)\nTruyền tham chiếu cho hàm (Passing arguments by reference) Truyền tham chiếu hằng (Pass by const reference) Tổng kết về phương pháp truyền tham chiếu cho hàm (Passing arguments by reference) Bài 23: Tiền khai báo và Định nghĩa Hàm (Forward declarations and Definitions of Functions)\nLỗi “identifier not found” Tiền khai báo và nguyên mẫu hàm (Forward declaration and function prototypes) Khai báo và định nghĩa trong C++ (Declarations and definitions in C++) Bài 24: Giới thiệu về cấu trúc điều khiển (Control flow introduction)\nTổng quan về cấu trúc điều khiển trong C++ Câu lệnh dừng (halt) Câu lệnh nhảy (Jumps) Cấu trúc rẽ nhánh có điều kiện (Conditional branches) Cấu trúc vòng lặp (Loops) Xử lý ngoại lệ (Exceptions handling) Bài 25: Câu điều kiện If và Toán tử điều kiện (If statements and Conditional operator)\nCâu điều kiện If Toán tử điều kiện (Conditional operator) Bài 26: Câu điều kiện Switch trong C++ (Switch statements)\nCâu điều kiện Switch (Switch statements) Khai báo và khởi tạo biến bên trong case statement Bài 27: Câu lệnh Goto trong C++ (Goto statements)\nTổng quan về câu lệnh Goto trong C++ Một số vấn đề của câu lệnh Goto Bài 28: Vòng lặp While trong C++ (While statements)\nTổng quan về cấu trúc vòng lặp Vòng lặp while (while statements) Bài 29: Vòng lặp Do while trong C++ (Do while statements)\nVòng lặp do while (do while statements) Bài 30: Vòng lặp For trong C++ (For statements)\nVòng lặp for (for statements) Bài 31: Từ khóa Break and continue trong C++\nTừ khóa break Từ khóa continue Bài 32: Phát sinh số ngẫu nhiên trong C++ (Random number generation)\nTổng quan về phát sinh số ngẫu nhiên Phát sinh số ngẫu nhiên trong C++ Phát sinh số ngẫu nhiên trong C++ 11 Bài 33: Mảng 1 chiều trong C++ (Arrays)\nTại sao lại sử dụng mảng? Tổng quan về mảng 1 chiều Khai báo và khởi tạo mảng 1 chiều Xuất các phần tử mảng 1 chiều Nhập dữ liệu cho mảng 1 chiều Phát sinh dữ liệu ngẫu nhiên cho mảng 1 chiều Bài 34: Các thao tác trên Mảng một chiều\nTruyền mảng vào hàm (passing arrays to functions) Nhập và xuất mảng 1 chiều Sao chép mảng 1 chiều Tìm kiếm phần tử trong mảng Sắp xếp mảng 1 chiều Thêm và xóa một phần tử trong mảng Bài 35: Mảng 2 chiều trong C++ (Two-dimensional arrays)\nMảng 2 chiều là gì? Khai báo và khởi tạo mảng 2 chiều Xuất các phần tử mảng 2 chiều Nhập các phần tử mảng 2 chiều Bài 36: Các thao tác trên Mảng 2 chiều\nTruyền mảng vào hàm (passing arrays to functions) Nhập và xuất mảng 2 chiều Tính tổng các phần tử trong mảng Tìm giá trị lớn nhất của mảng 2 chiều Bài 37: Mảng ký tự trong C++ (C-style strings)\nMảng ký tự (C-style strings) là gì? Khai báo và khởi tạo mảng ký tự (C-style strings) Xuất mảng ký tự (C-style strings) với std::cout Nhập mảng ký tự (C-style strings) với std::cin Bài 38: Các thao tác trên Mảng ký tự (C-style strings)\nMột số thao tác với mảng ký tự (C-style strings)\nhttps://www.freecodecamp.org/news/learn-c-with-free-31-hour-course/\n⌨️ (0:00:00) Introduction to data structures ⌨️ (0:06:33) Data Structures: List as abstract data type ⌨️ (0:19:40) Introduction to linked list ⌨️ (0:36:50) Arrays vs Linked Lists ⌨️ (0:49:05) Linked List - Implementation in C/C++ ⌨️ (1:03:02) Linked List in C/C++ - Inserting a node at beginning ⌨️ (1:15:50) Linked List in C/C++ - Insert a node at nth position ⌨️ (1:31:04) Linked List in C/C++ - Delete a node at nth position ⌨️ (1:43:32) Reverse a linked list - Iterative method ⌨️ (1:57:21) Print elements of a linked list in forward and reverse order using recursion ⌨️ (2:11:43) Reverse a linked list using recursion ⌨️ (2:20:38) Introduction to Doubly Linked List ⌨️ (2:27:50) Doubly Linked List - Implementation in C/C++ ⌨️ (2:43:09) Introduction to stack ⌨️ (2:51:34) Array implementation of stacks ⌨️ (3:04:42) Linked List implementation of stacks ⌨️ (3:15:39) Reverse a string or linked list using stack. ⌨️ (3:32:03) Check for balanced parentheses using stack ⌨️ (3:46:14) Infix, Prefix and Postfix ⌨️ (3:59:14) Evaluation of Prefix and Postfix expressions using stack ⌨️ (4:14:00) Infix to Postfix using stack ⌨️ (4:32:17) Introduction to Queues ⌨️ (4:41:35) Array implementation of Queue ⌨️ (4:56:33) Linked List implementation of Queue ⌨️ (5:10:48) Introduction to Trees ⌨️ (5:26:37) Binary Tree ⌨️ (5:42:51) Binary Search Tree ⌨️ (6:02:17) Binary search tree - Implementation in C/C++ ⌨️ (6:20:52) BST implementation - memory allocation in stack and heap ⌨️ (6:33:55) Find min and max element in a binary search tree ⌨️ (6:39:41) Find height of a binary tree ⌨️ (6:46:50) Binary tree traversal - breadth-first and depth-first strategies ⌨️ (6:58:43) Binary tree: Level Order Traversal ⌨️ (7:10:05) Binary tree traversal: Preorder, Inorder, Postorder ⌨️ (7:24:33) Check if a binary tree is binary search tree or not ⌨️ (7:41:01) Delete a node from Binary Search Tree ⌨️ (7:59:27) Inorder Successor in a binary search tree ⌨️ (8:17:23) Introduction to graphs ⌨️ (8:34:05) Properties of Graphs ⌨️ (8:49:19) Graph Representation part 01 - Edge List ⌨️ (9:03:03) Graph Representation part 02 - Adjacency Matrix ⌨️ (9:17:46) Graph Representation part 03 - Adjacency List\n","date":"Feb 20, 2022","img":"","permalink":"/courses/python/1_introduction/introduction/","series":["Khóa học python căn bản"],"tags":["c++"],"title":"Giới Thiệu Về Python"},{"categories":null,"content":"Tools sinh số ngẫu nhiên Việc phát sinh một con số được gọi là ngẫu nhiên được sử dụng rộng rãi vì nhiều thứ trong thế giới thực được xem là được xuất hiện một cách ngẫu nhiên. Vì vậy, để mô phỏng những quá trình xảy ra như ở thế giới thực, chúng ta cần sinh các con số một cách ngẫu nhiên. Ví dụ, hình dạng của đám mây, hình dạng của các dãy núi, rừng cây, khối đá, quá trình phát triển tế bào, quá trình tiến hóa, vân vân và mây mây.\nhttps://en.wikipedia.org/wiki/Randomized_algorithm Các bạn có thể tìm đọc để hiểu thêm về các thuật toán random/giả random.\nQuay \u0026nbsp; ","date":"Feb 20, 2022","img":"","permalink":"/utils/random/","series":null,"tags":["tools"],"title":"Sinh Số Ngẫu Nhiên"},{"categories":null,"content":" Giới thiệu Bước 1: Tạo bàn cờ và sinh nước đi Bước 2: Hàm lượng giá Bước 3. Tìm kiếm cây sử dụng minimax Bước 4: Cắt tỉa Alpha - Beta Giới thiệu Cờ tướng là một môn thể thao khá phổ biến ở Việt Nam. Các bạn có thể bắt gặp các bàn cờ ở các con hẻm của mỗi góc phố. Hoặc là khi các bộ bàn ghế đá thì người mua cũng thường nhờ thợ khắc lên bàn cờ tướng để hàng xóm láng giềng giải trí ngày cuối tuần. Trong bài viết này, mình sẽ hướng dẫn step by step ứng dụng chơi game cờ tướng đơn giản với một chút AI. Hi vọng sẽ giúp được các bạn trên con đường thực hành máy học.\nCác việc cần làm:\nTạo bàn cờ và sinh nước đi\nLượng giá bàn cờ\nÁp dụng minimax\nÁp dụng cắt tỉa alpha, beta\nBạn có thể chơi thử game cờ tướng mình có post ở đây: https://www.phamduytung.com/games/china_chess/\nBước 1: Tạo bàn cờ và sinh nước đi Mình không có gỏi lắm trong việc thiết kế mấy icon cho mấy con tướng, sĩ, tượng. Ngoài ra, công việc chính của chúng ta là phần làm sao cho máy tự đánh được, nên phần này mình sẽ xài các open source có sẵn, lượn lờ một chút trên mạng thì mình đã lụm được cái bàn cờ ở link https://github.com/lengyanyu258/xiangqiboardjs và thư viện sinh nước đi xiangqi.js. Thư viện xiangqi.js đã có sẵn các hàm kiểm tra tính hợp lệ của nước đi, nên mình chỉ việc lấy ra rồi dùng thôi, khỏi mất công phải viết lại.\nBàn cờ được chia làm 2 đội, là đội đen (black, ký hiệu b) và đội đỏ (red , ký hiệu r), mỗi đội gồm 16 quân, bao gồm 1 con tướng (General hoặc king , ký hiệu k), 2 con sỹ (Advisor hoặc guards, ministers, ký hiệu là a), 2 con tượng (Elephants hoặc bishops - ký hiệu là b), 2 con mã (Horses hoặc knights - ký hiệu là n, do chữ k trùng với king là con tướng, nên người ta xài chữ n), 2 con xe (Chariot hoặc rooks - ký hiệu là r), 2 con pháo (canons, ký hiệu là c ), 5 con chốt (Soldiers , ký hiệu là p ( do con chốt ở cờ đen và cờ đỏ có phiên âm tiếng trung khác nhau, chốt cờ đen đọc gần giống chữ \u0026ldquo;zú\u0026rdquo; (\u0026ldquo;pawn\u0026rdquo; hoặc \u0026ldquo;private\u0026rdquo; - tiếng anh), còn chốt cờ đỏ đọc là bing (\u0026ldquo;soldier\u0026rdquo; - tiếng anh) )).\nTổng cộng, ta có tướng, sỹ, tượng, mã, xe, pháo, chốt, 7 loại quân, tương đương với 7 ký hiệu, tổ hợp với 2 đội là đỏ và đen, tổ hợp với nhau, ta xác định được\nĐể bắt đầu, chúng ta sẽ code một hàm random bước đi đơn giản. Hàm có nhiệm vụ lấy ngẫu nhiên một bước đi trong danh sách các bước có thể đi, sau đó máy sẽ đánh bước đi đó.\n1 2 3function makeRandomMove () { 4 let possibleMoves = game.moves(); 5 6 // game over 7 if (possibleMoves.length === 0) return; // Không còn nước nào có thể đi, end game 8 9 let randomIdx = Math.floor(Math.random() * possibleMoves.length); // bốc đại 1 nước đi trong danh sách các bước có thể đi 10 game.move(possibleMoves[randomIdx]); 11 board.position(game.fen()); 12} Do thuật toán chúng ta cho máy chạy khá là ngốc, nên nó đánh cũng hơi ngốc. :)\nBước 2: Hàm lượng giá Dựa vào mức độ cơ động, tầm quang trọng của mỗi quân lính trên bàn cờ, chúng ta sẽ gán cho mỗi quân cờ một trọng số khác nhau thể hiện điều đó.\nVí dụ, chúng ta set các trọng số như sau:\ntướng của ta là 900 điểm, tướng của đối thủ là -900 điểm\nsỹ của ta là 20 điểm, sỹ của đối thủ là -20 điểm\ntượng của ta là 20 điểm, tượng của đối thủ là -20 điểm\nmã của ta là 40 điểm, mã của đối thủ là -40 điểm\nxe của ta là 90 điểm, xe của đối thủ là -90 điểm\npháo của ta là 45 điểm, pháo của đối thủ là -45 điểm\nchốt của ta là 15 điểm, chốt của đối thủ là -15 điểm\nHàm lượng giá ở trên khá ngây thơ, mọi quân cờ đều có điểm ngang nhau, không quan tâm vị trí đứng của nó.\nTrên thực tế, chúng ta thấy rằng, con tướng ở vị trí trung tâm thường là an toàn nhất, một khi tướng leo lên lầu 1 hoặc leo lầu 2, nghĩa là con tướng có khả năng bị đột tử cao hơn, nên chúng ta phải tinh chỉnh lại điểm của con tướng trong trường hợp này.\nMột ví dụ nữa là vị trí con mã, mã gần với thành của tướng địch hơn thì khả năng con xe chiếu bí tướng địch sẽ cao hơn con mã chưa qua sông.\nGiá trị lượng giá cho cờ tướng, các bạn có thể tham khảo ở link https://github.com/markdirish/xiangqi/blob/master/evaluate.js\nChúng ta sẽ duyệt lần lượt từ trái qua phải, từ trên xuống dưới, tính điểm của bàn cờ hiện tại.\nHàm lượng giá của bàn cờ xét như sau:\n1 2 3 4function evaluateBoard(board) { 5 var totalEvaluation = 0; 6 for (var i = 0; i \u0026lt; 10; i++) { 7 for (var j = 0; j \u0026lt; 9; j++) { 8 totalEvaluation = totalEvaluation + getPieceValue(board[i][j], i ,j); 9 } 10 } 11 return totalEvaluation; 12} 13 14 15 16function getPieceValue(piece, x, y) { 17 if (piece === null) { 18 return 0; 19 } 20 var getAbsoluteValue = function (piece, isRed, x ,y) { 21 if (piece.type === \u0026#39;p\u0026#39;) { //chốt 22 return 15 + ( isRed ? pEvalRed[x][y] : pEvalBlack[x][y] ); 23 } else if (piece.type === \u0026#39;r\u0026#39;) { //Xe 24 return 90 +( isRed ? rEvalRed[x][y] : rEvalBlack[x][y] ); 25 } else if (piece.type === \u0026#39;c\u0026#39;) { //pháo 26 return 45 +( isRed ? cEvalRed[x][y] : cEvalBlack[x][y] ); 27 } else if (piece.type === \u0026#39;n\u0026#39;) { // mã 28 return 40 +( isRed ? nEvalRed[x][y] : nEvalBlack[x][y] ); 29 } else if (piece.type === \u0026#39;b\u0026#39;) { // tượng 30 return 20 +( isRed ? bEvalRed[x][y] : bEvalBlack[x][y] ); 31 } else if (piece.type === \u0026#39;a\u0026#39;) { // sỹ 32 return 20 +( isRed ? aEvalRed[x][y] : aEvalBlack[x][y] ); 33 } else if (piece.type === \u0026#39;k\u0026#39;) { // tướng 34 return 900 +( isRed ? kEvalRed[x][y] : kEvalBlack[x][y] ); 35 } 36 throw \u0026#34;Unknown piece type: \u0026#34; + piece.type; 37 }; 38 39 var absoluteValue = getAbsoluteValue(piece, piece.color === \u0026#39;r\u0026#39;, x ,y); 40 return piece.color === \u0026#39;r\u0026#39; ? absoluteValue : -absoluteValue; 41} Bây giờ, chúng ta chỉ cần duyệt qua toàn bộ các nước có thể đi, tính xem nước đi nào có điểm số là lớn nhất, thì máy sẽ đi theo nước đi đó.\n1 2 3function getBestMove(game) { 4 5var newGameMoves = game.moves(); 6var bestMove = null; 7// set đại một số âm vô hạn 8var bestValue = -9999; 9 10for (var i = 0; i \u0026lt; newGameMoves.length; i++) { 11 var newGameMove = newGameMoves[i]; 12 game.move(newGameMove); 13 14 var boardValue = -evaluateBoard(game.board()) 15 game.undo(); 16 if (boardValue \u0026gt; bestValue) { 17 bestValue = boardValue; 18 bestMove = newGameMove 19 } 20} 21 22return bestMove; 23 24}; Vì vậy, ngoài việc xét điểm cho các loại quân, chúng ta sẽ có một bảng xét điểm cho các con\nKết quả có vẻ tốt hơn so với việc random bước đi trước đó, nhưng thuật toán vẫn còn hơi dốt dốt xíu, do máy chỉ tính 1 nước đi và chọn ra nước đi tốt nhất. Nên máy chưa có cái nhìn dài hơn. Có nhiều cách để cho máy có thể có góc nhìn xa hơn về thế cục của bàn cờ, một trong các cách được giới thiệu ở đây là sử dụng minimax\nBước 3. Tìm kiếm cây sử dụng minimax Thuật toán minimax thuộc nhóm duyệt theo chiều sâu (depth first search). Hai người chơi, một người được gọi là MAX, người còn lại gọi là MIN. Thuật toán được thiết kế để tìm nước đi tối ưu cho người MAX. Người MAX sẽ giữ node gốc, lần lượt duyệt đệ quy qua tất cả các node con theo chiều sâu nhất định đến khi duyệt qua tất cả các node hoặc là tìm được một đường đi mà đạt MAX.\nChi tiết hơn, người MAX sẽ đi đầu tiên. Nhiệm vụ của MAX là tìm nước đi sao cho điểm số của mình là cao nhất, nhiệm vụ của MIN là tìm nước đi để cực tiểu hoá điểm số của MAX.\nCác bạn có thể đọc thêm ở link https://en.wikipedia.org/wiki/Minimax.\nĐể triển khai minimax, đầu tiên, chúng ta sẽ sửa lại hàm getBestMove ở trên, thay vì gọi lượng giá bàn cờ evaluateBoard, chúng ta sẽ gọi hàm minimax\n1 2 3function minimaxRoot(depth, game, isMaximisingPlayer) { 4 var newGameMoves = game.moves(); 5 var bestMove = -9999; 6 var bestMoveFound; 7 8 for(var i = 0; i \u0026lt; newGameMoves.length; i++) { 9 var newGameMove = newGameMoves[i] 10 game.move(newGameMove); 11 var value = minimax(depth - 1, game, !isMaximisingPlayer); 12 game.undo(); 13 if(value \u0026gt;= bestMove) { 14 bestMove = value; 15 bestMoveFound = newGameMove; 16 } 17 } 18 return bestMoveFound; 19} với hàm minimax cũng cùng ý tưởng với hàm getBestMove ở trên, nhưng ta sẽ gọi đệ quy, luân phiên tính điểm máy, sau đó tính điểm người \u0026hellip; theo độ sâu ta đã thiết lập, để tìm ra đường đi có số điểm là lớn nhất.\n1 2function minimax (depth, game, isMaximisingPlayer) { 3 if (depth === 0) { 4 return -evaluateBoard(game.board()); 5 } 6 var newGameMoves = game.moves(); 7 if (isMaximisingPlayer) { 8 var bestMove = -9999; 9 for (var i = 0; i \u0026lt; newGameMoves.length; i++) { 10 game.move(newGameMoves[i]); 11 bestMove = Math.max(bestMove, minimax(depth - 1, game, !isMaximisingPlayer)); 12 game.undo(); 13 } 14 return bestMove; 15 } else { 16 var bestMove = 9999; 17 for (var i = 0; i \u0026lt; newGameMoves.length; i++) { 18 game.move(newGameMoves[i]); 19 bestMove = Math.min(bestMove, minimax(depth - 1, game, !isMaximisingPlayer)); 20 game.undo(); 21 } 22 return bestMove; 23 } 24}; Thuật toán này hoạt động khá hiệu quả, nhưng có một điểm yếu là nó sẽ vét cạn toàn bộ các trường hợp để tìm ra đường đi tối ưu nhất. Vì vậy, với giá trị độ sâu càng lớn thì thuật toán chạy càng chậm.\nBước 4: Cắt tỉa Alpha - Beta Cắt tỉa Alpha - Beta là một phương pháp tối ưu hoá của thuật toán minimax, phương pháp này giúp chúng ta bỏ qua một vài nhánh trong quá trình tìm kiếm, làm giới hạn phạm vi tìm kiếm, giúp mô hình hoạt động nhanh hơn.\nThuật toán sẽ hoạt động hiệu quả hơn nếu những bước tìm kiếm đầu tiên là những nước đi tốt nhất :)\nHàm minimax với alpla, beta được viết lại như sau\n1 2 3 4function minimax(depth, game, alpha, beta, isMaximisingPlayer) { 5 positionCount++; 6 if (depth === 0) { 7 return -evaluateBoard(game.board()); 8 } 9 10 var newGameMoves = game.moves(); 11 12 if (isMaximisingPlayer) { 13 var bestMove = -9999; 14 for (var i = 0; i \u0026lt; newGameMoves.length; i++) { 15 game.moves(newGameMoves[i]); 16 bestMove = Math.max(bestMove, minimax(depth - 1, game, alpha, beta, !isMaximisingPlayer)); 17 game.undo(); 18 alpha = Math.max(alpha, bestMove); 19 if (beta \u0026lt;= alpha) { 20 return bestMove; 21 } 22 } 23 return bestMove; 24 } else { 25 var bestMove = 9999; 26 for (var i = 0; i \u0026lt; newGameMoves.length; i++) { 27 game.moves(newGameMoves[i]); 28 bestMove = Math.min(bestMove, minimax(depth - 1, game, alpha, beta, !isMaximisingPlayer)); 29 game.undo(); 30 beta = Math.min(beta, bestMove); 31 if (beta \u0026lt;= alpha) { 32 return bestMove; 33 } 34 } 35 return bestMove; 36 } 37} Cảm ơn các bạn đã theo dõi bài viết. Xin chào và hẹn gặp lại các bạn ở bài viết kế tiếp.\nCác bạn có thể chơi game ở đây nha , link https://www.phamduytung.com/games/china_chess/\nMình sẽ update dần giao diện để cho game trở nên đẹp đẹp hơn.\n","date":"Aug 12, 2021","img":"https://unsplash.it/1920/1080?image=30","permalink":"/blog/2021-08-12-china_chess_alpha_beta_ai/","series":null,"tags":["Machine Learning","Normalization","Deep Learning","China Chess","Cờ Tướng","MiniMax","Alpha Beta Pruning"],"title":"Xây Dựng Chương Trình AI Đơn Giản Cho Game Cờ Tướng"},{"categories":null,"content":" Giới thiệu Vòng đời khi xây dựng chương trình máy học MLOps là gì Ví dụ sử dụng Pycaret Business Problem Exploratory Data Analysis Data Preparation Model Training \u0026amp; Selection Deployment \u0026amp; Monitoring Giới thiệu PyCaret là thư viện open-source machinelearning trong python, Thư viện tích hợp sẵn các mô hình cần thiết, giúp chúng ta train mô hình một lần trên nhiều thuật toán máy học khác nhau. Thư viện có hỗ trợ train trên GPU. Phiên bản hiện tại lúc mình viết bài viết này là 2.3.3. Các bạn có thể tham khảo thông tin thêm của thư viện ở link github https://github.com/pycaret/pycaret\nĐể cài đặt pycaret, các bạn sử dụng lệnh sau\n1 2pip install pycaret 3 4pip install pycaret[full] Bản full có cài thêm nhiều gói thư viện khác, các bạn có thể tham khảo các gói thư viện được cài thêm ở bản full qua link https://github.com/pycaret/pycaret/blob/master/requirements-optional.txt\nMình quan sát sơ qua thì bàn full có cài thêm mấy cái thư viện kết nối aws và gcs khá dư thừa, mình không xài tới, với ổ cứng máy mình cũng có hạn. Nên mình chỉ cài bản cơ bản và các thư viện cần thiết như scikit-optimize, tune-sklearn, xgboost \u0026hellip;\nVòng đời khi xây dựng chương trình máy học Business Problem : Như những ứng dụng khác, một ứng dụng máy học cũng được bắt đầu bằng một vấn đề thực tế trong cuộc sống, trong công việc. Phụ thuộc vào sự phức tạp của vấn đề, và các chi phí liên quan về mặt kinh doanh, chúng ta sẽ phân tích các yếu tố liên quan để xem xét có cần thiết phải phát triển chương trình sử dụng máy học hoặc tìm một giải pháp thay thế tốt hơn theo toàn bộ tiêu chí (thuyết vị lợi).\nData Sourcing \u0026amp; ETL : Sau khi hiểu bài toán, chúng ta sẽ thu thập các dữ liệu cần thiết.\nExploratory Data Analysis (EDA) : Dữ liệu ở trên là dữ liệu thô, chưa qua xử lý, nên có thể sẽ bị thu thập không đủ, thu thập thiếu. Chúng ta cần phải nắm rõ dữ liệu, phân tích sự cân bằng/ độ lệch của dữ liệu, xử lý nhiễu, xem phân bố của dữ liệu, xem độ tương quan giữa các đặc trưng, \u0026hellip;\nData Preparation : Sau khi phân tích, xào nấu dữ liệu đẹp đẽ, trơn tru, chúng ta sẽ bắt đầu chuẩn bị dữ liệu cho mô hình train, ví dụ chia dữ liệu thành tập train,test,validation, one-hot encoding, feature engineering, feature selection \u0026hellip;\nModel Training \u0026amp; Selection : Đây là phần nhàm chán nhất, thử nghiệm dữ liệu với các mô hình và tham số khác nhau, lựa chọn mô hình có kết quả tốt nhất trên tập validation. chờ mô hình train xong\nDeployment \u0026amp; Monitoring : Sau khi có được mô hình tốt nhất, chúng ta sẽ deploy ứng dụng, và theo dõi, tương tự như những ứng dụng khác thôi.\nTrong việc phát triển phần mềm, có một khái niệm đang nổi gần đây (lúc mình đang viết bài viết này) là devops. Giúp cho một số công việc nhàm chám được thực hiện một cách tự động. Trong máy học, chúng ta sẽ có khái niệm MLOps.\nMLOps là gì Định nghĩa theo wikipedia:\n1 2MLOps or ML Ops is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently Một bức hình bằng vạn câu chữ, xem bức hình trên, các bạn chắc về cơ bản cũng hiểu công việc của MLOps là gì rồi hen.\nÀ, đọc đến đây, các bạn có lẽ sẽ thắc mắc là sao đang giới thiệu Pycaret, mà sao lại lang mang qua MLOps làm gì? Thì mình cũng trả lời luôn là Pycaret là một trong những package giúp chúng ta MLOps ==\u0026gt; bớt nhàm chán khi phát triển ứng dụng machine learning rồi đó.\nVí dụ sử dụng Pycaret Mình sẽ trình bày phần này đúng theo machine learning life cycle, để đảm bảo việc giả lập sát với thực tế.\nBusiness Problem Bài toán Sarah Gets a Diamond, link chi tiết của bài toán ở https://hbsp.harvard.edu/product/UV0869-PDF-ENG. Bài toán này giúp người học khoá đó hiểu được sự khác nhau của linear-model, log-liner model, log-log mode. Nếu các bạn có nhu cầu tìm hiểu các model trên, có thể đăng ký khoá học trên hen. Ở đây, mình chỉ lấy mô tả chi tiết và data của khoá học.\nBối cảnh của bài toán diễn ra như sau. Grey muốn mua một chiếc nhẫn để cầu hôn Sarah. Sau một hồi tham khảo mấy thằng bạn từ thời nối khố, Grey quyết định sẽ mua nhẫn kim cương. Grey tiến hành đi thu thập thông tin của 6000 chiếc nhẫn kim cương khác nhau về giá, màu sắc, hình dạng \u0026hellip;\nMay mắn thay Grey có share dữ liệu này cho Pycaret, và chúng ta có thể sử dụng dữ liệu trên bằng cách load từ dataset của Pycaret\n1 2# load the dataset from pycaret 3from pycaret.datasets import get_data 4data = get_data(\u0026#39;diamond\u0026#39;) Và top 5 dữ liệu mẫu mà Grey thu thập là:\n1 2Carat Weight\tCut\tColor\tClarity\tPolish\tSymmetry\tReport\tPrice 30\t1.10\tIdeal\tH\tSI1\tVG\tEX\tGIA\t5169 41\t0.83\tIdeal\tH\tVS1\tID\tID\tAGSL\t3470 52\t0.85\tIdeal\tH\tSI1\tEX\tEX\tGIA\t3183 63\t0.91\tIdeal\tE\tSI1\tVG\tVG\tGIA\t4370 74\t0.83\tIdeal\tG\tSI1\tEX\tEX\tGIA\t3171 Exploratory Data Analysis Bước này sẽ phụ thuộc vào kinh nghiệm của người làm data. Kinh nghiệm của mình thì đầu tiên sẽ phân tích phân bố dữ liệu và phân tích mối tương quan giữa các biến liên tục trước đã, sau đó sẽ phân tích các yếu tố chuyên sâu hơn dựa vào cảm quan nhận được từ hai cái trên.\nQuan sát dữ liệu, chúng ta thấy rằng chỉ có hai thuộc tính Carat Weight và Price thuộc nhóm numerical variable, các thuộc tính còn lại thuộc nhóm categorical variable, nên mình không cần tính độ tương quan làm gì hết.\nMình có học từ link ở tài liệu tham khảo phía dưới, thư viện plotly.express, mình tham khảo thử thì thấy hàm vẽ scatter của thư viện có nhiều thuộc tính khá hay. Ví dụ mình thử phân tích kích thước viên đá kim cương với giá của chiếc nhẫn, chia theo màu sắc thì như thế nào\n1fig = px.scatter(data,x=\u0026#39;Carat Weight\u0026#39;, y=\u0026#39;Price\u0026#39;, animation_group=\u0026#39;Color\u0026#39;, 2 facet_col = \u0026#39;Color\u0026#39;, opacity = 0.25, template = \u0026#39;plotly_dark\u0026#39;, trendline=\u0026#39;ols\u0026#39;, 3 color=\u0026#34;Color\u0026#34;, trendline_color_override = \u0026#39;red\u0026#39;, title = \u0026#39;SARAH GETS A DIAMOND - A CASE STUDY\u0026#39;) 4fig.show() Nhìn hình trên, mình thấy rằng cùng 1 kích thước, màu H và màu I có giá xêm xêm nhau, màu E và F cũng tương tự, màu D có mức giá cao nhất. với kích thước 2.74, giá của chiếc nhẫn kim cương màu D cao hơn gấp đôi so với giá của nhẫn kim cương có màu H hoặc màu I\nCác bạn có thể thay thuộc tính facet_col = \u0026lsquo;Color\u0026rsquo; của hàm scatter bằng các tên cột như Cut\thoặc Clarity\thoặc\tSymmetry, sẽ có vài thứ hay ho có thể rút ra đó.\nSau khi phân tích dữ liệu, có cái nhìn sơ lược về các thuộc tính cũng như mối tương quang giữa chúng, chúng ta thường sẽ thường thực hiện các phép biến đổi để chuẩn hoá dữ liệu. Các phép biến đổi thường được xài là:\nChuẩn hoá dữ liệu: Scale dữ liệu về cùng một đoạn, ví dụ [-1,1] hoặc [0-1], 2 phương pháp phổ biến hay được sử dụng:\nMin-Max Z score Xử lý dữ liệu lệch: Các cột thuộc tính numberric sẽ được chuẩn hoá về phân phối chuẩn.\nTổng hợp dữ liệu: Sử dụng các thuộc tính có sẵn, kết hợp lại để tạo nên các thuộc tính mới.\nTrước hết, chúng ta sẽ xem histogram của biến Price\nTa thấy rằng, phân phối có ít quan sát hơn ở phía bên phải =\u0026gt; mô hình bị lệch phải (right-skewed hoặc skewed right, positively skewed distribution). Với dữ liệu bị bịnh này thì chúng ta sẽ dùng thuốc chữa là căn bậc hai, căn bậc ba hoặc là log.\nMình sẽ xài thuốc log. Sử dụng hàm log trong thư viện numpy\n1 2import numpy as np 3# create a copy of data 4data_copy = data.copy() 5# create a new feature Log_Price 6data_copy[\u0026#39;Log_Price\u0026#39;] = np.log(data[\u0026#39;Price\u0026#39;]) 7# plot histogram 8fig = px.histogram(data_copy, x=[\u0026#34;Log_Price\u0026#34;], title = \u0026#39;Histgram of Log Price\u0026#39;, template = \u0026#39;plotly_dark\u0026#39;) 9fig.show() Dữ liệu được đưa về dạng giống giống cái chuông úp, hình dạng của phân phối chuẩn.\nData Preparation Xài thư viện PyCaret khá sướng, chúng ta chỉ cần gọi hàm setup của thư viện là đủ\n1 2# initialize setup 3from pycaret.regression import * 4s = setup(data, target = \u0026#39;Price\u0026#39;, transform_target = True,transform_target_method=\u0026#34;yeo-johnson\u0026#34;, log_experiment = True, experiment_name = \u0026#39;diamond\u0026#39;) Bài toán thuộc dạng hồi quy, nên mình sẽ load toàn bộ các hàm thuộc regression vào.\nMọi việc còn lại, từ việc tính log của cột Price, đến việc tiền xử lý dữ liệu , \u0026hellip; đã được PyCaret lo hết. Chúng ta chỉ cần chịu khó đọc doc của thư viện để hiểu các tham số và ứng dụng nó vào là ổn.\nMột lưu ý là hàm transform mặc định xài box-cox, và pycaret ở thời điểm hiện tại chỉ hỗ trợ \u0026ldquo;box-cox\u0026rdquo; hoặc \u0026ldquo;yeo-johnson\u0026rdquo;. Nếu các bạn muốn xài log cho cột Price, thì phải thêm cột mới. Mình sẽ xài yeo-johnson thay cho mặc định box-cox.\nModel Training \u0026 Selection Tiếp đến việc train model và lựa chọn model cũng hết sức đơn giản, chúng ta chỉ cần gọi 1 dòng lệnh duy nhất\n1 2# compare all models 3best = compare_models() Xong. Thư viện tự động điều chỉnh tham số, lựa chọn tham số tốt nhất và mô hình tốt nhất cho chúng ta. Chúng ta chỉ cần ngồi, đợi máy chạy, xem kết quả.\nMột lưu ý là các bạn nếu không cài bản full thì nên xem file log để xem có báo lỗi thiếu thư viện hay không hen.\nSau khi có được thuật toán với mô hình tốt nhất , và các trọng số tốt nhất, chúng ta sẽ lưu mô hình lại vào file để sau này sử dụng.\n1 2# finalize the model 3final_best = finalize_model(best) 4# save model to disk 5save_model(final_best, \u0026#39;diamond-pipeline\u0026#39;) Deployment \u0026 Monitoring Đến phần này thì đơn giản rồi, các bạn có thể viết webapi như flask hoặc fastapi để sử dụng.\nPycaret có hỗ trợ mlflow, dùng để xem đường dẫn các model đã được huấn luyện, cũng như chi tiết các thông tin tham số, độ lỗi. Các bạn hãy gõ lệnh\n1!mlflow ui Nếu bạn chạy bằng terminal , thì chỉ cần gõ mlflow ui thôi, không cần dấu ! đâu, do mình chạy trên jupiter notebook nên phải thêm dấu ! câu lệnh mới hoạt động.\nSau khi chạy lện trên, bạn hãy mở trình duyệt web lên và nhập và địa chỉ http://localhost:5000, cái này cũng không có gì nhiều để đề cập, nên mình không show chi tiết ở đây, các bạn cứ vào đó vọc vạch, quậy phá hen.\nCác bạn có thể testing best model bằng câu lệnh sau\n1 2# create a copy of data and drop Price 3data1 = data.copy() 4# data1.drop(\u0026#39;Price\u0026#39;, axis=1, inplace=True) 5# generate predictions 6from pycaret.regression import predict_model 7predictions = predict_model(final_best, data=data1) 8predictions.head() Cột Label chính là cột giá của mô hình. Mình giữ lại cột giá để tiện so sánh.\n1 2\tCarat Weight\tCut\tColor\tClarity\tPolish\tSymmetry\tReport\tPrice\tLabel 30\t1.10\tIdeal\tH\tSI1\tVG\tEX\tGIA\t5169\t5365.265635 41\t0.83\tIdeal\tH\tVS1\tID\tID\tAGSL\t3470\t3525.863059 52\t0.85\tIdeal\tH\tSI1\tEX\tEX\tGIA\t3183\t3352.882096 63\t0.91\tIdeal\tE\tSI1\tVG\tVG\tGIA\t4370\t4485.753572 74\t0.83\tIdeal\tG\tSI1\tEX\tEX\tGIA\t3171\t3327.363225 Top 5 phần tử đầu tiên hiện ra cho mình thấy rằng, có vẻ mô hình dự đoán giá cao hơn một chút so với giá gốc.\nCảm ơn các bạn nhiều. Hẹn gặp lại trong các bài viết tiếp theo\nTham khảo\nhttps://github.com/pycaret/pycaret\nhttps://towardsdatascience.com/build-with-pycaret-deploy-with-fastapi-333c710dc786\nhttps://towardsdatascience.com/easy-mlops-with-pycaret-mlflow-7fbcbf1e38c6\n","date":"Jul 28, 2021","img":"https://unsplash.it/1920/1080?image=32","permalink":"/blog/2021-07-28-pycaret-flaskapi/","series":null,"tags":["Machine Learning","Deep Learning","PyCaret"],"title":"Tìm Hiểu Package PyCaret Trong Python"},{"categories":null,"content":" Giới thiệu Danh sách điều khiển truy cập - Access Control List (ACL) Điều khiển truy cập bắt buộc - Mandatory Access Control (MAC) Điều khiển truy cập tùy quyền - Discretionary Access Control (DAC) Điều khiển truy cập theo vai - Role Based Access Control (RBAC) Điều khiển truy cập theo thuộc tính - Attribute Based Access Control (ABAC) Giới thiệu Danh sách điều khiển truy cập - Access Control List Điều khiển truy cập bắt buộc - Mandatory Access Control Điều khiển truy cập tùy quyền - Discretionary Access Control (DAC) Điều khiển truy cập theo vai - Role Based Access Control (RBAC) Điều khiển truy cập theo thuộc tính - Attribute Based Access Control (ABAC) Danh sách điều khiển truy cập - Access Control List (ACL) Là mô hình cấp quyền truy cập dựa vào danh sách các quyền\nMô hình:\nSubject được quyền ( action ) trên object\rTuỳ từng bài toán khác nhau mà subject, action, object là khác nhau\rVí dụ:\rTrong môi trường phân quyền tập tin linux, subject là user, thread, action là READ/WRITE/ EXECUTE object là file, directory, tcp/udp port, thiết bị nhập xuất ...\rVí dụ:\nTrong hệ thống phân quyền của linux\rUser Alice được quyền đọc/ghi/thực thi trên file alice.sh\rUser Bob được quyền đọc trên file alice.sh\rỨng dụng:\nMô hình được ứng dụng trong Filesystem ACLs, POSIX ACL, NFSv4 ACL, Active Directory ACLs, Networking ACLs, SQL implementations.\rTham khảo:\nhttps://en.wikipedia.org/wiki/Access-control_list Điều khiển truy cập bắt buộc - Mandatory Access Control (MAC) Về cơ bản thì mô hình này cũng \u0026quot; là mô hình cấp quyền truy cập dựa vào danh sách các quyền\u0026quot;. Tuy nhiên, mô hình này sẽ kiểm soát quyền truy cập đến từng object của subject\nMô hình:\nSubject được quyền ( action ) trên object\rObject được quyền (action) bởi object\rVì ràng ở mức 2 đầu, nên mô hình này được ràng chặc chẽ hơn\rVí dụ:\nVí dụ: Ở một số tổ chức, user có quyền đọc ghi file (subject - action - object), tuy nhiên, có một số file tuyệt mật được phân quyền đọc/ ghi cho giám đốc (object - action - subject), nên user bình thường không thể đọc được.\rCác bạn có thể đọc thêm 3 ví dụ trong link của cornell mình có để bên dưới\rỨng dụng:\nSELinux\rWindows Vista và Windows Server 2008\r...\rTham khảo:\nhttps://en.wikipedia.org/wiki/Mandatory_access_control\rhttp://www.cs.cornell.edu/courses/cs5430/2015sp/notes/mac.php\rĐiều khiển truy cập tùy quyền - Discretionary Access Control (DAC) Là mô hình cấp quyền truy cập dựa vào danh sách các quyền. Mô hình này giống với ACL, chỉ có 1 điểm khác là subject có thể chuyển quyền mình đang có cho một subject khác\nMô hình:\nSubject được quyền ( action ) trên object\rSubject gán quyền (grant : action - object) cho Subject khác\rVí dụ: Alice có quyền đọc, ghi, thực thi file Alice.sh\nAlice gán quyền đọc file Alice.sh cho Bob\rỨng dụng:\nPhân quyền file trong hệ điều hành\r...\rĐiều khiển truy cập theo vai - Role Based Access Control (RBAC) Mô hình còn có tên gọi khác là Role Based Security, là mô hình cấp quyền truy cập dựa vào danh sách các quyền. Tuy nhiên, các subject sẽ được gán vào trong các Role và chúng ta sẽ cấp quyền cho các role.\nMô hình này có thể kết hợp với mô hình DAC (để tăng khả năng cấp quyền), hoặc MAC (để tăng tính bảo mật) mà không xung đột với 2 mô hình trên.\nMô hình:\nSubject thuộc Roles\rRoles được quyền ( action ) trên object\r=\u0026gt; các subject thuộc Roles được quyền (action) trên object\rVí dụ:\nAlice thuộc Role NhanVienTuyenDung, NhanVienIT\nRole NhanVienTuyenDung có quyền read, execute file\nRole NhanVienIT có quyền write file\n=\u0026gt; Alice có quyền read, write, execute file\nỨng dụng:\nCó rất nhiều ứng dụng của mô hình này, ví dụ các forum mã nguồn mở, cấp quyền trong hệ điều hành ....\rĐể tìm hiểu kỹ hơn về mô hình RBAC, các bạn có thể đọc quyển sách tham khảo ở dưới\nTham khảo :\nDavid F. Ferraiolo; D. Richard Kuhn; Ramaswamy Chandramouli (2007). Role-based Access Control (2nd ed.). Artech House. ISBN 978-1-59693-113-8.\rhttps://en.wikipedia.org/wiki/Role-based_access_control\rĐiều khiển truy cập theo thuộc tính - Attribute Based Access Control (ABAC) Mô hình còn có tên gọi khác là Policy Based Access Control hoặc Claims Based Access Control (CBAC), là mô hình cấp quyền truy cập dựa vào danh sách các quyền kết hợp với các thuộc tính.\nKiến trúc: Theo NIST đề xuất, kiến trúc của ABAC nên có các thành phần sau:\n- Policy Enforcement Point PEP: chịu trách nhiệm phân tích các yêu cầu truy xuất và gửi đến PDP để chứng thực.\r- Policy Decision Point PDP: nhận thông tin từ PEP và chịu trách nhiệm chứng thực yêu cầu có quyền truy xuất tới các tài nguyên hay không, trả về đồng ý hoặc từ chối. Nếu thiếu tông tin thì\r- Policy Information Point PIP: trả về các attribute mà PDP yêu cầu.\rThuộc tính: Bất kể thứ gì trên đời này đều có thể là thuộc tính. Tuy nhiên, chúng sẽ thường được phân làm 4 nhóm chính sau:\n- Subject attributes: Các thuộc tính về thông tin người dùng, ví dụ họ tên, ngày tháng năm sinh, quê quán, quốc tịch, địa chỉ, phòng ban, chức vụ, tên công việc, số cmnd, ....\r- Action attributes: Các thuộc tính về hành động như chạy , nảy, xoá, thêm, đọc, ghi ...\r- Object attributes: Các thuộc tính về thông tin của đối tượng muốn truy xuất, ví dụ như loại file, phần đuôi mở rộng, vị trí, ....\r- Contextual (environment) attributes: Các thuộc tính liên quan đến kịch bản diễn ra. Ví dụ hệ điều hành, ram, cpu, thời gian, múi giờ, ...\rVí dụ:\nToàn bộ nhân viên không được truy xuất database sau 21h đêm\rNhân viên Nguyễn Thị Lụa của GHN được quyền đổ danh sách freelancer ở Hà Nội, Hải Phòng, Hưng Yên\rỨng dụng:\nCó thể ứng dụng ABAC vào rất nhiều ứng dụng khác nhau để kiểm soát luồng truy cập tài nguyên của hệ thống. Tuy nhiên, việc xây dựng mô hình ACBA khá tốn kém về tài nguyên và đòi hỏi người quản lý phải có tư duy hệ thống vững chắc\rĐể tìm hiểu kỹ hơn về mô hình ABAC, các bạn có thể đọc quyển sách tham khảo ở dưới\nTham khảo :\nhttps://nvlpubs.nist.gov/nistpubs/specialpublications/NIST.SP.800-162.pdf\rhttps://arxiv.org/pdf/1306.2401.pdf\rhttps://en.wikipedia.org/wiki/Attribute-based_access_control\rCảm ơn các bạn đã chú ý quan tâm theo dõi. Xin chào và hẹn gặp lại ở các bài viết tiếp theo.\n","date":"Jul 2, 2021","img":"https://unsplash.it/1920/1080?image=33","permalink":"/blog/2021-07-02-mo-hinh-phan-quyen/","series":null,"tags":["ACL","mac","dac","rbac","abac"],"title":"Mô Hình Phân Quyền - Access Control"},{"categories":null,"content":" Giới thiệu Yêu cầu Giới thiệu Microsoft đã trình làng phiên bản WLS 2 với nhiều điểm cải tiến nổi trội. Trong bài viết này, mình sẽ hướng dẫn các bạn cài đặt wls 2 và upgrade các distro linux của mình xài WLS 2. Mình có một lưu ý nhỏ là nếu các distro linux của bạn không bị ràng gì thì các bạn nên xóa các linux distro hiện tại và cài mới lại linux. Vì quá trình upgrade chạy rất là lâu.\nYêu cầu Để cài đặt WLS 2, Các bạn bắc buộc phải nâng cấp lên các phiên bản \u0026ldquo;Windows 10 May 2020 (2004), Windows 10 May 2019 (1903), or Windows 10 November 2019 (1909)\u0026rdquo; hoặc các bản cập nhật sau đó.\nĐỂ xác định xem máy bạn đang xài phiên bản bao nhiêu, bạn nãy gõ mở cmd lên và gõ lệnh\n1systeminfo | findstr \u0026#34;OS\u0026#34; 2 3------ 4 5OS Name: Microsoft Windows 10 Home Single Language 6OS Version: 10.0.19043 N/A Build 19043 7OS Manufacturer: Microsoft Corporation 8OS Configuration: Standalone Workstation 9OS Build Type: Multiprocessor Free 10BIOS Version: American Megatrends Inc. S551LN.209, 7/8/2014 Nếu thỏa mãn các điều kiện trên, thì các bước chúng ta phải làm là:\n1dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 2 3------ 4 5Deployment Image Servicing and Management tool 6Version: 10.0.19041.844 7 8Image Version: 10.0.19043.1023 9 10Enabling feature(s) 11[==========================100.0%==========================] 12The operation completed successfully. Tiếp theo, chúng ta chạy lệnh\n1 2 3dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 4 5---------- 6 7Deployment Image Servicing and Management tool 8Version: 10.0.19041.844 9 10Image Version: 10.0.19043.1023 11 12Enabling feature(s) 13[==========================100.0%==========================] 14The operation completed successfully. Sau đó, bạn phải khởi động lại máy để window tiến hành cập nhật các gói thư viện cần thiết.\nSau khi khởi động lại máy xong, chúng ta sẽ gọi lệnh set phiên bản mặc định của wsl là bản 2 bằng lệnh:\n1 2wsl --set-default-version 2 Sau khi chạy lệnh này, sẽ có 1 trong 2 trường hợp xảy ra. Trường hợp 1\n1For information on key differences with WSL 2 please visit https://aka.ms/wsl2 Thì chúc mừng bạn, bạn đã enable thành công WSL 2\nTrường hợp thứ 2, bạn sẽ gặp output như thế này:\n1WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. Thì bạn này vào trang https://aka.ms/wsl2kernel như hướng dẫn, đọc kỹ file, down về file msi để cài Linux kernel vào. Sau đó chạy lại lệnh \u0026ldquo;wsl \u0026ndash;set-default-version 2\u0026rdquo;\nSau đó, các bạn tiến hành check lại phiên bản linux mình đang sử dụng\n1 2 wsl --list --verbose 3 4 ----- 5 6 NAME STATE VERSION 7* Ubuntu-18.04 Running 1 8 kali-linux Stopped 1 Như các bạn thấy ở trên, bản ubuntu 18.4 mình đang sử dụng đang ở version 1. Mình sẽ convert qua version 2 bằng lệnh\n1 2wsl --set-version Ubuntu-18.04 2 3 4------- 5Conversion in progress, this may take a few minutes... 6For information on key differences with WSL 2 please visit https://aka.ms/wsl2 Sau khi chạy dòng lệnh trên, các bạn chịu khó ngồi chờ một xíu, nó phụ thuộc vào cấu hình máy của các bạn. Kinh nghiệm của mình khi upgrade vài máy là nên tắt chương trình diệt virus như kaspersky, norton, BKAV, bit \u0026hellip;. đi. Tắt những ứng dụng sử dụng nhiều ram thì việc convert sẽ chạy nhanh hơn một chút.\nKết quả sau khi mình convert.\n1 2 NAME STATE VERSION 3* Ubuntu-18.04 Stopped 2 4 kali-linux Stopped 1 Cảm ơn các bạn đã chú ý theo dõi. Hẹn gặp lại ở các bài viết tiếp theo.\nLink hướng dẫn gốc từ trang chủ microsoft\nhttps://docs.microsoft.com/en-us/windows/wsl/install-win10\n","date":"May 30, 2021","img":"https://unsplash.it/1920/1080?image=34","permalink":"/blog/2021-05-30-upgrade-wls-to-wls2/","series":null,"tags":["wls2"],"title":"Nâng Cấp WSL Lên Bản WSL 2 Trên Window 10"},{"categories":null,"content":" Giới thiệu Bắt đầu Tiến hành turning Giới thiệu Trong quá trình giải các bài toán có sử dụng machine learning, vì để làm nhanh nên đôi khi mình sẽ sử dụng các tham số mặc định của mô hình để train. Một phần vì lý do chúng ta không biết cách chỉnh các tham só như thế nào, so với cái gì để có mô hình huấn luyện là tốt nhất. Ở bài viết này, mình sẽ sử dụng Learning Curves để tối ưu hóa các tham số của XGBoost. Các mô hình khác cũng làm tương tự thôi. Mình chọn XGBoost vì mô hình này thường cho kết quả khá tốt trên các cuộc thi ở Kaggle.\nBắt đầu Để bắt đầu thí nghiệm, chúng ta sẽ sinh ngẫu nhiên 60 ngàn dữ liệu có 1 ngàn thuộc tính bằng cách sử dụng hàm make_classification, sau đó sẽ chia dữ liệu thành 2 tập train và test với tỷ lệ 10% là tập test\n1X, y = make_classification(n_samples=60000, n_features=1000, n_informative=50, n_redundant=0, random_state=1) 2# split data into train and test sets 3X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1) Load mô hình XGBClassifier với các tham số là mặc định. Mô hình này được xem như là baseline và các cải tiến tham số ở sau sẽ so sánh kết quả trên mô hình này.\n1 2 3model = XGBClassifier() 4 5evalset = [(X_train, y_train), (X_test, y_test)] 6 7model.fit(X_train, y_train, eval_metric=\u0026#39;logloss\u0026#39;, eval_set=evalset) 8# evaluate performance 9yhat = model.predict(X_test) 10score = accuracy_score(y_test, yhat) 11print(\u0026#39;Accuracy: %.3f\u0026#39; % score) 12# retrieve performance metrics 13results = model.evals_result() 14# plot learning curves 15pyplot.plot(results[\u0026#39;validation_0\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;train\u0026#39;) 16pyplot.plot(results[\u0026#39;validation_1\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;test\u0026#39;) 17# show the legend 18pyplot.legend() 19# show the plot 20pyplot.show() Độ chính xác: Accuracy: 0.962. Lưu ý ràng độ chính xác khi thực nghiệm của mỗi lần chạy sẽ khác nhau, do data sinh ngẫu nhiên và một phần do sự ngẫu nhiên trong XGBoost.\nNhìn vào hình trên, chúng ta thấy rằng đường cong của tập train (đường màu xanh) có độ lỗi tốt hơn so với đường cong của tập test( đường màu đỏ)\nTiến hành turning Đầu tiên, nhìn vào đồ thị, ta thấy rằng đường cong vẫn còn có độ dốc, nên việc tăng số lần lặp có thể sẽ làm tăng thêm độ chính xác, thử thay đổi số lần lặp lên 500 xem sao.\nTrong XGBoost số lần lặp được tham số hóa bởi tham số n_estimators, chỉnh lại đoạn mã lệnh ở trên với một thay đổi nhỏ rồi chạy lại\n1 2model = XGBClassifier(n_estimators=500) Độ chính xác của mô hình tăng lên 1 chút, đối với thực nghiệm của mình là Accuracy: 0.981\nQuan sát đường cong của hình trên, ta thấy phần đuôi đoạn số lần lặp từ 270 đến 500 có độ dốc nhỏ, hầu như là bằng phẳng, có thể kết luận là việc huấn luyện ở đoạn này hầu như không cải tiến gì nhiều.\nMột nhận xét nữa là đoạn trước 150 có độ dốc khá lớn, có khả năng là hệ số học (learning reate) quá lớn, làm cho mô hình chưa đạt được cực tiểu, thử điều chỉnh hệ số học này nhỏ hơn là 0.01, thay vì 0.3 như giá trị mặc định xem sao.\nMột lưu ý là hệ số học nhỏ thì sẽ lâu hội tụ, nên chúng ta phải tăng số lần lặp lên. Ở đây đồng thời với việc giảm hệ số học xuống 0.01, mình còn tăng số lần lặp lên 1000.\nTrong XGBoost hệ số học được tham số hóa bởi tham số eta\n1 2model = XGBClassifier(n_estimators=1000, eta=0.01) Độ chính xác đạt được: Accuracy: 0.954\nTuy mô hình có độ chính xác giảm, nhưng nhìn vào đồ thị thì ta thấy mô hình vẫn còn độ dốc, nghĩa là mô hình sẽ cho kết quả tốt hơn nữa nếu ta tăng số vòng lặp.\nMột cách khách là thay đổi các chuẩn hóa (regularization ) bằng cách giảm các tham số số mẫu ( samples) và số đặc trưng (features) được dùng để xây dựng cây trong tập hợp. Hai tham số này được tham số hóa bởi tham số subsample và colsample_bytree. Giá trị mặc định của chúng là 1. Chúng ta sẽ thay đổi thành 0.35 xem sao nhé\n1 2model = XGBClassifier(n_estimators=5000, eta=0.01, subsample=0.35, colsample_bytree=0.35) Kết quả Accuracy: 0.970 Ở hai lần thí nghiệm trên, mình có các hướng xử lý có thể đi tiếp, một là tăng số lần lặp lên, vì độ dốc của mô hình vẫn còn, nên chúng ta hoàn toàn có thể thu được kết quả tốt hơn. Một cách khác là tăng learning rate lên để quá trình hội tụ được xảy ra nhanh hơn, ví dụ để eta = 0.05 hoặc 0.75 chẳn hạn.\nQuá trình này có thể tiếp tục, dựa vào quan sát của các bạn trên đường cong và hơn hết là sự hiệu biết thấu đáo của các bạn trên các tham số mà mô hình của bạn đang sử dụng. Chúc các bạn sẽ có một hướng đi tốt để giảm thiểu thời gian mò mẫm.\nCảm ơn các bạn đã chú ý theo dõi. Hẹn gặp lại ở các bài viết tiếp theo.\nNguồn tham khảo\nhttps://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\nhttps://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/\nhttps://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n","date":"Apr 11, 2021","img":"https://unsplash.it/1920/1080?image=35","permalink":"/blog/2021-04-11-xgboost_learning_curves/","series":null,"tags":["Machine Learning","XGBoost"],"title":"Tinh Chỉnh Thuật Toán XGBoost  Với Learning Curves"},{"categories":null,"content":" Giới thiệu SGD - Stochastic Gradient Descent Adam - Adaptive Moment Estimation AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients Kết luận Giới thiệu Hi các bạn, lại là mình đây, hôm nay mình sẽ cùng các bạn tìm hiểu thuật toán tối ưu hóa AdaBelief. Thuật toán này được sử dụng để thay cho thuật toán Adam optimizer mà các bạn hiện đang xài để huấn luyện mô hình Deep learning. Nào, chúng ta cùng bắt đầu tìm hiểu nhé.\nẨn sâu bên trong các thuật toán sử dụng Neural Network và một vài thuật toán machine learning đều sử dụng các hàm tối ưu hóa. Chúng ta có thể liệt kê ra một vài cái tên như RMSprop, SGD (Stochastic Gradient Descent), Adam (Adaptive Moment Estimation).\nMột vài các yếu tố hay được sử dụng để đánh giá một thuật toán optimizer:\nHội tụ nhanh (trong quá trình train)\nSự tổng quát hóa cao (vẫn nhận dạng được những mẫu chưa từng được huấn luyện)\nĐộ chính xác cao\nCác thuật toán tối ưu thuộc họ Adaptive thường có tốc độ hội tụ nhanh. Trong khi đó, các thuật toán thuộc họ SGD thường có sự tổng quát hóa cao. Gần đây, Juntang Zhuang và các cộng sự thuộc đại học Yale đã nghiên cứu và tạo ra thuật toán AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. Thuật toán này theo lời tác giả, hội tụ cả hai ưu điểm của họ Adaptive và SGD, là vừa có tốc độ hội tụ nhanh, vừa có tính tổng quát hóa cao Mã nguồn được tác giả công bố ở link https://github.com/juntang-zhuang/Adabelief-Optimizer.\nLời của tác giả:\nWe propose the AdaBelief optimizer, which adaptively scales the stepsize by the difference betweenpredicted gradient and observed gradient. To our knowledge, AdaBelief is the first optimizer toachieve three goals simultaneously: fast convergence as in adaptive methods, good generalization asin SGD, and training stability in complex settings such as GANs. Furthermore, Adabelief has the same parameters as Adam, hence is easy to tune. We validate the benefits of AdaBelief with intuitive examples, theoretical convergence analysis in both convex and non-convex cases, and extensiveexperiments on real-world datasets\nĐể hiểu về AdaBelief, trước tiên, chúng ta phải có một ít kiến thức cơ bản về SGD và Adam, nên chúng ta sẽ bắt đầu nói về SGD trước\nSGD - Stochastic Gradient Descent Thuật toán SGD là thuật toán tối ưu hóa cơ bản theo họ gradient. Thuật toán này rất triển khai, có nền tảng lý thuyết vững chắc, cực kỳ ổn định trong quá trình huấn luyện, kết quả đạt được có thể so sánh với các thuật toán khác. Ý tưởng của thuật toán khá đơn giản, đó là \u0026ldquo;tính giá trị gradient của mỗi tham số, và đi một bước nhỏ theo chiều của gradient\u0026rdquo;. Nếu chúng ta lặp đi lặp lại quá trình này, và ngẫu nhiên chọn (stochastic) một tập batch trong tập huấn luyện, mô hình chúng ta sẽ được cải tiến dần đến đểm hội tụ.\nTrong quá khứ, phần khó nhất của SGD là việc tính lại giá trị gradient cho toàn bộ các tham số trong mô hình. Nhưng hiện nay, các framwork máy học như Tensorflow, PyTouch, Caffee, Theano, \u0026hellip;. đã giúp chúng ta tính các giá trị gradient một cách tự động. Do đó, công việc của chúng ta hiện thời đơn giản hơn\n$$for \\text{ } i \\text{ } in \\text{ } range (m): $$ $$\\theta_i = \\theta_i - \\alpha ( \\hat y^{i} - y^i) x^i_j$$\nMột vấn đề chúng ta gặp phải trong quá trình huấn luyện DL với SGD là chậm, siêu chậm. Do thuật toán phải cập nhật toàn bộ các tham số, nên số lượng phép tính và lượng tài nguyên phần cứng được sử dụng rất là nhiều. Rất nhiều các biến thể của SGD đã được đề xuất để giải quyết vấn đề trên.\nAdam - Adaptive Moment Estimation Adam optimizer là một thuật toán kết hợp kỹ thuật của RMS prop và momentum. Thuật toán sử dụng hai internal states momentum (m) và squared momentum (v) của gradient cho các tham số. Sau mỗi batch huấn luyện, giá trị của m và v được cập nhật lại sử dụng exponential weighted averaging.\nMã giải của việc cập nhật m và v\n$$m_t = \\beta_1m_t-_1 + (1-\\beta_1)g_t $$ $$v_t = \\beta_2v_t-_1 + (1-\\beta_2)g^2_t$$\ntrong đó, beta được xem như là một siêu tham số. Công thức cập nhật theta như sau:\n$$\\theta_t = \\theta_t-_1 - \\alpha\\frac{m_t}{\\sqrt{v_t}+ \\epsilon }$$\ntrong đó, alpha là learning rate, epsion là giá trị được thêm vào để ngăng việc chia cho 0\nĐể việc descent được thực hiện nhanh hơn, thuật toán đã sử dụng hai kỹ thuật:\nTính exponential moving average của giá trị đạo hàm lưu vào biến m và sử dụng nó là tử số của việc cập nhật hướng. Với ý nghĩa là nếu m có giá trị lớn, thì việc descent đang đi đúng hướng và chúng ta cần bước nhảy lớn hơn để đi nhanh hơn. Tương tự, nếu giá trị m nhỏ, phần descent có thể không đi về hướng tối tiểu và chúng ta nên đi 1 bước nhỏ để thăm dò. Đây là phần momentum của thuật toán.\nTính exponential moving average của bình phương gía trị đạo hàm lưu vào biến v và sử dụng nó là phần mẫu số của việc cập nhật hướng. Với ý nghĩa như sau: Giả sử gradient mang các giá trị dương, âm lẫn lộn, thì khi cộng các giá trị lại theo công thức tính m ta sẽ được giá trị m gần số 0. Do âm dương lẫn lộn nên nó bị triệt tiêu lẫn nhau. Nhưng trong trường hợp này thì v sẽ mang giá trị lớn. Do đó, trong trường hợp này, chúng ta sẽ không hướng tới cực tiểu, chúng ta sẽ không muốn đi theo hướng đạo hàm trong trường hợp này. Chúng ta để v ở phần mẫu vì khi chia cho một giá trị cao, giá trị của các phần cập nhật sẽ nhỏ, và khi v có giá trị thấp, phần cập nhật sẽ lớn. Đây chính là phần tối ưu RMSProp của thuật toán.\nỞ đây, m được xem như là moment thứ nhất, v xem như là moment thứ hai, nên thuật toán có tên là \u0026ldquo;Adaptive moment estimation\u0026rdquo;.\nĐể lý giải vì sao Adam lại hội tụ nhanh hơn so với SGD, chúng ta có thể giải thích như sau: Exponential weighted averaging cho chúng ta giá trị xấp xỉ gradient mượt hơn qua mỗi lần lặp, dẫn tới tăng tínhs dừng. Sau đó, việc chia cho căng bậc 2 của giá trị v làm số lước của chúng ta giảm mạnh khi phương sai của giá trị gradient tăng lên. Điều này , như giải thích ở trên, có nghĩa là, khi hướng đi của mô hình chỉ ra không rõ ràng, thuật toán Adam thực hiện các bước đi nhỏ coi như là thăm dò thôi. Và sẽ thực hiện các bước đi lớn, nhanh khi hướng đi rõ ràng.\nThuật toán Adam hoạt động khá hiệu quả, nhưng bản thân nó cũng có những vấn đề. Tác giả của AdaBelief đã chỉ ra một vài điểm không hiệu quả của thuật toán\nAdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients ![Hình ảnh AdaBelief - Nguồn https://arxiv.org/pdf/2010.07468v5.pdf ] (adam_error.jpg)\nHãy nhìn vào hình trên, ở mục đánh dấu là số 3, giá trị G lớn vì đường cong ở đoạn đó dốc. Giá trị v cũng lớn. Do đó, nếu sử dụng thuật toán Adam ở đây, bước đi sẽ rất nhỏ. Việc di chuyển một bước đi nhỏ ở đây sẽ làm chậm quá trình hội tụ và không cần thiết. Bởi vì chúng ta tin tưởng rằng chúng ta đang đi đúng hướng, và chúng ta cần một bước đi dài hơn.\nAdaBelief sửa lỗi này bằng một thay đổi nhỏ trong thuật toán của adam. Thay vì tính bình phương của gradient, AdaBelief sẽ tính phương sai của gradient. Một sự thay đổi nhỏ nhưng mang lại giá trị to lớn.\n$$v_t = \\beta_2v_t-_1 + (1-\\beta_2)g^2_t $$ $$s_t = \\beta_2v_t-_1 + (1-\\beta_2)(g_t-m_t)^2$$\nTác giả không dùng biến v nữa, mà thay bằng biến s.\nVới việc dùng biến s. Trong trường hợp trên, g lớn và m lớn, thì s sẽ nhỏ. Và khi s ở phần mẫu nhỏ, chúng ta sẽ có bước đi xa hơn. Ở đây, AdaBelief đã giải quyết vấn đề\nQua đây, chúng ta cũng có thể giải thích vì sao có chữ \u0026ldquo;belief\u0026rdquo; trong từ AdaBelief. Giá trị phương sai được tính dựa vào kỳ vọng của giá trị gradient.\nMột chú ý nhỏ ở đây là mục số 1 và mục số 3 được coi là cải tiến của Adam so với momentum và SGD. Tất nhiên, AdaBelief cũng kế thừa mấy cái này.\nỞ mục đánh dấu số 1 trên hình, đường cong khá phẳng và giá trị đạo hàm gần như bằng 0. Nếu sử dụng SGD, chúng ta sẽ có một bước đi nhỏ. Trong khi đó, họ Adam sẽ cho chúng ta bước đi lớn hơn vì giá trị căng bậc hai của s hoặc v ở mẫu số sẽ cho ra một kết quả rất nhỏ.\nỞ mục đánh dấu số 2, đường cong ở đây rất dốc và hẹp, g và delta g ở đây rất lớn, cho nên ở đây chúng ta cần một bước di chuyển nhỏ. Nếu sử dụng SGD hoặc momentum thì sẽ đi một bước đi rất lớn do nhân với một lượng moving averages lớn. Trong khi đó, với Adam hoặc AdaBelief, chúng ta sẽ có giá trị căng bậc hai của s hoặc v ở mẫu số lớn nên bước đi sẽ nhỏ hơn.\nVề tốc độ hội tụ, tác giả có đề cập rõ và chi tiết trong bài báo, mình không đề cập lại nó nữa ở đây. Các bạn tự xem nhé.\nKết luận AdaBelief là thuật toán tối ưu hóa có nguồn gốc từ thuật toán Adam, không có thêm tham số ngoài, chỉ thay đổi 1 dòng code.\nThuật toán đã tăng tốc độ hội tụ cũng như mức tổng quát hóa.\nThuật toán thực hiện các bước đi dựa vào \u0026ldquo;belief\u0026rdquo; của hướng gradient ở thời điểm hiện tại.\nThuật toán giải quyết vấn đề \u0026ldquo;Large gradient, small curvature\u0026rdquo; bằng cách xem xét biên độ và dấu của gradient.\nNguồn:\nhttps://arxiv.org/abs/2010.07468\nhttps://medium.com/the-dl/understanding-the-new-adabelief-optimizer-2db70ef6de1e\nhttps://towardsdatascience.com/adabelief-optimizer-fast-as-adam-generalizes-as-good-as-sgd-71a919597af\n","date":"Jan 15, 2021","img":"https://unsplash.it/1920/1080?image=36","permalink":"/blog/2021-01-15---adabelief-optimizer/","series":null,"tags":["Machine Learning","Optimizer","SGD","Opencv"],"title":"Tìm Hiểu Thuật Toán Tối Ưu Hóa Adabelief Optimizer"},{"categories":null,"content":" Advantages of Reinforcement Learning Thiết lập bàn cờ Khởi tạo bàn cờ Kiểm tra Reward Thiết lập người chơi Khởi tạo Chọn nước đi Cập nhật trạng thái Huấn luyện mô hình Advantages of Reinforcement Learning Trong khi trong các phương pháp lý thuyết trò chơi nói chung, ví dụ thuật toán min-max, thuật toán luôn giả định chúng ta có một đối thủ hoàn hảo, công việc phải thực hiện là tối đa hóa phần thưởng của mình và giảm thiểu phần thưởng của đối thủ ( tối đa hóa điểm của mình và tối thiểu hóa điểm của đối thủ), trong học củng cố, chúng ta không cần giả định đối thủ của chúng ta là 1 thiên tài xuất chúng, nhưng chung ta vẫn thu được mô hình với kết quả rất tốt.\nBằng cách coi đối thủ là một phần của môi trường mà chúng ta có thể tương tác, sau một số lần lặp lại nhất định, đối thủ có thể lập kế hoạch trước mà không cần chúng ta phải làm gì cả. Ưu điểm của phương pháp này là giảm số lượng không gian tìm kiếm và giảm số phép toán suy luận phải thực hiện, nhưng nó có thể đạt được kỹ năng hiện đại chỉ bằng cách thử và học.\nTrong bài viết này, chúng ta sẽ làm các công việc sau:\nThứ nhất, huấn luyện mô hình cho 2 máy đấu với nhau mà thu được các trọng số cần thiết.\nThứ hai, cho người đánh với máy\nĐể hình thành bài toán học củng cố Reinforcement Learning , chúng ta cần phải xác định rõ 3 thành phần chính:\nState\nAction\nReward\nVới:\nState chính là bàn cờ với các nước đi của các người chơi. Chúng ta sẽ tạo một bàn cờ có kích thước 3x3, giá trị của mỗi ô cờ đều là 0. Vị trí người chơi 1 đặt quân sẽ được gán là 1. Vị trí người chơi 2 đặt quân sẽ được gán là -1.\nAction là vị trí người chơi sẽ đi quân khi biết state hiện tại (nghĩa là biết đối thủ đi nước nào, và có những nước nào hiện đang trên bàn cờ).\nReward: mang giá trị 0 hoặc 1. Khi kết thúc game sẽ trả về giá trị cho reward.\nỞ phần dưới đây, mình sẽ note lại code và sẽ comment trong code để cho rõ ý\nThiết lập bàn cờ Khởi tạo bàn cờ 1 2def __init__(self, p1, p2): 3 self.board = np.zeros((BOARD_ROWS, BOARD_COLS)) 4 self.p1 = p1 5 self.p2 = p2 6 self.isEnd = False 7 self.boardHash = None 8 # init p1 plays first 9 self.playerSymbol = 1 Chúng ta sẽ tạo một bàn cờ có kích thước 3x3, 2 biến người chơi. Người 1 là người chơi đầu tiên.\n1 2# Trả về danh sách các nước có thể đi 3def availablePositions(self): 4 positions = [] 5 for i in range(BOARD_ROWS): 6 for j in range(BOARD_COLS): 7 if self.board[i, j] == 0: 8 positions.append((i, j)) # need to be tuple 9 return positions 10 11# Cập nhật lại lên bàn cờ vị trí của người chơi đặt quân 12 13def updateState(self, position): 14 self.board[position] = self.playerSymbol 15 # switch to another player 16 self.playerSymbol = -1 if self.playerSymbol == 1 else 1 Kiểm tra Reward Sau mỗi nước đi của các kỳ thủ, chúng ta cần 1 hàm để kiểm tra xem kỳ thủ thắng hay thua và trả về kết quả cho reward như đề cập ở trên\n1 2def winner(self): 3 4\t# Kiểm tra theo dòng 5 6\tfor i in range(BOARD_ROWS): 7\tif sum(self.board[i, :]) == 3: 8\tself.isEnd = True 9\treturn 1 10\tif sum(self.board[i, :]) == -3: 11\tself.isEnd = True 12\treturn -1 13\t# kiểm tra theo cột 14 15\tfor i in range(BOARD_COLS): 16\tif sum(self.board[:, i]) == 3: 17\tself.isEnd = True 18\treturn 1 19\tif sum(self.board[:, i]) == -3: 20\tself.isEnd = True 21\treturn -1 22 23\t# kiểm tra theo đường chéo chính và theo đường chéo phụ 24 25\tdiag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)]) # đường chéo chính 26 27\tdiag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)]) # đường chéo phụ 28 29\tdiag_sum = max(abs(diag_sum1), abs(diag_sum2)) # lấy trị tuyệt đối của các nước đi, nếu bằng 3 nghĩa là có người chơi chiến thắng 30 31\tif diag_sum == 3: 32\tself.isEnd = True 33\tif diag_sum1 == 3 or diag_sum2 == 3: 34\treturn 1 35\telse: 36\treturn -1 37 38\t# Kiểm tra xem còn nước đi hay không 39\tif len(self.availablePositions()) == 0: 40\tself.isEnd = True 41\treturn 0 42 43\t# not end 44\tself.isEnd = False 45\treturn None 46 47# only when game ends 48def giveReward(self): 49\tresult = self.winner() 50\t# backpropagate reward 51\tif result == 1: 52\tself.p1.feedReward(1) 53\tself.p2.feedReward(0) 54\telif result == -1: 55\tself.p1.feedReward(0) 56\tself.p2.feedReward(1) 57\telse: 58\tself.p1.feedReward(0.1) 59\tself.p2.feedReward(0.5) Ở đây có một lưu ý. Khi cờ hòa thì chúng ta cũng xem rằng người đi trước thua, nên hệ số lúc cờ hòa sẽ là 0.1-0.5. Các bạn có thể thiết lập một giá trị khác, ví dụ 0.2-0.5 hoặc 0.5-0.5 tùy thích.\nThiết lập người chơi Người chơi cần có các phương thức sau:\nChọn nước đi dựa trên trạng thái hiện tại của bàn cờ.\nLưu lại trạng thái của ván cờ.\nCập nhật lại giá trị trạng thái sau mỗi ván.\nLưu và load các trọng số lên.\nKhởi tạo 1 2def __init__(self, name, exp_rate=0.2): 3 self.name = name 4 self.states = [] # record all positions taken 5 self.lr = 0.2 6 self.exp_rate = exp_rate 7 self.decay_gamma = 0.9 8 self.states_value = {} # state -\u0026gt; value Chọn nước đi 1 2def chooseAction(self, positions, current_board, symbol): 3\trandValue = np.random.uniform(0, 1) 4\tvalue_max = value = -999 5\tif randValue\u0026gt; self.exp_rate: 6 7\tfor p in positions: 8\tnext_board = current_board.copy() 9\tnext_board[p] = symbol 10\tnext_boardHash = self.getHash(next_board) 11\tvalue = -999 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash) 12\t# print(\u0026#34;value\u0026#34;, value) 13\tif value \u0026gt;= value_max: 14\tvalue_max = value 15\taction = p 16 17\tif value_max == -999 : 18\t# take random action 19\tidx = np.random.choice(len(positions)) 20\taction = positions[idx] 21 22\t# print(\u0026#34;{} takes action {}\u0026#34;.format(self.name, action)) 23\treturn action Cập nhật trạng thái Chúng ta sẽ cập nhật trạng thái với công thức sau\n$$ V(S_t) = V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)] $$\nDiễn giải ra tiếng việt, giá trị của trạng thái tại thời điểm t bằng giá trị tại thời điểm hiện tại cộng với độ lệch của trạng thái hiện tại và trạng thái tiếp theo nhân với một hệ số học alpha.\n1 2# at the end of game, backpropagate and update states value 3def feedReward(self, reward): 4 for st in reversed(self.states): 5 if self.states_value.get(st) is None: 6 self.states_value[st] = 0 7 self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st]) 8 reward = self.states_value[st] Huấn luyện mô hình Phần này nằm trong lớp State. Chúng ta sẽ lần lượt đi qua các quá trình luân phiên nhau giữa người chơi 1 và người chơi 2\nngười chơi chọn nước có thể đi -\u0026gt; cập nhật trạng thái -\u0026gt; kiểm tra thắng/thua -\u0026gt; người chơi chọn nước có thể đi \u0026hellip;\n1 2def play(self, rounds=100): 3\tfor i in range(rounds): 4\tif i % 1000 == 0: 5\tprint(\u0026#34;Rounds {}\u0026#34;.format(i)) 6\twhile not self.isEnd: 7\t# Player 1 8\tpositions = self.availablePositions() 9\tp1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol) 10\t# take action and upate board state 11\tself.updateState(p1_action) 12\tboard_hash = self.getHash() 13\tself.p1.addState(board_hash) 14\t# check board status if it is end 15 16\twin = self.winner() 17\tif win is not None: 18\t# self.showBoard() 19\t# ended with p1 either win or draw 20\tself.giveReward() 21\tself.p1.reset() 22\tself.p2.reset() 23\tself.reset() 24\tbreak 25 26\telse: 27\t# Player 2 28\tpositions = self.availablePositions() 29\tp2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol) 30\tself.updateState(p2_action) 31\tboard_hash = self.getHash() 32\tself.p2.addState(board_hash) 33 34\twin = self.winner() 35\tif win is not None: 36\t# self.showBoard() 37\t# ended with p2 either win or draw 38\tself.giveReward() 39\tself.p1.reset() 40\tself.p2.reset() 41\tself.reset() 42\tbreak Sau khi huấn luyện 100 ngàn lần, chúng ta sẽ chơi với máy, chỉ là 1 thay đổi nhỏ trong hàm chooseAction là thay vì lấy nước đi có trọng số lớn nhất, chúng ta sẽ cho người dùng nhập từ bàn phím dòng và cột vào\n1 2 3def chooseAction(self, positions): 4 while True: 5 row = int(input(\u0026#34;Input your action row:\u0026#34;)) 6 col = int(input(\u0026#34;Input your action col:\u0026#34;)) 7 action = (row, col) 8 if action in positions: 9 return action Và sửa lại hàm play một chút, bỏ loop 100k lần đi, bỏ gọi hàm cập nhật thưởng và bỏ các hàm reset đi\n1 2 3# play with human 4def play2(self): 5\twhile not self.isEnd: 6\t# Player 1 7\tpositions = self.availablePositions() 8\tp1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol) 9\t# take action and upate board state 10\tself.updateState(p1_action) 11\tself.showBoard() 12\t# check board status if it is end 13\twin = self.winner() 14\tif win is not None: 15\tif win == 1: 16\tprint(self.p1.name, \u0026#34;wins!\u0026#34;) 17\telse: 18\tprint(\u0026#34;tie!\u0026#34;) 19\tself.reset() 20\tbreak 21 22\telse: 23\t# Player 2 24\tpositions = self.availablePositions() 25\tp2_action = self.p2.chooseAction(positions) 26 27\tself.updateState(p2_action) 28\tself.showBoard() 29\twin = self.winner() 30\tif win is not None: 31\tif win == -1: 32\tprint(self.p2.name, \u0026#34;wins!\u0026#34;) 33\telse: 34\tprint(\u0026#34;tie!\u0026#34;) 35\tself.reset() 36\tbreak Mã nguồn hoàn chỉnh của chương trình\n1 2import numpy as np 3import pickle 4 5BOARD_ROWS = 3 6BOARD_COLS = 3 7 8 9class State: 10 def __init__(self, p1, p2): 11 self.board = np.zeros((BOARD_ROWS, BOARD_COLS)) 12 self.p1 = p1 13 self.p2 = p2 14 self.isEnd = False 15 self.boardHash = None 16 # init p1 plays first 17 self.playerSymbol = 1 18 19 # get unique hash of current board state 20 def getHash(self): 21 self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS)) 22 return self.boardHash 23 24 def winner(self): 25 # row 26 for i in range(BOARD_ROWS): 27 if sum(self.board[i, :]) == 3: 28 self.isEnd = True 29 return 1 30 if sum(self.board[i, :]) == -3: 31 self.isEnd = True 32 return -1 33 # col 34 for i in range(BOARD_COLS): 35 if sum(self.board[:, i]) == 3: 36 self.isEnd = True 37 return 1 38 if sum(self.board[:, i]) == -3: 39 self.isEnd = True 40 return -1 41 # diagonal 42 diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)]) 43 diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)]) 44 diag_sum = max(abs(diag_sum1), abs(diag_sum2)) 45 if diag_sum == 3: 46 self.isEnd = True 47 if diag_sum1 == 3 or diag_sum2 == 3: 48 return 1 49 else: 50 return -1 51 52 # tie 53 # no available positions 54 if len(self.availablePositions()) == 0: 55 self.isEnd = True 56 return 0 57 # not end 58 self.isEnd = False 59 return None 60 61 def availablePositions(self): 62 positions = [] 63 for i in range(BOARD_ROWS): 64 for j in range(BOARD_COLS): 65 if self.board[i, j] == 0: 66 positions.append((i, j)) # need to be tuple 67 return positions 68 69 def updateState(self, position): 70 self.board[position] = self.playerSymbol 71 # switch to another player 72 self.playerSymbol = -1 if self.playerSymbol == 1 else 1 73 74 # only when game ends 75 def giveReward(self): 76 result = self.winner() 77 # backpropagate reward 78 if result == 1: 79 self.p1.feedReward(1) 80 self.p2.feedReward(0) 81 elif result == -1: 82 self.p1.feedReward(0) 83 self.p2.feedReward(1) 84 else: 85 self.p1.feedReward(0.1) 86 self.p2.feedReward(0.5) 87 88 # board reset 89 def reset(self): 90 self.board = np.zeros((BOARD_ROWS, BOARD_COLS)) 91 self.boardHash = None 92 self.isEnd = False 93 self.playerSymbol = 1 94 95 def play(self, rounds=100): 96 for i in range(rounds): 97 if i % 1000 == 0: 98 print(\u0026#34;Rounds {}\u0026#34;.format(i)) 99 while not self.isEnd: 100 # Player 1 101 positions = self.availablePositions() 102 p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol) 103 # take action and upate board state 104 self.updateState(p1_action) 105 board_hash = self.getHash() 106 self.p1.addState(board_hash) 107 # check board status if it is end 108 109 win = self.winner() 110 if win is not None: 111 # self.showBoard() 112 # ended with p1 either win or draw 113 self.giveReward() 114 self.p1.reset() 115 self.p2.reset() 116 self.reset() 117 break 118 119 else: 120 # Player 2 121 positions = self.availablePositions() 122 p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol) 123 self.updateState(p2_action) 124 board_hash = self.getHash() 125 self.p2.addState(board_hash) 126 127 win = self.winner() 128 if win is not None: 129 # self.showBoard() 130 # ended with p2 either win or draw 131 self.giveReward() 132 self.p1.reset() 133 self.p2.reset() 134 self.reset() 135 break 136 137 138 # play with human 139 def play2(self): 140 while not self.isEnd: 141 # Player 1 142 positions = self.availablePositions() 143 p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol) 144 # take action and upate board state 145 self.updateState(p1_action) 146 self.showBoard() 147 # check board status if it is end 148 win = self.winner() 149 if win is not None: 150 if win == 1: 151 print(self.p1.name, \u0026#34;wins!\u0026#34;) 152 else: 153 print(\u0026#34;tie!\u0026#34;) 154 self.reset() 155 break 156 157 else: 158 # Player 2 159 positions = self.availablePositions() 160 p2_action = self.p2.chooseAction(positions) 161 162 self.updateState(p2_action) 163 self.showBoard() 164 win = self.winner() 165 if win is not None: 166 if win == -1: 167 print(self.p2.name, \u0026#34;wins!\u0026#34;) 168 else: 169 print(\u0026#34;tie!\u0026#34;) 170 self.reset() 171 break 172 173 174 def showBoard(self): 175 # p1: x p2: o 176 for i in range(0, BOARD_ROWS): 177 print(\u0026#39;-------------\u0026#39;) 178 out = \u0026#39;| \u0026#39; 179 for j in range(0, BOARD_COLS): 180 token = \u0026#34;\u0026#34; 181 if self.board[i, j] == 1: 182 token = \u0026#39;x\u0026#39; 183 if self.board[i, j] == -1: 184 token = \u0026#39;o\u0026#39; 185 if self.board[i, j] == 0: 186 token = \u0026#39; \u0026#39; 187 out += token + \u0026#39; | \u0026#39; 188 print(out) 189 print(\u0026#39;-------------\u0026#39;) 190 191 192class Player: 193 def __init__(self, name, exp_rate=0.3): 194 self.name = name 195 self.states = [] # record all positions taken 196 self.lr = 0.3 197 self.exp_rate = exp_rate 198 self.decay_gamma = 0.9 199 self.states_value = {} # state -\u0026gt; value 200 201 def getHash(self, board): 202 boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS)) 203 return boardHash 204 205 def chooseAction(self, positions, current_board, symbol): 206 randValue = np.random.uniform(0, 1) 207 value_max = value = -999 208 if randValue\u0026gt; self.exp_rate: 209 210 for p in positions: 211 next_board = current_board.copy() 212 next_board[p] = symbol 213 next_boardHash = self.getHash(next_board) 214 value = -999 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash) 215 # print(\u0026#34;value\u0026#34;, value) 216 if value \u0026gt;= value_max: 217 value_max = value 218 action = p 219 220 if value_max == -999 : 221 # take random action 222 idx = np.random.choice(len(positions)) 223 action = positions[idx] 224 225 # print(\u0026#34;{} takes action {}\u0026#34;.format(self.name, action)) 226 return action 227 228 # append a hash state 229 def addState(self, state): 230 self.states.append(state) 231 232 # at the end of game, backpropagate and update states value 233 def feedReward(self, reward): 234 for st in reversed(self.states): 235 if self.states_value.get(st) is None: 236 self.states_value[st] = 0 237 self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st]) 238 reward = self.states_value[st] 239 240 def reset(self): 241 self.states = [] 242 243 def savePolicy(self): 244 fw = open(\u0026#39;policy_\u0026#39; + str(self.name), \u0026#39;wb\u0026#39;) 245 pickle.dump(self.states_value, fw) 246 fw.close() 247 248 def loadPolicy(self, file): 249 fr = open(file, \u0026#39;rb\u0026#39;) 250 self.states_value = pickle.load(fr) 251 fr.close() 252 253 254class HumanPlayer: 255 def __init__(self, name): 256 self.name = name 257 258 def chooseAction(self, positions): 259 while True: 260 row = int(input(\u0026#34;Input your action row:\u0026#34;)) 261 col = int(input(\u0026#34;Input your action col:\u0026#34;)) 262 action = (row, col) 263 if action in positions: 264 return action 265 266 # append a hash state 267 def addState(self, state): 268 pass 269 270 # at the end of game, backpropagate and update states value 271 def feedReward(self, reward): 272 pass 273 274 def reset(self): 275 pass 276 277 278if __name__ == \u0026#34;__main__\u0026#34;: 279 # training 280 p1 = Player(\u0026#34;p1\u0026#34;) 281 p2 = Player(\u0026#34;p2\u0026#34;) 282 283 st = State(p1, p2) 284 print(\u0026#34;training...\u0026#34;) 285 st.play(100000) 286 287 p1.savePolicy() 288 289 # play with human 290 p1 = Player(\u0026#34;computer\u0026#34;, exp_rate=0) 291 p1.loadPolicy(\u0026#34;policy_p1\u0026#34;) 292 293 p2 = HumanPlayer(\u0026#34;human\u0026#34;) 294 295 st = State(p1, p2) 296 st.play2() Nguồn\nReinforcement Learning: An Introduction phiên bản 2 của Richard S. Sutton and Andrew G. Barto\nhttps://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542\n","date":"Dec 27, 2020","img":"https://unsplash.it/1920/1080?image=39","permalink":"/blog/2020-12-26---tic-tac-toe/","series":null,"tags":["Reinforcement Learning","TicTacToe","Opencv"],"title":"Reinforcement Learning Và Tictactoe"},{"categories":null,"content":" Mã nguồn Mã nguồn 1 2import cv2 3import numpy as np 4from random import choice 5 6def getColor(): 7\tlstColor = [[255,64,64],[255,165,0],[255,244,79],[102,255,0],[172,229,238],[148,87,235],[148,87,235],[241,156,187]] 8\treturn choice(lstColor) 9 10def getInfo(piece): 11\tif piece == \u0026#34;\u0026#34;: 12\tcoords = np.array([[0, 0]]) 13\telif piece == \u0026#34;I\u0026#34;: 14\tcoords = np.array([[0, 3], [0, 4], [0, 5], [0, 6]]) 15\telif piece == \u0026#34;T\u0026#34;: 16\tcoords = np.array([[1, 3], [1, 4], [1, 5], [0, 4]]) 17\telif piece == \u0026#34;L\u0026#34;: 18\tcoords = np.array([[1, 3], [1, 4], [1, 5], [0, 5]]) 19\telif piece == \u0026#34;J\u0026#34;: 20\tcoords = np.array([[1, 3], [1, 4], [1, 5], [0, 3]]) 21\telif piece == \u0026#34;S\u0026#34;: 22\tcoords = np.array([[1, 5], [1, 4], [0, 3], [0, 4]]) 23\telif piece == \u0026#34;Z\u0026#34;: 24\tcoords = np.array([[1, 3], [1, 4], [0, 4], [0, 5]]) 25\telse: 26\tcoords = np.array([[0, 4], [0, 5], [1, 4], [1, 5]]) 27 28\treturn coords, getColor() 29 30def display(board, coords, color, next_info, held_info, score, SPEED): 31\t# Generates the display 32 33\tborder = np.uint8(127 - np.zeros([20, 1, 3])) 34\tborder_ = np.uint8(127 - np.zeros([1, 23, 3])) 35 36\tdummy = board.copy() 37\tdummy[coords[:,0], coords[:,1]] = color 38 39\tright = np.uint8(np.zeros([20, 10, 3])) 40\tright[next_info[0][:,0] + 2, next_info[0][:,1]] = next_info[1] 41 42\tdummy = np.concatenate(( border, dummy, border, right, border), 1) 43\tdummy = np.concatenate((border_, dummy, border_), 0) 44\tdummy = dummy.repeat(20, 0).repeat(20, 1) 45\tdummy = cv2.putText(dummy, str(score), (325, 150), cv2.FONT_HERSHEY_DUPLEX, 1, [0, 0, 255], 2) 46 47\t# Instructions for the player 48\tindex_pos = 300 49\tx_index_pos = 300 50\tdummy = cv2.putText(dummy, \u0026#34;A - left\u0026#34;, (x_index_pos, index_pos), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234]) 51\tdummy = cv2.putText(dummy, \u0026#34;D - right\u0026#34;, (x_index_pos, index_pos+25), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234]) 52\tdummy = cv2.putText(dummy, \u0026#34;S - drain\u0026#34;, (x_index_pos, index_pos+50), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234]) 53\tdummy = cv2.putText(dummy, \u0026#34;W - rotate\u0026#34;, (x_index_pos, index_pos+75), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234]) 54\t# dummy = cv2.putText(dummy, \u0026#34;J - rotate left\u0026#34;, (45, 300), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255]) 55\t# dummy = cv2.putText(dummy, \u0026#34;L - rotate right\u0026#34;, (45, 325), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255]) 56\t# dummy = cv2.putText(dummy, \u0026#34;I - hold\u0026#34;, (45, 350), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255]) 57 58\tcv2.imshow(\u0026#34;Tetris\u0026#34;, dummy) 59\tkey = cv2.waitKey(int(1000/SPEED)) 60 61\treturn key 62 63def getNextPiece(): 64\tnext_piece = choice([\u0026#34;O\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;S\u0026#34;, \u0026#34;Z\u0026#34;, \u0026#34;L\u0026#34;, \u0026#34;J\u0026#34;, \u0026#34;T\u0026#34;]) 65 66\treturn next_piece 67 68SPEED = 1 # Controls the speed of the tetris pieces 69 70# Make a board 71 72board = np.uint8(np.zeros([20, 10, 3])) 73 74# Initialize some variables 75 76quit = False 77place = False 78drop = False 79switch = False 80held_piece = \u0026#34;\u0026#34; 81flag = 0 82score = 0 83next_piece =\u0026#34;\u0026#34; 84current_piece = \u0026#34;\u0026#34; 85# All the tetris pieces 86 87 88 89if __name__ == \u0026#34;__main__\u0026#34;: 90\tnext_piece = getNextPiece() 91\twhile not quit: 92\t# Check if user wants to swap held and current pieces 93\tif switch: 94\t# swap held_piece and current_piece 95\theld_piece, current_piece = current_piece, held_piece 96\tswitch = False 97\telse: 98\t# Generates the next piece and updates the current piece 99\tcurrent_piece = next_piece 100\tnext_piece = getNextPiece() 101 102\tif flag \u0026gt; 0: 103\tflag -= 1 104 105\t# Determines the color and position of the current, next, and held pieces 106 107\theld_info = getInfo(held_piece) 108 109\tnext_info = getInfo(next_piece) 110 111\tcoords, color = getInfo(current_piece) 112\tif current_piece == \u0026#34;I\u0026#34;: 113\ttop_left = [-2, 3] 114 115\tif not np.all(board[coords[:,0], coords[:,1]] == 0): 116\tbreak 117 118\twhile True: 119\t# Shows the board and gets the key press 120\tkey = display(board, coords, color, next_info, held_info, score, SPEED) 121\t# Create a copy of the position 122\tdummy = coords.copy() 123\tprint(\u0026#34;speed \u0026#34;,SPEED, \u0026#34;key \u0026#34;,key,\u0026#34; \u0026#34;, ord(\u0026#34;s\u0026#34;)) 124 125\tif key == ord(\u0026#34;s\u0026#34;): 126\tdrop = True 127 128\telif key == ord(\u0026#34;a\u0026#34;): 129\t# Moves the piece left if it isn\u0026#39;t against the left wall 130\tif np.min(coords[:,1]) \u0026gt; 0: 131\tcoords[:,1] -= 1 132\tif current_piece == \u0026#34;I\u0026#34;: 133\ttop_left[1] -= 1 134\telif key == ord(\u0026#34;d\u0026#34;): 135\t# Moves the piece right if it isn\u0026#39;t against the right wall 136\tif np.max(coords[:,1]) \u0026lt; 9: 137\tcoords[:,1] += 1 138\tif current_piece == \u0026#34;I\u0026#34;: 139\ttop_left[1] += 1 140\t# elif key == ord(\u0026#34;j\u0026#34;) or key == ord(\u0026#34;l\u0026#34;): 141\t# # Rotation mechanism 142\t# # arr is the array of nearby points which get rotated and pov is the indexes of the blocks within arr 143 144\t# if current_piece != \u0026#34;I\u0026#34; and current_piece != \u0026#34;O\u0026#34;: 145\t# if coords[1,1] \u0026gt; 0 and coords[1,1] \u0026lt; 9: 146\t# arr = coords[1] - 1 + np.array([[[x, y] for y in range(3)] for x in range(3)]) 147\t# pov = coords - coords[1] + 1 148 149\t# elif current_piece == \u0026#34;I\u0026#34;: 150\t# # The straight piece has a 4x4 array, so it needs seperate code 151 152\t# arr = top_left + np.array([[[x, y] for y in range(4)] for x in range(4)]) 153\t# pov = np.array([np.where(np.logical_and(arr[:,:,0] == pos[0], arr[:,:,1] == pos[1])) for pos in coords]) 154\t# pov = np.array([k[0] for k in np.swapaxes(pov, 1, 2)]) 155 156\t# # Rotates the array and repositions the piece to where it is now 157 158\t# if current_piece != \u0026#34;O\u0026#34;: 159\t# if key == ord(\u0026#34;j\u0026#34;): 160\t# arr = np.rot90(arr, -1) 161\t# else: 162\t# arr = np.rot90(arr) 163\t# coords = arr[pov[:,0], pov[:,1]] 164 165\telif key == ord(\u0026#34;w\u0026#34;): 166\t# Rotation mechanism 167\t# arr is the array of nearby points which get rotated and pov is the indexes of the blocks within arr 168 169\tif current_piece != \u0026#34;I\u0026#34; and current_piece != \u0026#34;O\u0026#34;: 170\tif coords[1,1] \u0026gt; 0 and coords[1,1] \u0026lt; 9: 171\tarr = coords[1] - 1 + np.array([[[x, y] for y in range(3)] for x in range(3)]) 172\tpov = coords - coords[1] + 1 173 174\telif current_piece == \u0026#34;I\u0026#34;: 175\t# The straight piece has a 4x4 array, so it needs seperate code 176 177\tarr = top_left + np.array([[[x, y] for y in range(4)] for x in range(4)]) 178\tpov = np.array([np.where(np.logical_and(arr[:,:,0] == pos[0], arr[:,:,1] == pos[1])) for pos in coords]) 179\tpov = np.array([k[0] for k in np.swapaxes(pov, 1, 2)]) 180 181\t# Rotates the array and repositions the piece to where it is now 182 183\tif current_piece != \u0026#34;O\u0026#34;: 184\tif key == ord(\u0026#34;j\u0026#34;): 185\tarr = np.rot90(arr, -1) 186\telse: 187\tarr = np.rot90(arr) 188\tcoords = arr[pov[:,0], pov[:,1]] 189\t# Hard drop set to true 190\t# drop = True 191\t# elif key == ord(\u0026#34;i\u0026#34;): 192\t# # Goes out of the loop and tells the program to switch held and current pieces 193\t# if flag == 0: 194\t# if held_piece == \u0026#34;\u0026#34;: 195\t# held_piece = current_piece 196\t# else: 197\t# switch = True 198\t# flag = 2 199\t# break 200\telif key == 8 or key == 27: 201\tquit = True 202\tbreak 203 204\t# Checks if the piece is overlapping with other pieces or if it\u0026#39;s outside the board, and if so, changes the position to the position before anything happened 205 206\tif np.max(coords[:,0]) \u0026lt; 20 and np.min(coords[:,0]) \u0026gt;= 0: 207\tif not (current_piece == \u0026#34;I\u0026#34; and (np.max(coords[:,1]) \u0026gt;= 10 or np.min(coords[:,1]) \u0026lt; 0)): 208\tif not np.all(board[coords[:,0], coords[:,1]] == 0): 209\tcoords = dummy.copy() 210\telse: 211\tcoords = dummy.copy() 212\telse: 213\tcoords = dummy.copy() 214 215\tif drop: 216\t# Every iteration of the loop moves the piece down by 1 and if the piece is resting on the ground or another piece, then it stops and places it 217 218\twhile not place: 219\tif np.max(coords[:,0]) != 19: 220\t# Checks if the piece is resting on something 221\tfor pos in coords: 222\tif not np.array_equal(board[pos[0] + 1, pos[1]], [0, 0, 0]): 223\tplace = True 224\tbreak 225\telse: 226\t# If the position of the piece is at the ground level, then it places 227\tplace = True 228 229\tif place: 230\tbreak 231 232\t# Keeps going down and checking when the piece needs to be placed 233 234\tcoords[:,0] += 1 235 236\tif current_piece == \u0026#34;I\u0026#34;: 237\ttop_left[0] += 1 238 239\tdrop = False 240 241\telse: 242\t# Checks if the piece needs to be placed 243\tif np.max(coords[:,0]) != 19: 244\tfor pos in coords: 245\tif not np.array_equal(board[pos[0] + 1, pos[1]], [0, 0, 0]): 246\tplace = True 247\tbreak 248\telse: 249\tplace = True 250 251\tif place: 252\t# Places the piece where it is on the board 253\tfor pos in coords: 254\tboard[tuple(pos)] = color 255 256\t# Resets place to False 257\tplace = False 258\tbreak 259 260\t# Moves down by 1 261 262\tcoords[:,0] += 1 263\tif current_piece == \u0026#34;I\u0026#34;: 264\ttop_left[0] += 1 265 266\t# Clears lines and also counts how many lines have been cleared and updates the score 267 268\tlines = 0 269 270\tfor line in range(20): 271\tif np.all([np.any(pos != 0) for pos in board[line]]): 272\tlines += 1 273\tboard[1:line+1] = board[:line] 274 275 276\tscore += lines*10 Mã nguồn này được kế thừa từ bài viết https://www.learnopencv.com/tetris-with-opencv-python/ và mình có modify lại theo sở thích cá nhân của mình. Còn một số bug mà mình chưa fix hết. Bạn đọc nào ghé ngang có đóng góp gì thì để lại comment giúp mình hen.\n","date":"Dec 26, 2020","img":"https://unsplash.it/1920/1080?image=40","permalink":"/blog/2020-12-25---tetric/","series":null,"tags":["Python","Tetris","Opencv"],"title":"Xây Dựng Game Xếp Gạch Bằng Opencv Và Python"},{"categories":null,"content":" Giá trị ngưỡng: Thuật toán Simple Thresholding Adaptive Thresholding Giá trị ngưỡng: Nói theo kiểu lúa hóa, trong opencv, ngưỡng là một số nằm trong đoạn từ 0 đến 255. Giá trị ngưỡng sẽ chia tách giá trị độ xám của ảnh thành 2 miền riêng biệt. Miền thứ nhất là tập hợp các điểm ảnh có giá trị nhỏ hơn giá trị ngưỡng. Miền thứ hai là tập hợp các các điểm ảnh có giá trị lớn hơn hoặc bằng giá trị ngưỡng.\nĐầu vào của một thuật toán phân ngưỡng trong opencv thường có input là ảnh nguồn (source image) và giá trị ngưỡng. Đầu ra là ảnh đích đã được phân ngưỡng (destination image). Một số thuật toán phân ngưỡng sẽ kèm thêm vài giá trị râu ria khác nữa, chúng ta sẽ không quan tâm đến chúng\nMã giải của thuật toán phân ngưỡng:\n1if src[i] \u0026gt;= T: 2\tdest[i] = MAXVAL 3else: 4\tdest [i] = 0 Có rất nhiều thuật toán phân ngưỡng dựa trên cách chúng ta xác định ngưỡng. Chúng ta sẽ tìm hiểu lần lượt các thuật toán trên.\nThuật toán Simple Thresholding Simple Thresholding thực hiện phân ngưỡng bằng cách thay thế giá trị lớn hơn hoặc bằng và giá trị bé hơn giá trị ngưỡng bằng một giá trị mới. Cụ thể chúng ta có thể xem mã nguồn bên dưới\n1 2import cv2 3import numpy as np 4from matplotlib import pyplot as plt 5 6img = cv2.imread(\u0026#39;gradient.png\u0026#39;,0) 7ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) 8ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV) 9ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC) 10ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO) 11ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV) 12 13titles = [\u0026#39;Original Image\u0026#39;,\u0026#39;BINARY\u0026#39;,\u0026#39;BINARY_INV\u0026#39;,\u0026#39;TRUNC\u0026#39;,\u0026#39;TOZERO\u0026#39;,\u0026#39;TOZERO_INV\u0026#39;] 14images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] 15 16for i in xrange(6): 17 plt.subplot(2,3,i+1),plt.imshow(images[i],\u0026#39;gray\u0026#39;) 18 plt.title(titles[i]) 19 plt.xticks([]),plt.yticks([]) 20 21plt.show() Hình ảnh và thuật toán của mô hình được lấy từ trang opencv-python-tutroals.readthedocs.io\nỞ đoạn code trên, chúng ta thiết lập giá trị ngưỡng là 127, với các điểm ảnh có giá trị lớn hơn hoặc bằng 127, chúng ta sẽ gán lại giá trị của nó thành 255. Và các điểm ảnh có giá trị bé hơn 127 sẽ được gán bằng 0 (mặc định).\n1 2 3double cv::threshold\t(\tInputArray src, 4OutputArray dst, 5double thresh, 6double maxval, 7int type 8) Thuật toán sample thresholding của opencv còn có 1 tham số nữa khá quan trọng nữa là loại ngưỡng (type). Hiện tại lúc mình viết bài viết này thì opencv hỗ trợ 8 loại là: THRESH_BINARY, THRESH_BINARY_INV, THRESH_TRUNC, THRESH_TOZERO, THRESH_TOZERO_INV, THRESH_MASK, THRESH_OTSU, THRESH_TRIANGLE. Ý nghĩa của từng loại như sau:\nTHRESH_BINARY: Có thể dịch là ngưỡng nhị phân. Ý nghĩa y hệt những gì mình đề cập ở trên.\nTHRESH_BINARY_INV: Ngưỡng nhị phân đảo ngược. Có thể hiểu là nó sẽ đảo ngược lại kết quả của THRESH_BINARY.\nTHRESH_TRUNC: Những giá trị điểm ảnh bé hơn ngưỡng sẽ giữ nguyên giá trị, những điểm ảnh lớn hơn hoặc ngưỡng sẽ được gán lại là maxvalue.\nTHRESH_TOZERO: Những điểm ảnh bé hơn ngưỡng sẽ bị gán thành 0, những điểm còn lại giữ nguyên.\nTHRESH_TOZERO_INV: Những điểm ảnh nhỏ hơn giá trị ngưỡng sẽ được giữ nguyên, những điểm ảnh còn lại sẽ bị gán thành 0.\nTHRESH_MASK: Ở bạn opencv4, hầu như không được xài.\nTHRESH_OTSU: Sử dụng thuật toán Otsu để xác định giá trị ngưỡng.\nTHRESH_TRIANGLE: Sử dụng thuật toán Triangle để xác định giá trị ngưỡng.\nGiá trị 127 là giá trị trung bình cộng của 0 và 255 làm tròn xuống. Giá trị ngưỡng của thuật toán này đòi hỏi người sử dụng phải có mức độ hiểu biết nhất định về các loại ảnh mình đang xử lý để chọn ngưỡng cho phù hợp.\nAdaptive Thresholding Thuật toán simple thresholding hoạt động khá tốt. Tuy nhiên, nó có 1 nhược điểm là giá trị ngưỡng bị/được gán toàn cục. Thực tế khi chụp, hình ảnh chúng ta nhận được thường bị ảnh hưởng của nhiễu, ví dụ như là bị phơi sáng, bị đèn flask, \u0026hellip;\nMột trong những cách được sử dụng để giải quyết vấn đề trên là chia nhỏ bức ảnh thành những vùng nhỏ (region), và đặt giá trị ngưỡng trên những vùng nhỏ đó -\u0026gt; adaptive thresholding ra đời. Opencv cung cấp cho chúng ta hai cách xác định những vùng nhỏ\n1import cv2 as cv 2import numpy as np 3from matplotlib import pyplot as plt 4img = cv.imread(\u0026#39;sudoku.png\u0026#39;,0) 5img = cv.medianBlur(img,5) 6ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY) 7th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\ 8 cv.THRESH_BINARY,11,2) 9th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\ 10 cv.THRESH_BINARY,11,2) 11titles = [\u0026#39;Original Image\u0026#39;, \u0026#39;Global Thresholding (v = 127)\u0026#39;, 12 \u0026#39;Adaptive Mean Thresholding\u0026#39;, \u0026#39;Adaptive Gaussian Thresholding\u0026#39;] 13images = [img, th1, th2, th3] 14for i in xrange(4): 15 plt.subplot(2,2,i+1),plt.imshow(images[i],\u0026#39;gray\u0026#39;) 16 plt.title(titles[i]) 17 plt.xticks([]),plt.yticks([]) 18plt.show() Hình ảnh và thuật toán của mô hình được lấy từ trang docs.opencv.org\n1 2 3void cv::adaptiveThreshold\t(\tInputArray src, 4OutputArray dst, 5double maxValue, 6int adaptiveMethod, 7int thresholdType, 8int blockSize, 9double C 10) Ở đây:\nblockSize: Kích thước của vùng, bắt buộc phải là một số lẻ lớn hơn 0.\nC: hằng số, giá trị từ -255 đến 255. Có thể gán C bằng 0 để đỡ rối.\nadaptiveMethod nhận vào một trong hai giá trị là cv.ADAPTIVE_THRESH_MEAN_C và cv.ADAPTIVE_THRESH_GAUSSIAN_C, đó là các phương pháp tính ngưỡng.\nADAPTIVE_THRESH_MEAN_C: Tính trung bình các láng giềng xung quanh điểm cần xét trong vùng blockSize * blockSize trừ đi giá trị hằng số C.\nADAPTIVE_THRESH_GAUSSIAN_C: Nhân giá trị xung quanh điểm cần xét với trọng số gauss rồi tính trung bình của nó, sau đó trừ đi giá trị hằng số C.\nthresholdType: Tương tự như Simple Thresholding đã trình bày ở trên.\nCảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.\nTham khảo\nhttps://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/\nhttps://www.learnopencv.com/opencv-threshold-python-cpp/\nhttps://medium.com/@anupriyam/basic-image-thresholding-in-opencv-5af9020f2472\n","date":"Dec 25, 2020","img":"","permalink":"/blog/2020-12-24-thresholding/","series":null,"tags":["python","thresholding","contour","opencv"],"title":"Ngưỡng (Thresholding) Trong Opencv"},{"categories":null,"content":" Xác định bias và variance Bias Unavoidable bias Avoidable bias Variance Tradeoff giữa bias và variance Cách giảm bias và variance Cách giảm bias Giảm variance Bức tranh tổng quát Tổng kết Việc huấn luyên mô hình máy học có thể sẽ gây ra cho bạn một chút khó khăn nếu bạn không hiểu những thứ bạn dang làm là đúng hay sai. Trong hầu hết các trường hợp, các mô hình học máy là các \u0026ldquo;hộp đen\u0026rdquo;, chúng ta chỉ có thể \u0026ldquo;nhìn thấy\u0026rdquo; dữ liệu đầu vào và độ chính xác mà mô hình trả ra. Chúng ta không biết bên trong nó đang làm cái gì. Việc hiểu lý do tại sao mô hình cho ra kết quả tệ hại là chìa khóa cho cái \u0026ldquo;cách\u0026rdquo; mà bạn cải tiến nó.\nTìm hiểu lý do \u0026ldquo;tại sao\u0026rdquo; mô hình cho ra kết quả tệ hại bằng cách \u0026ldquo;xác định bias và variance\u0026rdquo;.\nTìm hiểu \u0026ldquo;cách\u0026rdquo; cải tiến mô hình bằng việc thực hiện \u0026ldquo;giảm bias và variance\u0026rdquo;.\nXác định bias và variance Trước hết, chúng ta hãy bắt đầu nói về lỗi. Lỗi là phần không chính xác của mô hình trên tập test.\n$$ error = 1 - testing accuracy $$\nNếu mô hình đạt độ chính xác là 86% trên tập test, điều đó đồng nghĩa với độ lỗi là 14%. Trong 14% đó bao gồm bias và variance.\nBiểu đồ bias - variance. Nguồn towardsdatascience.com\nHai ý chính của hình trên cần làm rõ ở đây:\nBias là lỗi trên tập huấn luyện.\nVariance là gap giữa độ chính xác trên tập train và độ chính xác trên tập test.\nBạn hãy hình thật kỹ vào hình ở trên, nhìn đi nhìn lại 2, 3 lần. Nhắm mắt lại và nghiền ngẫm thật kỹ hai ý chính mình vừa đề cập ở trên.\nBias Bias mô tả khả năng học của mô hình. Giá trị bias lớn đồng nghĩa với việc mô hình cần phải học nhiều hơn nữa từ tập huấn luyện.\nNếu mô hình có độ chính xác 90% trên tập train, điều đó đồng nghĩa với việc bạn có 10% bias. Bias cũng được chia làm 2 nhóm, nhóm bias có thể tránh được (avoidable bias) và nhóm bias không thể tránh được (unavoidable bias).\n$$ bias = 1 - trainning accuracy $$\nUnavoidable bias Unavoidable bias hay còn được sử dụng dưới tên là optimal error rate. Đây là giới hạn trên của mô hình. Trong một số bài toán, ví dụ như là bài toán dự đoán giá chứng khoán, chúng ta - con người - không thể dự đoán chính xác 100%. Do đó, trong điều kiện lý tưởng nhất, tại một thời điểm nào đó, mô hình của chúng ta vẫn cứ trả ra kết quả sai.\nNếu bạn quyết định rằng mô hình có độ sai ít nhất là 4%. Nghĩa là chúng ta có 4% unavoidable bias.\nAvoidable bias Khác với optimal error rate và trainning error. Độ lỗi này xảy ra khi mô hình chúng ta chưa đủ độ tới. Chúng ta hoàn toàn có thể cái tiến mô hình này để giảm độ lỗi này về mức 0, v\nBiểu đồ bias - variance. Nguồn towardsdatascience.com\nBạn hãy để ý kỹ phần bias ở hình trên. Bias được chia làm 2 phần. Ở trên phần nét đứt là Unavoidable bias. Nó là điểm tới hạn của mô hình. Việc cần làm của chúng ta là huấn luyện, cải tiến mô hình, để cho đường trainning accuracy màu đỏ tiến sát với đường nét đứt.\nVariance Variance ý nghĩa của nó là mô tả mức độ tổng quát hóa của mô hình của bạn đối với dữ liệu mà nó chưa được huấn luyện. Và định nghĩa của nó là phần sai lệch giữa độ chính xác trên tập huấn luyện và độ chính xác tên tập test.\n$$ Variance = trainning accuracy - testing accuracy $$\nBiểu đồ variance. Nguồn towardsdatascience.com\nTradeoff giữa bias và variance Sự đánh đổi giữa bias và variace. Nguồn towardsdatascience.com\nMình nghĩ hình trên đủ nói lên tất cả ý mình muốn nói. Khi mô hình cảng trở nên phức tạp, thì bias sẽ giảm, nhưng mức độ tổng quát hóa cũng giảm theo (đồng nghĩa với việc variace sẽ tăng).\nCách giảm bias và variance Cách giảm bias Như đã nói ở phần trên, bias được chia thành 2 nhóm là Avoidable bias và unavoidable bias. Chúng ta không thể nào giảm Avoidable bias, nhưng chúng ta có thể giảm unavoidable bias bằng một trong các cách sau.\nTăng kích thước mô hình Việc tăng kích thước mô hình là một trong những cách làm giảm avoidable bias. Mô hình càng lớn thì có càng nhiều tham số phải điều chỉnh. Có nhiều tham sos đồng nghĩa với việc mô hình sẽ học được nhiều mối quan hệ phức tạp hơn. Chúng ta có thể tăng kích thước mô hình bằng cách thêm nhiều layer hơn nữa, hoặc thêm nhiều node hơn nữa cho mỗi layer.\nGiảm Regulation Việc giảm regulation cũng giúp mô hình tăng độ chính xác trên tập huấn luyên. Tuy nhiên, nếu chúng ta giảm regularization quá đà, mô hình sẽ không đạt được mức độ tổng quát hóa, và làm tăng variance. Đây là ví dụ dễ thấy nhất nhất về sự đánh đổi giữa bias và variance.\nGiảm Regulation . Nguồn towardsdatascience.com\nThay đổi kiến trúc mô hình Việc thay đổi kiến trúc mô hình cũng có thể giúp chúng ta đạt được độ chính xác cao hơn.\nMột số mục có thể thay đổi:\nThay đổi activation function ( ví dụ tanh, ReLU, sigmoid, LeakyReLU)\nThay đổi loại mô hình (ANN, CNN, RNN, KNNKNN, \u0026hellip;)\nThay đổi các tham số (learning rate, image size, \u0026hellip;)\nThay đổi thuật toán tối ưu (Adam, SGD, RMSprop, …)\nThêm đặc trưng mới Việc thêm đặc trưng mới giúp chúng ta cung cấp cho mô hình nhiều thông tin hơn. Chúng ta có thể thực hiện điều này thông qua kỹ thuật feature engineering.\nGiảm variance Thêm nhiều dữ liệu Thêm dữ liệu là cách đơn giản nhất, thường gặp nhất để tăng độ chính xác của mô hình trong trường hợp mô hình huấn luyện của chúng ta bị hight variance. Hiệu quả của việc thêm nhiều dữ liệu vào mô hình đã được đề cập ở bài báo có tựa đề là The Unreasonable Effectiveness of Recurrent Neural Networks của Andrej Karpathy (link: http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Việc thêm dữ liệu thường không ảnh hưởng đến độ lỗi bias, giúp làm giảm variance, nên đây là cách thường được sử dụng nhất.\nTăng Regularization Việc tăng Regularization giúp mô hình chống overfitting. Qua đó giúp giảm variance, và tăng bias :(. Một só cách Regularization hot ở thời điểm hiện lại là dropout ( với biến thể là Monte Carlo Dropout), BatchNorm\u0026hellip;\nGiảm kích thước mô hình Việc giảm kích thước mô hình giúp cho chúng ta giảm overfitting trên tập train. Mục tiêu của Việc này làm giảm khả năng liên kết những pattern của dữ liệu. Bởi vậy, mục tiêu của nó hoàn toàn tương tự như tăng Regularization. Trong thực tế, chúng ta thường sử dụng tăng thêm Regularization hơn là giảm kích thước mô hình để chống variace.\nLựa chọn đặc trưng (feature selection) Giảm chiều dữ liệu, bằng cách bỏ đi các đặc trưng thừa, giúp giảm nhiễu, là cách thường được sử dụng để giảm variace. Chúng ta có thể sử dụng PCA (Principal Component Analysis) để lọc ra các đặc trưng tốt hoặc kết hợp chúng với nhau để tạo các đặc trưng tốt hơn.\nBức tranh tổng quát Sau tất cả, chúng ta sẽ xây dựng được một bức tranh tổng quan về lỗi chúng ta đang mắc phải là gì và chúng ta nên làm gì để giảm độ lỗi đó.\nTổng quan . Nguồn towardsdatascience.com\nTổng kết Reducing Bias\nIncrease model size\nReduce regularization\nChange model architecture\nAdd features\nReducing Variance\nAdd More data\nDecrease model size\nAdd regularization\nFeature selection\nCảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.\nBài viết được lược dịch từ link https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b\nNguồn tự liệu từ bài viết được sử dụng trong cuốn sách Machine Learning Yearning của Andrew Ng. Các bạn có thể search theo từ khóa trên hoặc đăng ký trên site http://deeplearning.net/\n","date":"Apr 16, 2020","img":"","permalink":"/blog/2020-04-16-two-important-machine-learning-concepts-to-improve-every-model/","series":null,"tags":["machine learning","deep learning","bias","variance"],"title":"Hai Khái Niệm Quan Trọng Giúp Tăng Độ Chính Xác Của Các Mô Hình Trong Machine Learning"},{"categories":null,"content":" Đặt vấn đề Bài toán tìm kiếm văn bản tương đồng Vì sao phải dùng Min-Hashing Thuật toán MinHash Đặt vấn đề Giả sử bạn và tôi đều thích nghe nhạc trên trang mp3.zing.vn. Mỗi người đều nghe khoảng 100 bài nhạc khác nhau. Để đo sự giống nhau giữa danh sách bài hát bạn nghe và danh sách bài hát tôi nghe, thông thường chúng ta sẽ dùng độ đo Jaccard Similarity, được đo bằng cách lấy phần giao (intersection ) chia cho phần hợp (union). Nghĩa là đếm số lượng bài hát cả hai cùng nghe (phần giao) chia cho tổng số bài hát không lặp của cả hai.\nTrong trường hợp bạn và tôi đều nghe 100 bài, trong đó có 30 bài giống nhau, vậy phần giao là 30, phần hợp là 170, giá trị Jaccard Similarity sẽ là 30/170.\nĐộ đo Jaccard Similarity được sử dụng ở phương pháp apriori , FP Growth, \u0026hellip; mà các bạn đã có dịp học trong môn khai phá dữ liệu ở Đại học.\nBài toán tìm kiếm văn bản tương đồng Giả sử bạn quản lý một số lượng lớn văn bản (N= 1 tỷ), và xếp của bạn có nhu cầu nhóm những bài viết giống nhau thành từng cụm. Để:\nLoại bỏ bớt những kết quả trùng trong khung search.\nNhóm những bài viết vào từng nhóm sự kiện theo dòng thời gian, ví dụ sự kiện \u0026lsquo;cô gái giao gà\u0026rsquo;, sự kiện \u0026lsquo;dịch cúm corona\u0026rsquo;, \u0026hellip;\nVì một bất kể lý do nào đó mà trong lúc viết bài này tác giả chưa nghĩ ra.\nKhi đó, các vấn đều sau có thể sẽ phát sinh:\nNhiều phần nhỏ của văn bản này xuất hiện ở một vị trí lộn xộn nào ở một hoặc nhiều văn bản khác.\nVăn bản quá dài nên không thể lưu trữ hết lên bộ nhớ chính (RAM).\nCó quá nhiều cặp văn bản cần phải so sánh.\nĐể giải quyết bài toán trên, chúng ta sẽ tiếp cận theo hướng sau:\nShingling: Chuyển văn bản thành tập ký tự, tập từ \u0026hellip;.\nMin-Hashing: Chuyển tập ký tự thành 1 chuỗi số hash định danh.\nLocality-Sensitive Hashing: Tìm các văn bản tương đồng dựa vào chuỗi số định danh.\nỞ bài viết này, mình chỉ đề cập bước thứ 2 là Min-Hashing. Bước 1 và bước 3 bạn có thể tham khảo thêm trong khóa học, mình có để link bên dưới.\nVì sao phải dùng Min-Hashing Như bài toán đặt ra ở trên, chúng ta có 1 tỷ văn bản, chúng ta cần N(N-1)/2 = 5*10^17 phép tính Jaccard Similarity. Chúng ta có một server có thể thực hiện 5x10^6 phép so sánh, thì chúng ta phải mất 10^11 giây tương đương 31,710 năm để thực hiện xong.\nThuật toán MinHash sẽ giúp chúng ta một giá trị xấp xỉ giá trị của Jaccard Similarity của hai tập dữ liệu. Ưu điểm của MinHash:\nCó chiều dài đầu ra cố định\nKhông phụ thuộc vào chiều dài đầu vào.\nĐể tính giá trị xấp xỉ Jaccard Similarity (MinHash signatures), đầu tiên ta sẽ tính MinHash của hai tập data, được 2 giá trị hash, sau đó đếm giá trị trùng nhau của 2 chuỗi hash và chia chiều dài gía trị hash, chúng ta sẽ được một giá trị xấp xỉ giá trị Jaccard Similarity.\nVí dụ ta có hai tập tập dữ liệu {a,x,c,d} và {a,x,d,e} hai giá trị hash ta có tương ứng là 1234 và 1235, số ký tự trùng nhau là 3 (1,2,3), chiều dài là 4, vậy ta có giá trị Jaccard Similarity là 3/4.\nPhép tính này sẽ hơn việc tính Jaccard Similarity truyền thống, lý do là chúng ta không cần phải tính phần giao và phần hợp của hai tập dữ liệu ( trong trường hợp hai tập có nhiều giá trị thì việc tính càng lâu), và giá trị hash thường có chiều dài ngắn hơn so với số lượng phần trử trong tập dữ liệu, ngoài ra phép so sánh cũng đơn giản hơn nhiều.\nThuật toán MinHash Ý tưởng của thuật toán khá đơn giản:\nta có hàm hash:\n$$ h(x) = (ax+b)%c $$\nTrong đó:\nx là số nguyên đầu vào, a và b là hai số được chọn ngẫu nhiên với điều kiện a và b \u0026lt; x\nc là số nguyên tố được chọn ngẫu nhiên, với điều kiện c lớn hơn x.\nCách thuật toán thực hiện như sau:\nVới 1 văn bản, chạy thuật toán hash 10 lần, do ta có số a và b là ngẫu nhiên nên 10 lần chạy sẽ cho ra các kết quả khác nhau, lấy giá trị hash nhỏ nhất (do đó thuật toán có tên là min hash) làm thành phần đầu tiên của MinHash signature. Lặp lại quá trình trên 10 lần, chúng ta có MinHash signature với 10 giá trị.\nXong thuật toán, quá dễ.\nCảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.\nTham khảo\nKhóa học Mining of Massive Datasets chương 3 http://www.mmds.org/\nhttps://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/\n","date":"Jan 26, 2020","img":"","permalink":"/blog/2020-01-26-simhash/","series":null,"tags":["python","hash"],"title":"Simhash"},{"categories":null,"content":" Built-In Hashing Checksums Secure Hashing MD5– 16 bytes/128 bit SHA1–20 bytes/160 bits SHA256–32 bytes/256 bit và SHA512–64 bytes/512 bit Near-Duplicate Detection Perceptual Hashing Kết luận Built-In Hashing Python có xây dựng sẵn cho chúng ta một hàm hash, chúng ta cứ việc gọi ra và sử dụng.\n1hash(\u0026#34;pham duy tung\u0026#34;) 2-7141560399917772220 Một lưu ý nhỏ là giá trị của hàm hash sẽ khác nhau giữa các phiên bản python. Ví dụ ở trên mình xài python 3.8, với bản 3.6 sẽ là\n1hash(\u0026#34;pham duy tung\u0026#34;) 21568935795476364190 Checksums Chúng ta có thể sử dụng checksums để hash dữ liệu. Checksum được sử dụng trong thuật toán nén file ZIP để đảm bảo toàn vẹn dữ liệu sau khi nén. Thư viện zlib của python hỗ trợ 2 hàm tính checksum là adler32 và crc32. Để đảm bảo tốc độ chương trình và chỉ cần lấy hash đơn giản, chúng ta có thể sử dụng hàm Adler32. Tuy nhiên, nếu bạn muốn chương trình có độ tin cậy cao hoặc đơn giản là checksums, hãy sử dụng crc32. Các bạn có thể đọc bài viết ở đây https://www.leviathansecurity.com/blog/analysis-of-adler32 để hiểu hơn.\n1\u0026gt;\u0026gt;\u0026gt; import zlib 2\u0026gt;\u0026gt;\u0026gt; zlib.adler32(b\u0026#34;Pham Duy Tung\u0026#34;) 3524616855 4\u0026gt;\u0026gt;\u0026gt; zlib.crc32(b\u0026#34;Pham Duy Tung\u0026#34;) 53750031252 Secure Hashing Mã hóa an toàn (Secure Hashing) và bảo mật dữ liệu đã được nghiên cứu và ứng dụng từ nhiều năm về trước. Tiền thân là thuật toán MD5 đến SHA1, SHA256, SHA512\u0026hellip;. Mỗi thuật toán ra đời sau sẽ cải tiến độ bảo mật và giảm đụng độ của các thuật toán trước đó.\nMột số hàm hash phổ biến:\nMD5– 16 bytes/128 bit Chuỗi đầu ra của MD5 có kích thước 16 bytes hay 16*8 = 128 bits. Ở thời điểm hiện tại MD5 không còn là thuật toán phổ biến và không được khuyến khích dùng bởi các tổ chức bảo mật.\n1\u0026gt;\u0026gt;\u0026gt; import hashlib 2\u0026gt;\u0026gt;\u0026gt; hashlib.md5(b\u0026#34;Pham Duy Tung\u0026#34;).hexdigest() 3\u0026#39;58067430b9caa44f5ac1220b171f45c8\u0026#39; 4\u0026gt;\u0026gt;\u0026gt; len(hashlib.md5(b\u0026#34;Pham Duy Tung\u0026#34;).digest()) # Chiều dài của đầu ra là 16 bytes 516 Chú ý: Hàm hexdigest biểu diễn một byte thành một ký tự hex (2 ký tự đầu 58 của ví dụ trên là giá trị hex của số 88 trong hệ thập phân)\nSHA1–20 bytes/160 bits Đầu ra của SHA1 có chiều dài là 20 bytes tương ứng với 160 bit. Cũng giống như MD5, SHA1 cũng không được khuyến khích sử dụng ở trong các ứng dụng bảo mật.\n1\u0026gt;\u0026gt;\u0026gt; import hashlib 2\u0026gt;\u0026gt;\u0026gt; hashlib.sha1(b\u0026#34;Pham Duy Tung\u0026#34;).hexdigest() 3\u0026#39;b95b8716f15d89b6db67e2e788dea42d3fba5ee8\u0026#39; 4\u0026gt;\u0026gt;\u0026gt; len(hashlib.sha1(b\u0026#34;Pham Duy Tung\u0026#34;).digest()) 520 SHA256–32 bytes/256 bit và SHA512–64 bytes/512 bit Đây là hai hàm hash được khuyên là nên dùng ở thời điểm hiện tại\n1\u0026gt;\u0026gt;\u0026gt; hashlib.sha256(b\u0026#34;Pham Duy Tung\u0026#34;).hexdigest() 2\u0026#39;611b322b6b8ee570831c6061408ac5aa77fcdb572206d5d443855f5d3c1383c6\u0026#39; 3\u0026gt;\u0026gt;\u0026gt; len(hashlib.sha256(b\u0026#34;Pham Duy Tung\u0026#34;).digest()) 432 5\u0026gt;\u0026gt;\u0026gt; hashlib.sha512(b\u0026#34;Pham Duy Tung\u0026#34;).hexdigest() 6\u0026#39;ac1f6a2dd234bc15c1fa2be1db4e55ad4af8c476abb8e3d9ac3d4c74d3e151c23314e20925616e90a0bcb13a38b5531e064c586d65fed54504d713fdabee03f9\u0026#39; 7\u0026gt;\u0026gt;\u0026gt; len(hashlib.sha512(b\u0026#34;Pham Duy Tung\u0026#34;).digest()) 864 Near-Duplicate Detection Các thuật toán được giới thiệu ở trên, khi chúng ta thay đổi giá trị đầu vào, dù chỉ một giá trị nhỏ thôi ở một vài vị trí nào đó, thì kết quả trả ra lại khác nhau khá lớn. Tuy nhiên, đôi khi chúng ta gặp những bài toán tìm nội dung tương tự nhau hoặc gần như tương tự nhau. Ví dụ giống như google crawler dữ liệu xác định những bài văn copy paste từ những trang web khác nhau, hoặc phát hiện đạo văn, phát hiện đạo nhạc \u0026hellip;\nMột thuật toán khá phổ biến nằm trong nhóm này là SimHash. Thuật toán được google sử dụng để tìm ra các trang gần trùng nhau (theo wiki https://en.wikipedia.org/wiki/SimHash). Tác giả của thuật toán là Moses Charikar.\nĐể dùng Simhash, chúng ta phải cài đặt package từ kho của python\n1from simhash import Simhash 2 3\u0026gt;\u0026gt;\u0026gt; Simhash(\u0026#34;Pham Duy Tung\u0026#34;).value 417022061268703429674 5\u0026gt;\u0026gt;\u0026gt; Simhash(\u0026#34;Pham Duy Tung1\u0026#34;).value 617184261516160517290 Một trong những lưu ý quan trọng khi sử dụng SimHash ( tham khảo https://stackoverflow.com/questions/49820228/how-to-compare-the-similarity-of-documents-with-simhash-algorithm/49831194#49831194)\nSimHash thật sự hữu ích trong bài toán phát hiện văn bản trùng lắp.\nĐể tìm văn bản trùng lắp chính xác, dúng ta có thể sử dụng các thuật toán đơn giản mà hiệu quả như md5, sha1sha1.\nThuật toán phù hợp các văn bản lớn, không phù hợp cho các câu văn nhỏ.\nĐoạn code bên dưới là một ví dụ được dùng để tìm các văn bản có đạo nội dung.\n1 #assuming that you have a dictionary with document id as the key and the document as the value: 2# documents = { doc_id: doc } you can do: 3 4from simhash import simhash 5 6def split_hash(str, num): 7 return [ str[start:start+num] for start in range(0, len(str), num) ] 8 9hashes = {} 10for doc_id, doc in documents.items(): 11 hash = simhash(doc) 12 13 # you can either use the whole hash for higher precision or split into chunks for higher recall 14 hash_chunks = split_hash(hash, 4) 15 16 for chunk in hash_chunks: 17 if chunk not in hashes: 18 hashes[chunk] = [] 19 hashes[chunk].append(doc_id) 20 21# now you can print the duplicate documents: 22for hash, doc_list in hashes: 23 if doc_list \u0026gt; 1: 24 print(\u0026#34;Duplicates documents: \u0026#34;, doc_list) Ngoài SimHash, còn một thuật toán hash khá nổi tiếng nữa cũng được google sử dụng trong việc cá nhân hóa người dùng, đó là MinHash. Ở các bài viết tiếp theo mình sẽ viết về thuật toán này.\nPerceptual Hashing Loại hash cuối cùng chúng ta đề cập ở đây là perceptual hashing. Loại hash này được sử dụng để phát hiện sự khác nhau trong tập hình ảnh hoặc trong video.\nMột ví dụ của các thuật toán thuộc nhóm là là được dùng để phát hiện các frame ảnh trùng lắp trong video. Thuật toán được dùng để loại bỏ những nội dung trùng lắp, giúp tiết kiệm lưu trữ. Hoặc dùng trong các thuật toán tóm tắt video.\nẢnh 1 Ảnh 2\n1\u0026gt;\u0026gt;\u0026gt; import hashlib 2\u0026gt;\u0026gt;\u0026gt; from PIL import Image 3\u0026gt;\u0026gt;\u0026gt; image1 = Image.open(\u0026#34;google_free_ds1.png\u0026#34;) 4\u0026gt;\u0026gt;\u0026gt; image1 = Image.open(\u0026#34;google_free_ds_1.png\u0026#34;) 5\u0026gt;\u0026gt;\u0026gt; image2 = Image.open(\u0026#34;google_free_ds_2.png\u0026#34;) 6\u0026gt;\u0026gt;\u0026gt; hashlib.sha256(image1.tobytes()).hexdigest() 7\u0026#39;c57d0b5b1ca64077b45bdb65f817497834675232a2fc2ed76d6b8aa7955126b9\u0026#39; 8\u0026gt;\u0026gt;\u0026gt; hashlib.sha256(image2.tobytes()).hexdigest() 9\u0026#39;02ea5e51b19cf3748f91f9bbe26976e9e14dca4b47e0aaff88ab20030a695f44\u0026#39; Giá trị hash khác xa nhau, có vẻ chúng ta không thể nào sử dụng SHA256 trong bài toán này được. Lúc này, chúng ta sẽ tìm tới các thư viện thuộc nhóm Perceptual Hashing, một trong số chúng là ImageHash.\n1\u0026gt;\u0026gt;\u0026gt; import imagehash 2\u0026gt;\u0026gt;\u0026gt; hash1 = imagehash.average_hash(image1) 3\u0026gt;\u0026gt;\u0026gt; hash2 = imagehash.average_hash(image2) 4\u0026gt;\u0026gt;\u0026gt; hash1-hash2 524 Giá trị hash của hai ảnh trên là khác nhau, nhưng sự khác nhau là rất ít. Chứng tỏ hai ảnh trên có thể là bản sao của nhau.\nKết luận Trong bài viết này, chúng ta đã đề cập qua các cách khác nhau để hash dữ liệu trong Python. Phụ thuộc vào bài toán, chúng ta sẽ sử dụng các thuật toán với các tham số phù hợp. Hi vọng bài viết này sẽ ít nhiều giúp ích được cho các bạn.\nChú thích:\nẢnh cover của bài viết là ảnh của chùm sao thất tinh bắc đẩu mình chụp từ trang https://stellarium-web.org/.\nhash collision : Khi cho 2 input khác nhau vào hàm hash mà cùng ra một output -\u0026gt; collision.\nNguồn bài viết:\nhttps://medium.com/better-programming/how-to-hash-in-python-8bf181806141\n","date":"Jan 25, 2020","img":"","permalink":"/blog/2020-01-13-hash-in-python/","series":null,"tags":["python","hash"],"title":"Các Hàm Hash Có Sẵn Trong Python"},{"categories":null,"content":" Đặt vấn đề Thuật toán NMS Đặt vấn đề Sau khi thực hiện object detection feed một ảnh qua mạng neural, chúng ta sẽ thu được rất nhiều proposals (như hình ở dưới). Ở trạng thái này, có rất nhiều proposals là boding box cho một object duy nhất, điều này dẫn tới việc dư thừa. Chúng ta sử dụng thuật toán Non-maximum suppression (NMS) để giải quyết bài toán này.\nHình 1: Proposals box, hình được cắt từ bài báo\nThuật toán NMS Đầu vào:\nTập danh sách các proposals box ký hiệu là B với B ={b1,b2,\u0026hellip;,bn}, với bi là proposal thứ i.\nTập điểm của mỗi proposal box ký hiệu là S với S={s1,s2,\u0026hellip;,sn}, si là điểm confidence của box bi\nGiá trị ngưỡng overlap threshold N.\nCả hai giá trị bi và si đều là output của mạng neural network.\nĐầu ra:\nMột tập các proposals box D là tập các proposals đã loại bỏ dư thừa tương ứng với từng object trong hình.\nThuật toán:\nBước 1: Khởi tạo tập output D = {}\nBước 2: Chọn ra proposal box có điểm confidence cao nhất trong tập S, loại box đó ra khỏi tập S, B và thêm nó vào tập D.\nBước 3: Tính giá trị IOU giữa proposal box mới vừa loại ra ở bước 2 với toàn bộ proposal box trong tập B. Nếu có bất kỳ box nào đó có giá trị IOU lớn hơn giá trị ngưỡng N thì loại box đó ra khỏi B, S.\nBước 4: Lặp lại bước 2 đến khi nào không còn box nào có trong tập B.\nĐiểm yếu của thuật toán:\nNếu bạn đọc kỹ thuật toán, bạn sẽ thấy rằng toàn bộ quá trình loai bỏ những box dư thừa đều phụ thuộc vào giá trị ngưỡng N. Việc chọn lựa giá trị N chính là chìa khóa thành công của mô hình. Tuy nhiên, việc chọn giá trị ngưỡng này trong các bài toán khá khó. Và với việc chỉ sử dụng giá trị N, chúng ta sẽ gặp trường hợp dưới đây.\nGiả sửa giá trị ngưỡng N bạn chọn là 0.5. Có nghĩa là nếu box có giá trị lớn IOU đều bị loại bỏ, ngay cả với trường hợp điểm score si của nó có giá trị cao. Ngược lại, giả sử box có điểm score si thấp nhưng IOU của nó nhỏ hơn 0.5, ví dụ o.49, thì nó lại được nhận.\nVà để giải quyết bài toán này Navaneeth Bodla đã đề xuất một cải tiến nhỏ và đặt tên thuật toán là Soft-NMS. ý tưởng được đề ra như sau: Thay vì phải loại bỏ hoàn toàn proposal, chúng ta sẽ giảm giá trị confidence của box đi.\nsoft-nms, hình được cắt từ bài báo\nVới giá trị si được cập nhật lại như sau:\nsoft-nms, hình được cắt từ bài báo\nCảm ơn các bạn đã theo dõi bài viết. Hẹn gặp lại các bạn ở những bài viết tiếp theo.\nTham khảo\nhttps://medium.com/@yusuken/object-detction-1-nms-ed00d16fdcf9\nhttps://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\nhttps://arxiv.org/pdf/1704.04503.pdf\nhttps://arxiv.org/pdf/1705.02950.pdf\n","date":"Dec 25, 2019","img":"","permalink":"/blog/2019-12-25-nms/","series":null,"tags":["Machine learning","Deeplearning","AlexNet"],"title":"Tìm Hiểu Non-Maximum Suppression (NMS)"},{"categories":null,"content":" Kiến trúc mạng AlexNet Cải tiến của mô hình để giảm error rate Sử dụng ReLU thay cho TanH Local Response Normalization Overlapping Pooling Sử dụng Data Augmentation Dropout Sử dụng nhiều GPU Một số chi tiết khác về các learning param Kết quả Mạng CaffeNet Trong bài viết này, chúng ta sẽ tìm hiểu mô hình AlexNet từ nhóm của giáo sư Hinton. Tới thời điểm hiện tại (2019-05-27), bài viết của giáo sư đã có hơn 40316 lượt trích dẫn. Bài báo này có bước đóng góp cực kỳ quan trọng, là một đột phá lớn trong lĩnh vực deep learning, mở đầu cho sự quay lại của mạng neural network và đóng góp trực tiếp vào thành công của những chương trình trí tuệ nhân tạo tại thời điểm hiện tại.\nVề bài báo gốc của tác giả, mình có để ở phần trích dẫn bên dưới. Các bạn có nhu cầu tìm hiểu có thể tìm và đọc. Theo ý kiến riêng của mình, đây là một bài báo rất nên đọc và phải đọc. Trước đây mình đã có viết 1 bài về tập AlexNet nhưng chưa đầy đủ, bài đó mình chỉ giới thiệu phớt phớt qua mạng AlexNet. Trong bài viết này, mình sẽ trình bày kỹ hơn.\nSơ lược một chút, tập dữ liệu ImageNet là tập dataset có khoảng 15 triệu hình ảnh có độ phân giải cao đã được gán nhãn (có khoảng 22000 nhãn). Cuộc thi ILSVRC sử dụng một phần nhỏ của tập ImageNet với khoảng 1.2 triệu ảnh của 1000 nhãn (trung bình mỗi nhãn có khoảng 1.2 ngàn hình ảnh) làm tập train, 50000 ảnh làm tập validation và 150000 ảnh làm tập test (tập validation và tập test đều có 1000 nhãn thuộc tập train).\nKiến trúc mạng AlexNet Kiến trúc mô hình AlexNet\nMạng AlexNet bao gồm 8 lớp (tính luôn lớp input là 9), bao gồm:\nInput: có kích thước 224x224x3 (Scale ảnh đầu vào về dạng 224x224x3, thực chất ảnh của tập ImageNet có size tùy ý)\nLớp thứ nhất:\nConvolution Layer có kích thước 11x11x3 với stride size = 4 và pad = 0. Kết quả sau bước này ta được tập feature map có kích thước 55x55x96 (mình nghĩ là các bạn sẽ biết cách tính sao cho ra số 55, mình cũng đã đề cập vấn đề cách tính này ở 1 bài viết trước đây).\rTiếp theo là một Overlapping Max Pooling 3x3 có stride =2 =\u0026gt; feature maps = 27x27x96.\rTiếp theo là Local Response Normalization =\u0026gt; feature maps = 27x27x96.\rXong lớp thứ nhất\rLớp thứ hai:\nConvolutional Layer: 256 kernels có kích thước 5x5x48 (stride size = 1, pad = 2) =\u0026gt; 27x27x256 feature maps.\rOverlapping Max Pooling 3x3 có stride =2 =\u0026gt; feature maps = 13x13x256.\rTiếp theo là Local Response Normalization =\u0026gt; feature maps = 13x13x256.\rLớp thứ ba:\nConvolutional Layer: 384 kernels có kích thước 3x3x256 (stride size = 1, pad = 1) =\u0026gt; 13x13x384 feature maps.\rLớp thứ bốn: 384 kernels có kích thước 3x3x192 (stride size = 1, pad = 1) =\u0026gt; 13x13x384 feature maps.\nLớp thứ năm:\nConvolutional Layer: 256 kernels có kích thước 3x3x192 (stride size = 1, pad = 1) =\u0026gt; 13x13x256 feature maps.\rOverlapping Max Pooling 3x3 có stride =2 =\u0026gt; feature maps = 6x6x256.\rLớp thứ sáu:\nFull connected (hay còn gọi là Dense layer) với 4096 neurals\rLớp thứ bảy:\nFull connected với 4096 neurals\rLớp thứ tám:\nFull connected ra output 1000 neural (do có 1000 lớp)\rHàm độ lỗi được sử dụng là Softmax.\nTổng cộng, chúng ta có 60 triệu tham số được sử dụng để huấn luyện.\nCải tiến của mô hình để giảm error rate Sử dụng ReLU thay cho TanH Hàm kích hoạt ReLU và TanH\nCác mô hình neural network trước khi bài báo ra đời thường sử dụng hàm Tanh làm hàm kích hoạt. Mô hình AlexNet không sử dụng hàm TanH mà giới thiệu một hàm kích hoạt mới là ReLU. ReLU giúp cho quá trình huấn luyện chạy nhanh hơn gấp 6 lần so với kiến trúc tương tự sử dụng TanH, góp một phần vào việc độ lỗi trên tập huấn luyện là 25%.\nLocal Response Normalization Local Response Normalization và Batch Normalization\nTrong mạng AlexNet, nhóm tác giả sử dụng hàm chuẩn hóa là Local Response Normalization. Hàm này không phải là Batch Normalization mà các bạn hay sử dụng ở thời điểm hiện tại (xem hình ở trên, hai hàm có công thức tính toán hoàn toàn khác nhau). Việc sử dụng chuẩn hóa (Normalization) giúp tăng tốc độ hội tụ. Ngày nay, chúng ta không còn sử dụng Local Response Normalization nữa. Thay vào đó, chúng ta sử dụng Batch Normalization làm hàm chuẩn hóa.\nVới việc sử dụng hàm chuẩn hóa Local Response Normalization, độ lỗi top-1 error rate giảm 1.4%, top-5 giảm 1.2%.\nOverlapping Pooling Overlapping Pooling là pooling với stride nhỏ hơn kernel size. Một khái niệm ngược với Overlapping Pooling là Non-Overlapping Pooling với stride lớn hoăn hoặc bằng kernel.\nMạng AlexNet sử dụng Overlapping Pooling ở hidden layer thứ 1, 2 và 5 (Kernel size = 3x3, stride =2).\nVới việc sử dụng overlapping pooling, top-1 error rates giảm 0.4%, top-5 error rate giảm 0.3%.\nSử dụng Data Augmentation Dữ liệu của tập huấn luyện khá nhiều, 1.2 triệu mẫu. Nhưng chia ra cho 1000 lớp thì mỗi lớp có khoảng 1200, khá khiêm tốn phải không. Cho nên, tác giả đã nghĩ ra một cách khá hay để tăng số lượng hình ảnh mà vẫn giữ được tính IID của dữ liệu, đó là sử dụng các phép biến đổi affine trên dữ liệu ảnh gốc để thu thêm nhiều ảnh hơn.\nCó hai dạng Data Augentation được tác giả sử dụng\nDạng thứ nhất: Image translation và horizontal reflection (mirroring)\nImage translation được hiểu như sau: ảnh ImageNet gốc có kích thước 256x256 pixel, tác giả rút ra một ảnh con có kích thước 224x224 pixel, sau đó dịch qua trái 1 pixel và lấy 1 ảnh con tiếp theo có kích thước 224x224. Làm như vậy theo hàng, hết hàng làm theo cột. Cuối cùng tác giả có thể từ một bức hình 256x256 ban đầu rút trích thành 1024 hình có kích thước 224x224\nhorizontal reflection (mirroring) được hiểu là lấy ảnh phản chiếu của ánh gốc qua đường chéo chính. Ví dụ con báo dang có hướng tai của nó từ trái qua phải, ta lấy horizontal reflection của ảnh đó thì sẽ được con báo hướng tai từ phải qua trái.\nVới việc kết hợp Image translation và horizontal reflection (mirroring), tác giả có thể rút tối đa 2048 bức ảnh khác nhau chỉ từ 1 bức ảnh gốc =\u0026gt; với hơn 1000 bức ảnh của 1 nhãn có thể sinh ra tối đa là 2048000 bức ảnh, một con số khá lớn phải không các bạn.\nỞ tập test, tác giả sử dụng 4 hình 224x224 ở bốn góc cộng với 1 hình 224x224 ở trung tâm =\u0026gt; được 5 hình, đem 5 hình đó sử dụng horizontal reflection thì thu được 10 hình cho mỗi file test.\nDạng thứ hai: Thay đổi độ sáng\nThực hiện tính PCA trên tập train. Với mỗi hình trên tập train, thay đổi giá trị độ sáng\n$$[p_1, p_2, p_3][\\alpha_1 \\gamma_1, \\alpha_2 \\gamma_2, \\alpha_3 \\gamma_3]^T$$\nvới pi và gammai là giá trị trị riêng và vector riêng thứ i của ma trận hiệp phương sai 3x3 của ảnh, và alpha i là một giá trị ngẫu nhiên thuộc đoạn 1 và độ lệch chuẩn 0.1..\nVới việc sử dụng data augmentation, top-1 error rate giảm 1% độ lỗi.\nDropout Dropout\nVới mỗi layer sử dụng dropout, mỗi neural sẽ có cơ hội không đóng góp vào feed forward và backpropagation. Do đó, mỗi neural đều có cơ hội rất lớn đóng góp vào thuật toán, và chúng ta sẽ giảm thiểu tình trạng phụ thuộc vào một vài neural.\nKhông sử dụng dropout trong tập quá trình test.\nMạng AlexNet sử dụng giá trị xác xuất của dropout là 0.5 ở hai fully-connected layer. Dopout được xem như là một kỹ thuật chuẩn hóa nhằm mục đích giảm overfitting.\nSử dụng nhiều GPU Tại năm 2012, nhóm tác giả sử dụng card đồ họa NIVIDIA GTX 580 có 3GB bộ nhớ RAM. Cho nên, để có thể huấn luyện được mô hình AlexNet trên GPU, mô hình cần sử dụng 2 GPU.\nvì vậy việc sử dụng 2 hoặc nhiều GPU là do vấn đề thiếu bộ nhớ, chứ không phải là vấn đề tăng tốc quá trình train hơn so với 1 GPU\nNgoài ra, do giới hạn của GPU, nên mô hình AlexNet được tách ra làm 2 phần, mỗi phần được huấn luyện trên 1 GPU. Phiên bản 1 GPU của mô hình có tên là CaffeNet, và đòi hỏi chúng ta phải sử dụng GPU có bộ nhớ RAM lớn hơn hoặc bằng 6GB.\nMột số chi tiết khác về các learning param Batch size: 128\nMomemtum: 0.9\nWeight Decay: 0.0005\nLearning rate: 0.01, giá trị learning rate sẽ giảm đi 10 lần nếu validation error rate không thay đổi trong 1 khoảng thời gian. Số lần giảm là 3.\nEpoch: 90\nNhóm tác giả đã sử dụng 2 GPU 580 có 3GB GPU RAM và tốn 6 ngày để huấn luyện.\nKết quả Độ lỗi của AlexNet trên ILSVRC 2010\nTrong cuộc thi ILSVRC 2010, AlexNet đạt độ chính xác top-1 error 37.5% và top-5 error là 17.0%, kết quả này tốt hơn vượt trội so với các cách tiếp cận khác.\nĐộ lỗi của AlexNet trên ILSVRC 2012\nĐến cuộc thi ILSVRC 2012, độ lỗi của AlexNet trên tập validation giảm còn 18.2%.\nNếu lấy trung bình của dự đoán trên 5 mạng AlexNet được huấn luyện khác nhau, độ lỗi giảm còn 16.4%. Các lấy trung bình trên nhiều hơn 1 mạng CNN là một kỹ thuật boosting và được sử dụng trước đó ở bài toán phân loại số của mạng LeNet.\nỞ dòng số 3 là mạng AlexNet nhưng được thêm 1 convolution layer nữa (nên được ký hiệu là 1CNN*), độ lỗi trên tập validation giảm còn 16.4%.\nNếu lấy kết quả trung bình của 2 mạng neural net được chỉnh sửa (thêm 1 convolution layer) và 5 mạng AlexNet gốc (=\u0026gt; chúng ta có 7CNN*), độ lỗi trên tập validation giảm xuống 15.4%\nDemo kết quả top-5 của mạng AlexNet\nMạng CaffeNet Mạng này là phiên bản kiến trúc 1-GPU của AlexNet. Kiến trúc của mạng caffeNet như hình bên dưới:\nMạng caffeNet\nBạn thấy đó, thay vì có 2 phần trên và dưới như mô ình AlexNet ở trên, mô hình CaffeNet chỉ có 1 phần. Ví dụ lớp hidden layer thứ 7 mạng AlexNet gồm 2 phần, mỗi phần có kích thước 2048, còn ở phiên bản CaffeNet thì đã gộp lại thành 1 phần.\nTài liệu tham khảo\nhttps://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\nhttp://www.image-net.org/challenges/LSVRC/\nCảm ơn các bạn đã theo dõi bài viết, có chỗ nào bạn chưa rõ hoặc mình viết bị sai, các bạn vui lòng để lại comment để mình sửa lại cho đúng.\n","date":"May 27, 2019","img":"","permalink":"/blog/2019-05-27-alexnet/","series":null,"tags":["machine learning","deep learning","AlexNet","ILSVRC","dropout"],"title":"Tìm Hiểu Mạng AlexNet, Mô Hình Giành Chiến Thắng Tại Cuộc Thi ILSVRC 2012"},{"categories":null,"content":" Contour là gì Sử dụng contour trong opencv Ví dụ: Đếm số lượng quả bóng bay trong hình Contour là gì Các bạn có thể hiểu contour là \u0026ldquo;tập các điểm-liên-tục tạo thành một đường cong (curve) (boundary), và không có khoảng hở trong đường cong đó, đặc điểm chung trong một contour là các các điểm có cùng /gần xấu xỉ một giá trị màu, hoặc cùng mật độ. Contour là một công cụ hữu ích được dùng để phân tích hình dạng đối tượng, phát hiện đối tượng và nhận dạng đối tượng\u0026rdquo;.\nĐể tìm contour chính xác, chúng ta cần phải nhị phân hóa bức ảnh (nhớ là ảnh nhị phân nha các bạn, không phải ảnh grayscale đâu). Các kỹ thuật nhị phân hóa ảnh ở xử lý ảnh cơ bản có thể liệt kê đến là đặt ngưỡng, hoặc candy edge detection. Chúng ta sẽ không bàn kỹ về các cách đặt ngưỡng ( mặc dù có khá nhiều cách đặt ngưỡng, và trong opencv cũng có implement một vài phương pháp, nhưng nó không phải là mục tiêu của bài này, nên mình không đề cập ở đây) hoặc edge detection ở bài viết này, mà chúng ta sẽ đi vào các tìm contours bằng các sử dụng opencv luôn.\nTrong opencv, việc tìm một contour là việc tìm một đối tượng có màu trắng trên nền đen. Cho nên, các bạn hãy nhớ rằng hãy set đối tượng thành màu trắng và để nền là màu đen, đừng làm ngược lại nha.\nMột lưu ý nhỏ là tại thời điểm mình viết bài viết này, mình sử dụng phiên bản opencv3.6. Các bạn có thể sử dụng phiên bản opencv mới hơn, nhưng có thể những sample code mình để bên dưới sẽ không work, do không tương thích.\nSử dụng contour trong opencv Opencv hỗ trợ cho chúng ta hàm để tìm contour của một bức ảnh\n1modifiedImage, contours, hierarchy = cv2.findContours(binaryImage, typeofContour, methodofContour) Trong đó:\ncontours: Danh sách các contour có trong bức ảnh nhị phân. Mỗi một contour được lưu trữ dưới dạng vector các điểm\nhierarchy: Danh sách các vector, chứa mối quan hệ giữa các contour.\nmodifiedImage: Ảnh sau khi sử dụng contour, thường chúng ta không xài đối số này\nbinaryImage: Ảnh nhị phân gốc. Một chú ý quan trọng ở đây là sau khi sử dụng hàm findContours thì giá trị của binaryImage cũng thay đổi theo, nên khi sử dụng bạn có thể áp dụng binaryImage.copy() để không làm thay đổi giá trị của binaryImage\ntypeofContour: có các dạng sau: RETR_EXTERNAL, RETR_LIST, RETR_CCOMP, RETR_TREE, RETR_FLOODFILL.\nmethodofContour: Có các phương thức sau: CHAIN_APPROX_NONE, CHAIN_APPROX_SIMPLE, CHAIN_APPROX_TC89_L1, CHAIN_APPROX_TC89_KCOS.\nVí dụ về các sử dụng hàm\n1 2import numpy as np 3import cv2 4 5im = cv2.imread(\u0026#39;test.jpg\u0026#39;) # đọc ảnh màu 6imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) # chuyển ảnh màu sang dạng grayscale 7ret,thresh = cv2.threshold(imgray,127,255,0) # nhị phân hóa bức ảnh bằng cách đặt ngưỡng, với giá trị của ngưỡng là 127 8im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # tìm contour Opencv hỗ trợ chúng ta hàm để vẽ contor lên bức ảnh, giúp chúng ta nhìn rõ ràng hơn\n1cv2.drawContours(image, contours, contourIndex, colorCode, thickness) Với:\nimgage: ảnh, có thể là ảnh grayscale hoặc ảnh màu.\ncontours: danh sách các contour, là vector, nếu bạn muốn vẽ một contour, thì bạn phải cho nó vào trong một list.\ncontourIndex Vị trí của contor, thông thường chúng ta để -1\ncolorCode: Giá trị màu của contour chúng ta muốn vẽ, ở dạng BGR, nếu bạn muốn vẽ contour màu xanh lá cây thì set là (0,255,0).\nthickness : độ dày của đường contour cần vẽ, giá trị thickness càng lớn thì đường contor vẽ càng bự\nVí dụ: Đếm số lượng quả bóng bay trong hình Giả sử chúng ta có bức ảnh Bong bóng bay\nChúng ta thực hiện tìm contour của ảnh trên bằng cách\n1 2import numpy as np 3import cv2 4 5im = cv2.imread(\u0026#39;colorfull_ballon.jpg\u0026#39;) 6imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) # chuyển ảnh xám thành ảnh grayscale 7thresh = cv2.Canny(imgray, 127, 255) # nhị phân hóa ảnh 8_, contours, _ = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) 9 10cv2.drawContours(im, contours, -1, (0, 255, 0), 2) # vẽ lại ảnh contour vào ảnh gốc 11 12# show ảnh lên 13cv2.imshow(\u0026#34;ballons\u0026#34;, im) 14cv2.waitKey(0) Kết quả:\nContour màu xanh là đường curve bao quanh dữ liệu được rút trích được\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"May 26, 2019","img":"","permalink":"/blog/2019-05-26-contours/","series":null,"tags":["Machine Learning","Deep Learning","Opencv","Image Processing"],"title":"Contour"},{"categories":null,"content":" Chi tiết về mạng MobileNet Mô hình kiến trúc Depthwise Separable Convolution Làm mô hình gọn nhẹ hơn nữa So sánh MobileNet với các State-of-the-art đương thời Kết luận Trong bài viết này, chúng ta sẽ tìm hiểu mô hình MobileNetV1 từ nhóm tác giả đến từ Google. Điểm cải tiến (chắc là cải tiến :) của mô hình là sử dụng một cách tính tích chập có tên là Depthwise Separable Convolution để giảm kích thước mô hình và giảm độ phức tạp tính toán. Do đó, mô hình sẽ hữu ích khi chạy các ứng dụng trên di động và các thiết bị nhúng.\nLý do:\nMô hình có ít tham số hơn -\u0026gt; kích thước model sẽ nhỏ hơn.\nMô hình có ít phép tính cộng trừ nhân chia hơn -\u0026gt; độ phức tạp sẽ nhỏ hơn.\nHiện tại (2019-05-26), tại thời điểm viết bài, bài viết gốc của tác giả đã được 1594 lượt trích dẫn. Các bạn có thể tìm đọc bài báo gốc của tác giả tại trang https://arxiv.org/abs/1704.04861\nSố lượt trích dẫn bài báo MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications\nChi tiết về mạng MobileNet Mô hình kiến trúc Kiến trúc mạng MobileNet được trình bày bên dưới. Hình bên dưới được trích từ bài báo gốc của tác giả\nMô hình kiến trúc mạng MobileNet\nDiễn dịch ra ngôn ngữ tự nhiên, chúng ta thấy rằng mô hình có 30 lớp với các đặc điểm sau:\nLớp 1: Convolution layer với stride bằng 2\nLớp 2: Depthwise layer\nLớp 3: Pointwise layer\nLớp 4: Depthwise layer với stride bằng 2 (khác với bước 2, dw lớp 2 có stride size bằng 1)\nLớp 5: Pointwise layer\nLớp 30: Softmax, dùng để phân lớp.\nDepthwise Separable Convolution Depthwise separable convolution là một depthwise convolution theo sau bởi một pointwise convolution như hình bên dưới:\nCấu trúc của một Depthwise Separable Convolution\nDepthwise convolution: là một channel-wise DK×DK spatial convolution. Ví dụ ở hình trên, ta có 5 channels (các bạn để ý cục đầu tiên có 5 khối hộp, cục thứ 2 là phân tách 5 khối hộp ra thành ma trận mxn, cục thứ 3 là spatial convolution có kích thước kxk, cục thứ 4 là kết quả sau khi convolution, cục thứ 5 là ráp 5 cái kết quả của convolution lại ), do đó chúng ta sẽ có 5 DK×DK spatial convolution tương ứng với 5 channel trên.\nPointwise convolution: đơn giản là một convolution có kích thước 1x1 (như hình ở trên).\nVới M là số lượng input channel, N là số lượng output channel, Dk là kernel size, Df là feature map size (với dataset ImageNet thì input có kích thước là 224, do đó feature map ban đầu có Df = 224), chúng ta có thể tính được:\nChi phí tính toán của Depthwise convolution là :\n$$D_k \\cdot D_k \\cdot M \\cdot D_f \\cdot D_f$$\nChi phí tính toán của Pointwise convolution là :\n$$M \\cdot N \\cdot D_f \\cdot D_f$$\nTổng chi phí tính toán của Depthwise Separable Convolution là:\n$$D_k \\cdot D_k \\cdot M \\cdot D_f \\cdot D_f + M \\cdot N \\cdot D_f \\cdot D_f$$\nNếu chúng ta không sử dụng Depthwise Separable Convolution mà sử dụng phép convolution như bình thường, chi phí tính toán là\n$$ D_k \\cdot D_k \\cdot M \\cdot N \\cdot D_f \\cdot D_f$$\nDo đó, chi phí tính toán sẽ giảm:\n$$\\frac{D_k \\cdot D_k \\cdot M \\cdot D_f \\cdot D_f + M \\cdot N \\dot D_f \\cdot D_f}{D_k \\cdot D_k \\cdot M \\cdot N \\cdot D_f \\cdot D_f} = \\frac{1}{N} + \\frac{1}{D^2_k}$$\nGiả sử, chúng ta chọn kernel size Dk = 3, chúng ta sẽ giảm từ 8 đến 9 lần phép tính nhân =\u0026gt; giảm chi phí tính toán đi rất nhiều.\nMột chú ý nhỏ về kiến trúc ở đây, là sau mỗi convolution MobileNet sẽ sử dụng Batch Normalization (BN) và ReLU như hình bên dưới:\nStandard Convolution bên trái, Depthwise separable convolution với BN và ReLU bên phải\nSo sánh kết quả của việc sử dụng mạng 30 layer sử dụng thuần Convolution và mạng 30 layer sử dụng Depthwise Separable Convolution (MobileNet) trên tập dữ liệu ImageNet, chúng ta có bảng kết quả bên dưới\nStandard Convolution bên trái, Depthwise separable convolution với BN và ReLU bên phải\nMobileNet giảm 1% độ chính xác, nhưng số lượng tham số của mô hình và số lượng phép tính toán giảm đi rất rất nhiều, gần xấp xỉ 90%. Một con số đáng kinh ngạc.\nLàm mô hình gọn nhẹ hơn nữa Với mong muốn làm mô hình gọn nhẹ hơn nữa, nhóm tác giả đã thêm vào hai tham số alpha và rho.\nTham số alpha: Điều khiển số lượng channel (M và N).\nChi phí tính toán của depthwise separable convolution khi sử dụng thêm tham số alpha.\n$$D_k \\cdot D_k \\cdot \\alpha M \\cdot D_f \\cdot D_f + \\alpha M \\cdot \\alpha N \\cdot D_f \\cdot D_f$$\nGiá trị alpha nằm trong đoạn [0,1], nhóm tác giả set giá trị alpha có bước nhảy là 0.25, các giá trị cần xét là 0.25, 0.5, 0.75, 1. Trường hợp alpha = 1 chính là mạng MobileNet baseline của mình. Trong trường hợp thay đổi alpha, số phép tính toán, số tham số, cũng giảm đi rất nhiều, và tất nhiên, độ chính xác cũng giảm đi tương ứng.\nMạng MobileNet với alpha thay đổi\nPhân tích kỹ hình ở trên, ta thấy rằng với alpha bằng 0.75 và 0.5 giá trị độ chính xác còn nằm ở mức miễn cưỡng có thể chấp nhận được. Nhưng với alpha bằng 0.25 thì khó mà có thể chấp nhận được kết quả đó. Việc giảm phép tính toán và số lượng tham số dẫn đến kết quả tệ như trên quả là một điều không nên. Mình nghĩ ở đây nhóm tác giả để con số để có ý nghĩa so sánh.\nTham số rho: Tham số này được sử dụng để điều khiển độ phân giải của ảnh input.\nChi phí tính toán của depthwise separable convolution khi sử dụng thêm tham số rho.\n$$D_k \\cdot D_k \\cdot \\alpha M \\cdot \\rho D_f \\cdot \\rho D_f + \\alpha M \\cdot \\alpha N \\cdot \\rho D_f \\cdot \\rho D_f$$\nGiá trị rho cũng nằm trong đoạn [0,1]. Nhóm tác giả sử dụng các giá trị độ phân giải là 224 (độ phân giải gốc, tương ứng với rho =1), 192, 160, 128.\nMạng MobileNet với rho thay đổi\nGiá trị độ chính xác thay đổi theo hướng giảm khá mượt. Việc thay đổi rho chỉ làm giảm số lượng phép tính toán, không làm giảm số lượng tham số. Việc giảm độ chính xác có thể lý giải lý do là có một số hình có kích thước nhỏ nên khi giảm kích thước sẽ làm mất những đặc trưng cần thiết của đối tượng cần xét.\nSo sánh MobileNet với các State-of-the-art đương thời Khi so sánh 1.0 MobileNet-224 với GoogleNet và VGG 16 (hình bên dưới), chúng ta thấy rằng độ chính xác của cả 3 thuật toán là hầu như tương đương nhau. Nhưng 1.0 MobileNet-224 có số lượng tham số ít (75% so với GoogleNet) và số lượng phép toán nhỏ hơn rất nhiều =\u0026gt; chạy nhanh hơn.\nSo sánh 1.0 MobileNet-224 với GoogleNet và VGG 16 trên tập ImageNet\nVới mô hình 0.50 MobileNet-160, chúng ta có thể so sánh với mô hình Squeezenet và AlexNet (mô hình thắng giải nhất cuộc thi ILSVRC 2012). Một lần nữa, mô hình 0.50 MobileNet-160 cho kết quả tốt hơn, nhưng có số lượng phép tính toán ít hơn rất nhiều (hơi đáng buồn là số lượng tham số của mô hình 0.50 MobileNet-160 khá cao, số lượng tham số gấp đôi so với AlexNet và gần bằng Squeezenet) =\u0026gt; 0.50 MobileNet-160 train nhanh hơn, predict cũng nhanh hơn so với Squeezenet và AlexNet, nhưng tốn bộ nhớ RAM hơn.\nSo sánh 0.50 MobileNet-160 với Squeezenet và AlexNet trên tập ImageNet\nSo với mô hình Inception-v3 (mô hình thắng giải nhất cuộc thi ILSVRC 2015), MobileNet cho kết quả khá tốt, nhưng số tham số và số lượng phép tính toán nhỏ hơn rất nhiều\nSo sánh Mobile net và Inception-v3 trên tập Stanford Dog\nCác thí nghiệm ở dưới trên các tập dataset khác nhau chứng minh mức độ hiệu quả của MobileNet GPS Localization Via Photos\nFace Attribute Classification\nMMicrosoft COCO Object Detection Dataset\nFace Recognition\nKết luận MobileNet cho kết quả tốt ngang ngữa các state-of-the-art thắng giải nhất ở quá khứ, nhưng với mô hình có số lượng tham số nhỏ hơn và số phép tính toán ít hơn. Điều này đạt được là nhờ vào việc sử dụng Depthwise Separable Convolution.\nCảm ơn các bạn đã theo dõi bài viết, có chỗ nào bạn chưa rõ hoặc mình viết bị sai, các bạn vui lòng để lại comment để mình sửa lại cho đúng.\n","date":"May 25, 2019","img":"","permalink":"/blog/2019-05-26-mobilenetv1/","series":null,"tags":["machine learning","deep learning","MobileNetV1","Depthwise Separable Convolution","Light Weight Model","Width Multiplier","Resolution Multiplier"],"title":"Tìm Hiểu Mạng MobileNetV1"},{"categories":null,"content":" 1. Tạo chương trình đầu tiên bằng PredictionIO 1. Tạo chương trình đầu tiên bằng PredictionIO Đầu tiên, các bạn hãy tạo thư mục template ở đâu đó. Mình sẽ tạo ở trong thư mục /data/pio. Đường dẫn của mình sẽ là /data/pio/template\n1mdkir /data/pio/template Tiếp theo, chúng ta sẽ clone templte trên github về, các bạn thực hiện lệnh sau\n1git clone https://github.com/apache/predictionio-template-recommender.git 2cd predictionio-template-recommender Tiếp theo, chúng ta sẽ tạo một app đầu tiên, mình đặt tên là ourrecommendation, các bạn thích đặt tên gì thì đặt nha.\n1pio app new ourrecommendation Để liệt kê danh sách app đang có trong hệ thống, các bạn dùng lệnh\n1pio app list Kết quả trong máy mình tại thời điểm viết bài là\n1[INFO] [Pio$] Name | ID | Access Key | Allowed Event(s) 2[INFO] [Pio$] ourrecommendation | 1 | Z93rJZ7Xq2pXiQwVC6B5nRK6jRykcfyMI5huOijKbdDJeUeKEnVT-ph5nabptIX1 | (all) 3[INFO] [Pio$] Finished listing 1 app(s). Mình mới tạo app đầu tiên tên là ourrecommendation nên chỉ có 1 app trong hệ thống. Sau này sẽ có nhiều hơn. À, sau khi tạo app, thì hệ thống sẽ generate tự động cho app với một Access Key, ví dụ access key của app ourrecommendateion của mình là Z93rJZ7Xq2pXiQwVC6B5nRK6jRykcfyMI5huOijKbdDJeUeKEnVT-ph5nabptIX1. Các bạn sẽ có access key khác với access key của mình, nên đừng copy của mình về làm gì hết :).\nSau khi khởi tạo app xong, chúng ta sẽ import data vào hệ thống. Ở đây, mình sẽ download dữ liệu mẫu từ nguồn https://gist.githubusercontent.com/vaghawan/0a5fb8ddb85e03631dd500d7c8f0677d/raw/17487437dd8269588d9dd1ac859b129a43842ba5/data-sample.json. Sau khi download về các bạn import dữ liệu vào hệ thống bằng lệnh\n1pio import — appid 1 — input data-sample.json Với appod 1 là id của ourrecommendation chúng ta vừa mới tạo. Nếu quên appid, các bạn có thể xem lại bằng lệnh pio app list.\nSau khi import thành công, chúng ta sẽ thay đổi giá trị của trường appname trong file engine.json thành tên của app mình, là ourrecommendation\n1nano engine.json 2 3{ 4 \u0026#34;id\u0026#34;: \u0026#34;default\u0026#34;, 5 \u0026#34;description\u0026#34;: \u0026#34;Default settings\u0026#34;, 6 \u0026#34;engineFactory\u0026#34;: \u0026#34;org.example.recommendation.RecommendationEngine\u0026#34;, 7 \u0026#34;datasource\u0026#34;: { 8 \u0026#34;params\u0026#34; : { 9 \u0026#34;appName\u0026#34;: \u0026#34;ourrecommendation\u0026#34; 10 } 11 }, 12 \u0026#34;algorithms\u0026#34;: [ 13 { 14 \u0026#34;name\u0026#34;: \u0026#34;als\u0026#34;, 15 \u0026#34;params\u0026#34;: { 16 \u0026#34;rank\u0026#34;: 10, 17 \u0026#34;numIterations\u0026#34;: 20, 18 \u0026#34;lambda\u0026#34;: 0.01, 19 \u0026#34;seed\u0026#34;: 3 20 } 21 } 22 ] 23} Một lưu ý quang trọng là giá trị \u0026ldquo;org.example.recommendation.RecommendationEngine\u0026rdquo; trong \u0026ldquo;engineFactory\u0026rdquo; là của hệ thống. Và bạn đừng sửa, thay đổi chúng. Nói chung là ngoài giá trị của \u0026ldquo;appName\u0026rdquo; ra, bạn không nên thay đổi bất kỳ thức gì khác trong file engine.json.\nSau khi import file thành công. Chúng ta sẽ build app. Lệnh build có tác dụng kiểm tra lại hệ thống đã được cấu hình đúng và đủ chưa.\n1pio build Nếu build thành công, chúng ta sẽ thấy dòng chữ này.\n1 2[INFO] [Engine$] Build finished successfully. 3[INFO] [Pio$] Your engine is ready for training. Sau khi build thành công, chúng ta sẽ tiến hành huấn luyện mô hình\n1pio build Và chờ đợi dòng này xuất hiện\n1 2[INFO] [CoreWorkflow$] Training completed successfully. Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"May 7, 2019","img":"","permalink":"/blog/2019-05-07-predictio-mini-demo/","series":null,"tags":["Machine Learning","Deep Learning","PredictionIO","Forecast"],"title":"PredictionIO Phần 2 - Cài Đặt Chương Trình Demo"},{"categories":null,"content":" 1. Dropout là gì, nó có ý nghĩa gì trong mạng neural network 2. Tạo sao chúng ta cần dropout 3. Dropout 4. Một số đặc điểm rút ra được khi huấn luyện nhiều mô hình khác nhau sử dụng dropout 5. Thực nghiệm trong keras 1. Dropout là gì, nó có ý nghĩa gì trong mạng neural network Theo Wikipedia, thuật ngữ \u0026ldquo;dropout\u0026rdquo; đề cập đến việc bỏ qua các đơn vị (unit) (cả hai hidden unit và visible unit) trong mạng neural network.\nHiểu đơn giản là, trong mạng neural network, kỹ thuật dropout là việc chúng ta sẽ bỏ qua một vài unit trong suốt quá trình train trong mô hình, những unit bị bỏ qua được lựa chọn ngẫu nhiên. Ở đây, chúng ta hiểu \u0026ldquo;bỏ qua - ignoring\u0026rdquo; là unit đó sẽ không tham gia và đóng góp vào quá trình huấn luyện (lan truyền tiến và lan truyền ngược).\nVề mặt kỹ thuật, tại mỗi giai đoạn huấn luyện, mỗi node có xác suất bị bỏ qua là 1-p và xác suất được chọn là p\n2. Tạo sao chúng ta cần dropout Giả sử rằng bạn hiểu hoàn toàn những gì đã nói ở phần 1, câu hỏi đặt ra là tại sao chúng ta cần đến dropout, tại sao chúng ta cần phải loại bỏ một vài các unit nào đó trong mạng neural network?\nCâu trả lời cho câu hỏi này là để chống over-fitting\nKhi chúng ta sử dụng full connected layer, các neural sẽ phụ thuộc \u0026ldquo;mạnh\u0026rdquo; lẫn nhau trong suốt quá trình huấn luyện, điều này làm giảm sức mạng cho mỗi neural và dẫn đến bị over-fitting tập train.\n3. Dropout Đọc đến đây, bạn đã có một khái niệm cơ bản về dropout và động lực - động cơ để chúng ta sử dụng nó. Nếu bạn chỉ muốn có cái nhìn tổng quan về dropout trong neural network, hai sections trên đã cung cấp đầy đủ thông tin cho bạn, bạn có thể dừng tại đây. Phần tiếp theo, chúng ta sẽ nói kỹ hơn về mặt kỹ thuật của dropout.\nTrước đây, trong machine learning, người ta thường sử dụng regularization để ngăng chặn over-fititng. Regularization làm giảm over-fitting bằng cách thêm yếu tố \u0026ldquo;phạt\u0026rdquo; vào hàm độ lỗi (loss function). Bằng việc thêm vào điểm phạt này, mô hình được huấn luyện sẽ giúp các features weights giảm đi sự phụ thuộc lẫn nhau. Đối với những ai đã sử dụng Logistic Regression rồi thì sẽ không xa lạ với thuật ngữ phạt L1(Laplacian) và L2 (Gaussian).\nDropout là một kỹ thuật khác, một cách tiếp cận khác để regularization trong mạng neural netwoks.\nKỹ thuật dropout được thực hiện như sau:\nTrong pha train: với mỗi hidden layer, với mỗi trainning sample, với mỗi lần lặp, chọn ngẫu nhiên p phần trăm số node và bỏ qua nó (bỏ qua luôn hàm kích hoạt cho các node bị bỏ qua).\nTrong pha test: Sử dụng toàn bộ activations, nhưng giảm chúng với tỷ lệ p (do chúng ta bị miss p% hàm activation trong quá trình train).\nMô tả về kiến trúc mạng có và không có dropout\n4. Một số đặc điểm rút ra được khi huấn luyện nhiều mô hình khác nhau sử dụng dropout Dropout ép mạng neural phải tìm ra nhiều robust features hơn, với đặc điểm là chúng phải hữu ích hơn, tốt hơn, ngon hơn khi kết hợp với nhiều neuron khác.\nDropout đòi hỏi phải gấp đôi quá trình huấn luyện để đạt được sự hội tụ. Tuy nhiên, thời gian huấn luyện cho mỗi epoch sẽ ít hơn.\nVới H unit trong mô hình, mỗi unit đều có xác xuất bị bỏ qua hoặc được chọn, chúng ta sẽ có 2^H mô hình có thể có. Trong pha test, toàn bộ network được sử dụng và mỗi hàm activation được giảm đi với hệ số p.\nMột số nghiên cứu chỉ ra rằng, khi sử dụng Dropout và Batch Normalization (BN) cùng nhau thì kết quả rất tệ, trong cả lý thuyết và thực nghiệm, ví dụ nghiên cứu ở papper \u0026ldquo;Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift\u0026rdquo;, nguồn https://arxiv.org/abs/1801.05134, nhóm tác giả giải thích về mặt lý thuyết rằng: \u0026ldquo;đối với một neural, Dropout sẽ thay đổi phương sai của nó khi chúng ta chuyển trạng thái từ trian sang test. Còn BN thì không, BN vẫn tích luỹ đầy đủ thông tin trong quá trình huấn luyện. Do Dropout làm thay đổi phương sai nên sẽ xảy ra hiện tượng không đồng nhất về phương sai, dẫn đến hành vi suy luận không chắc chắn dẫn đến suy luận bị sai nhiều. Đặc biệt là khi kết hợp dropout và BN thì khiến cho suy luận càng sai lầm trầm trọng. \u0026ldquo;. Cho nên, trong một số trường hợp/bài toán chúng ta có thể dùng Dropout, trong một số trường hợp/ bài toán, người ta sử dụng BN và không sử dụng dropout.\nNgười ta thường dùng hệ số dropout là 0.5. Lý giải cho việc này, bạn có thể đọc bài báo http://papers.nips.cc/paper/4878-understanding-dropout.pdf. Nói nôm là việc sử dụng giảm 50% của dropout giúp kết quả đạt được là tốt nhất so với các phương pháp chuẩn hoá khác.\n5. Thực nghiệm trong keras Những vấn đề nói ở trên chỉ là lý thuyết. Bây giờ chúng ta sẽ bắt tay vào làm thực tế. Để xem thử dropout hoạt động như thế nào, chúng ta sẽ xây dựng mô hình deep net sử dụng keras và sử dụng tập dữ liệu cifar-10. Mô hình chúng ta xây dựng có 3 hidden layer với kích thước lần lượt là 64, 128, 256 và 1 full connected layer có kích thước 512 và output layer có kích thước 10 (do mình có 10 lớp).\nChúng ta sử dụng hàm kích hoạt là ReLU trên các hidden layer và sử dụng hàm sigmoid trên output layer. Sử dụng hàm lỗi categorical cross-entropy.\nTrong trường hợp mô hình có sử dụng dropout, chúng ta sẽ set dropout ở tất cả các layer và thay đổi tỷ lệ dropout nằm trong khoảng từ 0.0 đến 0.9 với bước nhảy là 0.1.\nMô hình setup với số epochs là 20. Bắt đầu xem nào.\nĐầu tiên, chúng ta sẽ load một vài thư viện cần thiết\n1import numpy as np 2import os 3 4import keras 5 6from keras.datasets import cifar10 7from keras.models import Sequential 8from keras.layers import Dense, Dropout, Activation, Flatten 9from keras.layers import Convolution2D, MaxPooling2D 10from keras.optimizers import SGD 11from keras.utils import np_utils 12from keras.preprocessing.image import ImageDataGenerator 13import matplotlib.pyplot as plt 14 15from pylab import rcParams 16rcParams[\u0026#39;figure.figsize\u0026#39;] = 20, 20 17 18from keras.datasets import cifar10 19 20(X_train, y_train), (X_test, y_test) = cifar10.load_data() 21 22 23print(\u0026#34;Training data:\u0026#34;) 24print(\u0026#34;Number of examples: \u0026#34;, X_train.shape[0]) 25print(\u0026#34;Number of channels:\u0026#34;,X_train.shape[3]) 26print(\u0026#34;Image size:\u0026#34;,X_train.shape[1], X_train.shape[2], X_train.shape[3]) 27 28print(\u0026#34;Test data:\u0026#34;) 29print(\u0026#34;Number of examples:\u0026#34;, X_test.shape[0]) 30print(\u0026#34;Number of channels:\u0026#34;, X_test.shape[3]) 31print(\u0026#34;Image size:\u0026#34;,X_test.shape[1], X_test.shape[2], X_test.shape[3]) Kết quả\n1Training data: 2Number of examples: 50000 3Number of channels: 3 4Image size: 32 32 3 5Test data: 6Number of examples: 10000 7Number of channels: 3 8Image size: 32 32 3 Chúng ta có 50000 hình train, và 10000 hình test. Mỗi hình là một ảnh RGB có kích thước 33x32x3 pixel.\ndataset cifar 10\nTiếp theo, chúng ta sẽ chuẩn hoá dữ liệu. Đây là 1 bước quan trọng trước khi huấn luyện mô hình\n1print( \u0026#34;mean before normalization:\u0026#34;, np.mean(X_train)) 2print( \u0026#34;std before normalization:\u0026#34;, np.std(X_train)) 3 4mean=[0,0,0] 5std=[0,0,0] 6newX_train = np.ones(X_train.shape) 7newX_test = np.ones(X_test.shape) 8for i in range(3): 9 mean[i] = np.mean(X_train[:,i,:,:]) 10 std[i] = np.std(X_train[:,i,:,:]) 11 12for i in range(3): 13 newX_train[:,i,:,:] = X_train[:,i,:,:] - mean[i] 14 newX_train[:,i,:,:] = newX_train[:,i,:,:] / std[i] 15 newX_test[:,i,:,:] = X_test[:,i,:,:] - mean[i] 16 newX_test[:,i,:,:] = newX_test[:,i,:,:] / std[i] 17 18 19X_train = newX_train 20X_test = newX_test 21 22print(\u0026#34;mean after normalization:\u0026#34;, np.mean(X_train)) 23print(\u0026#34;std after normalization:\u0026#34;, np.std(X_train)) 1mean before normalization: 120.70756512369792 2std before normalization: 64.1500758911213 3mean after normalization: 0.9062499999999979 4std after normalization: 0.4227421643271468 Full code đoạn huấn luyện\n1 2 3# In[3]:Specify Training Parameters 4 5batchSize = 512 #-- Training Batch Size 6num_classes = 10 #-- Number of classes in CIFAR-10 dataset 7num_epochs = 100 #-- Number of epochs for training 8learningRate= 0.001 #-- Learning rate for the network 9lr_weight_decay = 0.95 #-- Learning weight decay. Reduce the learn rate by 0.95 after epoch 10 11 12img_rows, img_cols = 32, 32 #-- input image dimensions 13 14Y_train = np_utils.to_categorical(y_train, num_classes) 15Y_test = np_utils.to_categorical(y_test, num_classes) 16 17 18 19batchSize = 512 #-- Training Batch Size 20num_classes = 10 #-- Number of classes in CIFAR-10 dataset 21num_epochs = 100 #-- Number of epochs for training 22learningRate= 0.001 #-- Learning rate for the network 23lr_weight_decay = 0.95 #-- Learning weight decay. Reduce the learn rate by 0.95 after epoch 24 25 26img_rows, img_cols = 32, 32 #-- input image dimensions 27 28Y_train = np_utils.to_categorical(y_train, num_classes) 29Y_test = np_utils.to_categorical(y_test, num_classes) 30 31 32# In[4]:VGGnet-10 33 34 35from keras.layers import Conv2D 36import copy 37result = {} 38y = {} 39loss = [] 40acc = [] 41dropouts = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] 42for dropout in dropouts: 43 print(\u0026#34;Dropout: \u0026#34;, (dropout)) 44 model = Sequential() 45 46 #-- layer 1 47 model.add(Conv2D(64, (3, 3), 48 border_mode=\u0026#39;valid\u0026#39;, 49 input_shape=( img_rows, img_cols,3))) 50 model.add(Dropout(dropout)) 51 model.add(Conv2D(64, (3, 3))) 52 model.add(Dropout(dropout)) 53 model.add(Activation(\u0026#39;relu\u0026#39;)) 54 model.add(MaxPooling2D(pool_size=(2, 2))) 55 56 ##--layer 2 57 model.add(Conv2D(128, (3, 3))) 58 model.add(Dropout(dropout)) 59 model.add(Activation(\u0026#39;relu\u0026#39;)) 60 model.add(MaxPooling2D(pool_size=(2, 2))) 61 62 ##--layer 3 63 model.add(Conv2D(256, (3, 3))) 64 model.add(Dropout(dropout)) 65 model.add(Activation(\u0026#39;relu\u0026#39;)) 66 model.add(MaxPooling2D(pool_size=(2, 2))) 67 68 ##-- layer 4 69 model.add(Flatten()) 70 model.add(Dense(512)) 71 model.add(Activation(\u0026#39;relu\u0026#39;)) 72 73 #-- layer 5 74 model.add(Dense(num_classes)) 75 76 #-- loss 77 model.add(Activation(\u0026#39;softmax\u0026#39;)) 78 79 sgd = SGD(lr=learningRate, decay = lr_weight_decay) 80 model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, 81 optimizer=\u0026#39;sgd\u0026#39;, 82 metrics=[\u0026#39;accuracy\u0026#39;]) 83 84 model_cce = model.fit(X_train, Y_train, batch_size=batchSize, epochs=20, verbose=1, shuffle=True, validation_data=(X_test, Y_test)) 85 score = model.evaluate(X_test, Y_test, verbose=0) 86 y[dropout] = model.predict(X_test) 87 print(\u0026#39;Test score:\u0026#39;, score[0]) 88 print(\u0026#39;Test accuracy:\u0026#39;, score[1]) 89 result[dropout] = copy.deepcopy(model_cce.history) 90 loss.append(score[0]) 91 acc.append(score[1]) 92 93 94 95# In[5]: plot dropout 96import numpy as np 97import matplotlib.pyplot as plt 98 99width = 0.1 100 101plt.bar(dropouts, acc, width, align=\u0026#39;center\u0026#39;) 102 103plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;major\u0026#39;, labelsize=35) 104plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;minor\u0026#39;, labelsize=35) 105 106plt.ylabel(\u0026#39;Accuracy\u0026#39;,size = 30) 107plt.xlabel(\u0026#39;Dropout\u0026#39;, size = 30) 108plt.show() 109 110 111# In[6]: plot non drop out 112 113import numpy as np 114import matplotlib.pyplot as plt 115 116width = 0.1 117 118plt.bar(dropouts, loss, width, align=\u0026#39;center\u0026#39;,color = \u0026#39;green\u0026#39;) 119 120plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;major\u0026#39;, labelsize=35) 121plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;minor\u0026#39;, labelsize=35) 122 123plt.ylabel(\u0026#39;Loss\u0026#39;,size = 30) 124plt.xlabel(\u0026#39;Dropout\u0026#39;, size = 30) 125plt.show() Kết quả\nNhìn hình kết quả ở trên, chúng ta có một số kết luận nhỏ như sau:\nGiá trị dropout tốt nhất là 0.2, khoảng dropout cho giá trị chấp nhận được là nằm trong đoạn từ 0 đến 0.5. Nếu dropout lớn hơn 0.5 thì kết quả hàm huấn luyện trả về khá tệ.\nGiá trị độ chính xác còn khá thấp =\u0026gt; 20 epochs là chưa đủ, cần huấn luyện nhiều hơn nữa.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"May 5, 2019","img":"","permalink":"/blog/2019-05-05-deep-learning-dropout/","series":null,"tags":["machine learning","deep learning","dropout","deep net"],"title":"Tìm Hiểu Về Dropout Trong Deep Learning, Machine Learning"},{"categories":null,"content":" 1. Giới thiệu về PredictionIO 2. Cơ chế hoạt động của PredictionIO Event Server Engine 3. Cài đặt PredictionIO trên môi trường Ubuntu Download và build Apache Prediction IO Biên dịch Prediction IO Download và giải nén các Dependencies Cấu hình chương trình 4.Khởi chạy hệ thống 1. Giới thiệu về PredictionIO PredictionIO là một \u0026ldquo;open source Machine Learning Server built on top of a state-of-the-art open source stack\u0026rdquo; giúp cho các developers và các data scientists tạo ra các engine dự đoán trong học máy. PredictionIO giúp chúng ta\nXây dựng và triển khai các ứng dụng, dịch vụ một cách nhanh chóng bằng cách tuỳ chỉnh lại các template đã sẵn có.\nTrả lời các câu truy vấn động trong thời gian thực.\nhuấn luyện và so sánh/đánh giá nhiều mô hình khác nhau dễ dàng.\nHợp nhất hoá dữ liệu từ nhiều nền tảng khác nhau hoặc trong thời gian thực để thực hiện phân tích dự đoán.\nHỗ trợ các thư viện máy học và xử lý dữ liệu như Spark MLLib và OpenNLP\nTự xây dựng, triển khai, customize một mô hình machine learning\n2. Cơ chế hoạt động của PredictionIO PredictionIO bao gồm các thành phần sau:\nPredictionIO platform: là nền tảng open source được apache xây dựng sẵn giúp chúng ta triển khai, xây dựng, đánh giá các mô hình máy học.\nEvent Server: là nơi giúp chúng ta chuẩn hoá các sự kiện từ nhiều nguồn khác nhau\nTemplate Gallery: là nơi chúng ta download các engine template máy học về. PredictionIO hỗ trợ cho chúng ta rất nhiều template mẫu khác nhau. Chúng ta sẽ lần lượt tìm hiểu và implement ở các bài viết tiếp theo.\nEvent Server PredictionIO Event Server chịu trách nhiệu thu thập dữ liệu từ các ứng dụng của bạn. Bạn có thể nhìn kỹ hơn ở hình bên dưới, các ứng dụng web, mobile app \u0026hellip; khi người dùng tương tác sẽ phát sinh các sự kiện (Event Data), ví dụ sự kiện người dùng thêm 1 đơn hàng vào giỏ hàng, người dùng xem sản phẩn A, người dùng xem sản phẩm C sau khi xem sản phẩm A\u0026hellip; Event Server sẽ ghi nhận lại đống dữ liệu này, chuẩn hoá lại. PredictionIO engine sau đó sẽ xây dựng mô hình dự đoán dựa trên các dữ liệu chúng ta thu thập được. Sau khi bạn có được mô hình tối ưu, chúng ta sẽ deploy các predict webservice, lắng nghe các truy vấn từ các ứng dụng và trả về kết quả trong thời gian thực.\nHình 1: Event server trong predictionio\nEvent Server sẽ thu thập dữ liệu của bạn trong thời gian thực hoặc theo chu kỳ. Sau đó, nó sẽ chuẩn hoá dữ liệu hỗn độn của bạn từ nhiều nguồn khác nhau thành một dạng chuẩn chung. Event Server chủ yếu phục vụ hai mục đính chính:\nCung cấp dữ liệu cho các engine để huấn luyện và đánh giá\nCung cấp dữ liệu dạng chuẩn để data analysis\nCũng giống như một database server, Event Server có thể được sử dụng để phục vụ cho nhiều ứng dụng khác nhau. Dữ liệu được phân tách cho các ứng dụng bằng \u0026ldquo;app_name\u0026rdquo; duy nhất. Cái này sẽ nói lại lúc xây dựng ứng dụng ở bên dưới.\nKhi một Event Server được triển khai, bạn có thể gửi dữ liệu cho một \u0026lsquo;app_name\u0026rsquo; cụ thể nào đó, app-name được định danh bằng access key. Dữ liệu được gửi đến Event Server sử dụng EventAPI sử dụng giao thức http (tham khảo thêm ở https://predictionio.apache.org/datacollection/eventapi/) hoặc sử dụng các PredictionIO SDK. Tham khảo thêm các SDK ở https://predictionio.apache.org/sdk/.\nTrong một số trường hợp, bạn muốn engine đọc dữ liệu từ một datastore nào đó thay vì Event Server. Bạn có thể thực hiện thông qua hướng dẫn ở https://predictionio.apache.org/start/customize/\nEngine Engine là nơi chịu trách nhiệu đưa ra các quyết định. Nó gồm một hoặc nhiều thuật toán học máy học khác nhau. Các Engine sẽ huấn luyện dữ liệu và xây dựng các mô hình dự đoán. Sau đó sẽ phát triển thành các webservice. Các webservice sẽ nhận các truy vấn từ ứng dụng, dự đoán và trả về kết quả cho ứng dụng.\nPredictionIO\u0026rsquo;s cung cấp cho chúng ta rất nhiều các template khác nhau đáp ứng gần như là đẩy đủ các mô hình máy học mà chúng ta cần. Bạn có thể dễ dàng tạo một mô hình máy học từ các template. Các thành phần của một template dược đặt tên là Data Source, Data Preparator, Algorithm(s), Serving, các bạn có thể dễ dàng customize lại tuỳ thuộc nhu cầu của bạn.\n3. Cài đặt PredictionIO trên môi trường Ubuntu Trong thời đại docker, các bạn có thể cài đặt PredictionIO dựa vào các docker được xây dựng sẵn đầy rẫy trên mạng, chúng giúp bạn đỡ tốn công sức hơn. Tuy nhiên, trong bài viết này, mình sẽ cài đặt từng thành phần PredictiIO trên ubuntu, không sử dụng docker.\nDownload và build Apache Prediction IO Chúng ta sẽ download Prediction IO từ trang github chính chủ. Phiên bản hiện tại là 0.14.0. Các bạn có thể lưu dữ liệu ở đâu tuỳ ý các bạn. Mình lưu ở thư mục /data/pio. Và trong suốt bài viết này, mình sẽ lưu các thứ liên quan trong thư mục /data/pio. Các bạn có cài đặt theo hướng dẫn của mình thì nhớ sửa lại cho đúng đường dẫn của các bạn. Chúng ta sẽ clone nguồn từ trang github predictionio. và sẽ switch qua branch release. Đây là branch chính thành phẩm, các branch khác đang trong giai đoạn phát triển nên có thể build không được. Lúc các bạn làm có thể nó đã phát triển lên bản 15, 16 hoặc 1.0 gì đó rồi. Các bạn cứ tự tin sử dụng phiên bản mới nhất.\n1git clone https://github.com/apache/predictionio.git 2git checkout release/0.14.0 Biên dịch Prediction IO Sau khi tải về bộ nguồn của Prediction IO, chúng ta sẽ tiền hành biên dịch. Quá trình biên dịch sẽ xảy ra khá lâu, các bạn kiên nhẫn chờ đợi\n1cd predictionio 2./make-distribution.sh Kết thúc quá trình biên dịch, các bạn sẽ thấy dòng chữ\n1PredictionIO binary distribution created at PredictionIO-0.14.0.tar.gz Vậy là chúng ta đã thành công. Việc tiếp theo là giải nén file PredictionIO-0.14.0.tar.gz để sử dụng\n1tar xvzf PredictionIO-0.14.0.tar.gz -C /data/pio Nhắc lại 1 lần nữa là do thời điểm hiện tại mình viết bài viết này, PredictionIO mới release bản 0.14.0 nên file tập tin sẽ là PredictionIO-0.14.0.tar.gz. Các bạn nhớ giải nén đúng với tên file ứng với phiên bản PredictionIO tương ứng nhé.\nDownload và giải nén các Dependencies Mình sẽ sử dụng Spark, ElasticSearch, Hbase và zookeeper, nên mình download hết về. Mình có thói quen sử dụng phiên bản mới nhất. Nên mình lên trang chủ và lấy link download mới nhất của chúng thôi. Tất cả các Dependencies mình dùng đều được bỏ vào trong thư mục vendors\n1 2cd PredictionIO-0.14.0 3mkdir vendors 4cd vendors 5wget https://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz 6 7wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.9.tar.gz 8 9wget https://www.apache.org/dyn/closer.lua/hbase/2.1.4/hbase-2.1.4-bin.tar.gz 10 11wget https://www-us.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz 12 13tar xvzf spark-2.4.2-bin-hadoop2.7.tgz 14 15tar xvzf elasticsearch-5.6.9.tar.gz 16 17tar xvzf hbase-2.1.4-bin.tar.gz 18 19tar xvzf zookeeper-3.4.14/zookeeper-3.4.14.tar.gz Cấu hình chương trình Cấu hình dependency Chúng ta sẽ cấu hình một chút để PredictionIO nhận ra các dependency của mình và cấu hình các dependency\nĐầu tiên, chúng ta sẽ chỉnh sửa file hbase-site.xml của HBase\n1nano /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/conf/hbase-site.xml Thay đoạn\n1\u0026lt;configuration\u0026gt; 2\u0026lt;/configuration\u0026gt; bằng đoạn\n1\u0026lt;configuration\u0026gt; 2 \u0026lt;property\u0026gt; 3 \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; 4 \u0026lt;value\u0026gt;file:///data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4\u0026lt;/value\u0026gt; 5 \u0026lt;/property\u0026gt; 6 \u0026lt;property\u0026gt; 7 \u0026lt;name\u0026gt;hbase.zookeeper.property.dataDir\u0026lt;/name\u0026gt; 8 \u0026lt;value\u0026gt;/data/pio/PredictionIO-0.14.0/vendors/zookeeper-3.4.14\u0026lt;/value\u0026gt; 9 \u0026lt;/property\u0026gt; 10\u0026lt;/configuration\u0026gt; Tiếp theo, chúng ta sẽ add đường dẫn java cho hbase\n1nano /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/conf/hbase-env.sh Thêm đoạn\n1 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ các bạn hãy thay đường dẫn java tương ứng với đường dẫn trong máy bạn. Nếu chưa có java thì các bạn hãy cài vào, nếu các bạn đã cài java mà không biết nó nằm ở đâu, các bạn có thể gọi lệnh bên dưới để xem đường dẫn\n1update-alternatives --config java Để chắc chắn rằng trong máy của bạn có cài java bạn hãy gọi lện java -version\nVí dụ trong máy mình\n1$java -version 2openjdk version \u0026#34;1.8.0_191\u0026#34; 3OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12) 4OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) Các bạn cố gắng sử dụng phiên bản java mới nhất. Nó sẽ tương thích tốt hơn với phiên bản mới nhất của HBase, hoặc đọc phiên bản java đề nghị trong trang chủ HBase. Tránh trường hợp sử dụng phiên bản java quá cũ HBase không hỗ trợ.\nCấu hình Prediction IO Chỉnh sửa file pio-env.sh.\n1 2nano /data/pio/PredictionIO-0.14.0/conf/pio-env.sh Mặc định PredictionIO sử dụng PosgresSQl làm event server. Mình không dùng nó mà thay thế bằng HBASE và ELASTICSEARCH.\nMột số thay đổi mình sẽ liệt kê bên dưới\n1SPARK_HOME=$PIO_HOME/vendors/spark-2.3.2-bin-hadoop2.7 2 3HBASE_CONF_DIR=$PIO_HOME/vendors/hbase-2.1.4/conf 4 5PIO_STORAGE_REPOSITORIES_METADATA_NAME=pio_meta 6PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH 7 8PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=pio_event 9PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE 10 11PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_model 12PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS 13 14#Comment các dòng này lại, do không dùng postgres 15# PIO_STORAGE_SOURCES_PGSQL_PASSWORD accordingly 16# PIO_STORAGE_SOURCES_PGSQL_TYPE=jdbc 17# PIO_STORAGE_SOURCES_PGSQL_URL=jdbc:postgresql://localhost/pio 18# PIO_STORAGE_SOURCES_PGSQL_USERNAME=pio 19# PIO_STORAGE_SOURCES_PGSQL_PASSWORD=pio 20 21PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME=$PIO_HOME/vendors/elasticsearch-5.6.9 22PIO_STORAGE_SOURCES_HBASE_HOME=$PIO_HOME/vendors/hbase-2.1.4 4.Khởi chạy hệ thống Chúng ta sẽ add path của PredictIO vào biến môi trường để sử dụng cho các lần sau\n1 2nano ~/.bashrc 3erport PATH=/data/pio/PredictionIO-0.14.0/bin:$PATH Hoặc có thể add path trong mỗi session\n1PATH=$PATH:/data/pio/PredictionIO-0.14.0/bin; export PATH Tiếp theo, chúng ta sẽ cấp quyền cho thư mục PredictionIO\n1sudo chmod -R 775 /data/pio Nếu không cấp quyền write cho thư mục thì PredictionIO không thể write log file được.\nChạy PredictionIO Server bằng cách gọi câu lệnh\n1pio-start-all Kết quả\n1Stopping PredictionIO Event Server... 2Stopping HBase... 3stopping hbase............. 4Stopping Elasticsearch... 5tgdd@U1604:/data/pio/PredictionIO-0.14.0/bin$ pio-start-all 6Starting Elasticsearch... 7Starting HBase... 8running master, logging to /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/bin/../logs/hbase-tgdd-master-U1604.out 9Waiting 10 seconds for Storage Repositories to fully initialize... 10Starting PredictionIO Event Server... Để kiểm tra hệ thống khi start có lỗi lầm gì không, chúng ta sử dụng lệnh\n1pio status Kết quả\n1[INFO] [Management$] Inspecting PredictionIO... 2[INFO] [Management$] PredictionIO 0.14.0 is installed at /data/pio/PredictionIO-0.14.0 3[INFO] [Management$] Inspecting Apache Spark... 4[INFO] [Management$] Apache Spark is installed at /data/spark-2.3.2-bin-hadoop2.7 5[INFO] [Management$] Apache Spark 2.3.2 detected (meets minimum requirement of 2.0.2) 6[INFO] [Management$] Inspecting storage backend connections... 7[INFO] [Storage$] Verifying Meta Data Backend (Source: ELASTICSEARCH)... 8[INFO] [Storage$] Verifying Model Data Backend (Source: LOCALFS)... 9[INFO] [Storage$] Verifying Event Data Backend (Source: HBASE)... 10[INFO] [Storage$] Test writing to Event Store (App Id 0)... 11[INFO] [HBLEvents] The table pio_event:events_0 doesn\u0026#39;t exist yet. Creating now... 12[INFO] [HBLEvents] Removing table pio_event:events_0... 13[INFO] [Management$] Your system is all ready to go. Bạn thấy dòng chữ [INFO] [Management$] Your system is all ready to go. thì yên tâm, hệ thống đã chạy thành công.\nĐể stop hệ thống, các bạn gọi lệnh\n1pio-stop-all Kết quả khi stop\n1Stopping PredictionIO Event Server... 2Stopping HBase... 3stopping hbase............. 4Stopping Elasticsearch... Vậy là chúng ta đã tiến hành cài đặt thành công PredictionIO Server rồi. Hẹn gặp bạn ở bài thứ hai, cài đặt các template cho PredictionIO và tiến hành dự đoán.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"May 4, 2019","img":"","permalink":"/blog/2019-05-04-setup-predictio/","series":null,"tags":["machine learning","deep learning","PredictionIO","forecast","dự đoán"],"title":"PredictionIO Phần 1 - Hướng Dẫn Cài Đặt"},{"categories":null,"content":" Lấy mẫu ngẫu nhiên Lấy mẫu phi ngẫu nhiên Lấy mẫu dữ liệu là một kỹ thuật rất quang trọng trong thống kê, là yếu tố quan trọng góp phần xác định độ chính xác của research/ survey. Nếu có bất kỳ sai sót gì trong quá trình lấy mẫu, nó sẽ ảnh hưởng trực tiếp đến kết quả cuối cùng. Có rất nhiều kỹ thuật giúp chúng ta thu thập mẫu dựa trên nhu cầu và tình huống chúng ta cần. Bài viết này sẽ giải thích một số kỹ thuật phổ biến nhất.\nĐể bắt đầu bài viết, chúng ta sẽ làm rõ mốt số khái niệm cơ bản là Quần thể - Population,mẫu - Sample và lấy mẫu - sampling\nQuần thể - population là tập hợp của các cá thể có một hoặc một số đặc điểm chung. Kích thước của một quần thể là số lượng cá thể trong quần thể đó.\nMẫu - sample là một tập con của quần thể. Quá trình chọn một mẫu được gọi là lấy mẫu -sampling. Kích thước mẫu là số lượng cá thể trong tập mẫu.\nHình 1: Ví dụ về lấy mẫu dữ liệu\nCó rất nhiều kỹ thuật lấy mẫu dữ liệu khác nhau, nhưng chúng ta có thể gom chúng vào 2 nhóm chính:\nLấy mẫu ngẫu nhiên - Probability Sampling\nLấy mẫu phi ngẫu nhiên - non-probability sampling\nHình 2: Ví dụ so về lấy mẫu ngẫu nhiên và lấy mẫu phi ngẫu nhiên\nSự khác biệt của hai nhóm trên là phương pháp lấy mẫu có sử dụng \u0026ldquo;hàm ngẫu nhiên\u0026rdquo; hay không. Với việc sử dụng hàm ngẫu nhiên, mỗi cá thể đều có cơ hội được lựa chọn ngang nhau và đều có cơ hội là một cá thể trong tập mẫu.\nLấy mẫu ngẫu nhiên Những thuật toán trong nhóm này sử dụng hàm \u0026ldquo;ngẫu nhiên\u0026rdquo; để đảm bảo rằng mọi phần tử đều có cơ hội lựa chọn ngang nhau. Một tên khác của phương pháp này là random sampling.\nMột số phương pháp thuộc nhóm này\nSimple Random Sampling\nStratified sampling\nSystematic sampling\nCluster Sampling\nMulti stage Sampling\nSimple Random Sampling Mỗi cá thể đều có cơ hội lựa chọn ngang nhau vào tập mẫu. Phương pháp này được sử dụng khi chúng ta không có bất kỳ thông tin gì về tập population.\nVí dụ: Chọn ngẫu nhiên 20 sinh viên trong lớp học 50 sinh viên. Mỗi sinh viên đều có cơ hội được chọn ngang nhau là 1/50.\nStratified sampling Kỹ thuật này phân chia mỗi cá thể trong quần thể thành từng nhóm nhỏ dựa trên sự tương đồng (similarity), nghĩa là các cá thể trong cùng 1 nhóm sẽ đồng nhất với nhau về một khía cạnh nào đó, và sẽ không giống với các nhóm khác về khía cạnh đó. Và chúng ta sẽ chọn ngẫu nhiên các các thể trong mỗi nhóm. Ở phương pháp này, chúng ta cần thông tin cho trước về tập quần thể để tạo các nhóm con.\nHình 2: lấy mẫu Stratified sampling\nỞ ví dụ trên, chúng ta sẽ chia tập quần thể thành các nhóm con mặc áo đỏ, mặc áo xanh, mặc áo vàng (phải biết trước được trong quần thể thằng nào mặc áo màu gì). Sau đó sẽ lựa chọn ngẫu nhiên 2 các thể trong mỗi nhóm.\nCluster Sampling Toàn bộ tập quần thể sẽ được chia thành từ cụm hoặc thành từng phần. Sau đó chúng ta sẽ chọn ngẫu nhiên từng cụm. Tất cả các cá thể trong cụm đó sẽ được sử dụng làm tập mẫu. Các cụm được định danh dựa trên các yếu tố xác định trước. Ví dụ ở trong hình ở trên, các cụm được định danh dựa vào màu sắc của áo mà người đó mặc. Điểm khác biệt ở phương pháp này so với phương pháp ở trên là phương pháp ở trên lựa chọn ngẫu nhiên một số các cá thể trong mỗi cụm. Còn phương pháp này sẽ lựa chọn ngẫu nhiên các cụm, và chọn hết tất cả các các thể trong cụm đó.\nMột số chiến lược để lựa chọn cụm:\nSingle Stage Cluster Sampling: Các cụm được lựa chọn ngẫu nhiên\nHình 3: Single Stage Cluster Sampling\nTwo Stage Cluster Sampling: Ở phương pháp này, chúng ta sẽ lựa chọn ngẫu nhiên các cụm, sau đó, trong mỗi cụm, chúng ta sẽ lựa chọn ngẫu nhiên các cá thể trong mỗi cụm\nHình 4: Two Stage Cluster Sampling\nSystematic Clustering Ở phương pháp này, việc lựa chọn cá thể là có quy luật và không ngẫu nhiên, từ cá thể đầu tiên. Các cá thể của tập mẫu được chọn ra từ tập quần thể dựa vào một quy luật nào đó. Đầu tiên, tất cả các cá thể trong tập quần thể phải được xắp xếp có thứ tự. Sau đó chúng ta sẽ lựa chọn ngẫu nhiên cá thể đầu tiên (mỗi cá thể đều có xác suất ngang nhau ở đây), và sử dụng quy luật nào đó để rút ra các cá thể tiếp theo.\nHình 5: Systematic Clustering\nNhư ví dụ ở trên, chúng ta xắp xếp các nhân vật áo vàng, xanh, đỏ ngẫu nhiên tuỳ ý theo sự lựa chọn của người ta. Quy luật là cứ 4 người sẽ lấy người cuối. Ấn nút ngẫu nhiên \u0026hellip; ta được số 3. Vậy là cá thể đầu tiên là nhân vật ở vị trí số 3, tiếp theo sẽ là nhân vật ở vị trí 7, 11, 15,19, 5, \u0026hellip;\nMulti-Stage Sampling Phương pháp này là sự kết hợp của một hoặc nhiều phương pháp được mô tả ở trên.\nQuần thể được chia thành nhiều cụm (cluster) và mỗi cụm được chia vào từng nhóm con (subgrop - strata) dựa trên sự tương đồng =\u0026gt; chúng ta được một tập các cụm con được gọi là stratum. Chúng ta sẽ lựa nhọn một hoặc một vài strata trong stratum. Quá trình này sẽ được lặp đi lặp lại đến khi không còn cụm nào có thể phân chia được nữa.\nVí dụ, các quốc gia có thể được phân chia thành từng bang, thành phố, thành thị, nông thôn. Và tất cả các khu vực có cùng ký tự đầu có thể được gom lại thành với nhau tạo thành một strata.\nHình 6: Multi-Stage Sampling\nLấy mẫu phi ngẫu nhiên Những kỹ thuật nằm trong nhóm này không sử dụng hàm ngẫu nhiên. Kỹ thuật này phụ thuộc vào khả năng hiểu biết của các nhà nghiên cứu (researcher) trên tập quần thể họ đang có để chọn lựa cá thể cho tập mẫu. Kết quả của việc lấy mẫu có thể bị lệch.\nMột số phương pháp thuộc nhóm này là:\nConvenience Sampling\nPurposive Sampling\nQuota Sampling\nReferral /Snowball Sampling\nConvenience Sampling Các cá thể được chọn dựa trên tính khả dụng của dữ liệu. Phương pháp này được sử dụng khi tính khả dụng của dữ liệu là hiếm và tốn kém. Do vậy, chúng ta sẽ lựa chọn mẫu dựa trên sự tiện lợi.\nVí dụ, Các nhà nghiên cứu thường hay sử dụng phương pháp này trong các giai đoạn đầu của các nghiên cứu khảo sát, vì nó dễ dàng, nhanh chóng và cho ra kết quả nhanh.\nPurposive Sampling Phương pháp lấy mẫu này dựa trên mục đích của nghiên cứu. Chỉ chọn ra những cá thể trong quần thể phù hợp nhất với mục đích nghiên cứu .\nVí dụ: Nếu chúng ta muốn hiểu được \u0026ldquo;suy nghĩ của những người quan tâm đến bằng thạc sỹ\u0026rdquo; thì tiêu chí lựa chọn cá thể là những người say yes trong câu hỏi \u0026ldquo;bạn có hứng thú với bậc thạc sỹ trong lĩnh vực \u0026hellip; không?\u0026rdquo;. Những người say \u0026ldquo;No\u0026rdquo; sẽ bị loại khỏi tập mẫu của chúng ta.\nQuota Sampling Phương pháp lấy mẫu này phụ thuộc vào một số tiêu chuẩn thiết lập từ trước. Tỷ lệ của các nhóm cá thể trong tập mẫu phải giống hết trong tập quần thể. Các cá thể được chọn cho đến khi chúng đạt đúng tỷ lệ của một loại dữ liệu.\nVí dụ: Giả sử chúng ta biết rằng trên trái đất này có 6 tỷ người, và 45% trong số đó là nam giới và 55% là nữ giới. Vậy thì chúng ta sẽ lấy mẫu làm sao cho tập mẫu chúng ta cũng phản ánh số đó, nghĩa là trong tập mẫu có 1000 người thì 45% trong số 1000 người đó phải là nam và 55% trong số 1000 người đó là nữ.\nReferral /Snowball Sampling Kỹ thuật này được sử dụng khi chúng ta không biết gì về tập quần thể hoặc tập quần thể hiếm. Lúc đó chúng ta sẽ tìm ra cá thể đầu tiên trong quần thể, rồi nhờ cá thể đầu tiên đó gợi ý các cá thể tiếp theo với điều kiện thoả nhu cẫu lấy mẫu của nghiên cứu. Cứ tiếp tục như vậy thì kích thước của tập mẫu sẽ tăng lên theo cấp nhân như kích thước quả quả cầu tuyết, nên kỹ thuật này còn có tên gọi khác là Snowball Sampling.\nHình 7: Ví dụ về Snowball Sampling\nVí dụ: Trong tình huống, ngữ cảnh là bạn muốn làm 1 bài khảo sát về những người bị nhiễm HIV, những người này thường có khuynh hướng không cởi mở ở mức độ công cộng và khó cho chúng ta tiếp cận để thu thập thông tin trực tiếp từ họ.\nNhóm khảo sát sẽ tiến hành liên hệ 1 người nào đó mà họ biết hoặc người nào đó xung phong làm cầu nối với các người bị nhiễm và thu thập thông tin từ họ (những người bị nhiễn tin tưởng người được xung phong hơn nhóm khảo sát. Vì nhóm khảo sát là người lạ).\nHi vọng sau bài viết này, các bạn có thêm nhiều ý tưởng hơn nữa về việc lấy mẫu và các cách để lấy mẫu trong ứng dụng thực tế.\nBài viết được lược dịch và một số hình ảnh được lấy từ nguồn https://towardsdatascience.com/sampling-techniques-a4e34111d808\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"May 4, 2019","img":"","permalink":"/blog/2019-05-04-sampling-method/","series":null,"tags":["machine learning","deep learning","sampleing","Probability Sampling","non-probability sampling"],"title":"Các Kỹ Thuật Lấy Mẫu"},{"categories":null,"content":" 1.\tPhân nhóm dựa trên phương thức học a.\tHọc có giám sát Phân lớp Hồi quy b. Học không giám sát Phân cụm Luật kết hợp c.\tHọc bán giám sát d.\tHọc tăng cường 2.\tPhân nhóm dựa trên sự tương đồng a.\tCác thuật toán hồi quy (Regression Algorithms) b.\tThuật toán dựa trên mẫu (Instance-based Algorithms) c.\tThuật toán chuẩn hoá (Regularization Algorithms) d.\tThuật toán cây quyết định (Decision Tree Algorithms) e.\tThuật toán Bayes (Bayesian Algorithms) f.\tThuật toán phân cụm (Clustering Algorithms) g.\tCác thuật toán luật kết hợp (Association Rule Learning Algorithms) h.\tThuật toán mạng nơron nhân tạo (Artificial Neural Network Algorithms) i.\tThuật toán học sâu (Deep Learning Algorithms) j.\tNhóm thuật toán Giảm chiều dữ liệu (Dimensionality Reduction Algorithms) k.\tThuật toán tập hợp (Ensemble Algorithms) l.\tCác thuật toán khác Ở bài trước mình đã trình bày định nghĩa và một số ứng dụng của Máy học (Machine Learning – ML), phân biệt ML với Trí tuệ nhân tạo (Artificial Intelligence – AI) cũng như mối quan hệ giữa AI, ML và Big Data. Từ bài viết này trở đi mình sẽ tập trung viết về ML, các thuật toán, cách sử dụng công cụ kèm theo một vài demo nhỏ giúp bạn đọc dễ hình dung và áp dụng. Để mở đầu cho chuỗi bài viết sắp tới, hôm nay mình sẽ trình bày cách phân nhóm các thuật toán ML.\nVới đa số mọi người, trước khi bắt đầu giải quyết một vấn đề nào đó, việc đầu tiên là chúng ta sẽ tìm hiểu xem liệu có ai đã gặp vấn đề này hoặc vấn đề tương tự như vậy hay không và cách họ giải quyết thế nào. Sau khi nắm được thông tin khái quát, công việc kế tiếp là chọn lựa và điều chỉnh giải pháp sao cho phù hợp với vấn đề của bản thân. Trong trường hợp vấn đề còn quá mới mẻ thì chúng ta mới phải bắt tay làm từ đầu, điều này hầu như rất hiếm, đặc biệt là trong thời đại công nghệ này, khi mà chỉ bằng một cú nhấp chuột, hàng ngàn thông tin, tư liệu về đề tài chúng ta quan tâm sẽ xuất hiện. Cũng giống như thế, ML hiện đã được nghiên cứu rộng khắp, rất nhiều công trình khoa học, thuật toán được cho ra đời. Với người mới bắt đầu mà nói thì chúng ta chưa cần phải làm gì cả ngoài việc nắm được các thuật toán cơ bản, đặc điểm của chúng để khi đối diện với một bài toán cụ thể trong thực tế chúng ta có thể biết được mình nên lựa chọn thuật toán nào cho phù hợp đã là điều rất tốt rồi.\nMặc dù có rất nhiều thuật toán học nhưng dựa vào phương thức học (learning style) hoặc sự tương đồng (similarity) về hình thức hay chức năng mà chúng có thể được gom thành từng nhóm. Sau đây mình sẽ trình bày tổng quan cả hai cách phân nhóm thuật toán học này.\n1.\tPhân nhóm dựa trên phương thức học Xét theo phương thức học, các thuật toán ML được chia làm bốn nhóm, bao gồm “Học có giám sát” (Supervised Learning), “Học không giám sát” (Unsupervised Learning), “Học bán giám sát” (hay học kết hợp - Semi-supervised Learning) và “Học tăng cường” (Reinforcement Learning).\na.\tHọc có giám sát Học có giám sát hay còn gọi là học có thầy là thuật toán dự đoán nhãn (label)/đầu ra (output) của một dữ liệu mới dựa trên tập dữ liệu huấn luyện mà trong đó mỗi mẫu dữ liệu đều đã được gán nhãn như minh hoạ ở Hình 1. Khi đó, thông qua một quá trình huấn luyện, một mô hình sẽ được xây dựng để cho ra các dự đoán và khi các dự đoán bị sai thì mô hình này sẽ được tinh chỉnh lại. Việc huấn luyện sẽ tiếp tục cho đến khi mô hình đạt được mức độ chính xác mong muốn trên dữ liệu huấn luyện. Điều này cũng giống như khi chúng ta đi học trên lớp, ta biết câu trả lời chính xác từ giáo viên (tập dữ liệu có nhãn) và từ đó ta sẽ sửa chữa nếu làm sai. Học có giám sát là nhóm phổ biến nhất trong các thuật toán ML.\nHình 1: Supervised Learning Algorithms\nMột cách toán học, học có giám sát là khi chúng ra có một tập hợp biến đầu vào $ X={x_1,x_2,…,x_N} $ và một tập hợp nhãn tương ứng $ Y={y_1,y_2,…,y_N} $, trong đó $ x_i$, $y_i $ là các vector. Các cặp dữ liệu biết trước $( x_i, y_i ) \\in X \\times Y $ được gọi là tập dữ liệu huấn luyện (training data). Từ tập dữ liệu huấn luyện này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập X sang một phần tử (xấp xỉ) tương ứng của tập Y:\n$$ y_i \\approx f(x_i), \\forall i=1, 2, …, N $$\nMục đích là xấp xỉ hàm số $f$ thật tốt để khi có một dữ liệu x mới, chúng ta có thể tính được nhãn tương ứng của nó $y=f(x)$.\nVí dụ: Trong nhận dạng chữ số viết tay, ta có ảnh của hàng nghìn trường hợp ứng với mỗi chữ số được viết bởi nhiều người khác nhau. Ta đưa các bức ảnh này vào một thuật toán học và chỉ cho nó biết “mỗi bức ảnh tương ứng với chữ số nào”. Sau khi thuật toán tạo ra một mô hình, tức là một hàm số nhận đầu vào là một bức ảnh và cho ra kết quả là một chữ số. Khi nhận được một bức ảnh mới mà mô hình “chưa từng gặp qua” và nó sẽ dự đoán xem bức ảnh đó tương ứng với chữ số nào.\nHình 2: Ảnh minh hoạ cho tập dữ liệu chữ số viết tay - MNIST\nĐối với những ai sử dụng mạng xã hội Facebook thì khá quen thuộc với tính năng phát hiện khuôn mặt trong một bức ảnh, bản chất của thuật toán dò tìm các khuôn mặt này là một thuật toán học có giám sát với tập huấn luyện là vô số ảnh đã được gán nhãn là mặt người hay không phải mặt người.\nCác thuật toán học có giám sát còn được phân ra thành hai loại chính là phân lớp (Classification) và hồi quy (Regression).\nPhân lớp Một bài toán được gọi là phân lớp nếu các nhãn của dữ liệu đầu vào được chia thành một số hữu hạn lớp (miền giá trị là rời rạc). Chẳng hạn như tính năng xác định xem một email có phải là spam hay không của Gmail; xác định xem hình ảnh của con vật là chó hay mèo. Hoặc ví dụ nhận dạng ký số viết tay ở trên cũng thuộc bài toán phân lớp, bao gồm mười lớp ứng với các số từ 0 đến 9. Tương tự cho ví dụ nhận dạng khuôn mặt với hai lớp là phải và không phải khuôn mặt, …\nHồi quy Một bài toán được xem là hồi quy nếu nhãn không được chia thành các nhóm mà là một giá trị thực cụ thể (miền giá trị là liên tục). Hầu hết các bài toán dự báo (giá cổ phiếu, giá nhà, …) thường được xếp vào bài toán hồi quy. Ví như, nếu một căn nhà rộng 150 m^2, có 7 phòng và cách trung tâm thành phố 10 km sẽ có giá là bao nhiêu? Lúc này kết quả dự đoán sẽ là một số thực.\nNếu như phát hiện khuôn mặt là bài toán phân lớp thì dự đoán tuổi là bài toán hồi quy. Tuy nhiên dự đoán tuổi cũng có thể coi là phân lớp nếu ta cho tuổi là một số nguyên dương N và khi đó ta sẽ có N lớp khác nhau tính từ 1. Một số thuật toán nổi tiếng thuộc về nhóm học có giám sát như:\nPhân lớp: k-Nearest Neighbors, mạng nơron nhân tạo, SVM, …\nHồi quy: Linear Regression, Logistic Regression, …\nb. Học không giám sát Trái với Supervised learning, học không giám sát hay học không thầy là thuật toán dự đoán nhãn của một dữ liệu mới dựa trên tập dữ liệu huấn luyện mà trong đó tất cả các mẫu dữ liệu đều chưa được gán nhãn hay nói cách khác là ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào như minh hoạ ở Hình 3. Điều này cũng giống như khi ta học mà không có thầy cô, sẽ không ai cho ta biết đáp án đúng là gì.\nHình 3: Unsupervised Learning Algorithms\nKhi đó, mục tiêu của thuật toán unsupervised learning không phải là tìm đầu ra chính xác mà sẽ hướng tới việc tìm ra cấu trúc hoặc sự liên hệ trong dữ liệu để thực hiện một công việc nào đó, ví như gom cụm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán.\nCác bài toán Unsupervised learning tiếp tục được chia nhỏ thành hai loại là phân cụm (Clustering) và luật kết hợp (Association Rule).\nPhân cụm Một bài toán phân cụm / phân nhóm toàn bộ dữ liệu X thành các nhóm/cụm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Chẳng hạn như phân nhóm khách hàng dựa vào độ tuổi, giới tính. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình dạng và màu sắc khác nhau, có thể là tam giác, vuông, tròn với màu xanh, đỏ, tím, vàng, sau đó yêu cầu trẻ phân chúng thành từng nhóm. Mặc dù ta không dạy trẻ mảnh nào tương ứng với hình nào hoặc màu nào, nhưng nhiều khả năng trẻ vẫn có thể phân loại các mảnh ghép theo màu sắc hoặc hình dạng.\nLuật kết hợp Là bài toán mà khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví như những khách hàng mua mặt hàng này sẽ mua thêm mặt hàng kia; hoặc khan giả xem phim này sẽ có xu hướng thích xem phim kia, dựa vào đó ta có thể xây dựng những hệ thống gợi ý khách hàng (Recommendation System) nhằm thúc đẩy nhu cầu mua sắm hoặc xem phim\u0026hellip;.\nMột số thuật toán thuộc nhóm học không giám sát như Apriori (Association Rule), k-Means (Clustering), …\nc.\tHọc bán giám sát Là bài toán mà khi tập dữ liệu đầu vào X là hỗn hợp các mẫu có nhãn và không có nhãn, trong đó số lượng có nhãn chỉ chiếm một phần nhỏ như minh hoạ ở Hình 4.\nPhần lớn các bài toán thực tế của ML thuộc nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được, chẳng hạn như ảnh y học hoặc các cặp câu song ngữ. Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet.\nHình 4: Semi-supervised Learning Algorithms\nVới bài toán này, mô hình phải tìm hiểu các cấu trúc để tổ chức dữ liệu cũng như đưa ra dự đoán. Vì đặc điểm trung gian nên ta có thể sử dụng unsupervised learning để khám phá và tìm hiểu cấu trúc trong dữ liệu đầu vào, đồng thời sử dụng supervised learning để dự đoán cho dữ liệu không được gán nhãn. Sau đó đưa dữ liệu vừa dự đoán trở lại làm dữ liệu huấn luyện cho supervised learning và sử dụng mô hình sau khi huấn luyện để đưa ra dự đoán về dữ liệu mới.\nMột số thuật toán học tăng cường như: Self Training, Generative models, S3VMs, Graph-Based Algorithms, Multiview Algorithms, …\nd.\tHọc tăng cường Học tăng tường hay học củng cố là bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất. Hiện tại, reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nước đi tiếp theo để đạt được điểm số cao nhất. Hình 5 là một ví dụ đơn giản sử dụng học tăng cường.\nHình 5: Minh hoạ cho học tăng cường được áp dụng trong lý thuyết trò chơi.\nAlphaGo - một phần mềm chơi cờ vây trên máy tính được xây dựng bởi Google DeepMind hay chương trình dạy máy tính chơi game Mario là những ứng dụng sử dụng học tăng cường.\nCờ vậy được xem là trò chơi có độ phức tạp cực kỳ cao với tổng số nước đi là xấp xỉ 1076110761, so với cờ vua là 1012010120, vì vậy thuật toán phải chọn ra một nước đi tối ưu trong số hàng tỉ tỉ lựa chọn. Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục tiêu cuối cùng của AlphaGo không phải là chơi như con người mà phải thắng được con người. Vì vậy, sau khi học xong các ván cờ của con người, AlphaGo tự chơi với chính nó thông qua hàng triệu ván cờ để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning.\nĐơn giản hơn cờ vây, tại một thời điểm cụ thể, người chơi game Mario chỉ cần bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào ứng với một chướng ngại vật cố định ở một vị trí cố định. Khi đó thuật toán trong ứng dụng dạy máy tính chơi game Mario sẽ nhận đầu vào là sơ đồ của màn hình tại thời điểm hiện hành, nhiệm vụ của thuật toán là tìm ra tổ hợp phím nên được bấm ứng với đầu vào đó. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa với thời gian bao lâu trong game, càng xa và càng nhanh thì điểm thưởng đạt được càng cao, tất nhiên điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra. Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa.\nCó nhiều cách khác nhau để thuật toán có thể mô hình hóa một vấn đề dựa trên sự tương tác của nó với dữ liệu đầu vào. Phân loại hoặc cách tổ chức thuật toán học máy này rất hữu ích vì nó buộc chúng ta phải suy nghĩ về vai trò của dữ liệu đầu vào và quy trình chuẩn bị mô hình và chọn một thuật toán phù hợp nhất cho vấn đề của chúng ta để có kết quả tốt nhất.\n2.\tPhân nhóm dựa trên sự tương đồng Dựa vào sự tương đồng về chức năng hay cách thức hoạt động mà các thuật toán sẽ được gom nhóm với nhau. Sau đây là danh sách các nhóm và các thuật toán theo từng nhóm.\na.\tCác thuật toán hồi quy (Regression Algorithms) Hồi quy là quá trình tìm mối quan hệ phụ thuộc của một biến (được gọi là biến phụ thuộc hay biến được giải thích, biến được dự báo, biến được hồi quy, biến phản ứng, biến nội sinh) vào một hoặc nhiều biến khác (được gọi là biến độc lập, biến giải thích, biến dự báo, biến hồi quy, biến tác nhân hay biến kiểm soát, biến ngoại sinh) nhằm mục đích ước lượng hoặc tiên đoán giá trị kỳ vọng của biến phụ thuộc khi biết trước giá trị của biến độc lập. Hình 6 tượng trưng cho ý tưởng của các thuật toán hồi quy.\nVí dụ như, dự đoán rằng nếu tăng lãi suất tiền gửi thì sẽ huy động được lượng tiền gửi nhiều hơn, khi đó ngân hàng A cần biết mối quan hệ giữa lượng tiền gửi và lãi suất tiền gửi, cụ thể hơn họ muốn biết khi tăng lãi suất thêm 0.1% thì lượng tiền gửi sẽ tăng trung bình là bao nhiêu.\nCác thuật toán hồi quy phổ biến nhất là:\nLinear Regression\nLogistic Regression\nLocally Estimated Scatterplot Smoothing (LOESS)\nMultivariate Adaptive Regression Splines (MARS)\nOrdinary Least Squares Regression (OLSR)\nStepwise Regression\nHình 6: Regression Algorithms\nb.\tThuật toán dựa trên mẫu (Instance-based Algorithms) Mô hình học tập dựa trên mẫu hay thực thể là bài toán ra quyết định dựa vào các trường hợp hoặc các mẫu dữ liệu huấn luyện được coi là quan trọng hay bắt buộc đối với mô hình.\nNhóm thuật toán này thường xây dựng cơ sở dữ liệu về dữ liệu mẫu và so sánh dữ liệu mới với cơ sở dữ liệu bằng cách sử dụng thước đo tương tự để tìm kết quả phù hợp nhất và đưa ra dự đoán. Khi đó trọng tâm được đặt vào đại diện của các thể hiện được lưu trữ như minh hoạ ở Hình 7.\nHình 7: Instance-based Algorithms\nCác thuật toán dựa trên thực thể phổ biến nhất là:\nk-Nearest Neighbor (kNN – k láng giềng gần nhất)\nLearning Vector Quantization (LVQ)\nLocally Weighted Learning (LWL)\nSelf-Organizing Map (SOM)\nc.\tThuật toán chuẩn hoá (Regularization Algorithms) Các thuật toán chuẩn hoá ra đời từ sự mở rộng các phương pháp đã có (điển hình là các phương pháp hồi quy) bằng cách xử phạt các mô hình dựa trên mức độ phức tạp của chúng. Việc ưu tiên các mô hình đơn giản hơn cũng tốt hơn trong việc khái quát hóa. Hình 8 tượng trưng cho ý tưởng của thuật toán chuẩn hoá.\nHình 8: Regularization Algorithms\nCác thuật toán chính quy phổ biến nhất là:\nElastic Net\nLeast Absolute Shrinkage and Selection Operator (LASSO)\nLeast-Angle Regression (LARS)\nRidge Regression\nd.\tThuật toán cây quyết định (Decision Tree Algorithms) Đây là phương pháp xây dựng mô hình ra quyết định dựa trên các giá trị thực của những thuộc tính trong dữ liệu. Sự quyết định được rẽ nhánh trong cấu trúc cây cho đến khi quyết định dự đoán được đưa ra cho một mẫu nhất định như minh hoạ ở Hình 9. Phương pháp này được sử dụng trong việc huấn luyện dữ liệu cho bài toán phân lớp và hồi quy. Vì sự nhanh chóng, chính xác nên phương pháp này rất được ưa chuộng trong ML.\nHình 9: Decision Tree Algorithms\nCác thuật toán cây quyết định phổ biến nhất bao gồm:\nChi-squared Automatic Interaction Detection (CHAID)\nClassification và Regression Tree – CART\nConditional Decision Trees\nC4.5 và C5.0\nDecision Stump\nIterative Dichotomiser 3 (ID3)\nM5\ne.\tThuật toán Bayes (Bayesian Algorithms) Đây là nhóm các thuật toán áp dụng Định lý Bayes cho bài toán phân loại và hồi quy.\nHình 10: Bayesian Algorithms\nCác thuật toán phổ biến nhất là:\nAveraged One-Dependence Estimators (AODE)\nBayesian Belief Network (BBN)\nBayesian Network (BN)\nGaussian Naive Bayes\nMultinomial Naive Bayes\nNaive Bayes\nf.\tThuật toán phân cụm (Clustering Algorithms) Tất cả các phương pháp đều sử dụng các cấu trúc vốn có trong dữ liệu để tổ chức tốt nhất dữ liệu thành các nhóm có mức độ phổ biến tối đa dựa vào trọng tâm (centroid) và thứ bậc (hierarchal) như thể hiện ở Hình 11.\nHình 11: Clustering Algorithms\nCác thuật toán phân cụm phổ biến nhất là:\nExpectation Maximisation (EM – cực đại hoá kỳ vọng)\nHierarchical Clustering\nk-Means\nk-Medians\ng.\tCác thuật toán luật kết hợp (Association Rule Learning Algorithms) Đây là những thuật toán sẽ rút trích ra các quy tắc giải thích tốt nhất mối quan hệ giữa các biến trong dữ liệu. Các quy tắc này có thể giúp khám phá ra các tính chất quan trọng và hữu ích trong các tập dữ liệu lớn và cao chiều trong thương mại cùng các lĩnh vực khác. Hình 12 minh hoạ cho ý tưởng của thuật toán luật kết hợp.\nHình 12: Association Rule Learning Algorithms\nCác thuật toán luật kết hợp phổ biến nhất là:\nApriori algorithm\nEclat algorithm\nFP-Growth algorithm\nh.\tThuật toán mạng nơron nhân tạo (Artificial Neural Network Algorithms) Mạng nơron nhân tạo là các mô hình được lấy cảm hứng từ cấu trúc và chức năng của mạng lưới thần kinh sinh học. Hình 13 minh hoạ cho một mạng truyền thẳng. Nhóm thuật toán này có thể được sử dụng cho bài toán phân lớp và hồi quy với rất nhiều biến thể khác nhau cho hầu hết các vấn đề. Tuy nhiên, trong bài viết này mình chỉ trình bày các thuật toán cổ điển và phổ biến nhất:\nBack-Propagation (mạng lan truyền ngược)\nPerceptron (Mạng lan truyền thẳng)\nMulti-layer perceptron (Mạng truyền thẳng đa lớp)\nHopfield Network\nRadial Basis Function Network (RBFN)\nHình 13: Artificial Neural Network Algorithms\ni.\tThuật toán học sâu (Deep Learning Algorithms) Thực chất Deep Learning là một bản cập nhật hiện đại cho Artificial Neural Networks nhằm khai thác khả năng tính toán của máy tính, tuy nhiên vì sự phát triển lớn mạnh của chúng nên mình tách ra thành một nhóm riêng.\nDeep Learning quan tâm đến việc xây dựng các mạng thần kinh lớn hơn, phức tạp hơn nhiều, và làm sao để khai thác hiệu quả các bộ dữ liệu lớn chứa rất ít dữ liệu đã được gán nhãn. Hình 14 minh hoạ cho ý tưởng của Deep learning.\nHình 14: Deep Learning Algorithms\nCác thuật toán học sâu phổ biến nhất là:\nConvolutional Neural Network (CNN)\nDeep Belief Networks (DBN)\nDeep Boltzmann Machine (DBM)\nStacked Auto-Encoders\nj.\tNhóm thuật toán Giảm chiều dữ liệu (Dimensionality Reduction Algorithms) Giống như các phương pháp phân cụm, giảm không gian tìm kiếm và khai thác cấu trúc vốn có trong dữ liệu nhưng theo cách không giám sát hoặc để tóm tắt hay mô tả dữ liệu sử dụng ít thông tin hơn là mục tiêu của nhóm phương pháp này. Hình 15 minh hoạ cho việc giảm chiều dữ liệu.\nĐiều này có thể hữu ích để trực quan hóa dữ liệu hoặc đơn giản hóa dữ liệu mà sau đó có thể được sử dụng trong phương pháp học có giám sát. Nhiều trong số các phương pháp này có thể được điều chỉnh để sử dụng trong phân lớp và hồi quy.\nHình 15: Dimensional Reduction Algorithms\nCác thuật toán Giảm chiều dữ liệu phổ biến như:\nFlexible Discriminant Analysis (FDA)\nLinear Discriminant Analysis (LDA)\nMixture Discriminant Analysis (MDA)\nMultidimensional Scaling (MDS)\nPartial Least Squares Regression (PLSR)\nPrincipal Component Analysis (PCA)\nPrincipal Component Regression (PCR)\nProjection Pursuit\nQuadratic Discriminant Analysis (QDA)\nSammon Mapping\nk.\tThuật toán tập hợp (Ensemble Algorithms) Ensemble methods là những phương pháp kết hợp các mô hình yếu hơn được huấn luyện độc lập và phần dự đoán của chúng sẽ được kết hợp theo một cách nào đó để đưa ra dự đoán tổng thể như minh họa ở Hình 16.\nNhóm thuật toán này khá mạnh và được nghiên cứu nhiều, đặc biệt là về cách để kết hợp các mô hình với nhau.\nHình 16: Ensemble Algorithms\nMột số thuật toán phổ biến như:\nAdaBoost\nBoosting\nBootstrapped Aggregation (Bagging)\nGradient Boosting Machines (GBM)\nGradient Boosted Regression Trees (GBRT)\nRandom Forest\nStacked Generalization (blending)\nl.\tCác thuật toán khác Còn rất nhiều các thuật toán khác không được liệt kê ở đây, chẳng hạn như Support Vector Machines (SVM), mình đang phân vân rằng liệu thuật toán này nên được đưa vào nhóm nào đó hay đứng một mình. Nếu dựa vào danh sách các biến thể và mức độ phát triển thì SVM có thể được tách thành một nhóm riêng – nhóm thuật toán sử dụng véctơ hỗ trợ.\nThêm vào đó, các thuật toán được hình thành từ các nhiệm vụ đặc biệt, hoăc các thuật toán từ những nhánh con đặc biệt của ML cũng không được liệt kê vào các nhóm, chẳng hạn như:\nFeature selection algorithms\nAlgorithm accuracy evaluation\nPerformance measures\nCó dịp mình sẽ bổ sung hoặc đề cập đến những thuật toán này ở một bài viết khác.\nMặc dù rất hữu ích (dựa vào nhóm, người dùng sẽ dễ dàng nhớ được bản chất của thuật toán) nhưng phương pháp phân nhóm này chưa hoàn hảo ở điểm có những thuật toán có thể phù hợp với nhiều danh mục như Learning Vector Quantization, vừa là phương pháp lấy cảm hứng từ mạng thần kinh (neural network), vừa là phương pháp dựa trên cá thể (instance-based). Hoặc là thuật toán có cùng tên mô tả bài toán và nhóm thuật toán như Hồi quy (Regression) và Phân cụm (Clustering). Đối với những trường hợp này ta có thể giải quyết bằng cách liệt kê các thuật toán hai lần hoặc bằng cách chọn nhóm một cách chủ quan. Để tránh trùng lặp các thuật toán và giữ cho mọi thứ đơn giản thì có lẽ chọn nhóm theo cách chủ quan sẽ phù hợp hơn.\nĐể giúp các bạn dễ nhớ cũng như tổng kết cho phần này mình đã vẽ một sơ đồ các thuật toán phân theo nhóm và sắp xếp theo alphabet, các bạn có thể xem thểm ở Hình 17 bên dưới.\nHình 17: Sơ đồ phân nhóm thuật toán theo sự tương đồng\nHy vọng bài viết này sẽ mang lại hữu ích cho bạn đọc, nhất là giúp bạn có dược cái nhìn tổng quan về những gì hiện có và một số ý tưởng về cách liên kết các thuật toán với nhau.\nDanh sách các nhóm và thuật toán được liệt kê trong bài viết chỉ đảm bảo được yếu tố phổ biến tuy nhiên sẽ không đầy đủ. Vậy nên nếu bạn biết thêm thuật toán hoặc nhóm nào chưa được liệt kê ở đây hoặc kể cả cách phân nhóm thuật toán khác, cũng như sau khi đọc mà các bạn có bất kỳ góp ý, câu hỏi giúp cải thiện bài viết tốt hơn, các bạn có thể để lại bình luận nhằm chia sẻ cùng mình và những bạn đọc khác nhé.\nTài liệu tham khảo: A Tour of Machine Learning Algorithms by Jason Brownlee in Understand Machine Learning Algorithms\nSemi-Supervised Learning Tutorial by Xiaojin Zhu\nhttps://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms\nTop 10 algorithms in data mining by Xindong Wu · Vipin Kumar · J. Ross Quinlan · Joydeep Ghosh · Qiang Yang · Hiroshi Motoda · Geoffrey J. McLachlan · Angus Ng · Bing Liu · Philip S. Yu · Zhi-Hua Zhou · Michael Steinbach · David J. Hand · Dan Steinberg.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"Apr 19, 2019","img":"","permalink":"/blog/2019-04-19-deep-learning-view/","series":null,"tags":["machine learning","deep learning","học có giám sát","học không giám sát","học tăng cường"],"title":"Phân Nhóm Các Thuật Toán Học Máy"},{"categories":null,"content":" Nghiên cứu dữ liệu Phân tích dữ liệu Làm sạch dữ liệu Xử lý missing values Tạo đặc trưng Huấn luyện mô hình Nghiên cứu dữ liệu Trong thực tế, Walmart đã chạy các chương trình khuyến mãi trong các ngày lễ lớn trong năm. Có 4 ngày lễ lớn đó là Siêu cúp bóng bầu dục Mỹ (Super Bowl - tổ chức vào chủ nhật đầu tiên của tháng Hai. Đây là một sự kiện thể thao lớn và ngày tổ chức Super Bowl được người Mỹ coi là ngày lễ quốc gia của Hoa Kỳ (theo wiki https://vi.wikipedia.org/wiki/Super_Bowl)), ngày lễ lao động (Labor Day - ngày một tháng 5), lễ tạ ơn (Thanksgiving, ngày lễ tạ ơn ở Mỹ được tổ chức vào ngày thứ Năm lần thứ tư của tháng 11, còn ở Canada ngày lễ tạ ơn được tổ chức vào ngày thứ hai lần thứ hai của tháng 10, theo wiki https://en.wikipedia.org/wiki/Thanksgiving), lễ giáng sinh (Christmas ngày 24 và 25 tháng 12 theo wiki https://en.wikipedia.org/wiki/Christmas ). Những tuần có chứa những ngày lễ lớn này được đánh trọng số gấp 5 lần những tuần khác. Chúng ta phải xây dựng mô hình để mô hình hoá các tác động của việc giảm giá trong các tuần lễ này khi không có dữ liệu lịch sử đầy đủ.\nTập dữ liệu được cung cấp bao gồm:\nTập train: chứa dữ liệu số bán từ 05-02-2010 đến 01-11-2012. Các trường dữ liệu là: store number - mã cửa hàng, Dept number - mã sản phẩm, Date - Tuần, Weekly_Sales - số bán, IsHoliday - Nếu tuần đó có chứa các holidate thì đánh 1 ngược lại đánh 0.\nTập test: Chứa dữ liệu test, có các cột thuộc tính như tập train\nTập features: Chứa thông tin thêm về của hàng, bao gồm store - mã cửa hàng, Date - ngày, Temperature - Nhiệt độ, Fuel_Price - giá dầu (ở mỹ, mỗi khu vực khác nhau sẽ có giá nhiên liệu khác nhau), MarkDown1, MarkDown2,\u0026hellip; , MarkDown5 - một chỉ số gì đó mà tác giả không cung cấp định nghĩa cho chúng ta, CPI - chỉ số giá tiêu dùng, Unemployment - tình trạng thất nghiệp, IsHoliday - Tuần có chứa ngày nghỉ.\nPhân tích dữ liệu Mình sẽ import một số thư viện cần thiết\n1import pandas as pd 2import numpy as np 3 4#Do some statistics 5from scipy.misc import imread 6from scipy import sparse 7import scipy.stats as ss 8import math 9 10#Nice graphing tools 11import matplotlib 12import matplotlib.pyplot as plt 13import seaborn as sns Đọc các file data lên, merge các file lại với nhau\n1 2 3train = pd.read_csv(\u0026#39;data/train.csv\u0026#39;) 4test = pd.read_csv(\u0026#39;data/test.csv\u0026#39;) 5feature = pd.read_csv(\u0026#39;data/features.csv\u0026#39;) 6 7train = train.merge(feature, how=\u0026#39;left\u0026#39;, on=[\u0026#39;Store\u0026#39;,\u0026#39;Date\u0026#39;]) 8test = test.merge(feature, how=\u0026#39;left\u0026#39;, on=[\u0026#39;Store\u0026#39;,\u0026#39;Date\u0026#39;]) 9 10 11# Merge in store info 12stores = pd.read_csv(\u0026#34;data/stores.csv\u0026#34;) 13train = train.merge(stores, how=\u0026#39;left\u0026#39;, on=\u0026#39;Store\u0026#39;) 14test = test.merge(stores, how=\u0026#39;left\u0026#39;, on=\u0026#39;Store\u0026#39;) 15print(train.head()) Kết quả\n1 Store Dept Date Weekly_Sales IsHoliday_x Temperature Fuel_Price MarkDown1 MarkDown2 MarkDown3 MarkDown4 MarkDown5 CPI Unemployment IsHoliday_y Type Size Split 20 1 1 2010-02-05 24924.50 False 42.31 2.572 NaN NaN NaN NaN NaN 211.096358 8.106 False A 151315 Train 31 1 1 2010-02-12 46039.49 True 38.51 2.548 NaN NaN NaN NaN NaN 211.242170 8.106 True A 151315 Train 42 1 1 2010-02-19 41595.55 False 39.93 2.514 NaN NaN NaN NaN NaN 211.289143 8.106 False A 151315 Train 53 1 1 2010-02-26 19403.54 False 46.63 2.561 NaN NaN NaN NaN NaN 211.319643 8.106 False A 151315 Train 64 1 1 2010-03-05 21827.90 False 46.50 2.625 NaN NaN NaN NaN NaN 211.350143 8.106 False A 151315 Train Mới có 5 dòng đầu tiên mà thấy các chỉ số markdown Nan rồi.\nChúng ta tiến hành một số phân tích dữ liệu. À, Mình sẽ merge dữ liệu train và test lại rồi phân tích thống kê\n1df = pd.concat([train,test],axis=0) # Join train and test 2 3print(df.describe()) Kết quả\n1 CPI Dept Fuel_Price MarkDown1 MarkDown2 MarkDown3 MarkDown4 MarkDown5 Size Store Temperature Unemployment Weekly_Sales 2count 498472.000000 536634.000000 536634.000000 265596.000000 197685.000000 242326.000000 237143.000000 266496.000000 536634.00000 536634.000000 536634.000000 498472.000000 421570.000000 3mean 172.090481 44.277301 3.408310 7438.004144 3509.274827 1857.913525 3371.556866 4324.021158 136678.55096 22.208621 58.771762 7.791888 15981.258123 4std 39.542149 30.527358 0.430861 9411.341379 8992.047197 11616.143274 6872.281734 13549.262124 61007.71180 12.790580 18.678716 1.865076 22711.183519 5min 126.064000 1.000000 2.472000 -2781.450000 -265.760000 -179.260000 0.220000 -185.170000 34875.00000 1.000000 -7.290000 3.684000 -4988.940000 625% 132.521867 18.000000 3.041000 2114.640000 72.500000 7.220000 336.240000 1570.112500 93638.00000 11.000000 45.250000 6.623000 2079.650000 750% 182.442420 37.000000 3.523000 5126.540000 385.310000 40.760000 1239.040000 2870.910000 140167.00000 22.000000 60.060000 7.795000 7612.030000 875% 213.748126 74.000000 3.744000 9303.850000 2392.390000 174.260000 3397.080000 5012.220000 202505.00000 33.000000 73.230000 8.549000 20205.852500 9max 228.976456 99.000000 4.468000 103184.980000 104519.540000 149483.310000 67474.850000 771448.100000 219622.00000 45.000000 101.950000 14.313000 693099.360000 Phân tích một chút:\nBỏ qua cột Dept và Store vì nó là mã sản phẩm và mã của hàng, người ta thích đặt số bao nhiêu thì đặt.\nCác chỉ số MarkDown có độ lệch chuẩn khá cao.\nNhiệt độ min là -7.29, max là 101.95, trung bình là 58, nên không thể là độ C được, có thể là độ F\nXem thử hệ số tương quan giữa các column như thế nào\n1sns.set(style=\u0026#34;white\u0026#34;) 2 3# Compute the correlation matrix 4corr = df.corr() 5 6# Generate a mask for the upper triangle 7mask = np.zeros_like(corr, dtype=np.bool) 8mask[np.triu_indices_from(mask)] = True 9 10# Set up the matplotlib figure 11f, ax = plt.subplots(figsize=(11, 9)) 12 13# Generate a custom diverging colormap 14cmap = sns.diverging_palette(220, 10, as_cmap=True) 15 16# Draw the heatmap with the mask and correct aspect ratio 17sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, 18 square=True, linewidths=.5, cbar_kws={\u0026#34;shrink\u0026#34;: .5}) 19 20plt.show() Hệ số tương quan giữa các cột trong dữ liệu\nPhân tích một chút, chúng ta thấy rằng MarkDown5 hầu như không có liên quan gì đến các column còn lại. Hệ số trải từ -0.3 đến 0.3 chứng tỏ mổi quan hệ giữa các cột là khá lỏng lẻo. Chỉ số giá tiêu dùng tương quan tỷ lệ nghịch với tình trạng thất nghiệp (hợp lý không nhỉ). Kích thước cửa hàng càng bự thì bán càng nhiều (ok hiển nhiên), sản phẩm có mã càng lớn thì bán càng nhiều (? có lẽ là sản phẩm mới, người mỹ thích mua sản phẩm mới chăng). Và một vấn đề quan trọng là giá nhiên liệu, isHoliday, nhiệt độ không có mối tương quan với weekly sales. Chỉ số CPI và tình trạng thất nghiệp cũng ảnh hưởng không lớn với weekly sales.\nThử plot lên hình ảnh về số lượng bán và kích thước cửa hàng xem sao\n1plt.scatter( df[\u0026#39;Size\u0026#39;],df[\u0026#39;Weekly_Sales\u0026#39;]) 2plt.show() Tương quan giữa số bán và kích thước cửa hàng\nNhìn vào hình trên, chúng ta thấy rằng cửa hàng có kích thước nhỏ số bán cũng không tăng đột biến khi gặp ngày lễ, cửa hàng kích thước siêu bự có tỷ lệ đột biến thấp, cửa hàng trung trung có đột biến, ở khúc size 125000 và số bán là 700000. Chúng ta hãy xem những ngày có số bán lớn rơi vào ngày nào. Dựa vào bảng desription ở phía trên đã phân tích, trung bình của số bán là 15981 và lệch chuẩn là 22711, cộng lại là 15981 + 22711 = 38692, nhìn trên đô thị thì phần đột biến khá lớn. Max là 700000, min là 0 (cái này nhìn hình, không phải số thực tế ở bảng mô tả), mình sẽ lấy ra những ngày có số bán lớn hơn 350000 (vượt qua ngưỡng trung bình + độ lệch chuẩn rất nhều -\u0026gt; ngoại lệ là đây) xem những ngày đó là ngày gì\n1 2print(df.loc[df[\u0026#39;Weekly_Sales\u0026#39;] \u0026gt;350000].head(10)) In ra top 10 thằng đầu tiên\n1 2 CPI Date Dept Fuel_Price IsHoliday_x IsHoliday_y MarkDown1 MarkDown2 MarkDown3 MarkDown4 MarkDown5 Size Split Store Temperature Type Unemployment Weekly_Sales 337201 126.669267 2010-11-26 72 2.752 True True NaN NaN NaN NaN NaN 205863 Train 4 48.08 A 7.127 381072.11 437253 129.836400 2011-11-25 72 3.225 True True 561.45 137.88 83340.33 44.04 9239.23 205863 Train 4 47.96 A 5.143 385051.04 588428 126.983581 2010-12-24 7 3.236 False False NaN NaN NaN NaN NaN 126512 Train 10 57.06 B 9.003 406988.63 695373 126.669267 2010-11-26 72 3.162 True True NaN NaN NaN NaN NaN 126512 Train 10 55.33 B 9.003 693099.36 795377 126.983581 2010-12-24 72 3.236 False False NaN NaN NaN NaN NaN 126512 Train 10 57.06 B 9.003 404245.03 895425 129.836400 2011-11-25 72 3.760 True True 174.72 329.00 141630.61 79.00 1009.98 126512 Train 10 60.68 B 7.874 630999.19 9115222 126.669267 2010-11-26 72 3.162 True True NaN NaN NaN NaN NaN 112238 Train 12 47.66 B 14.313 359995.60 10115274 129.836400 2011-11-25 72 3.622 True True 5391.83 8.00 63143.29 49.27 2115.67 112238 Train 12 53.25 B 12.890 360140.66 11128984 182.544590 2010-12-24 7 3.141 False False NaN NaN NaN NaN NaN 200898 Train 14 30.59 A 8.724 356867.25 12135665 182.783277 2010-11-26 72 3.039 True True NaN NaN NaN NaN NaN 200898 Train 14 46.15 A 8.724 474330.10 Nhìn vào bảng trên, chúng ta thấy rằng 10 ngày đầu tiên tập trung chủ yếu ở tháng 11 và tháng 12, tháng 12 là 24-25 tháng 12 -\u0026gt; ngày noel, còn tháng 11 là 25-26 tháng 11 (ngày gì vậy ta, trong mô tả không thấy) Tra lịch thì ngày 25 tháng 11 năm 2011 trúng thứ sáu, tra trên mạng một thông tin khá quan trong là \u0026ldquo;Black Friday sẽ rơi vào khoảng ngày 23-29 tháng 11\u0026rdquo; -\u0026gt; không nghi ngờ gì nữa có thể là ngày này đây. Thử tra tiếp ngày 26 tháng 11 năm 2010, cũng là thứ sáu luôn -\u0026gt; ngày black friday và ngày noel có sức mua điên cuồng quá.\nMình dùng một kỹ thuật nhỏ là giảm dần số bán, để ra số bán tối thiểu mà ngày black friday và ngày nodel vẫn còn giữ vị trí thống trị. Kỹ thuật khá đơn giản thôi, từ giá trị 350000, mỗi lần mình sẽ giảm đi 10000, và đếm số lần xuất hiện của các ngày, nếu có ngày nào đó nằm ngoài tuần chứa black friday và nodel thì mình dừng. Sau một hồi tìm kiếm và số bán đã xuất hiện, đó là 290000\n1print(df.loc[df[\u0026#39;Weekly_Sales\u0026#39;] \u0026gt;290000,\u0026#34;Date\u0026#34;].value_counts()) 12010-11-26 16 22011-11-25 14 32010-12-24 8 42011-12-23 3 52010-02-05 1 Làm sạch dữ liệu Xử lý missing values Một vấn đề khá quan trọng là trong tập dữ liệu này missing value khá nhiều, thử đếm số lượng null trong data cho ta biết được rằng\n1CPI 38162 2Date 0 3Dept 0 4Fuel_Price 0 5IsHoliday_x 0 6IsHoliday_y 0 7MarkDown1 271038 8MarkDown2 338949 9MarkDown3 294308 10MarkDown4 299491 11MarkDown5 270138 12Size 0 13Split 0 14Store 0 15Temperature 0 16Type 0 17Unemployment 38162 18Weekly_Sales 115064 Các giá trị MarkDown bị null khá nhiều, cách đơn giản nhất là set 0 cho tất cả các giá trị null ( Mình lưu log lại những index null của các markdown).\n1df = df.assign(md1_present = df[\u0026#39;MarkDown1\u0026#39;]notnull()) 2df = df.assign(md2_present = df[\u0026#39;MarkDown2\u0026#39;]notnull()) 3df = df.assign(md3_present = df[\u0026#39;MarkDown3\u0026#39;]notnull()) 4df = df.assign(md4_present = df[\u0026#39;MarkDown4\u0026#39;]notnull()) 5df = df.assign(md5_present = df[\u0026#39;MarkDown5\u0026#39;].notnull()) 6 7df.fillna(0, inplace=True) Tạo đặc trưng Đặc trưng holiday\n1df[\u0026#39;IsHoliday\u0026#39;] = \u0026#39;IsHoliday_\u0026#39; + df[\u0026#39;IsHoliday_x\u0026#39;].map(str) 2holiday_dummies = pd.get_dummies(df[\u0026#39;IsHoliday\u0026#39;]) Đặc trưng ngày tháng\nRút trích tháng\n1df[\u0026#39;DateType\u0026#39;] = [datetime.strptime(date, \u0026#39;%Y-%m-%d\u0026#39;).date() for date in df[\u0026#39;Date\u0026#39;].astype(str).values.tolist()] 2df[\u0026#39;Month\u0026#39;] = [date.month for date in df[\u0026#39;DateType\u0026#39;]] 3df[\u0026#39;Month\u0026#39;] = \u0026#39;Month_\u0026#39; + df[\u0026#39;Month\u0026#39;].map(str) 4Month_dummies = pd.get_dummies(df[\u0026#39;Month\u0026#39;] ) Rút trích ngày trước giáng sinh và black friday\n1df[\u0026#39;Black_Friday\u0026#39;] = np.where((df[\u0026#39;DateType\u0026#39;]==datetime(2010, 11, 26).date()) | (df[\u0026#39;DateType\u0026#39;]==datetime(2011, 11, 25).date()), \u0026#39;yes\u0026#39;, \u0026#39;no\u0026#39;) 2df[\u0026#39;Pre_christmas\u0026#39;] = np.where((df[\u0026#39;DateType\u0026#39;]==datetime(2010, 12, 23).date()) | (df[\u0026#39;DateType\u0026#39;]==datetime(2010, 12, 24).date()) | (df[\u0026#39;DateType\u0026#39;]==datetime(2011, 12, 23).date()) | (df[\u0026#39;DateType\u0026#39;]==datetime(2011, 12, 24).date()), \u0026#39;yes\u0026#39;, \u0026#39;no\u0026#39;) 3df[\u0026#39;Black_Friday\u0026#39;] = \u0026#39;Black_Friday_\u0026#39; + df[\u0026#39;Black_Friday\u0026#39;].map(str) 4df[\u0026#39;Pre_christmas\u0026#39;] = \u0026#39;Pre_christmas_\u0026#39; + df[\u0026#39;Pre_christmas\u0026#39;].map(str) 5Black_Friday_dummies = pd.get_dummies(df[\u0026#39;Black_Friday\u0026#39;] ) 6Pre_christmas_dummies = pd.get_dummies(df[\u0026#39;Pre_christmas\u0026#39;] ) Thêm các đặc trưng vào trong dữ liệu\n1 2df = pd.concat([df,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies],axis=1) Thêm đặc trưng trung vị của từng loại cửa hàng vào từng tháng, do một số của hàng sẽ bị NA ở cột số bán ở một thời điểm nào đó, nên chúng ta replace số bán là 0 có vẻ không hợp lý lắm. Mình chọn cách là thay thế bằng trung bình của số bán trong tháng của cửa hàng cùng loại. Nhưng trước tiên thì tính trung bình số bán của từng loại cửa hàng cái đã.\n1 2medians = pd.DataFrame({\u0026#39;Median Sales\u0026#39; :df.loc[df[\u0026#39;Split\u0026#39;]==\u0026#39;Train\u0026#39;].groupby(by=[\u0026#39;Type\u0026#39;,\u0026#39;Dept\u0026#39;,\u0026#39;Store\u0026#39;,\u0026#39;Month\u0026#39;,\u0026#39;IsHoliday\u0026#39;])[\u0026#39;Weekly_Sales\u0026#39;].median()}).reset_index() 3print(medians.head()) Kết quả\n1 Type Dept Store Month IsHoliday Median Sales 20 Type_A Dept_1 Store_1 Month_1 IsHoliday_False 17350.585 31 Type_A Dept_1 Store_1 Month_10 IsHoliday_False 23388.030 42 Type_A Dept_1 Store_1 Month_11 IsHoliday_False 19551.115 53 Type_A Dept_1 Store_1 Month_11 IsHoliday_True 19865.770 64 Type_A Dept_1 Store_1 Month_12 IsHoliday_False 39109.390 thêm dữ liệu vào trong data chính, loại bỏ NA và tạo key cho mỗi dòng để dễ dàng truy xuất\n1df = df.merge(medians, how = \u0026#39;outer\u0026#39;, on = [\u0026#39;Type\u0026#39;,\u0026#39;Dept\u0026#39;,\u0026#39;Store\u0026#39;,\u0026#39;Month\u0026#39;,\u0026#39;IsHoliday\u0026#39;]) 2 3# Fill NA 4df[\u0026#39;Median Sales\u0026#39;].fillna(df[\u0026#39;Median Sales\u0026#39;].loc[df[\u0026#39;Split\u0026#39;]==\u0026#39;Train\u0026#39;].median(), inplace=True) 5 6# Create a key for easy access 7 8df[\u0026#39;Key\u0026#39;] = df[\u0026#39;Type\u0026#39;].map(str)+df[\u0026#39;Dept\u0026#39;].map(str)+df[\u0026#39;Store\u0026#39;].map(str)+df[\u0026#39;Date\u0026#39;].map(str)+df[\u0026#39;IsHoliday\u0026#39;].map(str) Chúng ta sẽ dự đoán số bán của tuần kế tiếp dựa vào kết quả số bán của tuần hiện tại, nên trong dữ liệu sẽ lưu trên ngày của tuần trước đó để dễ truy xuất. Vì 1 tuần có 7 ngày, chúng ta sẽ lưu giá trị là ngày ở cột hiện tại - 7\n1df[\u0026#39;DateLagged\u0026#39;] = df[\u0026#39;DateType\u0026#39;]- timedelta(days=7) Và giờ đây, chúng ta sẽ lặp qua toàn bộ các dòng trên tập dữ liệu, kiểm tra xem có dòng nào số bán nan hông, nếu có thì sẽ thay bằng trung bình đã tính ở trên. Ở đây mình tạo một sorted dataset để truy xuất cho nhanh\n1 2#Make a sorted dataframe. This will allow us to find lagged variables much faster! 3sorted_df = df.sort_values([\u0026#39;Store\u0026#39;, \u0026#39;Dept\u0026#39;,\u0026#39;DateType\u0026#39;], ascending=[1, 1,1]) 4sorted_df = sorted_df.reset_index(drop=True) # Reinitialize the row indices for the loop to work 5 6sorted_df[\u0026#39;LaggedSales\u0026#39;] = np.nan # Initialize column 7sorted_df[\u0026#39;LaggedAvailable\u0026#39;] = np.nan # Initialize column 8last=df.loc[0] # intialize last row for first iteration. Doesn\u0026#39;t really matter what it is 9row_len = sorted_df.shape[0] 10for index, row in sorted_df.iterrows(): 11 lag_date = row[\u0026#34;DateLagged\u0026#34;] 12 # Check if it matches by comparing last weeks value to the compared date 13 # And if weekly sales aren\u0026#39;t 0 14 if((last[\u0026#39;DateType\u0026#39;]== lag_date) \u0026amp; (last[\u0026#39;Weekly_Sales\u0026#39;]\u0026gt;0)): 15 sorted_df.set_value(index, \u0026#39;LaggedSales\u0026#39;,last[\u0026#39;Weekly_Sales\u0026#39;]) 16 sorted_df.set_value(index, \u0026#39;LaggedAvailable\u0026#39;,1) 17 else: 18 sorted_df.set_value(index, \u0026#39;LaggedSales\u0026#39;,row[\u0026#39;Median Sales\u0026#39;]) # Fill with median 19 sorted_df.set_value(index, \u0026#39;LaggedAvailable\u0026#39;,0) 20 21 last = row #Remember last row for speed 22 if(index%int(row_len/10)==0): #See progress by printing every 10% interval 23 print(str(int(index*100/row_len))+\u0026#39;% loaded\u0026#39;) 24 25print(sorted_df[[\u0026#39;Dept\u0026#39;, \u0026#39;Store\u0026#39;,\u0026#39;DateType\u0026#39;,\u0026#39;LaggedSales\u0026#39;,\u0026#39;Weekly_Sales\u0026#39;,\u0026#39;Median Sales\u0026#39;]].head()) 19% loaded 219% loaded 329% loaded 439% loaded 549% loaded 659% loaded 769% loaded 879% loaded 989% loaded 1099% loaded 11 Dept Store DateType LaggedSales Weekly_Sales Median Sales 120 Dept_1 Store_1 2010-02-05 23510.49 24924.50 23510.49 131 Dept_1 Store_1 2010-02-12 24924.50 46039.49 37887.17 142 Dept_1 Store_1 2010-02-19 46039.49 41595.55 23510.49 153 Dept_1 Store_1 2010-02-26 41595.55 19403.54 23510.49 164 Dept_1 Store_1 2010-03-05 19403.54 21827.90 21280.40 Công việc đơn giản tiếp theo là merge dữ liệu vào data chính và tính độ lệch giữa 2 tuần bán\n1# Merge by store and department 2df = df.merge(sorted_df[[\u0026#39;Dept\u0026#39;, \u0026#39;Store\u0026#39;,\u0026#39;DateType\u0026#39;,\u0026#39;LaggedSales\u0026#39;,\u0026#39;LaggedAvailable\u0026#39;]], how = \u0026#39;inner\u0026#39;, on = [\u0026#39;Dept\u0026#39;, \u0026#39;Store\u0026#39;,\u0026#39;DateType\u0026#39;]) 3df[\u0026#39;Sales_dif\u0026#39;] = df[\u0026#39;Median Sales\u0026#39;] - df[\u0026#39;LaggedSales\u0026#39;] Và bây giờ , thay vì ta ước lượng weekly sales, chúng ta sẽ ước lượng độ lệch giữa week sales và median sales (đây là một cách trong những cách để tính điểm dừng của dữ liệu time series)\n1df[\u0026#39;Difference\u0026#39;] = df[\u0026#39;Median Sales\u0026#39;] - df[\u0026#39;Weekly_Sales\u0026#39;] Huấn luyện mô hình Lựa chọn các đặc trưng huấn luyện\n1selector = [ 2 #\u0026#39;Month\u0026#39;, 3 \u0026#39;CPI\u0026#39;, 4 \u0026#39;Fuel_Price\u0026#39;, 5 \u0026#39;MarkDown1\u0026#39;, 6 \u0026#39;MarkDown2\u0026#39;, 7 \u0026#39;MarkDown3\u0026#39;, 8 \u0026#39;MarkDown4\u0026#39;, 9 \u0026#39;MarkDown5\u0026#39;, 10 \u0026#39;Size\u0026#39;, 11 \u0026#39;Temperature\u0026#39;, 12 \u0026#39;Unemployment\u0026#39;, 13 14 15 16 \u0026#39;md1_present\u0026#39;, 17 \u0026#39;md2_present\u0026#39;, 18 \u0026#39;md3_present\u0026#39;, 19 \u0026#39;md4_present\u0026#39;, 20 \u0026#39;md5_present\u0026#39;, 21 22 \u0026#39;IsHoliday_False\u0026#39;, 23 \u0026#39;IsHoliday_True\u0026#39;, 24 \u0026#39;Pre_christmas_no\u0026#39;, 25 \u0026#39;Pre_christmas_yes\u0026#39;, 26 \u0026#39;Black_Friday_no\u0026#39;, 27 \u0026#39;Black_Friday_yes\u0026#39;, 28 \u0026#39;LaggedSales\u0026#39;, 29 \u0026#39;Sales_dif\u0026#39;, 30 \u0026#39;LaggedAvailable\u0026#39; 31 ] Tách dữ liệu train và test riêng ra\n1 2train = df.loc[df[\u0026#39;Split\u0026#39;]==\u0026#39;Train\u0026#39;] 3test = df.loc[df[\u0026#39;Split\u0026#39;]==\u0026#39;Test\u0026#39;] Lấy ngẫu nhiên 20% dữ liệu ở tập train để validation\n1# Set seed for reproducability 2np.random.seed(42) 3X_train, X_val, y_train, y_val = train_test_split(train[selector], train[\u0026#39;Difference\u0026#39;], test_size=0.2, random_state=42) Huấn luyện bằng neural network sử dụng lstm\n1 2adam_regularized = Sequential() 3 4 # First hidden layer now regularized 5 model.add(Dense(32,activation=\u0026#39;relu\u0026#39;, 6 input_dim=X_train.shape[1], 7 kernel_regularizer = regularizers.l2(0.01))) 8 9 # Second hidden layer now regularized 10 adam_regularized.add(Dense(16,activation=\u0026#39;relu\u0026#39;, 11 kernel_regularizer = regularizers.l2(0.01))) 12 13 # Output layer stayed sigmoid 14 adam_regularized.add(Dense(1,activation=\u0026#39;linear\u0026#39;)) 15 16 # Setup adam optimizer 17 adam_optimizer=keras.optimizers.Adam(lr=0.01, 18 beta_1=0.9, 19 beta_2=0.999, 20 epsilon=1e-08) 21 22 # Compile the model 23 adam_regularized.compile(optimizer=adam_optimizer, 24 loss=\u0026#39;mean_absolute_error\u0026#39;, 25 metrics=[\u0026#39;acc\u0026#39;]) 26 27 # Train 28 history=adam_regularized.fit(X_train, y_train, # Train on training set 29 epochs=10, # We will train over 1,000 epochs 30 batch_size=2048, # Batch size 31 verbose=0) # Suppress Keras output 32 print(\u0026#39;eval\u0026#39;,model.evaluate(x=X_val,y=y_val)) 33 34 # Plot network 35 plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;Adam Regularized\u0026#39;) 36 plt.xlabel(\u0026#39;Epochs\u0026#39;) 37 plt.ylabel(\u0026#39;loss\u0026#39;) 38 plt.legend() 39 plt.show() 1eval: [1457.0501796214685, 0.002312783168124545] Độ lỗi trên tập train\nĐộ lỗi trên tập train giảm xuống đến gần 1450 thì đừng hẳn, không thể giảm được nữa\nGiá trị độ lệch trên tập evaluation là 1457.0501796214685\nThử huấn luyện bằng random forest\n1regr = RandomForestRegressor(n_estimators=20, criterion=\u0026#39;mse\u0026#39;, max_depth=None, 2 min_samples_split=2, min_samples_leaf=1, 3 min_weight_fraction_leaf=0.0, max_features=\u0026#39;auto\u0026#39;, 4 max_leaf_nodes=None, min_impurity_decrease=0.0, 5 min_impurity_split=None, bootstrap=True, 6 oob_score=False, n_jobs=1, random_state=None, 7 verbose=2, warm_start=False) 8 9 #Train on data 10 regr.fit(X_train, y_train.ravel()) 11 y_pred_random = regr.predict(X_val) 12 13 y_val = y_val.to_frame() 14 15 # Transform forest predictions to observe direction of change 16 direction_true1= y_val.values 17 direction_predict = y_pred_random 18 19 y_val[\u0026#39;Predicted\u0026#39;] = y_pred_random 20 df_out = pd.merge(train,y_val[[\u0026#39;Predicted\u0026#39;]],how = \u0026#39;left\u0026#39;,left_index = True, right_index = True,suffixes=[\u0026#39;_True\u0026#39;,\u0026#39;_Pred\u0026#39;]) 21 df_out = df_out[~pd.isnull(df_out[\u0026#39;Predicted\u0026#39;])] 22 23 df_out[\u0026#39;prediction\u0026#39;] = df_out[\u0026#39;Median Sales\u0026#39;]-df_out[\u0026#39;Predicted\u0026#39;] 24 25 print(\u0026#34;Medians: \u0026#34;+str(sum(abs(df_out[\u0026#39;Difference\u0026#39;]))/df_out.shape[0])) 26 print(\u0026#34;Random Forest: \u0026#34;+str(sum(abs(df_out[\u0026#39;Weekly_Sales\u0026#39;]-df_out[\u0026#39;prediction\u0026#39;]))/df_out.shape[0])) Kết quả\n1 29% loaded 319% loaded 429% loaded 539% loaded 649% loaded 759% loaded 869% loaded 979% loaded 1089% loaded 1199% loaded 12 Dept Store DateType LaggedSales Weekly_Sales Median Sales 130 Dept_1 Store_1 2010-02-05 23510.49 24924.50 23510.49 141 Dept_1 Store_1 2010-02-12 24924.50 46039.49 37887.17 152 Dept_1 Store_1 2010-02-19 46039.49 41595.55 23510.49 163 Dept_1 Store_1 2010-02-26 41595.55 19403.54 23510.49 174 Dept_1 Store_1 2010-03-05 19403.54 21827.90 21280.40 18[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. 19building tree 1 of 20 20[Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 6.5s remaining: 0.0s 21building tree 2 of 20 22building tree 3 of 20 23building tree 4 of 20 24building tree 5 of 20 25building tree 6 of 20 26building tree 7 of 20 27building tree 8 of 20 28building tree 9 of 20 29building tree 10 of 20 30building tree 11 of 20 31building tree 12 of 20 32building tree 13 of 20 33building tree 14 of 20 34building tree 15 of 20 35building tree 16 of 20 36building tree 17 of 20 37building tree 18 of 20 38building tree 19 of 20 39building tree 20 of 20 40[Parallel(n_jobs=1)]: Done 20 out of 20 | elapsed: 2.2min finished 41[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. 42[Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 0.0s remaining: 0.0s 43[Parallel(n_jobs=1)]: Done 20 out of 20 | elapsed: 1.1s finished 44Medians: 1545.7406070759525 45Random Forest: 1356.4670052620745 Trung bình lệch của random forest là 1356, giá trị này nhỏ hơn so với giá trị output của lstm trả về.\nThử huấn luyện bằng XGBoost\n1 2param_dist = { \u0026#39;max_depth\u0026#39;:5} 3 4 model = XGBRegressor(**param_dist) 5 6 #Train on data 7 model.fit(X_train, y_train.ravel()) 8 y_pred_random = model.predict(X_val) 9 10 y_val = y_val.to_frame() 11 12 # Transform forest predictions to observe direction of change 13 direction_true1= y_val.values 14 direction_predict = y_pred_random 15 16 y_val[\u0026#39;Predicted\u0026#39;] = y_pred_random 17 df_out = pd.merge(train,y_val[[\u0026#39;Predicted\u0026#39;]],how = \u0026#39;left\u0026#39;,left_index = True, right_index = True,suffixes=[\u0026#39;_True\u0026#39;,\u0026#39;_Pred\u0026#39;]) 18 df_out = df_out[~pd.isnull(df_out[\u0026#39;Predicted\u0026#39;])] 19 20 df_out[\u0026#39;prediction\u0026#39;] = df_out[\u0026#39;Median Sales\u0026#39;]-df_out[\u0026#39;Predicted\u0026#39;] 21 22 print(\u0026#34;Medians: \u0026#34;+str(sum(abs(df_out[\u0026#39;Difference\u0026#39;]))/df_out.shape[0])) 23 print(\u0026#34;XGB Regressor: \u0026#34;+str(sum(abs(df_out[\u0026#39;Weekly_Sales\u0026#39;]-df_out[\u0026#39;prediction\u0026#39;]))/df_out.shape[0])) Kết quả\n1 2Medians: 1545.7406070759525 3XGB Regressor: 1354.1976755192593 Kết quả cũng gần như bằng Random forest :).\nGiờ mình sẽ dùng random forest để tạo file submission\n1 2 3rf_model = RandomForestRegressor(n_estimators=80, criterion=\u0026#39;mse\u0026#39;, max_depth=None, 4 min_samples_split=2, min_samples_leaf=1, 5 min_weight_fraction_leaf=0.0, max_features=\u0026#39;auto\u0026#39;, 6 max_leaf_nodes=None, min_impurity_decrease=0.0, 7 min_impurity_split=None, bootstrap=True, 8 oob_score=False, n_jobs=1, random_state=None, 9 verbose=0, warm_start=False) 10 11#Train on data 12rf_model.fit(train[selector], train[\u0026#39;Difference\u0026#39;]) 13final_y_prediction = rf_model.predict(test[selector]) 14 15testfile = pd.concat([test.reset_index(drop=True), pd.DataFrame(final_y_prediction)], axis=1) 16testfile[\u0026#39;prediction\u0026#39;] = testfile[\u0026#39;Median Sales\u0026#39;]-testfile[0] 17 18submission = pd.DataFrame({\u0026#39;id\u0026#39;:pd.Series([\u0026#39;\u0026#39;.join(list(filter(str.isdigit, x))) for x in testfile[\u0026#39;Store\u0026#39;]]).map(str) + \u0026#39;_\u0026#39; + 19 pd.Series([\u0026#39;\u0026#39;.join(list(filter(str.isdigit, x))) for x in testfile[\u0026#39;Dept\u0026#39;]]).map(str) + \u0026#39;_\u0026#39; + 20 testfile[\u0026#39;Date\u0026#39;].map(str), 21 \u0026#39;Weekly_Sales\u0026#39;:testfile[\u0026#39;prediction\u0026#39;]}) 22 23submission.to_csv(\u0026#39;submission.csv\u0026#39;,index=False) Sau khi submit mô hình, mình đạt được kết quả là 4455.96312 trên private board, và 4419.17292 trên publish board. Đây là một kết quả khá tệ (đứng hạng khoảng top 300). Sau khi mình nhìn lại mô hình thì phát hiện một số vấn đề.\nCác đặc trưng trong file features.csv nó không có mối tương quan gì hết với số bán như phân tích ở trên -\u0026gt; mình mạnh dạng bỏ luôn file features.csv, không quan tâm đến nó nữa, tập trung vào file chính.\nBỏ mấy cái lag luôn, thử forecast chính vào cái số bán luôn xem sao\nVới cửa hàng nào thì xây dựng mô hình cho cửa hàng và sản phẩm đó, không xây dựng một mô hình tổng quát áp dụng cho toàn cửa hàng. với những cửa hàng không có trong tập train hoặc những sản phẩm mà cửa hàng đó chưa bán trước đây (nói chung là không có trong tập train) thì mới áp dụng mô hình của toàn cửa hàng cho nó.\nKết quả là mình đạt được 2736 trên private board và 2657.40087 trên publish board (top 30), kết quả trên vẫn làm cho mình chưa hài lòng lắm.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Apr 17, 2019","img":"","permalink":"/blog/2019-04-17-walmart-store-sales-forecasting/","series":null,"tags":["walmart","forecast","dự đoán"],"title":"Dự Đoán Doanh Số Bán Của Các Cửa Hàng Walmart"},{"categories":null,"content":" Thực hiện Thu thập hình ảnh và tiền xử lý Thực hiện Đây là một bài toán tiếp cận bằng Deep Learning, nên việc thu thập nhiều dữ liệu có ý nghĩa rất quang trọng trong việc đóng góp vào độ chính xác của mô hình. Ở đây, chúng ta sẽ download tập dữ liệu ảnh của http://places2.csail.mit.edu/download.html và sử dụng mạng UNet để huấn luyện mô hình.\nThu thập hình ảnh và tiền xử lý Dữ liệu sẽ được download tại địa chỉ http://data.csail.mit.edu/places/places365/train_256_places365challenge.tar. Tập trên có kích thước 108 GB. Đây là tập ảnh thuộc hệ màu RGB. Chúng ta sẽ chuyển tập ảnh trên về hệ màu grayscale làm ảnh gốc cho quá trình huấn luyện. Có một mẹo nhỏ cho chúng ta rút ngắn quá trình huấn luyện nhưng vẫn đảm bảo được độ chính xác của mô hình là ngoài kênh màu RGB mà chúng ta hay xài, trên thế giới còn có kênh màu HSV, trong đó nếu chúng ta chuyển một ảnh ở kênh màu RGB về hệ màu HSV, và bỏ đi các giá trị H, S, chỉ giữ lại giá trị V, thì chất lượng ảnh xám của nó gần như là tương đương với ảnh grayscale sử dụng công thức \u0026ldquo;thần thánh\u0026rdquo; mà chúng ta được học ở môn xử lý ảnh grayscale =0.30*R + 0.59*G + 0.11*B\nVì vậy, thay vì việc input là giá trị xám của ảnh, output là giá trị của các kênh màu RGB, chúng ta sẽ chuyển đổi bài toán lại là input là giá trị xám, output là giá trị H và S.\nMô hình mạng Unet\nMạng UNet là một mạng neural network được dùng khá phổ biến trong các cuộc thi phân đoạn ảnh, độ chính xác của nó so với các thuật toán khác là vượt trội hoàn toàn. Ở đây, chúng ta có 2 hướng tiếp cận, một là build một mạng Unet và random init weight rồi huấn luyện nó, cách thứ hai là build mạng unet sử dụng pretrain model rồi huấn luyện. Bởi vì đặc trưng của các pretrain model hoạt động khá tốt và được huấn luyện trên tập dataset lớn, nên mình sẻ sử dụng nó ở bài viết này. Song song đó, mình sẽ cung cấp một giải pháp kèm theo sử để sử dụng mạng mà không dùng pretrain model.\nÚ tưởng chính của mạng UNet tựa tựa như auto-encoder, từ ảnh gốc ban đầu, chúng sẽ được nén thông tin lại qua các phép biến đổi Conv2D (như các chú thích màu sắc của mũi tên trong hình trên), sau đó sẽ được \u0026ldquo;giải nén\u0026rdquo; về lại ảnh gốc ban đầu. Việc huấn luyện coi như là hoàn tất 100% nếu ảnh gốc với ảnh giải nén là là giống nhau hoàn toàn.\nBài viết sẽ được cập nhật\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Apr 16, 2019","img":"","permalink":"/blog/2019-04-16-colorfull-grayscale-to-color/","series":null,"tags":["machine learning","deep learning","neural network","amazone","thế giới di động","mwg"],"title":"Thử Làm Ứng Dụng Tô Màu Ảnh Xám Thành Ảnh Màu Sử Dụng Tensorflow"},{"categories":null,"content":" Lời mở đầu Thực hiện Lời mở đầu Ở trong bài viết này, chúng ta sẽ sử dụng tập dữ liệu là tập dữ liệu ở ở link https://www.kaggle.com/alxmamaev/flowers-recognition. Tập dữ liệu này bao gồm 4242 hình cảnh của 5 loại hoa hồng (rose), hoa mặt trời (sunflower), hoa bồ công anh (dandelion), hoa cúc (daisy) và hoa tulip. Nhóm tác giả đã thu thập dữ liệu dựa trên các trang web flicr, google images, yandex. Tập hình ảnh được chia thành 5 lớp, mỗi lớp có khoảng 800 hình, có kích thước xấp xỉ 320x320 pixel. Các hình ảnh có kích thước không đồng nhất với nhau.\nThực hiện Dữ liệu sau khi giản nén có dạng\n1data_dir/classname1/*.* 2data_dir/classname2/*.* 3... Cấu trúc lưu trũ như này đúng với mô hình của mình nên chúng ta cần nên chúng ta không thay đổi gì về câu trúc nữa, tiến hành viết code\nĐầu tiên, chúng ta sẽ load dataset lên và tranform nó để đưa vào huấn luyện.\n1import sys 2import os 3from collections import defaultdict 4import numpy as np 5import scipy.misc 6 7 8def preprocess_input(x0): 9 x = x0 / 255. 10 x -= 0.5 11 x *= 2. 12 return x 13 14 15def reverse_preprocess_input(x0): 16 x = x0 / 2.0 17 x += 0.5 18 x *= 255. 19 return x 20 21 22def dataset(base_dir, n): 23 print(\u0026#34;base dir: \u0026#34;+base_dir) 24 print(\u0026#34;n: \u0026#34;+str(n)) 25 n = int(n) 26 d = defaultdict(list) 27 for root, subdirs, files in os.walk(base_dir): 28 for filename in files: 29 file_path = os.path.join(root, filename) 30 assert file_path.startswith(base_dir) 31 32 suffix = file_path[len(base_dir):] 33 34 suffix = suffix.lstrip(\u0026#34;/\u0026#34;) 35 suffix = suffix.lstrip(\u0026#34;\\\\\u0026#34;) 36 if(suffix.find(\u0026#39;/\u0026#39;)\u0026gt;-1): #linux 37 label = suffix.split(\u0026#34;/\u0026#34;)[0] 38 else: #window 39 label = suffix.split(\u0026#34;\\\\\u0026#34;)[0] 40 d[label].append(file_path) 41 print(\u0026#34;walk directory complete\u0026#34;) 42 tags = sorted(d.keys()) 43 44 processed_image_count = 0 45 useful_image_count = 0 46 47 X = [] 48 y = [] 49 50 for class_index, class_name in enumerate(tags): 51 filenames = d[class_name] 52 for filename in filenames: 53 processed_image_count += 1 54 if processed_image_count%100 ==0: 55 print(class_name+\u0026#34;\\tprocess: \u0026#34;+str(processed_image_count)+\u0026#34;\\t\u0026#34;+str(len(d[class_name]))) 56 img = scipy.misc.imread(filename) 57 height, width, chan = img.shape 58 assert chan == 3 59 aspect_ratio = float(max((height, width))) / min((height, width)) 60 if aspect_ratio \u0026gt; 2: 61 continue 62 # We pick the largest center square. 63 centery = height // 2 64 centerx = width // 2 65 radius = min((centerx, centery)) 66 img = img[centery-radius:centery+radius, centerx-radius:centerx+radius] 67 img = scipy.misc.imresize(img, size=(n, n), interp=\u0026#39;bilinear\u0026#39;) 68 X.append(img) 69 y.append(class_index) 70 useful_image_count += 1 71 print(\u0026#34;processed %d, used %d\u0026#34; % (processed_image_count, useful_image_count)) 72 73 X = np.array(X).astype(np.float32) 74 #X = X.transpose((0, 3, 1, 2)) 75 X = preprocess_input(X) 76 y = np.array(y) 77 78 perm = np.random.permutation(len(y)) 79 X = X[perm] 80 y = y[perm] 81 82 print(\u0026#34;classes:\u0026#34;,end=\u0026#34; \u0026#34;) 83 for class_index, class_name in enumerate(tags): 84 print(class_name, sum(y==class_index),end=\u0026#34; \u0026#34;) 85 print(\u0026#34;X shape: \u0026#34;,X.shape) 86 87 return X, y, tags Đoạn code trên khá đơn giản và dễ hiểu. Lưu ý ở đây là với những bức ảnh có tỷ lệ width và height \u0026gt; 2 thì mình sẽ loại chúng ra khỏi tập dữ liệu.\nTiếp theo, chúng ta sẽ xây dựng mô hình dựa trên mô hình Resnet50 có sẵn của kares, do sử dụng pretrain model, nên n-1 lớp trước đó sẽ không được huấn luyện và chúng ta sẽ sử dụng dụng các weight có sẵn đã được huấn luyện trên tập ImageNet rút đặc trưng cho mô hình. Chúng ta chỉ cần thêm một lớp full connected và softmax để phân lớp các loại hoa, công việc của chúng ta hiện tại là tìm ra trọng số của lớp full connected cuối cùng (thay vì huấn luyện lại hết toàn bộ mô hình).\n1 2# create the base pre-trained model 3def build_model(nb_classes): 4 base_model = ResNet50(weights=\u0026#39;imagenet\u0026#39;, include_top=False) 5 6 # add a global spatial average pooling layer 7 x = base_model.output 8 x = GlobalAveragePooling2D()(x) 9 # let\u0026#39;s add a fully-connected layer 10 x = Dense(1024, activation=\u0026#39;relu\u0026#39;)(x) 11 # and a logistic layer 12 predictions = Dense(nb_classes, activation=\u0026#39;softmax\u0026#39;)(x) 13 14 # this is the model we will train 15 model = Model(inputs=base_model.input, outputs=predictions) 16 17 # first: train only the top layers (which were randomly initialized) 18 # i.e. freeze all convolutional ResNet50 layers 19 for layer in base_model.layers: 20 layer.trainable = False 21 22 return model Visualize một chút xíu về kiến trúc inceptionV3 mình đang dùng.\n1__________________________________________________________________________________________________ 2Layer (type) Output Shape Param # Connected to 3================================================================================================== 4input_1 (InputLayer) (None, None, None, 3 0 5__________________________________________________________________________________________________ 6conv1_pad (ZeroPadding2D) (None, None, None, 3 0 input_1[0][0] 7__________________________________________________________________________________________________ 8conv1 (Conv2D) (None, None, None, 6 9472 conv1_pad[0][0] 9__________________________________________________________________________________________________ 10bn_conv1 (BatchNormalization) (None, None, None, 6 256 conv1[0][0] 11__________________________________________________________________________________________________ 12activation_1 (Activation) (None, None, None, 6 0 bn_conv1[0][0] 13__________________________________________________________________________________________________ 14pool1_pad (ZeroPadding2D) (None, None, None, 6 0 activation_1[0][0] 15__________________________________________________________________________________________________ 16max_pooling2d_1 (MaxPooling2D) (None, None, None, 6 0 pool1_pad[0][0] 17__________________________________________________________________________________________________ 18res2a_branch2a (Conv2D) (None, None, None, 6 4160 max_pooling2d_1[0][0] 19__________________________________________________________________________________________________ 20bn2a_branch2a (BatchNormalizati (None, None, None, 6 256 res2a_branch2a[0][0] 21__________________________________________________________________________________________________ 22activation_2 (Activation) (None, None, None, 6 0 bn2a_branch2a[0][0] 23__________________________________________________________________________________________________ 24res2a_branch2b (Conv2D) (None, None, None, 6 36928 activation_2[0][0] 25__________________________________________________________________________________________________ 26bn2a_branch2b (BatchNormalizati (None, None, None, 6 256 res2a_branch2b[0][0] 27__________________________________________________________________________________________________ 28activation_3 (Activation) (None, None, None, 6 0 bn2a_branch2b[0][0] 29__________________________________________________________________________________________________ 30res2a_branch2c (Conv2D) (None, None, None, 2 16640 activation_3[0][0] 31__________________________________________________________________________________________________ 32res2a_branch1 (Conv2D) (None, None, None, 2 16640 max_pooling2d_1[0][0] 33__________________________________________________________________________________________________ 34bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024 res2a_branch2c[0][0] 35__________________________________________________________________________________________________ 36bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024 res2a_branch1[0][0] 37__________________________________________________________________________________________________ 38add_1 (Add) (None, None, None, 2 0 bn2a_branch2c[0][0] 39 bn2a_branch1[0][0] 40__________________________________________________________________________________________________ 41activation_4 (Activation) (None, None, None, 2 0 add_1[0][0] 42__________________________________________________________________________________________________ 43res2b_branch2a (Conv2D) (None, None, None, 6 16448 activation_4[0][0] 44__________________________________________________________________________________________________ 45bn2b_branch2a (BatchNormalizati (None, None, None, 6 256 res2b_branch2a[0][0] 46__________________________________________________________________________________________________ 47activation_5 (Activation) (None, None, None, 6 0 bn2b_branch2a[0][0] 48__________________________________________________________________________________________________ 49res2b_branch2b (Conv2D) (None, None, None, 6 36928 activation_5[0][0] 50__________________________________________________________________________________________________ 51bn2b_branch2b (BatchNormalizati (None, None, None, 6 256 res2b_branch2b[0][0] 52__________________________________________________________________________________________________ 53activation_6 (Activation) (None, None, None, 6 0 bn2b_branch2b[0][0] 54__________________________________________________________________________________________________ 55res2b_branch2c (Conv2D) (None, None, None, 2 16640 activation_6[0][0] 56__________________________________________________________________________________________________ 57bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024 res2b_branch2c[0][0] 58__________________________________________________________________________________________________ 59add_2 (Add) (None, None, None, 2 0 bn2b_branch2c[0][0] 60 activation_4[0][0] 61__________________________________________________________________________________________________ 62activation_7 (Activation) (None, None, None, 2 0 add_2[0][0] 63__________________________________________________________________________________________________ 64res2c_branch2a (Conv2D) (None, None, None, 6 16448 activation_7[0][0] 65__________________________________________________________________________________________________ 66bn2c_branch2a (BatchNormalizati (None, None, None, 6 256 res2c_branch2a[0][0] 67__________________________________________________________________________________________________ 68activation_8 (Activation) (None, None, None, 6 0 bn2c_branch2a[0][0] 69__________________________________________________________________________________________________ 70res2c_branch2b (Conv2D) (None, None, None, 6 36928 activation_8[0][0] 71__________________________________________________________________________________________________ 72bn2c_branch2b (BatchNormalizati (None, None, None, 6 256 res2c_branch2b[0][0] 73__________________________________________________________________________________________________ 74activation_9 (Activation) (None, None, None, 6 0 bn2c_branch2b[0][0] 75__________________________________________________________________________________________________ 76res2c_branch2c (Conv2D) (None, None, None, 2 16640 activation_9[0][0] 77__________________________________________________________________________________________________ 78bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024 res2c_branch2c[0][0] 79__________________________________________________________________________________________________ 80add_3 (Add) (None, None, None, 2 0 bn2c_branch2c[0][0] 81 activation_7[0][0] 82__________________________________________________________________________________________________ 83activation_10 (Activation) (None, None, None, 2 0 add_3[0][0] 84__________________________________________________________________________________________________ 85res3a_branch2a (Conv2D) (None, None, None, 1 32896 activation_10[0][0] 86__________________________________________________________________________________________________ 87bn3a_branch2a (BatchNormalizati (None, None, None, 1 512 res3a_branch2a[0][0] 88__________________________________________________________________________________________________ 89activation_11 (Activation) (None, None, None, 1 0 bn3a_branch2a[0][0] 90__________________________________________________________________________________________________ 91res3a_branch2b (Conv2D) (None, None, None, 1 147584 activation_11[0][0] 92__________________________________________________________________________________________________ 93bn3a_branch2b (BatchNormalizati (None, None, None, 1 512 res3a_branch2b[0][0] 94__________________________________________________________________________________________________ 95activation_12 (Activation) (None, None, None, 1 0 bn3a_branch2b[0][0] 96__________________________________________________________________________________________________ 97res3a_branch2c (Conv2D) (None, None, None, 5 66048 activation_12[0][0] 98__________________________________________________________________________________________________ 99res3a_branch1 (Conv2D) (None, None, None, 5 131584 activation_10[0][0] 100__________________________________________________________________________________________________ 101bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048 res3a_branch2c[0][0] 102__________________________________________________________________________________________________ 103bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048 res3a_branch1[0][0] 104__________________________________________________________________________________________________ 105add_4 (Add) (None, None, None, 5 0 bn3a_branch2c[0][0] 106 bn3a_branch1[0][0] 107__________________________________________________________________________________________________ 108activation_13 (Activation) (None, None, None, 5 0 add_4[0][0] 109__________________________________________________________________________________________________ 110res3b_branch2a (Conv2D) (None, None, None, 1 65664 activation_13[0][0] 111__________________________________________________________________________________________________ 112bn3b_branch2a (BatchNormalizati (None, None, None, 1 512 res3b_branch2a[0][0] 113__________________________________________________________________________________________________ 114activation_14 (Activation) (None, None, None, 1 0 bn3b_branch2a[0][0] 115__________________________________________________________________________________________________ 116res3b_branch2b (Conv2D) (None, None, None, 1 147584 activation_14[0][0] 117__________________________________________________________________________________________________ 118bn3b_branch2b (BatchNormalizati (None, None, None, 1 512 res3b_branch2b[0][0] 119__________________________________________________________________________________________________ 120activation_15 (Activation) (None, None, None, 1 0 bn3b_branch2b[0][0] 121__________________________________________________________________________________________________ 122res3b_branch2c (Conv2D) (None, None, None, 5 66048 activation_15[0][0] 123__________________________________________________________________________________________________ 124bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048 res3b_branch2c[0][0] 125__________________________________________________________________________________________________ 126add_5 (Add) (None, None, None, 5 0 bn3b_branch2c[0][0] 127 activation_13[0][0] 128__________________________________________________________________________________________________ 129activation_16 (Activation) (None, None, None, 5 0 add_5[0][0] 130__________________________________________________________________________________________________ 131res3c_branch2a (Conv2D) (None, None, None, 1 65664 activation_16[0][0] 132__________________________________________________________________________________________________ 133bn3c_branch2a (BatchNormalizati (None, None, None, 1 512 res3c_branch2a[0][0] 134__________________________________________________________________________________________________ 135activation_17 (Activation) (None, None, None, 1 0 bn3c_branch2a[0][0] 136__________________________________________________________________________________________________ 137res3c_branch2b (Conv2D) (None, None, None, 1 147584 activation_17[0][0] 138__________________________________________________________________________________________________ 139bn3c_branch2b (BatchNormalizati (None, None, None, 1 512 res3c_branch2b[0][0] 140__________________________________________________________________________________________________ 141activation_18 (Activation) (None, None, None, 1 0 bn3c_branch2b[0][0] 142__________________________________________________________________________________________________ 143res3c_branch2c (Conv2D) (None, None, None, 5 66048 activation_18[0][0] 144__________________________________________________________________________________________________ 145bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048 res3c_branch2c[0][0] 146__________________________________________________________________________________________________ 147add_6 (Add) (None, None, None, 5 0 bn3c_branch2c[0][0] 148 activation_16[0][0] 149__________________________________________________________________________________________________ 150activation_19 (Activation) (None, None, None, 5 0 add_6[0][0] 151__________________________________________________________________________________________________ 152res3d_branch2a (Conv2D) (None, None, None, 1 65664 activation_19[0][0] 153__________________________________________________________________________________________________ 154bn3d_branch2a (BatchNormalizati (None, None, None, 1 512 res3d_branch2a[0][0] 155__________________________________________________________________________________________________ 156activation_20 (Activation) (None, None, None, 1 0 bn3d_branch2a[0][0] 157__________________________________________________________________________________________________ 158res3d_branch2b (Conv2D) (None, None, None, 1 147584 activation_20[0][0] 159__________________________________________________________________________________________________ 160bn3d_branch2b (BatchNormalizati (None, None, None, 1 512 res3d_branch2b[0][0] 161__________________________________________________________________________________________________ 162activation_21 (Activation) (None, None, None, 1 0 bn3d_branch2b[0][0] 163__________________________________________________________________________________________________ 164res3d_branch2c (Conv2D) (None, None, None, 5 66048 activation_21[0][0] 165__________________________________________________________________________________________________ 166bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048 res3d_branch2c[0][0] 167__________________________________________________________________________________________________ 168add_7 (Add) (None, None, None, 5 0 bn3d_branch2c[0][0] 169 activation_19[0][0] 170__________________________________________________________________________________________________ 171activation_22 (Activation) (None, None, None, 5 0 add_7[0][0] 172__________________________________________________________________________________________________ 173res4a_branch2a (Conv2D) (None, None, None, 2 131328 activation_22[0][0] 174__________________________________________________________________________________________________ 175bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024 res4a_branch2a[0][0] 176__________________________________________________________________________________________________ 177activation_23 (Activation) (None, None, None, 2 0 bn4a_branch2a[0][0] 178__________________________________________________________________________________________________ 179res4a_branch2b (Conv2D) (None, None, None, 2 590080 activation_23[0][0] 180__________________________________________________________________________________________________ 181bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024 res4a_branch2b[0][0] 182__________________________________________________________________________________________________ 183activation_24 (Activation) (None, None, None, 2 0 bn4a_branch2b[0][0] 184__________________________________________________________________________________________________ 185res4a_branch2c (Conv2D) (None, None, None, 1 263168 activation_24[0][0] 186__________________________________________________________________________________________________ 187res4a_branch1 (Conv2D) (None, None, None, 1 525312 activation_22[0][0] 188__________________________________________________________________________________________________ 189bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096 res4a_branch2c[0][0] 190__________________________________________________________________________________________________ 191bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096 res4a_branch1[0][0] 192__________________________________________________________________________________________________ 193add_8 (Add) (None, None, None, 1 0 bn4a_branch2c[0][0] 194 bn4a_branch1[0][0] 195__________________________________________________________________________________________________ 196activation_25 (Activation) (None, None, None, 1 0 add_8[0][0] 197__________________________________________________________________________________________________ 198res4b_branch2a (Conv2D) (None, None, None, 2 262400 activation_25[0][0] 199__________________________________________________________________________________________________ 200bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024 res4b_branch2a[0][0] 201__________________________________________________________________________________________________ 202activation_26 (Activation) (None, None, None, 2 0 bn4b_branch2a[0][0] 203__________________________________________________________________________________________________ 204res4b_branch2b (Conv2D) (None, None, None, 2 590080 activation_26[0][0] 205__________________________________________________________________________________________________ 206bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024 res4b_branch2b[0][0] 207__________________________________________________________________________________________________ 208activation_27 (Activation) (None, None, None, 2 0 bn4b_branch2b[0][0] 209__________________________________________________________________________________________________ 210res4b_branch2c (Conv2D) (None, None, None, 1 263168 activation_27[0][0] 211__________________________________________________________________________________________________ 212bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096 res4b_branch2c[0][0] 213__________________________________________________________________________________________________ 214add_9 (Add) (None, None, None, 1 0 bn4b_branch2c[0][0] 215 activation_25[0][0] 216__________________________________________________________________________________________________ 217activation_28 (Activation) (None, None, None, 1 0 add_9[0][0] 218__________________________________________________________________________________________________ 219res4c_branch2a (Conv2D) (None, None, None, 2 262400 activation_28[0][0] 220__________________________________________________________________________________________________ 221bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024 res4c_branch2a[0][0] 222__________________________________________________________________________________________________ 223activation_29 (Activation) (None, None, None, 2 0 bn4c_branch2a[0][0] 224__________________________________________________________________________________________________ 225res4c_branch2b (Conv2D) (None, None, None, 2 590080 activation_29[0][0] 226__________________________________________________________________________________________________ 227bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024 res4c_branch2b[0][0] 228__________________________________________________________________________________________________ 229activation_30 (Activation) (None, None, None, 2 0 bn4c_branch2b[0][0] 230__________________________________________________________________________________________________ 231res4c_branch2c (Conv2D) (None, None, None, 1 263168 activation_30[0][0] 232__________________________________________________________________________________________________ 233bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096 res4c_branch2c[0][0] 234__________________________________________________________________________________________________ 235add_10 (Add) (None, None, None, 1 0 bn4c_branch2c[0][0] 236 activation_28[0][0] 237__________________________________________________________________________________________________ 238activation_31 (Activation) (None, None, None, 1 0 add_10[0][0] 239__________________________________________________________________________________________________ 240res4d_branch2a (Conv2D) (None, None, None, 2 262400 activation_31[0][0] 241__________________________________________________________________________________________________ 242bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024 res4d_branch2a[0][0] 243__________________________________________________________________________________________________ 244activation_32 (Activation) (None, None, None, 2 0 bn4d_branch2a[0][0] 245__________________________________________________________________________________________________ 246res4d_branch2b (Conv2D) (None, None, None, 2 590080 activation_32[0][0] 247__________________________________________________________________________________________________ 248bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024 res4d_branch2b[0][0] 249__________________________________________________________________________________________________ 250activation_33 (Activation) (None, None, None, 2 0 bn4d_branch2b[0][0] 251__________________________________________________________________________________________________ 252res4d_branch2c (Conv2D) (None, None, None, 1 263168 activation_33[0][0] 253__________________________________________________________________________________________________ 254bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096 res4d_branch2c[0][0] 255__________________________________________________________________________________________________ 256add_11 (Add) (None, None, None, 1 0 bn4d_branch2c[0][0] 257 activation_31[0][0] 258__________________________________________________________________________________________________ 259activation_34 (Activation) (None, None, None, 1 0 add_11[0][0] 260__________________________________________________________________________________________________ 261res4e_branch2a (Conv2D) (None, None, None, 2 262400 activation_34[0][0] 262__________________________________________________________________________________________________ 263bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024 res4e_branch2a[0][0] 264__________________________________________________________________________________________________ 265activation_35 (Activation) (None, None, None, 2 0 bn4e_branch2a[0][0] 266__________________________________________________________________________________________________ 267res4e_branch2b (Conv2D) (None, None, None, 2 590080 activation_35[0][0] 268__________________________________________________________________________________________________ 269bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024 res4e_branch2b[0][0] 270__________________________________________________________________________________________________ 271activation_36 (Activation) (None, None, None, 2 0 bn4e_branch2b[0][0] 272__________________________________________________________________________________________________ 273res4e_branch2c (Conv2D) (None, None, None, 1 263168 activation_36[0][0] 274__________________________________________________________________________________________________ 275bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096 res4e_branch2c[0][0] 276__________________________________________________________________________________________________ 277add_12 (Add) (None, None, None, 1 0 bn4e_branch2c[0][0] 278 activation_34[0][0] 279__________________________________________________________________________________________________ 280activation_37 (Activation) (None, None, None, 1 0 add_12[0][0] 281__________________________________________________________________________________________________ 282res4f_branch2a (Conv2D) (None, None, None, 2 262400 activation_37[0][0] 283__________________________________________________________________________________________________ 284bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024 res4f_branch2a[0][0] 285__________________________________________________________________________________________________ 286activation_38 (Activation) (None, None, None, 2 0 bn4f_branch2a[0][0] 287__________________________________________________________________________________________________ 288res4f_branch2b (Conv2D) (None, None, None, 2 590080 activation_38[0][0] 289__________________________________________________________________________________________________ 290bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024 res4f_branch2b[0][0] 291__________________________________________________________________________________________________ 292activation_39 (Activation) (None, None, None, 2 0 bn4f_branch2b[0][0] 293__________________________________________________________________________________________________ 294res4f_branch2c (Conv2D) (None, None, None, 1 263168 activation_39[0][0] 295__________________________________________________________________________________________________ 296bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096 res4f_branch2c[0][0] 297__________________________________________________________________________________________________ 298add_13 (Add) (None, None, None, 1 0 bn4f_branch2c[0][0] 299 activation_37[0][0] 300__________________________________________________________________________________________________ 301activation_40 (Activation) (None, None, None, 1 0 add_13[0][0] 302__________________________________________________________________________________________________ 303res5a_branch2a (Conv2D) (None, None, None, 5 524800 activation_40[0][0] 304__________________________________________________________________________________________________ 305bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048 res5a_branch2a[0][0] 306__________________________________________________________________________________________________ 307activation_41 (Activation) (None, None, None, 5 0 bn5a_branch2a[0][0] 308__________________________________________________________________________________________________ 309res5a_branch2b (Conv2D) (None, None, None, 5 2359808 activation_41[0][0] 310__________________________________________________________________________________________________ 311bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048 res5a_branch2b[0][0] 312__________________________________________________________________________________________________ 313activation_42 (Activation) (None, None, None, 5 0 bn5a_branch2b[0][0] 314__________________________________________________________________________________________________ 315res5a_branch2c (Conv2D) (None, None, None, 2 1050624 activation_42[0][0] 316__________________________________________________________________________________________________ 317res5a_branch1 (Conv2D) (None, None, None, 2 2099200 activation_40[0][0] 318__________________________________________________________________________________________________ 319bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192 res5a_branch2c[0][0] 320__________________________________________________________________________________________________ 321bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192 res5a_branch1[0][0] 322__________________________________________________________________________________________________ 323add_14 (Add) (None, None, None, 2 0 bn5a_branch2c[0][0] 324 bn5a_branch1[0][0] 325__________________________________________________________________________________________________ 326activation_43 (Activation) (None, None, None, 2 0 add_14[0][0] 327__________________________________________________________________________________________________ 328res5b_branch2a (Conv2D) (None, None, None, 5 1049088 activation_43[0][0] 329__________________________________________________________________________________________________ 330bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048 res5b_branch2a[0][0] 331__________________________________________________________________________________________________ 332activation_44 (Activation) (None, None, None, 5 0 bn5b_branch2a[0][0] 333__________________________________________________________________________________________________ 334res5b_branch2b (Conv2D) (None, None, None, 5 2359808 activation_44[0][0] 335__________________________________________________________________________________________________ 336bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048 res5b_branch2b[0][0] 337__________________________________________________________________________________________________ 338activation_45 (Activation) (None, None, None, 5 0 bn5b_branch2b[0][0] 339__________________________________________________________________________________________________ 340res5b_branch2c (Conv2D) (None, None, None, 2 1050624 activation_45[0][0] 341__________________________________________________________________________________________________ 342bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192 res5b_branch2c[0][0] 343__________________________________________________________________________________________________ 344add_15 (Add) (None, None, None, 2 0 bn5b_branch2c[0][0] 345 activation_43[0][0] 346__________________________________________________________________________________________________ 347activation_46 (Activation) (None, None, None, 2 0 add_15[0][0] 348__________________________________________________________________________________________________ 349res5c_branch2a (Conv2D) (None, None, None, 5 1049088 activation_46[0][0] 350__________________________________________________________________________________________________ 351bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048 res5c_branch2a[0][0] 352__________________________________________________________________________________________________ 353activation_47 (Activation) (None, None, None, 5 0 bn5c_branch2a[0][0] 354__________________________________________________________________________________________________ 355res5c_branch2b (Conv2D) (None, None, None, 5 2359808 activation_47[0][0] 356__________________________________________________________________________________________________ 357bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048 res5c_branch2b[0][0] 358__________________________________________________________________________________________________ 359activation_48 (Activation) (None, None, None, 5 0 bn5c_branch2b[0][0] 360__________________________________________________________________________________________________ 361res5c_branch2c (Conv2D) (None, None, None, 2 1050624 activation_48[0][0] 362__________________________________________________________________________________________________ 363bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192 res5c_branch2c[0][0] 364__________________________________________________________________________________________________ 365add_16 (Add) (None, None, None, 2 0 bn5c_branch2c[0][0] 366 activation_46[0][0] 367__________________________________________________________________________________________________ 368activation_49 (Activation) (None, None, None, 2 0 add_16[0][0] 369__________________________________________________________________________________________________ 370global_average_pooling2d_1 (Glo (None, 2048) 0 activation_49[0][0] 371__________________________________________________________________________________________________ 372dense_1 (Dense) (None, 1024) 2098176 global_average_pooling2d_1[0][0] 373__________________________________________________________________________________________________ 374dense_2 (Dense) (None, 5) 5125 dense_1[0][0] 375================================================================================================== 376Total params: 25,691,013 377Trainable params: 2,103,301 378Non-trainable params: 23,587,712 379__________________________________________________________________________________________________ Phần train lại sẽ có khoảng hơn 2 triệu tham số, phần layer ở trước đó không train là khoảng 23 triệu tham số.\nChia tập dữ liệu ra thành 5 phần, 4 phần làm tập train, 1 phần làm tập validation.\n1X, y, tags = dataset.dataset(data_directory, n) 2nb_classes = len(tags) 3 4 5sample_count = len(y) 6train_size = sample_count * 4 // 5 7X_train = X[:train_size] 8y_train = y[:train_size] 9Y_train = np_utils.to_categorical(y_train, nb_classes) 10X_test = X[train_size:] 11y_test = y[train_size:] 12Y_test = np_utils.to_categorical(y_test, nb_classes) chúng ta tiến hành thực hiện ImageDataGenerator để có được nhiều dữ liệu mẫu hơn và chống overfit, trong keras đã có sẵn hàm\n1datagen = ImageDataGenerator( 2 featurewise_center=False, 3 samplewise_center=False, 4 featurewise_std_normalization=False, 5 samplewise_std_normalization=False, 6 zca_whitening=False, 7 rotation_range=45, 8 width_shift_range=0.25, 9 height_shift_range=0.25, 10 horizontal_flip=True, 11 vertical_flip=False, 12 channel_shift_range=0.5, 13 zoom_range=[0.5, 1.5], 14 brightness_range=[0.5, 1.5], 15 fill_mode=\u0026#39;reflect\u0026#39;) 16 17datagen.fit(X_train) Cuối cùng, chúng ta sẽ xây dựng mô hình và tiến hành huấn luyện, lưu mô hình. Quá trình này tốn hơi nhiều thời gian.\n1 2model = net.build_model(nb_classes) 3model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#34;accuracy\u0026#34;]) 4 5# train the model on the new data for a few epochs 6 7print(\u0026#34;training the newly added dense layers\u0026#34;) 8 9samples_per_epoch = X_train.shape[0]//batch_size*batch_size 10steps_per_epoch = samples_per_epoch//batch_size 11validation_steps = X_test.shape[0]//batch_size*batch_size 12 13model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True), 14 samples_per_epoch=samples_per_epoch, 15 epochs=nb_epoch, 16 steps_per_epoch = steps_per_epoch, 17 validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size), 18 validation_steps=validation_steps, 19 ) 20 21 22net.save(model, tags, model_file_prefix) Thử download một vài hình ảnh trên mạng về rồi test thử xem sao\n![Hình ảnh] (flower-classifition_demo.jpg)\nKết quả khá tốt phải không các bạn.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Apr 15, 2019","img":"","permalink":"/blog/2019-04-15-phan-loai-hoa/","series":null,"tags":["Machine learning","Deeplearning","hoa hồng","hoa mặt trời","hoa bồ công anh","hoa cúc","hoa tulip"],"title":"Phân Loại Hoa Sử Dụng Pretrain Model"},{"categories":null,"content":" Dự đoán chuỗi thời gian Các thuộc tính của time series Vì sao chúng ta lại quan tâm đến tính dừng của dữ liệu Cách xác định tính dừng của dữ liệu Phương pháp dự đoán chuỗi thời gian cơ bản Phương pháp dự đoán dựa vào mạng neural network Sử dụng mạng Echo State Networks Dự doán chuỗi time series Tối ưu hoá các tham số Hyper parameters Trong cuốn The West Wing Script Book của Aaron Sorkin, ông ấy đã có một câu như thế này \u0026ldquo;There (is) order and even great beauty in what looks like total chaos. If we look closely enough at the randomness around us, patterns will start to emerge.\u0026rdquo;. Mình xin phép không dịch câu nói trên ra, bởi vì mình dịch khá tệ, và câu nói này khá nổi tiếng (đã được trích dẫn khá nhiều trên các bài viết của các bloger khác). Nhưng câu nói đó khá phù hợp với môi trường chứng khoán, nơi mà mọi thứ đều không rõ ràng và khá \u0026ldquo;hỗn loạn\u0026rdquo;.\nDự đoán chuỗi thời gian Giá cổ phiếu trên thị trường chứng khoán thường được quy vào bài toán là time series. Các công ty đầu tư hoặc các nhà nghiên cứu, các nhà đầu tư hiện nay thường sử dụng phương pháp stochastic hoặc các cải tiến của phương pháp stochastic (ví dụ mô hình ARIMA, RegARIMA,\u0026hellip;) để đưa ra các dự đoán hợp lý phù hợp với các giá trị quá khứ. Mục tiêu cuối cùng là tìm ra một mô hình khả dĩ nhất để phản ánh quy luật của thị trường và sử dụng nó để sinh ra lợi nhuận (trở nên giàu có hơn :)).\nCác thuộc tính của time series Một trong các thuộc tính của chuỗi thời gian là tính dừng (stationary). Một chuỗi time series được gọi là có tính dừng nếu các thuộc tính có ý nghĩa thống kê của nó (ví dụ như là trung bình, độ lệch chuẩn) không đổi theo thời gian. Ở đây, chúng ta luận bàn nho nhỏ một chút vì sao tính dừng rất quang trọng trong chuỗi thời gian.\nTrước hết, hầu hết các mô hình về time series hiện tại được xây dựng trên một giả định tính dừng của chuỗi thời gian. Có nghĩa là nếu chuỗi thời gian ở trong quá khứ có một hành vi nào đó, thì khả năng cao là nó sẽ lặp lại trong tương lai. Ngoài ra, các lý thuyết liên quan đến tính dừng của chuỗi time series đã được các nhà nghiên cứu khai thác một cách triệt để và dễ ràng implement hơn là các lý thuyết về non-stationary trong time series.\nTính dừng được định nghĩa bằng các tiêu chí rõ ràng và nghiêm ngặt. Tuy nhiên, trong bài toán thực tế, chúng ta có thể giả định rằng một chuỗi time series được coi là có tính dừng nếu các thuộc tính thống kê không đổi theo thời gian, nghĩa là:\nGiá trị trung bình không thay đổi. Nếu giá trị trung bình thay đổi, chuỗi thời gian sẽ có khuynh hướng đi lên hoặc đi xuống. Hình ảnh bên dưới, mô tả trực quan một chuỗi thời gian có tính dừng (trung bình không thay đổi), và một chuỗi thời gian không có tính dừng (trung bình thay đổi). Giá trị phương sai không thay đổi. Thuộc tính này còn được gọi là đồng đẳng (homoscedasticity). Hình bên dưới mô tả một chuỗi có phương sai thay đổi (không có tính dừng) và một chuỗi có phương sai bất biến (có tính dừng). Tính tự tương tự không phụ thuộc vào thời gian Vì sao chúng ta lại quan tâm đến tính dừng của dữ liệu Chúng ta quan tâm đến tính dừng của dữ liệu, đơn giản là bởi vì nếu dữ liệu không có tính dừng, chúng ta không thể xây dựng mô hình chuỗi thời gian (như đã nói ở trên, các nghiên cứu hiện nay đều dựa trên một cơ sở là dữ liệu có tính dừng). Trong trường hợp bạn có trong tay dữ liệu thuộc dạng time series, và một tiêu chí nào đó trong 3 tiêu chí mình đã liệu kê ở trên bị vi phạm, suy ra là dữ liệu của bạn không có tính dừng. Bạn phải chuyển đổi dữ liệu bạn đang có để cho nó có tính dừng. May mắn rằng cũng có nhiều nghiên cứu thực hiện việc này, ví dụ như \u0026ldquo;khử xu hướng (detrending)\u0026rdquo;, khử sai biệt (differencing)\u0026hellip;\nNếu bạn mới chỉ bắt đầu phân tích chuỗi thời gian, bạn sẽ thấy việc làm trên khá là stupid. Lý thuyết tốt nhất hiện nay cho chuỗi thời gian là chia nhỏ nó ra thành các thành phần như là xu hướng (linear trend), mùa vụ (seasonal), chu kỳ, và yếu tố ngẫu nhiên. Dự đoán cho từng phần một, sau đó lấy tổng chúng lại.\nĐối với những ai đã quen thuộc với biến đổi Fourier, thì sẽ dễ dàng \u0026ldquo;cảm\u0026rdquo; hơn cái mình vừa nói ở trên.\nCách xác định tính dừng của dữ liệu Khá khó để xác định một biểu đồ chuỗi time series có tính dừng hay không (quan sát biểu đồ bằng mắt). Cho nên chúng ta sẽ sử dụng kiểm định Dickey-Fuller. Đây là một kiểm định thống kê để kiểm tra xem chuỗi dữ liệu có tính dừng hay không. Với giả thuyết null là chuỗi time series là một chuỗi không có tính dừng. Nếu giá trị nhỏ hơn một ngưỡng p-value nào đó (thường là 0.05), chúng ta có quyền bác bỏ giả định null, và nói rằng chuỗi thời gian đang có là có tính dừng. Ở bài viết này, mình không đề cập đến mô hình kiểm định - vốn được học trong môn xác xuất thống kê. Các bạn có nhu cầu tìm hiểu thì có thể search trên google hoặc là xem lại sách xác suất thống kê.\nPhương pháp dự đoán chuỗi thời gian cơ bản Phương pháp cơ bản nhất, đơn giản nhất, và để áp dụng nhất dược sử dụng để dự đoán chuỗi thời gian là moving average. Mô hình này thực hiện tính trung bình của t giá trị cuối cùng làm giá trị dự đoán của điểm tiếp theo. Ví dụ như để dự đoán giá chứng khoán của ngày thứ 2 của tuần tiếp theo, chúng ta sẽ lấy trung bình giá đóng của của 5 ngày trước đó (giá từ thứ hai đến thứ sáu tuần này).\nĐến đây, các bạn đã có một số hiểu biết về time series. Một mô hình khá nổi tiếng là ARIMA đã được sử dụng nhiều để phân tích và dự báo. Cách thực hiện của mô hình trên được trình bày tóm gọn trong hình mô tả bên dưới.\nPhương pháp dự đoán dựa vào mạng neural network Thực tế, có rất nhiều mạng neural network đã được áp dụng để dự đoán mô hình chứng khoán. Các bạn có thể tìm đọc lại các bài viết trước đây của mình về sử dụng LSTM trong dự báo chứng khoán. Mô hình chứng khoán bằng mạng neural network nói chung phải đối mặt với một vấn đề khá \u0026ldquo;xương xẩu\u0026rdquo; là xử lý nhiễu và vanishing gradients. Trong đó, việc xử lý vanishing gradients là quan trọng nhất. Bản chất của mạng neural network là tối ưu hoá hàm lan truyền ngược bằng cách sử dụng đạo hàm giữa các lớp layer để chúng \u0026lsquo;học\u0026rsquo;. Trải qua nhiều layer, giá trị của đạo hàm sẽ càng ngày nhỏ dần vào xấp xỉ bằng 0. Giả sử chúng ta có một mô hình có 100 lớp hidden layer, chúng ta nhân 100 lần số 0.1 với nhau và boom, giá trị cuối cùng chung ta nhận được là 0, nghĩa là chúng ta chẳng học được cái gì cả.\nMay mắn thay, tới thời điểm hiện tại, chúng ta có 3 cách để xử lý vấn đề trên:\nClipping gradients\nLSTM (Long Short Term Memory) hoặc GRU (Gate Recurrent Units)\nEcho states RNNs\nKỹ thuật clipping gradients sử dụng một mẹo là khi giá trị đạo hàm quá lớn hoặc quá nhỏ, chúng ta sẽ không lấy đạo hàm nữa. Kỹ thuật này thoạt nhìn có vẻ hay, nhưng nó không thể ngăn chúng ta mất mát thông tin và đây là một ý tưởng khá tệ.\nRNN (LSTM hoặc GRU) là một kỹ thuật khác là điều chỉnh các kết nối theo một vài quy luật nhất định, ví dụ output của layer tầng 1 có thể là input của layer tầng 10, chứ không nhất thiết là input của layer tầng 2 như cách thông thường. Kỹ thuật này khá tốt về mặt lý thuyết. Tuy nhiên, có một vấn đề khá lớn khi sử dụng là chúng ta phải tính toán kỹ các kết nối để đảm bảo hệ thống hoạt động ổn đinh. Mô hình được xây dựng trên kỹ thuật này khá bự, làm cho thuật toán chạy chậm. Ngoài ra, tính hội tụ của thuật toán không được đảm bảo. Mô hình LSTM đơn giản mình có để ở hình bên dưới.\nMạng echo states network, là một mô hình mới được nghiên cứu gần đây, bản chất nó là một mảng recurrent neural network với các hidden layer liên kết \u0026ldquo;lỏng lẻo\u0026rdquo; với nhau. Lớp này được gọi là \u0026lsquo;reservoir\u0026rsquo; (như hình mô tả bên dưới).\nTrong mô hình mạng echo state network, chúng ta chỉ cần huấn luyện lại trọng số của lớp output, việc này giúp chúng ta rút ngắn thời gian huấn luyện mô hình, và tăng tốc qusa trình training.\nSử dụng mạng Echo State Networks Về nguyên lý hoạt động của mô hình này, mình sẽ không đề cập ở đây. Chủ đề về mạng Echo State Networks mình sẽ nghiên cứu kỹ lưỡng và đề cập ở trong bài viết sắp tới. Mục tiêu của bài viết này là sử dụng mô hình Echo State Networks trong bài toán time series.\nDự doán chuỗi time series Trước tiên, chúng ta sẽ import một số thư viện cần thiết, thư viện ESN đã có sẵn tại đường dẫn pyESN, các bạn download về rồi dùng\n1 2 3import numpy as np 4import pandas as pd 5import seaborn as sns 6from matplotlib import pyplot as plt 7import warnings 8warnings.filterwarnings(\u0026#39;ignore\u0026#39;) 9 10# This is the library for the Reservoir Computing got it by: https://github.com/cknd/pyESN 11from pyESN import ESN Tiếp theo chúng ta sẽ đọc file\n1 2data = open(\u0026#34;amazon.txt\u0026#34;).read().split() 3data = np.array(data).astype(\u0026#39;float64\u0026#39;) Chúng ta sẽ xây dựng một mô hình ESN đơn giản\n1 2n_reservoir= 500 3sparsity=0.2 4rand_seed=23 5spectral_radius = 1.2 6noise = .0005 7 8 9esn = ESN(n_inputs = 1, 10 n_outputs = 1, 11 n_reservoir = n_reservoir, 12 sparsity=sparsity, 13 random_state=rand_seed, 14 spectral_radius = spectral_radius, 15 noise=noise) 16 17\t``` 18 19Để đơn giản, mình sẽ tạo mô hình với dữ liệu tào lao như sau:input là một vector toàn số 1, output là các điểm dữ liệu của mình. Cho mô hình ESN học với số lượng phần tử là 1500, sau đó sẽ dự đoán 10 điểm dữ liệu tiếp theo. Với bước nhảy là 10, lặp 10 lần. Sau quá trình lặp, mình thu được 100 điểm dự đoán 20 21 22```python 23trainlen = 1500 24future = 10 25futureTotal=100 26pred_tot=np.zeros(futureTotal) 27 28for i in range(0,futureTotal,future): 29 pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i]) # dữ liệu từ ngày i đến ngày i + trainlen 30 prediction = esn.predict(np.ones(future)) 31 pred_tot[i:i+future] = prediction[:,0] # dự đoán cho ngày i+ trainlen + 1 đến ngày i + trainlen + future 32 33 34\t``` 35 36Vẽ mô hình cùi mía của mình mới làm lên để xem dữ liệu dự đoán và dữ liệu thực tế chênh lệch như thế nào 37 38```python 39plt.figure(figsize=(16,8)) 40plt.plot(range(1000,trainlen+futureTotal),data[1000:trainlen+futureTotal],\u0026#39;b\u0026#39;,label=\u0026#34;Data\u0026#34;, alpha=0.3) 41#plt.plot(range(0,trainlen),pred_training,\u0026#39;.g\u0026#39;, alpha=0.3) 42plt.plot(range(trainlen,trainlen+futureTotal),pred_tot,\u0026#39;k\u0026#39;, alpha=0.8, label=\u0026#39;Free Running ESN\u0026#39;) 43 44lo,hi = plt.ylim() 45plt.plot([trainlen,trainlen],[lo+np.spacing(1),hi-np.spacing(1)],\u0026#39;k:\u0026#39;, linewidth=4) 46 47plt.title(r\u0026#39;Ground Truth and Echo State Network Output\u0026#39;, fontsize=25) 48plt.xlabel(r\u0026#39;Time (Days)\u0026#39;, fontsize=20,labelpad=10) 49plt.ylabel(r\u0026#39;Price ($)\u0026#39;, fontsize=20,labelpad=10) 50plt.legend(fontsize=\u0026#39;xx-large\u0026#39;, loc=\u0026#39;best\u0026#39;) 51sns.despine() 52plt.show() Độ phức tạp của mô hình là khá nhỏ khi so với mô hình RNN. Lý do là về bản chất, chúng ta chỉ huấn luyện trên trọng số của output layer, nó là một hàm tuyến tính. Do vậy, độ phức tạp tính toán chỉ giống như là việc tính một hàm hồi quy tuyến tính. Trong thực tế, độ phức tạp tính toán sẽ là O(N) với N là ố lượng hidden unit trong reservoir.\nTối ưu hoá các tham số Hyper parameters Ở phần trước, chúng ta set đại các tham số spectral_radius = 1.2 và noise = .0005. Trong thực tế, chúng ta phải tìm các siêu tham số này bằng cách tìm ra mô hình trả về MSE là nhỏ nhất.\nSử dụng kỹ thuật Grid Search với ngưỡng spectrum_radius nằm trong đoạn [0.5, 1.5] và noise nằm trong đoạn noise [0.0001, 0.01], chú ý là các bạn có thể search ở đoạn lớn hơn. Kết quả thu được:\n1def MSE(yhat, y): 2 return np.sqrt(np.mean((yhat.flatten() - y)**2)) 3 4\tn_reservoir= 500 5sparsity = 0.2 6rand_seed = 23 7radius_set = [0.9, 1, 1.1] 8noise_set = [ 0.001, 0.004, 0.006] 9 10radius_set = [0.5, 0.7, 0.9, 1, 1.1,1.3,1.5] 11noise_set = [ 0.0001, 0.0003,0.0007, 0.001, 0.003, 0.005, 0.007,0.01] 12 13 14 15radius_set_size = len(radius_set) 16noise_set_size = len(noise_set) 17 18trainlen = 1500 19future = 2 20futureTotal= 100 21 22loss = np.zeros([radius_set_size, noise_set_size]) 23 24for l in range(radius_set_size): 25 rho = radius_set[l] 26 for j in range(noise_set_size): 27 noise = noise_set[j] 28 29 pred_tot=np.zeros(futureTotal) 30 31 esn = ESN(n_inputs = 1, 32 n_outputs = 1, 33 n_reservoir = n_reservoir, 34 sparsity=sparsity, 35 random_state=rand_seed, 36 spectral_radius = rho, 37 noise=noise) 38 39 for i in range(0,futureTotal,future): 40 pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i]) 41 prediction = esn.predict(np.ones(future)) 42 pred_tot[i:i+future] = prediction[:,0] 43 44 loss[l, j] = MSE(pred_tot, data[trainlen:trainlen+futureTotal]) 45 print(\u0026#39;rho = \u0026#39;, radius_set[l], \u0026#39;, noise = \u0026#39;, noise_set[j], \u0026#39;, MSE = \u0026#39;, loss[l][j] ) Kết quả\n1 2(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 20.367056799629353) 3(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 22.44956008062169) 4(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 24.574909979223666) 5(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 25.862558649155638) 6(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 29.882933676750657) 7(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 32.63942614291128) 8(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 36.441245548726) 9(\u0026#39;rho = \u0026#39;, 0.5, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 44.77637915282457) 10(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 19.560517902720054) 11(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 20.12742795009036) 12(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 20.81801427735713) 13(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 21.26142619965559) 14(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 23.270880660885513) 15(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 26.061347331527354) 16(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 30.298361979419834) 17(\u0026#39;rho = \u0026#39;, 0.7, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 39.17074955771047) 18(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 18.612970860501118) 19(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 18.681815816990774) 20(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 18.835785386862582) 21(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 18.982346096338105) 22(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 20.81632098844061) 23(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 24.60968377490799) 24(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 30.231007189936882) 25(\u0026#39;rho = \u0026#39;, 0.9, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 41.28587340583505) 26(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 18.23852181110818) 27(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 18.27010615150326) 28(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 18.36078059388596) 29(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 18.47920006882226) 30(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 20.613227951906246) 31(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 25.153712109142973) 32(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 31.700838835741898) 33(\u0026#39;rho = \u0026#39;, 1, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 44.23736750779224) 34(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 17.981571756431556) 35(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 18.009398312163942) 36(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 18.09054736889828) 37(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 18.218795249276663) 38(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 20.82610561349463) 39(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 26.272452530336505) 40(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 33.91532767431614) 41(\u0026#39;rho = \u0026#39;, 1.1, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 48.22002405965967) 42(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 17.72839068197909) 43(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 17.799908079894703) 44(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 17.92917208443474) 45(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 18.143905288756557) 46(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 22.20343747458126) 47(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 30.05977704513729) 48(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 40.56654468067572) 49(\u0026#39;rho = \u0026#39;, 1.3, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 59.43231026660687) 50(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.0001, \u0026#39;, MSE = \u0026#39;, 17.627409489404897) 51(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.0003, \u0026#39;, MSE = \u0026#39;, 17.835052829116567) 52(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.0007, \u0026#39;, MSE = \u0026#39;, 18.100099619981393) 53(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.001, \u0026#39;, MSE = \u0026#39;, 18.481406587483956) 54(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.003, \u0026#39;, MSE = \u0026#39;, 24.887601182697498) 55(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.005, \u0026#39;, MSE = \u0026#39;, 36.34166374510305) 56(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.007, \u0026#39;, MSE = \u0026#39;, 50.99612645577753) 57(\u0026#39;rho = \u0026#39;, 1.5, \u0026#39;, noise = \u0026#39;, 0.01, \u0026#39;, MSE = \u0026#39;, 75.94229622771246) Kết quả thu được là giá trị MSE tốt nhất là spectrum radius = 1.5 và nnoise = 0.0001\nThử dự đoán giá cổ phiếu của tập đoàn thế giới di động (Mã cổ phiếu MWG) xem sao\nỞ hình trên, mình không tiến hành grid search mà lấy lại các hyper parameters cũ để huấn luyện mô hình. Kết quả như hình trên mình thấy cũng khá tốt rồi, nên mình không tiến hành grid search lại để tìm kết quả tốt hơn.\nDựa vào kết quả chúng ta thu được, có thể nói rằng mô hình ESN dự đoán khá tốt dữ liệu thuộc dạng time series với độ hỗn loạn cao. Đây là một kết luận nhỏ của mình dựa vào bằng chứng trên việc mình test trên tập dữ liệu ngẫu nhiên mà mình có.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Apr 4, 2019","img":"","permalink":"/blog/2019-04-04-predicting-stock-prices-with-echo-state-networks/","series":null,"tags":["machine learning","deep learning","neural network","amazone","thế giới di động","mwg"],"title":"Dự Đoán Giá Cổ Phiếu Bằng Mô Hình Mạng Echo State Networks"},{"categories":null,"content":" Hướng dẫn ban đầu Bạn huấn luyện một hình mất hơn 12 tiếng đồng hồ. Mọi thứ khá ổn: loss function giảm. Nhưng khi bạn mang mô hình ra predict thì điều tồi tệ nhất xảy ra: Tất cả trả về đều là 0, không có cái nào nhận dạng chính xác cả. \u0026ldquo;Điều gì đã xảy ra, bạn đã làm gì sai?\u0026rdquo;. Bạn hỏi máy tính, nó không trả lời bạn. Bạn đập bàn, đập ghế trong cơn tức giận và chẳng giải quyết được điều gì cả.\nCó rất nhiều nguyên nhân gây ra vấn đề này. Việc cần làm của các bạn là phải tìm ra chính xác nguyên nhân và \u0026ldquo;sửa\u0026rdquo; nó, sau đó tốn hơn 12 tiếng đồng hồ để huấn luyện lại :), rồi lại sửa \u0026hellip;\nHướng dẫn ban đầu Nếu bạn gặp tình trạng như phần mô tả ở trên, bạn hãy thực hiện các bước mình mô tả bên dưới thử xem vấn đề của bạn là gì?\nBắt đầu huấn luyện mô hình bằng một mô hình đơn giản mà bạn biết chắc rằng nó hoạt động tốt với tập dữ liệu bạn đang có. Ví dụ, trong bài toán object detection, hãy sử dụng mô hình VGG. Và bạn hãy cố gắng sửa dụng standard loss nếu có thể.\nBỏ qua những thứ râu ria như là regularization hoặc data augmentation. Hãy tập trung vào xây dựng một mô hình cho một kết quả khả quan cái đã, sau đó mới cải tiến bằng các thứ râu ria trên sau.\nNếu bạn finetuning một mô hình, bạn hãy kiểm tra thật kỹ quá trình tiền xử lý dữ liệu. Chắc chắn rằng quá trình tiền xử lý của bạn giống y chang quá trình tiền xử lý của mô hình gốc.\nChắc chắn 100% rằng giá trị đầu vào là đúng.\nBắt đầu bằng một tập sample nhỏ (từ 2 đến 20 mẫu). Huấn luyện nó đến khi bị overfit và bổ sung thêm mẫu huấn luyện sau khi mô hình của bạn bị overfit.\nBổ sung thên các yếu tố râu ria như augmentation/regularization, custom loss functions, thử với một mô hình phức tạp hơn.\nNếu những cách trên vẫn không thành công. Mô hình vẫn trả về giá trị zero. Bạn có thể mắc phải một số lỗi được liệt kê bên dưới.\nKiểm tra rằng dữ liệu của bạn đưa vào mạng neural netwok thật sự có ý nghĩa và đúng. Ví dụ, hãy đảm bảo rằng bạn không nhầm lẫn / swap giá trị giữa width và height của hình ảnh, hoặc một lý do nào đó bạn đưa vào một zero image, hoặc bạn chỉ huấn luyện duy nhất một batch (ví dụ dữ liệu bạn lớn, chia làm 10 batch, và code nhầm sao đó chỉ đưa input là batch số 1 vào).\nMột trường hợp nữa là khi input và output của bạn chẳng có mối liên hệ gì với nhau, và không cách nào nhận biết rằng nó phụ thuộc nhau bởi vì bản chất của dữ liệu là như vậy, hoặc input của bạn đang có chưa đủ chứng cứ để suy ra output. Một ví dụ của trường hợp này là giá chứng khoáng.\nKiểm tra kỹ dữ liệu train để đảm bảo không có đánh nhãn sai\nKiểm tra xem dữ liệu có bị mất cân bằng không. Hãy sử dụng các kỹ thuật để cân bằng lại dữ liệu.\nĐảm bảo rằng trong 1 batch chứa dữ liệu của nhiều hơn 1 nhãn. Hãy xáo trộn ngẫu nhiên dữ liệu để tránh lỗi này.\nBài báo https://arxiv.org/abs/1609.04836 chỉ ra rằng khi bạn huấn luyện mô hình với batch size lớn có thể làm giảm tính tổng quát của mô hình.\nKhoá học CS231 đã chỉ ra một lỗi khá phổ biến: \u0026ldquo;Bất kỳ một quá trình tiền xử lý nào cũng phải thực hiện trên tập train, và sau đó áp dụng vào tập validation,test\u0026rdquo;. Ví dụ, chúng ta tính trung bình trên toàn bộ dữ liệu, rồi sau đó chia tập dữ liệu thành train, test, predict là không đúng. Hành động đúng là chia tập dữ liệu thành train, test, vali trước, sau đó tính giá trị trung bình trên từng kênh màu trên tập train, rồi mới lấy giá trị trung bình đó áp cho tập test và tập validate.\nMột vấn đề khác có thể là \u0026ldquo;Look for correct loss at chance performance\u0026rdquo;:\nVí dụ, với tập dữ liệu CIFAR-10 sử dụng softmax classifier, ở lần đầu tiên, giá trị loss mong đợi của chúng ta là 2.303, bởi vì có 1 thằng đúng, 10 thằng sai, xác suất là 1/10 = 0.1. softmax loss là -ln(0.1) = 2.302.\nVới dữ liệu CIFAR-10 dùng SVM, ở lần lặp đầu tiên, giá trị loss chúng ta kỳ vọng là 9 (với mỗi lớp sai, giá trị margin sẽ là 1).\nNếu các giá trị trả ra không giống như mong đợi, vấn đề xảy ra là do giá trị init không đúng.\nMột vấn đề nữa là khi tăng giá trị regularization thì cũng đồng thời tăng giá trị loss. =\u0026gt; Nếu loss không tăng =\u0026gt; có vấn đề.\nBài viết được lược dịch từ https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?fbclid=IwAR1Qj6jJW87oKi_LR7xWMDOMZDTx8xwLZEhCCMuvOw63ztwdD1MknZVjm_Q\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Apr 2, 2019","img":"","permalink":"/blog/2019-04-02-37-reason-neural-network-not-working/","series":null,"tags":["machine learning","deep learning","neural network"],"title":"Các Lý Do Mạng Neural Network Không Hoạt Động Không Chính Xác"},{"categories":null,"content":" Trí tuệ nhân tạo Dữ liệu lớn Máy học và mối quan hệ với Trí tuệ nhân tạo cùng Dữ liệu lớn Mối quan hệ giữa ML với AI và Big Data Trong vài năm trở lại đây (khoảng từ 2013) truyền thông trong và ngoài nước có khá nhiều bài viết giật tít về “Cách mạng công nghiệp lần thứ tư” hay “Thời đại công nghiệp 4.0”. Cùng với các cụm từ này, “Trí tuệ nhân tạo”, “Máy học”, “Dữ liệu lớn” lại được nhắc đến với tần suất cao hơn. Vậy thì những thuật ngữ này có ý nghĩa gì và giữa chúng có mối liên hệ nào với nhau hay không? Trong bài viết này, chúng ta sẽ cùng tìm hiểu.\nTrí tuệ nhân tạo Năm 2016, trong “Trận thách đấu của Google DeepMind” được tổ chức tại Hàn Quốc, AlphaGo (một phần mềm chơi cờ vây trên máy tính được xây dựng bởi Google DeepMind) đã dành chiến thắng 4/5 ván trước Lee Sedol (người từng 18 lần vô địch giải cờ vây thế giới) là sự kiện quan trọng khiến con người có thể tin tưởng vào tương lai và sức mạnh của trí tuệ nhân tạo.\nSau khi trận đấu kết thúc, chính phủ Hàn Quốc công bố rằng họ sẽ đầu từ 863 triệu USD (khoảng 1 nghìn tỷ won) vào nghiên cứu trí tuệ nhân tạo trong vòng vài năm tiếp theo.\nTính tới nay, lượng dữ liệu các trận đấu cờ vây được nhận vào giúp AlphaGO có kinh nghiệm tương đương với 80 năm chơi cờ vây liên tục. Một con số đáng ngạc nhiên và ngưỡng mộ.\nNhư vậy trí tuệ nhân tạo là gì?\nTrí tuệ nhân tạo (AI - Artificial Intelligence) là một nhánh nghiên cứu trong lĩnh vực khoa học máy tính và từ lâu đã được rất nhiều các nhà nghiên cứu quan tâm. Thuật ngữ AI được đặt bởi nhà khoa học máy tính người Mỹ - John McCarthy vào năm 1956 tại Hội nghị Dartmouth. Cho đến thời điểm hiện tại thì có khá nhiều những phát biểu khác nhau về AI bởi các chuyên gia, chẳng hạn như:\nAI là khoa học nghiên cứu giúp tạo ra máy tính có khả năng suy nghĩ, đầy trí tuệ như tên của chính nó (Haugeland, 1985).\nAI là khoa học nghiên cứu các hoạt động trí não thông qua các mô hình tính toán (Chaniaka và McDemott, 1985).\nAI là khoa học nghiên cứu cách để máy tính có thể thực hiện được những công việc mà con người làm tốt hơn máy (Rich và Knight, 1991).\nAI là khoa học nghiên cứu các mô hình máy tính có thể nhận thức, lập luận và hành động (Winston, 1992).\nAI là khoa học nghiên cứu các hành vi thông minh mô phỏng các vật thể nhân tạo (Nilsson, 1998)\nAI là khoa học nghiên cứu các hành vi thông minh nhằm giải quyết các vấn đề được đặt ra đối với các chương trình máy tính (Học viện Kỹ thuật Quân sự).\nNhư vậy, từ những định nghĩa trên chúng ta có thể rút ra định nghĩa tổng quát rằng trí tuệ nhân tạo hay trí thông minh nhân tạo là trí tuệ được biểu diễn bởi bất kỳ một hệ thống nhân tạo nào. Hệ thống đó sẽ mô phỏng các quá trình hoạt động trí tuệ của con người, bao gồm quá trình học tập, lập luận và tự sửa lỗi. Do đó, trí thông minh nhân tạo liên quan đến cách hành xử, sự học hỏi và khả năng thích ứng thông minh của máy móc nói chung và máy tính nói riêng.\nCách đây vài năm, đối với phần đông chúng ta – những người không nghiên cứu chuyên sâu về AI sẽ cho rằng AI là một phương thức để nhân bản con người bằng máy móc và được ứng dụng trong chế tạo robot. Tuy nhiên AI hiện tại không phải chỉ là những con robot mà nó có thể biểu hiện dưới bất cứ hình dạng nào, thậm chí vô hình vô dạng, nhằm cung cấp lời giải cho các vấn đề của cuộc sống thực tế trên hầu hết các lĩnh vực, chẳng hạn như:\nTrong lĩnh vực chăm sóc sức khỏe: AI góp phần cải thiện tình trạng sức khỏe bệnh nhân, và giúp giảm chi phí điều trị. Một trong những hệ thống công nghệ chăm sóc sức khỏe tốt nhất phải kể đến là IBM Watson, được mệnh danh là “Bác sĩ biết tuốt” khi mà hệ thống này có khả năng hiểu được các ngôn ngữ tự nhiên và có khả năng phản hồi các câu hỏi được yêu cầu hoặc cho phép bệnh nhân tra cứu thông tin về tinh hình sức khoẻ của mình. IBM Watson có thể lướt duyệt cùng lúc hàng triệu hồ sơ bệnh án để cung cấp cho các bác sĩ những lựa chọn điều trị dựa trên bằng chứng chỉ trong vòng vài giây nhờ khả năng tổng hợp dữ liệu khổng lồ và tốc độ xử lý mạnh mẽ. “Bác sĩ biết tuốt” khai thác dữ liệu bệnh nhân và các nguồn dữ liệu sẵn có khác nhằm tạo ra giả thuyết và từ đó xậy dựng một lược đồ điểm tin cậy giúp “Bác sĩ thật” đưa ra quyết định điều trị cuối cùng. Ngoài ra, ứng dụng AI nổi bậc khác trong lĩnh vực này cần phải kể đến là chatbot - chương trình máy tính trực tuyến để trả lời các câu hỏi và hỗ trợ khách hàng, sắp xếp các cuộc hẹn hoặc trợ giúp bệnh nhân thông qua quá trình thanh toán và các trợ lý y tế ảo cung cấp phản hồi y tế cơ bản.\nTrong lĩnh vực kinh doanh: Các tác vụ mà con người thực hiện lặp đi lặp lại giờ đây đã được tự động hoá quy trình bằng robot. Các thuật toán Machine Learning được tích hợp trên các nền tảng phân tích và CRM (Customer Relationship Management - quản lý quan hệ khách hàng) để khám phá các thông tin về cách phục vụ khách hàng tốt hơn. Chatbots được tích hợp trên các trang web nhằm cung cấp dịch vụ ngay lập tức cho khách hàng. Một số hệ thống trợ lý ảo nổi tiếng giúp sắp xếp, nhắc cuộc họp, tìm kiếm thông tin như Google Assistant, Alexa, Siri. Hiện nay các hệ thống này đã bắt đầu được tích hợp vào trong các thiết bị gia dụng như máy giặt, tủ lạnh, lò vi sóng, … giúp người sử dụng có thể điều khiển thiết bị bằng câu lệnh thoại.\nTrong lĩnh vực giáo dục: Công nghệ thực tế ảo làm thay đổi cách dạy và học. Sinh viên có thể đeo kính VR và có cảm giác như đang ngồi trong lớp nghe giảng bài hay nhập vai để chứng kiến những trận đánh giả lập, ngắm nhìn di tích, điều này giúp mang lại cảm xúc và ghi nhớ sâu sắc nội dung học. Hoặc khi đào tạo nghề phi công, học viên đeo kính sẽ thấy phía trước là cabin và học lái máy bay như thật để thực hành giúp giảm thiểu rủi ro trong quá trình bay thật.\nTrong lĩnh vực tài chính: AI áp dụng cho các ứng dụng tài chính cá nhân như Mint hay Turbo Tax giúp tăng cường các định chế tài chính.\nTrong lĩnh vực pháp luật: Quá trình khám phá, chọn lọc thông qua các tài liệu trong luật pháp thường áp đảo đối với con người. Tự động hóa quá trình này giúp tiết kiệm thời gian và quá trình làm việc hiệu quả hơn. Các trợ lý ảo giúp trả lời các câu hỏi đã được lập trình sẵn.\nTrong lĩnh vực sản xuất: Đây là lĩnh vực đi đầu trong việc kết hợp robot vào luồng công việc. Robot công nghiệp được sử dụng để thực hiện các nhiệm vụ đơn lẻ và đã được tách ra khỏi con người. Xe tự động lái Tesla là một ứng dụng điển hình trong lĩnh vực này.\nTrong lĩnh vực bảo mật thông tin: rất nhiều hệ thống nhận diện và bảo mật thông minh được xây dựng, phải kể đến như FaceID - bảo mật thông qua nhận diện khuôn mặt của Apple, Facebook với khả nhận diện khuôn mặt để gợi ý tag. Bên cạnh các nước phương Tây thì Trung Quốc hiện đang là quốc gia đi đầu trong việc sử dụng AI để nhận diện và quản lý công dân.\nTừ những ứng dụng trên ta có thể thấy rằng nói đến AI là nói về não bộ chứ không phải là nói về một cơ thể, là phần mềm chứ không phải là phần cứng.\nDữ liệu lớn Một cách tổng quát thì dữ liệu là thông tin dưới dạng ký hiệu, chữ viết, chữ số, hình ảnh, âm thanh hoặc dạng tương tự. Từ thế kỷ thứ 3 trước CN, Thư viện Alexandria được coi là nơi chứa đựng toàn bộ kiến thức của loài người. Ngày nay, tổng lượng dữ liệu trên toàn thế giới đủ để chia đều cho mỗi đầu người một lượng nhiều gấp 320 lần lượng dữ liệu mà các sử gia tin rằng Thư viện Alexandria từng lưu trữ – ước tính vào khoảng 120 exabyte. Các nhà thống kê cho rằng, nếu tất cả những dữ liệu này được ghi vào đĩa CD và xếp chồng chúng lên nhau thì sẽ có tới 5 chồng đĩa mà mỗi chồng đều có độ cao bằng khoảng cách từ Trái Đất đến Mặt Trăng.\nSự bùng nổ dữ liệu này chỉ mới xuất hiện gần đây. Cách đây không lâu, vào năm 2000, chỉ một phần tư lượng dữ liệu lưu trữ trên toàn thế giới ở dạng kỹ thuật số, ba phần tư còn lại được người ta lưu trên giấy tờ, phim, và các phương tiện analog khác. Nhưng do lượng dữ liệu kỹ thuật số bùng nổ quá nhanh – cứ 3 năm lại tăng gấp đôi, làm cho tỉ lệ này nhanh chóng đảo ngược. Hiện nay, chỉ dưới 2% tổng lượng dữ liệu chưa được chuyển sang lưu trữ ở dạng kỹ thuật số.\nDưới đây là một vài ví dụ nhỏ minh hoạ cho sự dùng nổ của dữ liệu hiện nay:\nTheo Forbes, lượng dữ liệu mà người dùng tạo ra mỗi ngày là 2.5 tỷ tỷ bytes, một con số rất đáng kinh ngạc và dự đoán con số này sẽ tiếp tục bùng nổ nữa cùng với sự phát triển của Internet vạn vật (IoT – Internet of thing), khi mà hệ thống các thiết bị thông minh được kết nối và tương tác với nhau cũng như tương tác với người dùng, đồng thời thu thập dữ liệu. Dự báo có khoảng 200 tỷ thiết bị như thế vào năm 2020. Giả sử chỉ xét đến thiết bị tìm kiếm bằng giọng nói, hiện tại:\nCó 33 triệu thiết bị qua giọng nói đang lưu thông.\n8 triệu người dùng điều khiển giọng nói mỗi tháng.\nCác câu lệnh tìm kiếm bằng giọng nói trên Google trong năm 2016 tăng 35 lần so với năm 2008.\nTheo thống kê, hiện nay có hơn 7 tỷ người sử dụng internet. Trung bình Google xử lý hơn 40.000 tìm kiếm mỗi giây (tức khoảng 3.5 tỷ tìm kiếm mỗi ngày, nếu tính cả những cổ máy tìm kiếm khác ngoại trừ Google thì con số này lên tới 5 tỷ lượt/ngày, 100 tỷ lượt/tháng) và những con số này sẽ tiếp tục tăng lên theo từng giây.\nRất đông người yêu thích các phương tiện truyền thông xã hội và dĩ nhiên việc sử dụng chúng cũng sẽ tạo ra dữ liệu. Theo báo cáo Data Never Sleép 5.0 của Domo, trên các phương tiện truyền thông cứ mỗi một phút sẽ có (nguồn http://www.internetlivestats.com/google-search-statistics/):\n527.760 bức ảnh được chia sẻ bởi người sử dụng Snapchat .\n456.000 tweet được gửi lên Twitter.\n46.740 bức ảnh được đăng bởi người dùng Instagram.\nHơn 120 người có công việc ổn định tham gia LinkedIn.\nVới khoảng 2 tỷ người dùng, Facebook vẫn là mạng xã hội lớn nhất hành tinh và dưới đây là các số liệu liên quan đến Facebook (nguồn http://newsroom.fb.com/company-info/):\nHơn 900 triệu người thật sự sử dụng Facebook mỗi ngày, 82.8% trong số đó ở ngoài Mỹ và Canada.\n307 triệu / 2 tỷ là người Châu Âu.\nCứ mỗi giây lại có 5 tài khoản mới được tạo ra.\n510.000 bình luận được đăng tải và 293.000 trạng thái được cập nhật mỗi phút.\nHơn 300 triệu bức ảnh được tải lên mỗi ngày.\n15.000 ảnh GIF được gửi thông qua Facebook Messenger.\nCũng thuộc sở hữu của Facebook, Instagram cũng có những con số ấn tượng:\n600 triệu người dùng.\n400 triệu người hoạt động mỗi ngày.\n100 triệu người sử dụng tính năng Stories mỗi ngày.\nLiên quan đến số lượng người dùng và dữ liệu chúng ta không thể không nhắc đến Youtube khi mà cứ mỗi một phút sẽ có khoảng 300 giờ video được đăng tải trên Youtube (nguồn https://www.youtube.com/yt/about/press/).\nTrong thời đại công nghệ, việc thông qua các trang web hẹn hò để tìm nửa còn lại không còn là điều xa lạ. Với hơn 20 tỷ lượt kết đôi, Tinder xứng đáng là nhịp cầu công nghệ thành công bậc nhất hiện tại. Cứ mỗi phút trôi qua Tinder có khoảng 990.000 lượt vuốt và hơn 26 triệu lượt hẹn hò mỗi ngày.\nNgoài việc liên kết, trao đổi với nhau qua mạng xã hội, trong công việc mọi người thường sử dụng email, skype để thư từ, liên lạc. Tính đến năm 2019 có khoảng 9 tỷ người sử dụng email và dưới đây là một vài con số thống kê các sự kiện xảy ra trong một phút:\nNgười dùng gửi đi 16 triệu văn bản.\n156 triệu email được gửi đi với khoảng 16 triệu văn bản.\n103.447.520 thư rác được gửi đi.\n154.200 cuộc gọi Skype.\nKhông còn quá khó khăn trong việc lưu giữ các khoảnh khắc, ngày nay khi mà bất cứ ai cũng có thể sở hữu một chiếc điện thoại thông minh (smartphone) và ai cũng là nhiếp ảnh gia, cứ như thế có hàng nghìn tỷ bức ảnh được cho ra đời và lưu trữ trên điện thoại.\nThông qua những ví dụ vừa nêu có thể chúng ta sẽ nghĩ rằng dữ liệu lớn thuần tuý chỉ là vấn đề về kích cỡ, và nếu điều này là đúng thì dữ liệu bao nhiêu được cho là “lớn”?\nĐể trả lời câu hỏi này ta quay lại một chút về lịch sử của thuật ngữ “Big Data”. Không giống với AI và ML, Big Data không phải là một ngành khoa học chính thống mà chỉ là một thuật ngữ truyền thông mới xuất hiện trong vài năm trở lại đây. Nó không khác gì thuật ngữ “kỷ nguyên phần mềm” hay “cách mạng công nghiệp”. Mặc dù thuật ngữ này mới xuất hiện nhưng khối lượng dữ liệu tích tụ kể từ khi mạng Internet xuất hiện vào cuối thế kỷ trước cũng không phải là nhỏ từ ví dụ về thư viện Alexandria. Vậy thì câu hỏi đặt ra là tại sao với khối lượng khổng lồ như thế mà thời đó vẫn không gọi là Big Data? Câu trả lời là mặc dù được bao quanh bởi dữ liệu khổng lồ nhưng ở thời điểm đó con người không biết làm gì với chúng ngoài lưu trữ và sao chép. Cho đến khi các nhà khoa học nhận ra rằng trong đống dữ liệu này đang ẩn chứa một khối lượng tri thức khổng lồ. Những tri thức ấy có thể giúp ta hiểu thêm về con người và xã hội. Chẳng hạn như từ danh sách các bộ phim yêu thích của một cá nhân, chúng ta có thể rút ra được sở thích xem phem của người đó và gợi ý những bộ phim cùng thể loại. Hoặc từ danh sách tìm kiếm của cộng đồng mạng chúng ta sẽ biết được vấn đề nóng hổi nhất đang được quan tâm và sẽ tập trung đăng tải nhiều tin tức hơn về vấn đề đó, …\nNhư vậy, bùng nổ thông tin không phải là lý do duy nhất dẫn đến sự ra đời của cụm từ Big Data mà Big Data chỉ thực sự bắt đầu khi chúng ta hiểu được giá trị của thông tin ẩn chứa trong dữ liệu và có đủ tài nguyên cũng như công nghệ để có thể khai tác chúng trên quy mô lớn. Và không có gì ngạc nhiên khi Máy học chính là thành phần mấu chốt của công nghệ đó.\nMáy học và mối quan hệ với Trí tuệ nhân tạo cùng Dữ liệu lớn Để máy tính có khả năng suy nghĩ và trí tuệ như con người thì đòi hỏi máy tính phải có khả năng “học” mà không cần phải lập trình để thực hiện các tác vụ cụ thể đó. Về phía các nhà nghiên cứu AI, họ muốn xem thử liệu máy tính có thể học dữ liệu như thế nào? Từ đó thuật ngữ Máy học hay Học máy (ML – Machine Learning) được hình thành. Mặc dù không có nhiều định nghĩa như AI nhưng ML lại có 2 định nghĩa khá tường minh như sau:\nMáy học là ngành học cung cấp cho máy tính khả năng học hỏi mà không cần được lập trình một cách rõ ràng (Arthur Samuel, 1959).\nTheo Giáo sư Tom Mitchell – Carnegie Mellon University: Máy học là 1 chương trình máy tính được nói là học hỏi từ kinh nghiệm E từ các tác vụ T và với độ đo hiệu suất P nếu hiệu suất của nó áp dụng trên tác vụ T và được đo lường bởi độ đo P tăng từ kinh nghiệm E.\nMột vài ví dụ minh hoạ cho định nghĩa của Tom Mitchell:\n•\tVí dụ 1: Giả sử như ta muốn máy tính xác định một tin nhắn có phải là SPAM hay không thì:\nTác vụ T: Xác định 1 tin nhắn có phải SPAM hay không?\nKinh nghiệm E: Xem lại những tin nhắn được đánh dấu là SPAM xem có những đặc tính gì để có thể xác định nó là SPAM.\nĐộ đo P: Là phần trăm số tin nhắn SPAM được phân loại đúng.\n•\tVí dụ 2: Chương trình nhận dạng chữ số viết tay (bao gồm các chữ số từ 0 đến 9)\nTác vụ T: nhận dạng được ảnh chứa ký tự số.\nKinh nghiệm E: Đặc trưng để phân loại ký tự số từ tập dữ liệu số cho trước.\nĐộ đo P: Độ chính xác của quá trình nhận dạng.\nMối quan hệ giữa ML với AI và Big Data Trong phần 1 và phần 2 chúng ta luôn thấy sự xuất hiện của ML, đây là lý do vì sao mình không tách riêng mối quan hệ giữa các khái niệm này ra một phần riêng mà để chung trong nội dung của ML. Vậy thì mối liên hệ đó là gì?\nMột cách hàn lâm thì AI là ngành khoa học được sinh ra với mục tiêu là làm cho máy tính có được trí thông minh như con người. Mục tiêu này vẫn khá mơ hồ vì không phải ai cũng đồng ý với một định nghĩa thống nhất về trí thông minh. Các nhà khoa học phải định nghĩa một số mục tiêu cụ thể hơn, một trong số đó là việc làm cho máy tính lừa được Turing Test. Turing Test được tạo ra bởi Alan Turing (1912 – 1954), người được xem là cha để của ngành khoa học máy tính hiện đại, nhằm phân biệt xem người đối diện có phả là người hay không.\nNhư vậy, AI thể hiện một của mục tiêu con người, trong khi ML là một phương tiện được kỳ vọng sẽ giúp con người đạt được mục tiêu đó. Và trên thực tế thì ML đã mang nhân loại đi rất xa trên quãng đường chinh phục AI. Dù có mối quan hệ chặc chẽ với nhau nhưng chúng không hẳn là trùng khớp vì môt bên là mục tiêu (AI), một bên là phương tiện (ML). Chinh phục AI mặc dù vẫn là mục đích tối thượng của ML, nhưng hiện tại ML tập trung vào những mục tiêu ngắn hạn hơn như làm cho máy tính có khả năng nhận thức cơ bản của con người như nghe, nhìn, hiểu được ngôn ngữ, giải toán, lập trình, …, các khả năng này ứng với các lĩnh vực cụ thể trong AI như:\nThị giác máy tính (computer vision): mục tiêu của lĩnh vực này là làm cho máy tính có thể nhìn như con người. Những ứng dụng quan trọng có thể kể đến trong lĩnh vực này như là nhận dạng chữ/ chứ số viết tay, nhận dạng khuôn mặt, dáng đi, cử chỉ, phân loại loài hoa, nhãn hiệu, phát hiện đồ vât, …. Từ tập hình ảnh ban đầu, các thuật toán ML sẽ tiến hành xử lý, phân tích để rút ra các đặc trưng chính giúp nhận dạng đối tượng hoặc phân biệt các đối tượng với nhau.\nXử lý Ngôn ngữ tự nhiên (Natural Language Processing – NLP): Mục tiêu là giúp cho máy tính có thể hiểu như con người. Dịch máy là một trong những ứng dụng điển hình của NLP, dịch nội dung của một đoạn văn bản từ ngôn ngữ này sang ngôn ngữ khác (Google Translate). Xuất phát từ “Từ điển” hoặc tập các cặp câu song ngữ, tập luật ngữ pháp của mỗi ngôn ngữ được tạo bởi người có chuyên môn về những ngôn ngữ đó, các thuật toán máy học sẽ tiến hành phân tích để tách câu, tách từ, xác định từ loại, phân tích cú pháp để từ đó lấy ra ngữ nghĩa phù hợp rồi ghép lại với nhau và cho ra nội dung ở ngôn ngữ tương ứng. Ngoài ra, tóm tắt văn bản dựa vào các từ khoá của từng lĩnh vực cũng là một bài toán ML rất được quan tâm trong vài năm trở lại đây, khi mà mỗi ngày lượng tin tức cần phải đọc là quá nhiều.\nXử lý tiếng nói (Speech Language Processing): nhằm làm cho máy tính có thể nghe được như người. Tổng hộp tiếng nói (text to speech) để đọc sách cho người khiếm thị, tạo sub cho các video (speech to text) để hỗ trợ cho người khiếm thính hoặc hỗ trợ cho việc học ngôn ngữ; nhận dạng giọng nói (speech recognition) giúp phát hiện tội phạm là một số ứng dụng điển hình trong lĩnh vực này.\nThay vì cố gắng “dạy” máy tính cách làm một việc gì đó, chẳng hạn như lái xe hơi, điều mà các chuyên gia AI cần làm là cung cấp “đủ” dữ liệu cho một máy tính để nó có thể tính ra xác suất của tất cả mọi thứ mà người ta muốn tính toán, ví như xác suất người đi đường gặp đèn giao thông màu xanh, màu đỏ, màu vàng, … thì chuẩn xác hơn.\nDo đó, nhiệm vụ thực sự của ML trong AI là “học” mà thực chất của việc học này là rút trích thông tin hữu ích cho từng bài toán trong “tập dữ liệu” cho trước. Lúc này mối quan hệ giữa ML và Big Data sẽ được bộc lộ, đó là nếu khối lượng dữ liệu của Big Data càng gia tăng thì ML sẽ phát triển hơn, có khả năng rút trích được nhiều thông tin giá trị hơn hay dự đoán chính xác hơn, ngược lại thì giá trị của Big Data phụ thuộc vào khả năng khai thác tri thức từ dữ liệu của ML, vì nó sẽ thực sự là Big Data khi khối lượng dữ liệu đó mang lại thông tin hữu ích.\nViệc sử dụng những khối lượng thông tin theo cách này đòi hỏi chúng ta phải có sự thay đổi trong cách tiếp cận dữ liệu. Một là thu thập và sử dụng thật nhiều dữ liệu thay vì chấp nhận lấy những mẫu thống kê với số lượng nhỏ như các nhà thống kê vẫn làm từ hơn một thế kỷ nay. Hai là không nhất thiết phải kén chọn sàng lọc ra dữ liệu sạch, vì kinh nghiệm thực tiễn cho thấy rằng một chút sai lệch trong thông tin vẫn có thể chấp nhận được, và việc sử dụng một lượng khổng lồ những dữ liệu ô hợp đem lại nhiều ích lợi hơn là dữ liệu tuy chính xác nhưng dung lượng quá ít. Ba là trong nhiều trường hợp, chúng ta không nhất thiết phải cố tìm ra nguyên nhân đằng sau các hiện tượng.Ví dụ, không cần phải cố tìm hiểu chính xác vì sao một cỗ máy bị hỏng, thay vào đó các nhà nghiên cứu có thể thu thập và phân tích thật nhiều dữ liệu về chúng cùng tất cả mọi thứ liên quan, từ đó rút ra quy luật làm cơ sở dự đoán các sự vật, sự việc trong tương lai.\nDưới đây là một số tài liệu mình đã sử dụng để tham khảo trong qua trình viết bài:\nIntroduction to Machine Learning of Alex Smola and S.V.N. Vishwanathan.\nArtificial Intelligence (third edition) of The McGraw-Hill Companies, write by Elaine Rich, Kevin Knight and Shivashankar B Nair.\nhttps://i4iam.files.wordpress.com/2013/08/artificial-intelligence-by-rich-and-knight.pdf\nhttps://en.wikipedia.org/wiki/Artificial_intelligence\nhttps://searchenterpriseai.techtarget.com/definition/AI-Artificial-Intelligence\nhttp://vienthongke.vn/tin-tuc/43-tin-tuc/2176-thoi-dai-cua-du-lieu-lon-big-data\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.\n","date":"Apr 2, 2019","img":"","permalink":"/blog/2019-04-02-deep-learning-view/","series":null,"tags":["machine learning","deep learning"],"title":"Trí Tuệ Nhân Tạo, Máy Học, Dữ Liệu Lớn"},{"categories":null,"content":" Bắt đầu Visualize dữ liệu Bounding Boxes Resize Images Mini Masks Anchors Prediction Bắt đầu Đầu tiên, chúng ta sẽ download tập dataset balloon tại https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip, giải nén và bỏ trong thư mục datasets. Tiếp đó, các bạn donwload file balloon.py và visualize.py về. File đầu tiên hỗ trợ chúng ta đọc dữ liệu của dataset balloon và file thứ hai hỗ trợ visualize hình ảnh một cách trực quan. Cả hai file mình đều lấy mã nguồn của Matterport trên https://github.com/matterport/Mask_RCNN/ Tiến hành import các thư viện cần thiết về.\n1import os 2import sys 3import itertools 4import math 5import logging 6import json 7import re 8import random 9from collections import OrderedDict 10import numpy as np 11import matplotlib 12import matplotlib.pyplot as plt 13import matplotlib.patches as patches 14import matplotlib.lines as lines 15from matplotlib.patches import Polygon 16 17 18import balloon 19import utils 20import visualize 21 22config = balloon.BalloonConfig() 23BALLOON_DIR = \u0026#34;datasets/balloon\u0026#34; Thông tin của tập train bao gồm\n1dataset = balloon.BalloonDataset() 2dataset.load_balloon(BALLOON_DIR, \u0026#34;train\u0026#34;) 3 4# Must call before using the dataset 5dataset.prepare() 6 7print(\u0026#34;Image Count: {}\u0026#34;.format(len(dataset.image_ids))) 8print(\u0026#34;Class Count: {}\u0026#34;.format(dataset.num_classes)) 9for i, info in enumerate(dataset.class_info): 10 print(\u0026#34;{:3}. {:50}\u0026#34;.format(i, info[\u0026#39;name\u0026#39;])) 1Image Count: 61 2Class Count: 2 3 0. BG 4 1. balloon Vậy là có tổng cộng 61 hình train. Dữ liệu được đánh làm 2 nhãn, một nhãn là background, một nhãn là balloon.\nVisualize dữ liệu Chúng ta sẽ load một vài hình lên xem người ta đã mask dữ liệu như thế nào. Ở đây, với mỗi hình ảnh, mình sẽ load 1 hình gốc và 4 hình của 4 quả bóng tương ứng trong hình, nếu trong hình có nhiều hơn 4 quả bóng thì chỉ vẽ 4 quả bóng đầu tiên\n1 2 3n_col = 5 4 5# Load and display random samples 6fig, axs = plt.subplots(nrows=4, ncols=n_col, figsize=(9.3, 6),subplot_kw={\u0026#39;xticks\u0026#39;: [], \u0026#39;yticks\u0026#39;: []}) 7fig.subplots_adjust(left=0.03, right=0.97, hspace=0.3, wspace=0.05) 8image_ids = np.random.choice(dataset.image_ids, 4) 9# for image_id in image_ids: 10# for ax, image_id in zip(axs.flat, image_ids): 11 12for index in range(0,4): 13 image_id = image_ids[index] 14 15 image = dataset.load_image(image_id) 16 mask, class_ids = dataset.load_mask(image_id) 17 print(mask.shape) 18 print(len(class_ids)) 19 20 axs.flat[index*n_col].imshow(image) 21 axs.flat[index*n_col].set_title(\u0026#39;img\u0026#39;) 22 23 for sub_index in range(0,len(class_ids)): 24 if sub_index \u0026gt;= n_col: 25 break 26 axs.flat[index*n_col +1 + sub_index].imshow(mask[:,:,sub_index]) 27 axs.flat[index*n_col + 1+sub_index].set_title(str(dataset.class_names[class_ids[sub_index]])) 28 29 30plt.tight_layout() 31plt.show() Các bạn có thể sử dụng hàm display_top_masks của tác giả Mask R-CNN để xem thử, hàm của họ hơi khác của mình một chút.\n1 2image_ids = np.random.choice(dataset.image_ids, 4) 3for image_id in image_ids: 4 image = dataset.load_image(image_id) 5 mask, class_ids = dataset.load_mask(image_id) 6 visualize.display_top_masks(image, mask, class_ids, dataset.class_names) Bounding Boxes Chúng ta có 2 cách để lấy Bounding Boxes của các hình. Một là lấy trực tiếp từ tập dataset (đối với những dataset có lưu bounding box), hai là rút trích bounding box từ các toạ độ mask. Chúng ta nên thực hiện cách hai, lý do là chúng ta sẽ dùng các kỹ thuật Data Generator để sinh nhiều ảnh hơn cung cấp cho thuật toán train. Lúc này, việc tính lại bounding box sẽ dễ dàng hơn.\n1 2# Load random image and mask. 3image_id = random.choice(dataset.image_ids) 4image = dataset.load_image(image_id) 5mask, class_ids = dataset.load_mask(image_id) 6 7# Compute Bounding box 8bbox = utils.extract_bboxes(mask) 9 10# Display image and additional stats 11print(\u0026#34;image_id \u0026#34;, image_id, dataset.image_reference(image_id)) 12 13# Display image and instances 14visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names) Resize Images Các ảnh trong tập train có các kích thước khác nhau. Các bạn có thể xem các hình ở trên, có ảnh có kích thước này, có ảnh có kích thước kia. Chúng ta sẽ resize chúng về cùng một kích thước (ví dụ 1024x1024) để làm đầu vào cho tập huấn luyện. Và chúng ta sẽ sử dụng zero padding để lấp đầy những khoảng trống của những ảnh không đủ kích thước.\n1 2 3 4# Load random image and mask. 5image_id = np.random.choice(dataset.image_ids, 1)[0] 6image = dataset.load_image(image_id) 7mask, class_ids = dataset.load_mask(image_id) 8original_shape = image.shape 9# Resize 10image, window, scale, padding, _ = utils.resize_image( 11 image, 12 min_dim=config.IMAGE_MIN_DIM, 13 max_dim=config.IMAGE_MAX_DIM, 14 mode=config.IMAGE_RESIZE_MODE) 15mask = utils.resize_mask(mask, scale, padding) 16# Compute Bounding box 17bbox = utils.extract_bboxes(mask) 18 19# Display image and additional stats 20print(\u0026#34;image_id: \u0026#34;, image_id, dataset.image_reference(image_id)) 21print(\u0026#34;Original shape: \u0026#34;, original_shape) 22print(\u0026#34;Resize shape: \u0026#34;, image.shape) 23# Display image and instances 24visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names) Kết quả\n1image_id: 9 datasets/balloon\\train\\15290896925_884ab33fd3_k.jpg 2Original shape: (1356, 2048, 3) 3Resize shape: (1024, 1024, 3) Lưu ý một điều là ở đây, mình sử dụng random image, nên nếu các bạn chạy lại câu lệnh như mình thì kết quả ra phần nhiều sẽ khác mình. Tuy nhiên, Resize shape luôn là (1024, 1024, 3).\nMini Masks Một vấn đề khá nghiêm trọng ở đây là chúng ta cần khá nhiều bộ nhớ để lưu các masks. Numpy sử dụng 1 byte để lưu 1 giá trị bit. Do đó, với kích thước ảnh là 1024x1024, chúng ta cần 1MB bộ nhớ ram để lưu trữ. Nếu chúng ta có tập dataset tầm 1000 bức ảnh thì cần đến 1GB bộ nhớ, khá là lớn. Ngoài việc tốn bộ nhớ lữu trữ, chúng còn làm chậm tốc độ huấn luyện mô hình nữa.\nĐể cải tiến, chúng ta có thể sử dụng một trong hai cách sau:\nCách thứ nhất: Thay vì lưu toàn bộ mask của toàn bức ảnh, chúng ta chỉ lưu những pixel của mask trong bounding box. Với việc sử dụng cách này, chúng ta sẽ tiết kiệm kha khá bộ nhớ chính. Cách thứ hai: Chúng ta có thể resize mask về một kích thước chuẩn nào đó, ví dụ 48x48 pixel. Với những mask có kích thước lớn hơn 48x48, chúng sẽ bị mất thông tin. Mình không thích cách thứ hai cho lắm. Tuy nhiên, theo lý giải của nhóm tác giả Mask R-CNN, thì hầu hết việc gán các đường biên (object annotations) thường không chính xác cho lắm (thừa hoặc thiếu một vài chỗ), cho nên, việc mất mát thông tin với lượng nhỏ này hầu như là không đáng kể.\nĐể đánh giá hiệu quả của hàm mask resizing, chúng ta sẽ chạy đoạn code bên dưới và xem ảnh kết quả. Đoạn code trên mình sử dụng 2 hàm compose_image_meta và load_image_gt của tác giả ở đường dẫn https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py. Mình có modify lại hàm load_image_gt một chút để hợp với ý mình hơn.\n1############################## 2# Data Formatting 3############################## 4 5def compose_image_meta(image_id, original_image_shape, image_shape, 6 window, scale, active_class_ids): 7 \u0026#34;\u0026#34;\u0026#34;Takes attributes of an image and puts them in one 1D array. 8 image_id: An int ID of the image. Useful for debugging. 9 original_image_shape: [H, W, C] before resizing or padding. 10 image_shape: [H, W, C] after resizing and padding 11 window: (y1, x1, y2, x2) in pixels. The area of the image where the real 12 image is (excluding the padding) 13 scale: The scaling factor applied to the original image (float32) 14 active_class_ids: List of class_ids available in the dataset from which 15 the image came. Useful if training on images from multiple datasets 16 where not all classes are present in all datasets. 17 \u0026#34;\u0026#34;\u0026#34; 18 meta = np.array( 19 [image_id] + # size=1 20 list(original_image_shape) + # size=3 21 list(image_shape) + # size=3 22 list(window) + # size=4 (y1, x1, y2, x2) in image cooredinates 23 [scale] + # size=1 24 list(active_class_ids) # size=num_classes 25 ) 26 return meta 27 28 29def load_image_gt(dataset, config, image_id, augment=False, augmentation=None, 30 use_mini_mask=False): 31 \u0026#34;\u0026#34;\u0026#34;Load and return ground truth data for an image (image, mask, bounding boxes). 32 augment: (deprecated. Use augmentation instead). If true, apply random 33 image augmentation. Currently, only horizontal flipping is offered. 34 augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation. 35 For example, passing imgaug.augmenters.Fliplr(0.5) flips images 36 right/left 50% of the time. 37 use_mini_mask: If False, returns full-size masks that are the same height 38 and width as the original image. These can be big, for example 39 1024x1024x100 (for 100 instances). Mini masks are smaller, typically, 40 224x224 and are generated by extracting the bounding box of the 41 object and resizing it to MINI_MASK_SHAPE. 42 Returns: 43 image: [height, width, 3] 44 shape: the original shape of the image before resizing and cropping. 45 class_ids: [instance_count] Integer class IDs 46 bbox: [instance_count, (y1, x1, y2, x2)] 47 mask: [height, width, instance_count]. The height and width are those 48 of the image unless use_mini_mask is True, in which case they are 49 defined in MINI_MASK_SHAPE. 50 \u0026#34;\u0026#34;\u0026#34; 51 # Load image and mask 52 image = dataset.load_image(image_id) 53 mask, class_ids = dataset.load_mask(image_id) 54 original_shape = image.shape 55 image, window, scale, padding, crop = utils.resize_image( 56 image, 57 min_dim=config.IMAGE_MIN_DIM, 58 min_scale=config.IMAGE_MIN_SCALE, 59 max_dim=config.IMAGE_MAX_DIM, 60 mode=config.IMAGE_RESIZE_MODE) 61 mask = utils.resize_mask(mask, scale, padding, crop) 62 63 # Random horizontal flips. 64 # TODO: will be removed in a future update in favor of augmentation 65 if augment: 66 logging.warning(\u0026#34;\u0026#39;augment\u0026#39; is deprecated. Use \u0026#39;augmentation\u0026#39; instead.\u0026#34;) 67 if random.randint(0, 1): 68 image = np.fliplr(image) 69 mask = np.fliplr(mask) 70 71 # Augmentation 72 # This requires the imgaug lib (https://github.com/aleju/imgaug) 73 if augmentation: 74 import imgaug 75 76 # Augmenters that are safe to apply to masks 77 # Some, such as Affine, have settings that make them unsafe, so always 78 # test your augmentation on masks 79 MASK_AUGMENTERS = [\u0026#34;Sequential\u0026#34;, \u0026#34;SomeOf\u0026#34;, \u0026#34;OneOf\u0026#34;, \u0026#34;Sometimes\u0026#34;, 80 \u0026#34;Fliplr\u0026#34;, \u0026#34;Flipud\u0026#34;, \u0026#34;CropAndPad\u0026#34;, 81 \u0026#34;Affine\u0026#34;, \u0026#34;PiecewiseAffine\u0026#34;] 82 83 def hook(images, augmenter, parents, default): 84 \u0026#34;\u0026#34;\u0026#34;Determines which augmenters to apply to masks.\u0026#34;\u0026#34;\u0026#34; 85 return augmenter.__class__.__name__ in MASK_AUGMENTERS 86 87 # Store shapes before augmentation to compare 88 image_shape = image.shape 89 mask_shape = mask.shape 90 # Make augmenters deterministic to apply similarly to images and masks 91 det = augmentation.to_deterministic() 92 image = det.augment_image(image) 93 # Change mask to np.uint8 because imgaug doesn\u0026#39;t support np.bool 94 mask = det.augment_image(mask.astype(np.uint8), 95 hooks=imgaug.HooksImages(activator=hook)) 96 # Verify that shapes didn\u0026#39;t change 97 assert image.shape == image_shape, \u0026#34;Augmentation shouldn\u0026#39;t change image size\u0026#34; 98 assert mask.shape == mask_shape, \u0026#34;Augmentation shouldn\u0026#39;t change mask size\u0026#34; 99 # Change mask back to bool 100 mask = mask.astype(np.bool) 101 102 # Note that some boxes might be all zeros if the corresponding mask got cropped out. 103 # and here is to filter them out 104 _idx = np.sum(mask, axis=(0, 1)) \u0026gt; 0 105 mask = mask[:, :, _idx] 106 class_ids = class_ids[_idx] 107 # Bounding boxes. Note that some boxes might be all zeros 108 # if the corresponding mask got cropped out. 109 # bbox: [num_instances, (y1, x1, y2, x2)] 110 bbox = utils.extract_bboxes(mask) 111 112 # Active classes 113 # Different datasets have different classes, so track the 114 # classes supported in the dataset of this image. 115 active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32) 116 source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\u0026#34;source\u0026#34;]] 117 active_class_ids[source_class_ids] = 1 118 119 # Resize masks to smaller size to reduce memory usage 120 if use_mini_mask: 121 if USE_MINI_MASK_SHAPE: 122 mask = utils.minimize_mask(bbox, mask, MINI_MASK_SHAPE) 123 else: 124 mask = utils.minimize_mask(bbox, mask, mask.shape[:2]) 125 126 # Image meta data 127 image_meta = compose_image_meta(image_id, original_shape, image.shape, 128 window, scale, active_class_ids) 129 130 return image, image_meta, class_ids, bbox, mask 131 132 133image_id = np.random.choice(dataset.image_ids, 1)[0] 134image, image_meta, class_ids, bbox, mask = load_image_gt( 135 dataset, config, image_id, use_mini_mask=False) 136 137 138visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 5))]) 139 140image, image_meta, class_ids, bbox, mask = load_image_gt( 141 dataset, config, image_id, use_mini_mask=True) 142 143 144visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 5))]) 145 146USE_MINI_MASK_SHAPE = True 147 148image, image_meta, class_ids, bbox, mask = load_image_gt( 149 dataset, config, image_id, use_mini_mask=True) 150 151 152visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 5))]) 153 154mask = utils.expand_mask(bbox, mask, image.shape) 155visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names) Với ảnh ở line 1 là ảnh gốc ban đầu và các full mask của bức ảnh, ảnh ở line 2 là chỉ lấy mask của bounding box, ảnh ở line 3 là lấy mask ở bounding box và scale ảnh (do scale ảnh nên ở line 3 các bạn sẽ thấy mask có hình răng cưa, khác với các mask line 2). Line 4 là ảnh ở line 3 được revert back lại hình gốc ban đầu. Các bạn có để ý thấy rằng nó sẽ bị răng cưa ở biên cạnh chứ không được smooth như ảnh gốc. Nếu chúng ta không làm object annotations kỹ, thì object cũng sẽ bị răng cưa như trên.\nAnchors Thứ tự của các anchor thật sự rất quan trọng. Trong quá trình train, thứ tự của các anchor như thế nào thì trong quá trình test, validation, prediction phải dùng y hệt vậy.\nTrong mạng FPN, các anchor phải được xắp xếp theo cách mà chúng ta có thể dễ dàng liên kết với giá trị output\nXắp xếp các anchor theo thứ tự các lớp của pyramid. Tất cả các anchor của level đầu tiên, tiếp theo là các anchor của các lớp thứ hai, lớp thư ba\u0026hellip; Việc xắp xếp theo cách này sẽ giúp chúng ta dễ dàng phân tách các lớp anchor và dễ hiểu theo lẽ tự nhiên.\nTrong mỗi level, xắp xếp các anchor trong mỗi level bằng thứ tự xử lý của các feature map. Thông thường, một convolution layer sẽ dịch chuyển trên feature map bắt đầu từ vị trí trái - trên (top - left) đi xuống phải dưới (từ trái qua phải, xuống hàng rồi lại từ trái qua phải).\nTrên mỗi cell của feature map, chúng ta sẽ xắp xếp các anchor theo các ratios.\nAnchor Stride:\n1 2backbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE) 3anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, 4 config.RPN_ANCHOR_RATIOS, 5 backbone_shapes, 6 config.BACKBONE_STRIDES, 7 config.RPN_ANCHOR_STRIDE) 8 9# Print summary of anchors 10num_levels = len(backbone_shapes) 11anchors_per_cell = len(config.RPN_ANCHOR_RATIOS) 12print(\u0026#34;Total anchors: \u0026#34;, anchors.shape[0]) 13print(\u0026#34;ANCHOR Scales: \u0026#34;, config.RPN_ANCHOR_SCALES) 14print(\u0026#34;BACKBONE STRIDE: \u0026#34;, config.BACKBONE_STRIDES) 15print(\u0026#34;ratios: \u0026#34;, config.RPN_ANCHOR_RATIOS) 16print(\u0026#34;Anchors per Cell: \u0026#34;, anchors_per_cell) 17# print(\u0026#34;Anchors stride: \u0026#34;, config.RPN_ANCHOR_STRIDE) 18print(\u0026#34;Levels: \u0026#34;, num_levels) 19anchors_per_level = [] 20for l in range(num_levels): 21 num_cells = backbone_shapes[l][0] * backbone_shapes[l][1] 22 print(\u0026#34;backbone_shapes in level \u0026#34;,l,\u0026#39; \u0026#39;,backbone_shapes[l][0],\u0026#39;x\u0026#39;,backbone_shapes[l][1]) 23 print(\u0026#34;num_cells in level \u0026#34;,l,\u0026#39; \u0026#39;,num_cells) 24 anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2) 25 print(\u0026#34;Anchors in Level {}: {}\u0026#34;.format(l, anchors_per_level[l])) 1Total anchors: 261888 2ANCHOR Scales: (32, 64, 128, 256, 512) 3BACKBONE STRIDE: [4, 8, 16, 32, 64] 4ratios: [0.5, 1, 2] 5Anchors per Cell: 3 6Levels: 5 7backbone_shapes in level 0 256 x 256 8num_cells in level 0 65536 9Anchors in Level 0: 196608 10backbone_shapes in level 1 128 x 128 11num_cells in level 1 16384 12Anchors in Level 1: 49152 13backbone_shapes in level 2 64 x 64 14num_cells in level 2 4096 15Anchors in Level 2: 12288 16backbone_shapes in level 3 32 x 32 17num_cells in level 3 1024 18Anchors in Level 3: 3072 19backbone_shapes in level 4 16 x 16 20num_cells in level 4 256 21Anchors in Level 4: 768 Trong kiến trức FPN, feature map tại một số layer đầu tiên là những feature map có độ phân giải lớn. Ví dụ, nếu bức ảnh đầu vào có kích thước là 1024x1024 pixel, và kích thước của mỗi anchor lớp đầu tiên là 32x32 pixel (giá trị đầu tiên của RPN_ANCHOR_SCALES (32, 64, 128, 256, 512)) và bước nhảy (STRIDE) của lớp đầu tiên là 4 (giá trị đầu tiên của BACKBONE_STRIDES ([4, 8, 16, 32, 64])). Từ những dữ kiện này, ta có thể suy ra được là sẽ sinh ra backbone cell có kích thước 256x256 pixel =\u0026gt; 256x256 = 65536 anchor. Với mỗi backbone cell, chúng ta thực hiện phép scale với 3 tỷ lệ khác nhau là [0.5, 1, 2], vậy chúng ta có tổng cộng là 65536x3 = 196608 anchor (xấp xỉ 200k anchor). Để ý một điều là kích thước của một anchor là 32x32 pixel, và bước nhảy là 4, cho nên chúng ta sẽ bị chống lấn (overlap) 28 pixel của anchor 1 và anchor 2 ngay sau nó.\nMột điều thú vị là, nếu ta tăng bước nhảy lên gấp 2 lần, ví dụ từ 4 pixel lấy một anchor lên 8 pixel lấy một anchor, thì số lượng anchor giảm đi đến 4 lần (196608 anchor ở level 0 so với 49152 anchor ở level 1).\nThử vẽ tất cả các anchor của tất cả các level ở điểm giữa một bức ảnh bức kỳ lên, mỗi một level sẽ dùng một màu khác nhau, chúng ta được một hình như bên dưới.\n1# Visualize anchors of one cell at the center of the feature map of a specific level 2 3# Load and draw random image 4image_id = np.random.choice(dataset.image_ids, 1)[0] 5image, image_meta, _, _, _ = modellib.load_image_gt(dataset, config, image_id) 6fig, ax = plt.subplots(1, figsize=(10, 10)) 7ax.imshow(image) 8levels = len(backbone_shapes) 9 10kn_color =np.array( [(255,0,0),(0,255,0),(0,0,255),(128,0,0),(0,128,0),(0,0,128)])/255. 11 12for level in range(levels): 13 # colors = visualize.random_colors(levels) 14 colors = kn_color 15 # Compute the index of the anchors at the center of the image 16 level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels 17 level_anchors = anchors[level_start:level_start+anchors_per_level[level]] 18 print(\u0026#34;Level {}. Anchors: {:6} Feature map Shape: {} \u0026#34;.format(level, level_anchors.shape[0], 19 backbone_shapes[level])) 20 center_cell = backbone_shapes[level] // 2 21 center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1]) 22 level_center = center_cell_index * anchors_per_cell 23 center_anchor = anchors_per_cell * ( 24 (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \\ 25 + center_cell[1] / config.RPN_ANCHOR_STRIDE) 26 level_center = int(center_anchor) 27 28 # Draw anchors. Brightness show the order in the array, dark to bright. 29 for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]): 30 y1, x1, y2, x2 = rect 31 p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor=\u0026#39;none\u0026#39;, 32 edgecolor=np.array(colors[level]) / anchors_per_cell) 33 print(i) 34 ax.add_patch(p) 35 36 37plt.show() Nhìn ảnh trên,các bạn phần nào đó mường tượng ra các anchor sẽ như thế nào rồi phải không.\nPrediction Để tiến hành detect vị trí quả bóng và mask của quả bóng, chúng ta download một ảnh small party nhỏ trên internet về và kiểm chứng.\n1 2import os 3 4import tensorflow as tf 5 6import cv2 7 8DEVICE = \u0026#34;/cpu:0\u0026#34; 9ROOT_DIR = os.path.abspath(\u0026#34;../../\u0026#34;) 10MODEL_DIR = os.path.join(ROOT_DIR, \u0026#34;logs\u0026#34;) 11# Create model in inference mode 12 13class InferenceConfig(config.__class__): 14 # Run detection on one image at a time 15 GPU_COUNT = 1 16 IMAGES_PER_GPU = 1 17 18config = InferenceConfig() 19config.display() 20 21with tf.device(DEVICE): 22 model = modellib.MaskRCNN(mode=\u0026#34;inference\u0026#34;, model_dir=MODEL_DIR, 23 config=config) 24 25 26weights_path = \u0026#34;mask_rcnn_balloon.h5\u0026#34; 27 28# Load weights 29print(\u0026#34;Loading weights \u0026#34;, weights_path) 30# model.load_weights(weights_path, by_name=True) 31 32imgpath = \u0026#34;datasets\\\\balloon\\\\test\\\\t1.png\u0026#34; 33# imgpath = \u0026#34;datasets/balloon/val/14898532020_ba6199dd22_k.jpg\u0026#34; 34 35image = cv2.imread(imgpath) 36 37image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) 38 39 40 41ds_name = [\u0026#39;BG\u0026#39;, \u0026#39;balloon\u0026#39;] 42 43 44results = model.detect([image], verbose=1) 45 46def get_ax(rows=1, cols=1, size=16): 47 \u0026#34;\u0026#34;\u0026#34;Return a Matplotlib Axes array to be used in 48 all visualizations in the notebook. Provide a 49 central point to control graph sizes. 50 51 Adjust the size attribute to control how big to render images 52 \u0026#34;\u0026#34;\u0026#34; 53 _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows)) 54 return ax 55# Display results 56ax = get_ax(1) 57r = results[0] 58visualize.display_instances(image, r[\u0026#39;rois\u0026#39;], r[\u0026#39;masks\u0026#39;], r[\u0026#39;class_ids\u0026#39;], 59 dataset.class_names, r[\u0026#39;scores\u0026#39;], ax=ax, 60 title=\u0026#34;Predictions\u0026#34;) 61plt.show() Kết quả nhận dạng khá chính xác phải không các bạn.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Mar 25, 2019","img":"","permalink":"/blog/2019-03-25-mask-rcnn-balloon/","series":null,"tags":["machine learning","deep learning","Mask R-CNN","balloon","bóng bay"],"title":"Tìm Hiểu Mask R-CNN Và Ví Dụ Phân Vùng Quả Bóng Bay Sử Dụng Deep Learning"},{"categories":null,"content":" Thêm dấu tiếng việt là một trong những bài toán khá hay trong xử lý ngôn ngữ tự nhiên. Ở đây, mình đã tiến hành thu thập dữ liệu bài báo của nhiều nguồn khác nhau như zing.vn, vnexpress, kenh14.vn \u0026hellip; làm kho ngữ liệu và xây dựng mô hình.\nĐể tiến hành thực nghiệm, mình sẽ lấy một số đoạn văn mẫu ở trang tin tức của thế giới di động (https.www.thegioididong.com) (mình không crawl nội dung tin tức ở trang này làm dữ liệu học).\nỞ bài viết link https://www.thegioididong.com/tin-tuc/3-ngay-cuoi-tuan-mua-laptop-online-tang-them-pmh-den-400k-tra-gop-0--1151334, mình lấy đoạn mở đầu \u0026ldquo;Từ ngày 15/3 đến 17/3, nhiều mẫu laptop tại Thế Giới Di Động sẽ được ưu đãi mạnh, tặng phiếu mua hàng đến 400 ngàn đồng, trả góp 0% và nhiều quà tặng hấp dẫn khác khi mua theo hình thức ONLINE. Nếu đang có nhu cầu mua laptop, bạn hãy nhanh chóng xem qua danh sách sản phẩm dưới đây nhé.\u0026rdquo;, bỏ dấu của câu đi, thì mình được câu\n\u0026ldquo;Tu ngay 15/3 den 17/3, nhieu mau laptop tai The Gioi Di Dong se duoc uu dai manh, tang phieu mua hang den 400 ngan dong, tra gop 0% va nhieu qua tang hap dan khac khi mua theo hinh thuc ONLINE. Neu dang co nhu cau mua laptop, ban hay nhanh chong xem qua danh sach san pham duoi day nhe.\u0026rdquo;\nSử dụng mô hình mình đã huấn luyện, thu được kết quả như sau:\n\u0026ldquo;Từ ngày 15/3 đến 17/3 m t m, nhiều mẫu laptoP tạI thế giỚi di động sẽ được ưu đãi mạnh, tang phiếu mua hàng đến 400 ngàn đồng, trả góp 0 r% và nhiều quà tặng hấp dẫn khác khi mua theo hìNH THỨc Onfine. nếu đang có nhu cầu mua laptop, bạn hãy nhanh chóng xem qua danh sách sản phẩm dưới\u0026rdquo;\nKết quả khá khả quan phải không các bạn, còn một số lỗi nhỏ ở phần nhận dạng ký tự hoa nữa. Mình sẽ fix lại ở các bài viết sau.\nMình thí nghiệm tiếp với phần đầu bài viết https://www.thegioididong.com/tin-tuc/apple-ban-ra-thi-truong-35-trieu-cap-tai-nghe-airpods-nam-2018-1155181. Đoạn \u0026ldquo;Hôm nay, báo cáo của Counterpoint Research cho thấy, trong năm 2018 Apple đã bán được khoảng 35 triệu cặp tai nghe không dây AirPods. Theo hãng phân tích này, AirPods hiện là tai nghe không dây phổ biến nhất.\u0026rdquo;, bỏ dấu tiếng việt là thu được \u0026ldquo;Hom nay, bao cao cua Counterpoint Research cho thay, trong nam 2018 Apple da ban duoc khoang 35 trieu cap tai nghe khong day AirPods. Theo hang phan tich nay, AirPods hien la tai nghe khong day pho bien nhat.\u0026rdquo;\nKết quả của mô hình: \u0026ldquo;Hôm nay, bạo cáo của Coorteenria eEeeroa c ttt, trong năm 2018 apple đã bán được khoảng 35 triệu cặp tại nghe không đầy aitcoDs. theo Hàng phân tích này, airxoDs Hiện là tai nghe không dạy phổ biến nhất.\u0026rdquo;\nMô hình của mình cho lặp 50 lần. Mình tiến hành thí nghiệm và publish mô hình ở lần lặp thứ 10.\nMã nguồn file predict\n1from keras.models import load_model 2model = load_model(\u0026#39;a_best_weight.h5\u0026#39;) 3 4from collections import Counter 5 6import numpy as np 7 8import utils 9import string 10import re 11 12alphabet = set(\u0026#39;\\x00 _\u0026#39; + string.ascii_lowercase + string.digits + \u0026#39;\u0026#39;.join(utils.ACCENTED_TO_BASE_CHAR_MAP.keys())) 13 14print(\u0026#34;alphabet\u0026#34;,alphabet) 15codec = utils.CharacterCodec(alphabet, utils.MAXLEN) 16 17def guess(ngram): 18 text = \u0026#39; \u0026#39;.join(ngram) 19 text += \u0026#39;\\x00\u0026#39; * (utils.MAXLEN - len(text)) 20 if utils.INVERT: 21 text = text[::-1] 22 preds = model.predict_classes(np.array([codec.encode(text)]), verbose=0) 23 rtext = codec.decode(preds[0], calc_argmax=False).strip(\u0026#39;\\x00\u0026#39;) 24 if len(rtext)\u0026gt;0: 25 index = rtext.find(\u0026#39;\\x00\u0026#39;) 26 if index\u0026gt;-1: 27 rtext = rtext[:index] 28 return rtext 29 30 31def add_accent(text): 32 # lowercase the input text as we train the model on lowercase text only 33 # but we keep the map of uppercase characters to restore cases in output 34 is_uppercase_map = [c.isupper() for c in text] 35 text = utils.remove_accent(text.lower()) 36 37 outputs = [] 38 words_or_symbols_list = re.findall(\u0026#39;\\w[\\w ]*|\\W+\u0026#39;, text) 39 40 # print(words_or_symbols_list) 41 42 for words_or_symbols in words_or_symbols_list: 43 if utils.is_words(words_or_symbols): 44 outputs.append(_add_accent(words_or_symbols)) 45 else: 46 outputs.append(words_or_symbols) 47 # print(outputs) 48 output_text = \u0026#39;\u0026#39;.join(outputs) 49 50 # restore uppercase characters 51 output_text = \u0026#39;\u0026#39;.join(c.upper() if is_upper else c 52 for c, is_upper in zip(output_text, is_uppercase_map)) 53 return output_text 54 55def _add_accent(phrase): 56 grams = list(utils.gen_ngram(phrase.lower(), n=utils.NGRAM, pad_words=utils.PAD_WORDS_INPUT)) 57 58 guessed_grams = list(guess(gram) for gram in grams) 59 # print(\u0026#34;phrase\u0026#34;,phrase,\u0026#39;grams\u0026#39;,grams,\u0026#39;guessed_grams\u0026#39;,guessed_grams) 60 candidates = [Counter() for _ in range(len(guessed_grams) + utils.NGRAM - 1)] 61 for idx, gram in enumerate(guessed_grams): 62 for wid, word in enumerate(re.split(\u0026#39; +\u0026#39;, gram)): 63 candidates[idx + wid].update([word]) 64 output = \u0026#39; \u0026#39;.join(c.most_common(1)[0][0] for c in candidates if c) 65 return output.strip(\u0026#39;\\x00 \u0026#39;) 66 67 68 69# print(add_accent(\u0026#39;do,\u0026#39;)) 70# print(add_accent(\u0026#39;7.3 inch,\u0026#39;)) 71# print(add_accent(\u0026#39;Truoc do, tren san khau su kien SDC 2018, giam doc cao cap mang marketing san pham di dong cua Samsung, ong Justin Denison da cam tren tay nguyen mau cua thiet bi nay. Ve co ban, no chang khac gi mot chiec may tinh bang 7.3 inch, duoc cau thanh tu nhieu lop phu khac nhau nhu polyme, lop man chong soc, lop phan cuc voi do mong gan mot nua so voi the he truoc, lop kinh linh hoat va mot tam lung da nang co the bien thanh man hinh. Tat ca se duoc ket dinh bang mot loai keo cuc ben, cho phep chiec may nay co the gap lai hang tram ngan lan ma khong bi hu hong.\u0026#39;)) 72# print(add_accent(\u0026#39;man hinh. Tat ca se duoc ket dinh bang mot loai keo cuc ben, cho phep chiec may nay co the gap lai hang tram ngan lan ma khong bi hu hong.\u0026#39;)) 73print(add_accent(\u0026#39;Hom nay, bao cao cua Counterpoint Research cho thay, trong nam 2018 Apple da ban duoc khoang 35 trieu cap tai nghe khong day AirPods. Theo hang phan tich nay, AirPods hien la tai nghe khong day pho bien nhat.\u0026#39;)) Mã nguồn file utils\n1import re 2import string 3import time 4from contextlib import contextmanager 5import numpy as np 6 7 8 9# maximum string length to train and predict 10# this is set based on our ngram length break down below 11MAXLEN = 32 12 13# minimum string length to consider 14MINLEN = 3 15 16# how many words per ngram to consider in our model 17NGRAM = 5 18 19# inverting the input generally help with accuracy 20INVERT = True 21 22# mini batch size 23BATCH_SIZE = 128 24 25# number of phrases set apart from training set to validate our model 26VALIDATION_SIZE = 100000 27 28# using g2.2xl GPU is ~5x faster than a Macbook Pro Core i5 CPU 29HAS_GPU = True 30 31PAD_WORDS_INPUT = True 32 33### Ánh xạ từ không dấu sang có dấu 34 35ACCENTED_CHARS = { 36\t\u0026#39;a\u0026#39;: u\u0026#39;a á à ả ã ạ â ấ ầ ẩ ẫ ậ ă ắ ằ ẳ ẵ ặ\u0026#39;, 37\t\u0026#39;o\u0026#39;: u\u0026#39;o ó ò ỏ õ ọ ô ố ồ ổ ỗ ộ ơ ớ ờ ở ỡ ợ\u0026#39;, 38\t\u0026#39;e\u0026#39;: u\u0026#39;e é è ẻ ẽ ẹ ê ế ề ể ễ ệ\u0026#39;, 39\t\u0026#39;u\u0026#39;: u\u0026#39;u ú ù ủ ũ ụ ư ứ ừ ử ữ ự\u0026#39;, 40\t\u0026#39;i\u0026#39;: u\u0026#39;i í ì ỉ ĩ ị\u0026#39;, 41\t\u0026#39;y\u0026#39;: u\u0026#39;y ý ỳ ỷ ỹ ỵ\u0026#39;, 42\t\u0026#39;d\u0026#39;: u\u0026#39;d đ\u0026#39;, 43} 44 45### Ánh xạ từ có dấu sang không dấu 46ACCENTED_TO_BASE_CHAR_MAP = {} 47for c, variants in ACCENTED_CHARS.items(): 48\tfor v in variants.split(\u0026#39; \u0026#39;): 49\tACCENTED_TO_BASE_CHAR_MAP[v] = c 50 51# \\x00 ký tự padding 52 53### Những ký tự cơ bản, bao gồm ký tự padding, các chữ cái và các chữ số 54BASE_ALPHABET = set(\u0026#39;\\x00 _\u0026#39; + string.ascii_lowercase + string.digits) 55 56### Bộ ký tự bao gồm những ký tự cơ bản và những ký tự có dấu 57ALPHABET = BASE_ALPHABET.union(set(\u0026#39;\u0026#39;.join(ACCENTED_TO_BASE_CHAR_MAP.keys()))) 58 59 60def is_words(text): 61\treturn re.fullmatch(\u0026#39;\\w[\\w ]*\u0026#39;, text) 62 63# Hàm bỏ dấu khỏi một câu 64def remove_accent(text): 65\t\u0026#34;\u0026#34;\u0026#34; remove accent from text \u0026#34;\u0026#34;\u0026#34; 66\treturn u\u0026#39;\u0026#39;.join(ACCENTED_TO_BASE_CHAR_MAP.get(char, char) for char in text) 67 68#hàm thêm padding vào một câu 69def pad(phrase, maxlen): 70\t\u0026#34;\u0026#34;\u0026#34; right pad given string with \\x00 to exact \u0026#34;maxlen\u0026#34; length \u0026#34;\u0026#34;\u0026#34; 71\treturn phrase + u\u0026#39;\\x00\u0026#39; * (maxlen - len(phrase)) 72 73 74def gen_ngram(words, n=3, pad_words=True): 75\t\u0026#34;\u0026#34;\u0026#34; gen n-grams from given phrase or list of words \u0026#34;\u0026#34;\u0026#34; 76\tif isinstance(words, str): 77\twords = re.split(\u0026#39;\\s+\u0026#39;, words.strip()) 78 79\tif len(words) \u0026lt; n: 80\tif pad_words: 81\twords += [\u0026#39;\\x00\u0026#39;] * (n - len(words)) 82\tyield tuple(words) 83\telse: 84\tfor i in range(len(words) - n + 1): 85\tyield tuple(words[i: i + n]) 86 87def extract_phrases(text): 88\t\u0026#34;\u0026#34;\u0026#34; extract phrases, i.e. group of continuous words, from text \u0026#34;\u0026#34;\u0026#34; 89\treturn re.findall(r\u0026#39;\\w[\\w ]+\u0026#39;, text, re.UNICODE) 90 91 92@contextmanager 93def timing(label): 94\tbegin = time.monotonic() 95\tprint(label, end=\u0026#39;\u0026#39;, flush=True) 96\ttry: 97\tyield 98\tfinally: 99\tduration = time.monotonic() - begin 100\tprint(\u0026#39;: took {:.2f}s\u0026#39;.format(duration)) 101 102class CharacterCodec(object): 103 def __init__(self, alphabet, maxlen): 104 self.alphabet = list(sorted(set(alphabet))) 105 self.index_alphabet = dict((c, i) for i, c in enumerate(self.alphabet)) 106 self.maxlen = maxlen 107 108 def encode(self, C, maxlen=None): 109 maxlen = maxlen if maxlen else self.maxlen 110 X = np.zeros((maxlen, len(self.alphabet))) 111 for i, c in enumerate(C[:maxlen]): 112 X[i, self.index_alphabet[c]] = 1 113 return X 114 115 def try_encode(self, C, maxlen=None): 116 try: 117 return self.encode(C, maxlen) 118 except KeyError: 119 return None 120 121 def decode(self, X, calc_argmax=True): 122 if calc_argmax: 123 X = X.argmax(axis=-1) 124 return \u0026#39;\u0026#39;.join(self.alphabet[x] for x in X) link donwnload mô hình ở lần lặp thứ 10 ở https://github.com/AlexBlack2202/alexmodel/blob/master/a_best_weight.h5?raw=true\nÀ, kết quả của câu nói phần mở đầu là \u0026ldquo;mẹ nói rằng em rất đậm đang\u0026rdquo;. Hi hi, may quá.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Mar 16, 2019","img":"","permalink":"/blog/2019-03-16-vietnamese-accent/","series":null,"tags":["machine learning","nlp","thêm dấu tiếng việt"],"title":"Thêm Dấu Tiếng Việt Cho Câu Không Dấu"},{"categories":null,"content":"Privacy Policy ==============\nLast updated: February 10, 2024\nThis Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\nWe use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy.\nInterpretation and Definitions Interpretation 1 2The words of which the initial letter is capitalized have meanings defined 3under the following conditions. The following definitions shall have the same 4meaning regardless of whether they appear in singular or in plural. 5 6## Definitions 7 8~~~~~~~~~~~ 9 10For the purposes of this Privacy Policy: 11 12 * Account means a unique account created for You to access our Service or 13 parts of our Service. 14 15 * Affiliate means an entity that controls, is controlled by or is under 16 common control with a party, where \u0026#34;control\u0026#34; means ownership of 50% or 17 more of the shares, equity interest or other securities entitled to vote 18 for election of directors or other managing authority. 19 20 * Company (referred to as either \u0026#34;the Company\u0026#34;, \u0026#34;We\u0026#34;, \u0026#34;Us\u0026#34; or \u0026#34;Our\u0026#34; in this 21 Agreement) refers to Phạm Duy Tùng. 22 23 * Cookies are small files that are placed on Your computer, mobile device or 24 any other device by a website, containing the details of Your browsing 25 history on that website among its many uses. 26 27 * Country refers to: Vietnam 28 29 * Device means any device that can access the Service such as a computer, a 30 cellphone or a digital tablet. 31 32 * Personal Data is any information that relates to an identified or 33 identifiable individual. 34 35 * Service refers to the Website. 36 37 * Service Provider means any natural or legal person who processes the data 38 on behalf of the Company. It refers to third-party companies or 39 individuals employed by the Company to facilitate the Service, to provide 40 the Service on behalf of the Company, to perform services related to the 41 Service or to assist the Company in analyzing how the Service is used. 42 43 * Third-party Social Media Service refers to any website or any social 44 network website through which a User can log in or create an account to 45 use the Service. 46 47 * Usage Data refers to data collected automatically, either generated by the 48 use of the Service or from the Service infrastructure itself (for example, 49 the duration of a page visit). 50 51 * Website refers to Phạm Duy Tùng, accessible from 52 \u0026lt;https://www.phamduytung.com/\u0026gt; 53 54 * You means the individual accessing or using the Service, or the company, 55 or other legal entity on behalf of which such individual is accessing or 56 using the Service, as applicable. 57 58 59# Collecting and Using Your Personal Data 60--------------------------------------- 61 62## Types of Data Collected Personal Data While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\nUsage Data Usage Data Usage Data is collected automatically when using the Service.\nUsage Data may include information such as Your Device\u0026rsquo;s Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\nWhen You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\nWe may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\nInformation from Third-Party Social Media Services The Company allows You to create an account and log in to use the Service through the following Third-party Social Media Services:\nGoogle Facebook Instagram Twitter LinkedIn If You decide to register through or otherwise grant us access to a Third- Party Social Media Service, We may collect Personal data that is already associated with Your Third-Party Social Media Service\u0026rsquo;s account, such as Your name, Your email address, Your activities or Your contact list associated with that account.\nYou may also have the option of sharing additional information with the Company through Your Third-Party Social Media Service\u0026rsquo;s account. If You choose to provide such information and Personal Data, during registration or otherwise, You are giving the Company permission to use, share, and store it in a manner consistent with this Privacy Policy.\nTracking Technologies and Cookies We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\nCookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies. Web Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity). Cookies can be \u0026ldquo;Persistent\u0026rdquo; or \u0026ldquo;Session\u0026rdquo; Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser.\nWe use both Session and Persistent Cookies for the purposes set out below:\nNecessary / Essential Cookies\nType: Session Cookies\nAdministered by: Us\nPurpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.\nCookies Policy / Notice Acceptance Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies identify if users have accepted the use of cookies on the Website.\nFunctionality Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.\nFor more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\nUse of Your Personal Data 1 2The Company may use Personal Data for the following purposes: 3 4 * To provide and maintain our Service , including to monitor the usage of 5 our Service. 6 7 * To manage Your Account: to manage Your registration as a user of the 8 Service. The Personal Data You provide can give You access to different 9 functionalities of the Service that are available to You as a registered 10 user. 11 12 * For the performance of a contract: the development, compliance and 13 undertaking of the purchase contract for the products, items or services 14 You have purchased or of any other contract with Us through the Service. 15 16 * To contact You: To contact You by email, telephone calls, SMS, or other 17 equivalent forms of electronic communication, such as a mobile 18 application\u0026#39;s push notifications regarding updates or informative 19 communications related to the functionalities, products or contracted 20 services, including the security updates, when necessary or reasonable for 21 their implementation. 22 23 * To provide You with news, special offers and general information about 24 other goods, services and events which we offer that are similar to those 25 that you have already purchased or enquired about unless You have opted 26 not to receive such information. 27 28 * To manage Your requests: To attend and manage Your requests to Us. 29 30 * For business transfers: We may use Your information to evaluate or conduct 31 a merger, divestiture, restructuring, reorganization, dissolution, or 32 other sale or transfer of some or all of Our assets, whether as a going 33 concern or as part of bankruptcy, liquidation, or similar proceeding, in 34 which Personal Data held by Us about our Service users is among the assets 35 transferred. 36 37 * For other purposes : We may use Your information for other purposes, such 38 as data analysis, identifying usage trends, determining the effectiveness 39 of our promotional campaigns and to evaluate and improve our Service, 40 products, services, marketing and your experience. 41 42 43We may share Your personal information in the following situations: 44 45 * With Service Providers: We may share Your personal information with 46 Service Providers to monitor and analyze the use of our Service, to 47 contact You. 48 * For business transfers: We may share or transfer Your personal information 49 in connection with, or during negotiations of, any merger, sale of Company 50 assets, financing, or acquisition of all or a portion of Our business to 51 another company. 52 * With Affiliates: We may share Your information with Our affiliates, in 53 which case we will require those affiliates to honor this Privacy Policy. 54 Affiliates include Our parent company and any other subsidiaries, joint 55 venture partners or other companies that We control or that are under 56 common control with Us. 57 * With business partners: We may share Your information with Our business 58 partners to offer You certain products, services or promotions. 59 * With other users: when You share personal information or otherwise 60 interact in the public areas with other users, such information may be 61 viewed by all users and may be publicly distributed outside. If You 62 interact with other users or register through a Third-Party Social Media 63 Service, Your contacts on the Third-Party Social Media Service may see 64 Your name, profile, pictures and description of Your activity. Similarly, 65 other users will be able to view descriptions of Your activity, 66 communicate with You and view Your profile. 67 * With Your consent : We may disclose Your personal information for any 68 other purpose with Your consent. 69 70## Retention of Your Personal Data The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\nThe Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\nTransfer of Your Personal Data 1 2Your information, including Personal Data, is processed at the Company\u0026#39;s 3operating offices and in any other places where the parties involved in the 4processing are located. It means that this information may be transferred to — 5and maintained on — computers located outside of Your state, province, country 6or other governmental jurisdiction where the data protection laws may differ 7than those from Your jurisdiction. 8 9Your consent to this Privacy Policy followed by Your submission of such 10information represents Your agreement to that transfer. 11 12The Company will take all steps reasonably necessary to ensure that Your data 13is treated securely and in accordance with this Privacy Policy and no transfer 14of Your Personal Data will take place to an organization or a country unless 15there are adequate controls in place including the security of Your data and 16other personal information. 17 18## Delete Your Personal Data 19~~~~~~~~~~~~~~~~~~~~~~~~~ 20 21You have the right to delete or request that We assist in deleting the 22Personal Data that We have collected about You. 23 24Our Service may give You the ability to delete certain information about You 25from within the Service. 26 27You may update, amend, or delete Your information at any time by signing in to 28Your Account, if you have one, and visiting the account settings section that 29allows you to manage Your personal information. You may also contact Us to 30request access to, correct, or delete any personal information that You have 31provided to Us. 32 33Please note, however, that We may need to retain certain information when we 34have a legal obligation or lawful basis to do so. 35 36## Disclosure of Your Personal Data Business Transactions If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\nLaw enforcement Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\nOther legal requirements The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\nComply with a legal obligation Protect and defend the rights or property of the Company Prevent or investigate possible wrongdoing in connection with the Service Protect the personal safety of Users of the Service or the public Protect against legal liability Security of Your Personal Data 1 2The security of Your Personal Data is important to Us, but remember that no 3method of transmission over the Internet, or method of electronic storage is 4100% secure. While We strive to use commercially acceptable means to protect 5Your Personal Data, We cannot guarantee its absolute security. 6 7# Children\u0026#39;s Privacy 8------------------ 9 10Our Service does not address anyone under the age of 13. We do not knowingly 11collect personally identifiable information from anyone under the age of 13. 12If You are a parent or guardian and You are aware that Your child has provided 13Us with Personal Data, please contact Us. If We become aware that We have 14collected Personal Data from anyone under the age of 13 without verification 15of parental consent, We take steps to remove that information from Our 16servers. 17 18If We need to rely on consent as a legal basis for processing Your information 19and Your country requires consent from a parent, We may require Your parent\u0026#39;s 20consent before We collect and use that information. 21 22# Links to Other Websites 23----------------------- 24 25Our Service may contain links to other websites that are not operated by Us. 26If You click on a third party link, You will be directed to that third party\u0026#39;s 27site. We strongly advise You to review the Privacy Policy of every site You 28visit. 29 30We have no control over and assume no responsibility for the content, privacy 31policies or practices of any third party sites or services. 32 33# Changes to this Privacy Policy 34------------------------------ 35 36We may update Our Privacy Policy from time to time. We will notify You of any 37changes by posting the new Privacy Policy on this page. 38 39We will let You know via email and/or a prominent notice on Our Service, prior 40to the change becoming effective and update the \u0026#34;Last updated\u0026#34; date at the top 41of this Privacy Policy. 42 43You are advised to review this Privacy Policy periodically for any changes. 44Changes to this Privacy Policy are effective when they are posted on this 45page. 46 47# Contact Us 48---------- 49 50If you have any questions about this Privacy Policy, You can contact us: 51 52 * By email: alexblack2202@gmail.com 53 54 * By visiting this page on our website: 55 \u0026lt;https://www.phamduytung.com/contact/\u0026gt; ","date":"Feb 28, 2019","img":"","permalink":"/privacy/","series":null,"tags":null,"title":"Privacy Policy"},{"categories":null,"content":"Website Terms and Conditions of Use 1. Terms By accessing this Website, accessible from https://www.phamduytung.com/, you are agreeing to be bound by these Website Terms and Conditions of Use and agree that you are responsible for the agreement with any applicable local laws. If you disagree with any of these terms, you are prohibited from accessing this site. The materials contained in this Website are protected by copyright and trade mark law.\n2. Use License Permission is granted to temporarily download one copy of the materials on Phạm Duy Tùng's Website for personal, non-commercial transitory viewing only. This is the grant of a license, not a transfer of title, and under this license you may not:\nmodify or copy the materials; use the materials for any commercial purpose or for any public display; attempt to reverse engineer any software contained on Phạm Duy Tùng's Website; remove any copyright or other proprietary notations from the materials; or transferring the materials to another person or \"mirror\" the materials on any other server. This will let Phạm Duy Tùng to terminate upon violations of any of these restrictions. Upon termination, your viewing right will also be terminated and you should destroy any downloaded materials in your possession whether it is printed or electronic format. These Terms of Service has been created with the help of the Terms Of Service Generator.\n3. Disclaimer All the materials on Phạm Duy Tùng's Website are provided \"as is\". Phạm Duy Tùng makes no warranties, may it be expressed or implied, therefore negates all other warranties. Furthermore, Phạm Duy Tùng does not make any representations concerning the accuracy or reliability of the use of the materials on its Website or otherwise relating to such materials or any sites linked to this Website.\n4. Limitations Phạm Duy Tùng or its suppliers will not be hold accountable for any damages that will arise with the use or inability to use the materials on Phạm Duy Tùng's Website, even if Phạm Duy Tùng or an authorize representative of this Website has been notified, orally or written, of the possibility of such damage. Some jurisdiction does not allow limitations on implied warranties or limitations of liability for incidental damages, these limitations may not apply to you.\n5. Revisions and Errata The materials appearing on Phạm Duy Tùng's Website may include technical, typographical, or photographic errors. Phạm Duy Tùng will not promise that any of the materials in this Website are accurate, complete, or current. Phạm Duy Tùng may change the materials contained on its Website at any time without notice. Phạm Duy Tùng does not make any commitment to update the materials.\n6. Links Phạm Duy Tùng has not reviewed all of the sites linked to its Website and is not responsible for the contents of any such linked site. The presence of any link does not imply endorsement by Phạm Duy Tùng of the site. The use of any linked website is at the user's own risk.\n7. Site Terms of Use Modifications Phạm Duy Tùng may revise these Terms of Use for its Website at any time without prior notice. By using this Website, you are agreeing to be bound by the current version of these Terms and Conditions of Use.\n8. Your Privacy Please read our Privacy Policy.\n9. Governing Law Any claim related to Phạm Duy Tùng's Website shall be governed by the laws of vn without regards to its conflict of law provisions.\n","date":"Feb 28, 2019","img":"","permalink":"/teamofservices/","series":null,"tags":null,"title":"Team of Services"},{"categories":null,"content":" Bài toán người giao hàng là gì Cài đặt chương trình và thực thi Xây dựng vector state Xây dựng hàm fitness function Xác định loại bài toán Xác định thuật toán tối ưu Bài toán người giao hàng là gì Người giao hàng là bài toán cơ bản trong nhóm bài toán tối ưu. Bài toán được phát biểu như sau: Có một người giao hàng cần đi giao hàng tại n thành phố. Xuất phát từ một thành phố nào đó, đi qua các thành phố khác để giao hàng và trở về thành phố ban đầu. Mỗi thành phố chỉ đến một lần, khoảng cách từ một thành phố đến các thành phố khác là xác định được. Hãy tìm một chu trình (một đường đi khép kín thỏa mãn điều kiện trên) sao cho tổng độ dài các cạnh là nhỏ nhất.\nCó rất nhiều cách để giải bài toán này, các bạn đọc có thể search google để tìm thêm cách giải khác, ở đây, mình sẽ trình bày cách sử dụng thư viện mlrose của python để giải quyết bài toán trên.\nCài đặt chương trình và thực thi Chúng ta giả định rằng người giao hàng sẽ đi qua 5 thành phố, và mỗi thành phố sẽ có 2 giá trị x và y tương ứng với toạ độ của các thành phố đó trên bản đồ.\n1input = [ 2[9, 12], 3[24, 15], 4[12 ,30], 5[4 ,3], 6[13, 27], 7] Theo phần trước, chúng ta sẽ xây dựng 4 phần\nXây dựng vector state Đơn giản là một vector x có số lượng phần tử bằng số lượng thành phố mà người giao hàng sẽ viết thăm\nx = [x0,x1,2,x3,x4], trong đó, giá trị x1 là chỉ số của thành phố người giao hàng sẽ ghé đầu tiên, x0 là toạ độ thành phố bắt đầu\nXây dựng hàm fitness function Mục tiêu của bài toán là tìm đường đi ngăn nhất, nên chúng ta có thể dễ dàng xây dựng hàn fitness bằng cách tính khoảng cách euclide giữa các thành phố.\n1 2def fitness_fun(state): 3 distance = 0 4 5 for index in range(1, len(state)): 6 dist = np.linalg.norm(input[state[index-1]]-input[state[index]]) 7 8 distance = distance + dist 9 10 dist = np.linalg.norm(input[state[0]]-input[state[len(state)-1]]) 11 distance = distance + dist 12 13 return distance 14 15fitness_cust = mlrose.CustomFitness(fitness_fun,\u0026#39;tsp\u0026#39;) Xác định loại bài toán Đây là bài toán rời rạc không lặp, nên ta sẽ sử dụng hàm TSPOpt, length = 5 do số lượng phần tử của state là 5, maximize=False do bài toán tìm đường đi ngắn nhất .\n1problem_fit = mlrose.TSPOpt(length = 5, fitness_fn = fitness_cust, 2 maximize=False) Xác định thuật toán tối ưu Chúng ta vẫn tiếp tục sử dụng thuật toán simulated_annealing như trước xem kết quả như thế nào\n1#Define decay schedule 2schedule = mlrose.ExpDecay() 3 4# Define initial state 5init_state = np.array([0, 1, 2, 3, 4]) 6 7# Set random seed 8np.random.seed(1) 9 10# Solve problem using simulated annealing 11best_state, best_fitness = mlrose.simulated_annealing(problem, schedule = schedule, 12 max_attempts = 10, max_iters = 500, 13 init_state = init_state) 14 15print(\u0026#39;The best state found is: \u0026#39;, best_state) 16print(\u0026#39;The fitness at the best state is: \u0026#39;, best_fitness) Kết quả\n1The best state found is: [1 4 2 0 3] 2The fitness at the best state is: 71.30882356753094 Đây là kết quả tối ưu của bài toán.\nThử thay bằng giải thuật di truyền GA, với tỷ lệ đột biến là 0.2\n1best_state, best_fitness = mlrose.genetic_alg(problem,mutation_prob = 0.2) 2 3print(\u0026#39;The best state found is: \u0026#39;, best_state) 4print(\u0026#39;The fitness at the best state is: \u0026#39;, best_fitness) Kết quả\n1 2The best state found is: [0 2 4 1 3] 3The fitness at the best state is: 71.30882356753094 Thử thay đổi tập dữ liệu input có nhiều số phần tử hơn\n1input =[(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), (1, 5), (2, 3)] Kết quả\n1The best state found is: [3 4 5 6 7 0 1 2] 2The fitness at the best state is: 17.34261754766733 Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Feb 8, 2019","img":"","permalink":"/blog/2019-02-08-randomized-optimization-in-python-v1/","series":null,"tags":["chấm điểm công dân","china","China social credit system","credit system"],"title":"Tối Ưu Hoá Ngẫu Nhiên - Bài Toán Người Giao Hàng"},{"categories":null,"content":" Bài toán tối ưu hoá là gì Ví dụ Tại sao lại dùng Randomized Optimization? Giải bài toán tối ưu bằng thư viện mlrose Bài toán 8 hậu Định nghĩa state Định nghĩa fitness funtion Xác định loại bài toán Xác định thuật toán tối ưu Bài toán tối ưu hoá là gì Theo Russell and Norvig bài toán tối ưu hoá là bài toán mà \u0026ldquo;the aim is to find the best state according to an objective function\u0026rdquo; (mình xin phép để nguyên câu tiếng anh).\nTrong đó, state trong từ best state phụ thuộc vào ngữ cảnh của bài toán. Ví dục\nTrong ngữ cảnh là mạng neural network, state chính là các trọng số (weight), best state là tìm các trọng số tối ưu Trong bài toán 8 hậu, state là vị trí của các con hậu, best state là vị trí tốt nhất thoả yêu cầu, cũng chính là lời giải. Trong bài toán người giao hàng, state là các thành phố người giao hàng đi qua. Trong bài toán tô màu cho mỗi quốc gia trên bản đồ, state là màu được tô cho mỗi quốc gia Nói đến đây, các bạn chắc cũng đã hiểu được khái niệm state là gì rồi. Điều quan trọng ở đây là chúng ta có thể biểu diễn state dưới dạng một con số, hoặc một mảng các giá trị số. (nghĩa là chúng ta phải chuyển đổi màu, thành phố, \u0026hellip; dưới dạng số) thì mới có thể tính toán được.\nTừ best trong chữ best state được biểu diễn bởi một hàm toán học (mà chúng ta quen thuộc với các từ như là objective funtion, fitness funtion, cost funtion, loss function , v.v). Cái mà chúng ta muốn là cực đại hoặc cực tiểu hoá nó (để có được kết quả tốt nhất). Hàm này nhận đầu vào là state array và trả về \u0026ldquo;fitness\u0026rdquo; value.\nCho nên, chúng ta có thể định nghĩa đơn giản bài toán tối ưu là việc tìm các giá trị tối ưu để cực đại/ cực tiểu hoá một hàm toán học.\nVí dụ Một ví dụ xàm xàm như sau\nTa có một (state) vector x = [x0,x1,x2,x3,x4] thuộc đoạn [0,1] một hàm f(x) = x0 + x1 + x2 + x3 + x4, tìm các giá trị x để f đạt cực đại.\nRõ ràng, bằng việc tính nhẩm, chúng ta biết được rằng giá trị cực đại của hàm trên là 5, và lời giải cho bài toán trên là x = [1,1,1,1,1].\nCòn theo toán học cấp 3, ta sẽ tính đạo hàm riêng phần của từng phần tử (cái này đơn giản, mình không nhắc lại), và cũng đạt được x = [1,1,1,1,1]\nTại sao lại dùng Randomized Optimization? Trong bài toán ở trên, chúng ta có thể dễ dàng nhẩm được giá trị tối ưu một cách nhanh chóng. Tuy nhiên, trong thực tế, bài toán sẽ khó hơn một chút, và có nhiều hàm chúng ta không thể dễ dàng tìm được giá trị đạo hàm một cách nhanh chóng được (tốn thời gian rất lâu để giải bài toán ). Lúc này, chúng ta sẽ dùng Randomized optimization.\nRandomized optimization sẽ bắt đầu tại một điểm ngẫu nhiên \u0026ldquo;best\u0026rdquo; state nào đó, sau đó sẽ sinh ngẫu nhiên một state khác (thường là láng giềng của \u0026ldquo;best\u0026rdquo; state hiện tại). Nếu state mới đạt giá trị finest tốt hơn \u0026ldquo;best\u0026rdquo; state hiện tại thì gán \u0026ldquo;best\u0026rdquo; state bằng state mới. Quá trình này lặp đi lặp lại cho đến khi không thể tìm được state mới này tốt hơn \u0026ldquo;best\u0026rdquo; state hiện tại.\nKhông có gì bảo đảm rằng randomized optimization sẽ tìm được lời giải tối ưu. Ví dụ như hình trên, thuật toán chỉ có thể dừng ở local maximin, rồi đứng yên ở đó. Tuy nhiên, nếu chúng ta thiết lập số lần lặp đủ lớn, thuật toán thông thường sẽ trả về kết quả tốt hơn.\nỞ đây, chúng ta có một sự đánh đổi trade-off giữa thời gian tìm ra lời giải tối ưu và chất lượng của lời giải.\nGiải bài toán tối ưu bằng thư viện mlrose Để giải bài toán tối ưu bằng thư viện mlrose, chúng ta sẽ phải định nghĩa 4 thứ:\nĐịnh nghĩa state vector Định nghĩa hàm fitness function Xác định loại bài toán Chọn một thuật toán tối ưu hoá ngẫu nhiên để chạy. Để đơn giản, chúng ta sẽ giải quyết bài toán 8 hậu bằng thư viện mlrose.\nBài toán 8 hậu Nhắc lại một chút về bài toán 8 hậu. Trong bàn cờ vua có kích thước 8x8, chúng ta phải chọn vị trí đặt 8 con hậu sao cho trên mỗi dòng, cột và đường chéo của một con hậu bất kỳ đang đứng không giáp mặt với con hậu khác.\nĐịnh nghĩa state Đây rõ ràng là bài toán tối ưu, và bước đầu tiên ta sẽ định nghĩa một vector trạng thái x = [x0, x1, x2, x3, x4, x5, x6, x7], quy ước toạ độ 0,0 là vị trí trái dưới. Giá trị của xi là vị trị cột của con hậu dòng i đang đứng.\nVí dụ, ở hình trên, ta có x = [6, 1, 7, 5, 0, 2, 3, 4], với x0 = 6 nghĩa là con hậu đang ở cột 0 dòng 6 (góc toạ độ chúng ta khảo sát là trái dưới)\nHình trên không phải là lời giải tối ưu cho bài toán, vì con hậu ở cột 5, cột 6 và cột 7 giáp mặt nhau theo đường chéo.\nĐịnh nghĩa fitness funtion Trong thư viện mlrose đã định nghĩa sẵn hàm fitness function cho một số bài toán đơn giản, ví dụ như trong bài toán 8 hậu vừa rồi. Tuy nhiên, chúng ta sẽ không sử dụng hàm có sẵn đó, mà sẽ tự viết một hàm fitness riêng. Có nhiều cách để định nghĩa hàm fitness khác nhau cho bài toán này. Ở đay, chúng ta sẽ xây dựng một hàm có input là vị trí của các con hậu output là một con số thông báo số lượng con hậu không giáp nhau. Nếu số lượng là 8 thì input chính là lời giải của bài toán.\n1# Define alternative N-Queens fitness function for maximization problem 2def queens_max(state): 3 4 # Initialize counter 5 fitness = 0 6 7 # For all pairs of queens 8 for i in range(len(state) - 1): 9 for j in range(i + 1, len(state)): 10 11 # Check for horizontal, diagonal-up and diagonal-down attacks 12 if (state[j] == state[i]) \\ 13 or (state[j] == state[i] + (j - i)) \\ 14 or (state[j] == state[i] - (j - i)): 15 16 # If no attacks, then increment counter 17 fitness += 1 18 break 19 20 21 return fitness 22 23fitness_cust = mlrose.CustomFitness(queens_max) Xác định loại bài toán Thư viện mlrose cung cấp cho chúng ta các lớp để định nghĩa 3 loại bài toán tối ưu:\nDiscreteOpt: Lớp này được sử dụng để giải các bài toán có giá trị trạng thái là rời rạc. Và tập các trạng thái sẽ được cung cấp trước. Mỗi phần tử trong state chỉ nhận một giá trị trong tập trạng thái. và mỗi phần tử trong tập trạng thái chỉ thuộc về một phần tử trong state.\nContinuousOpt: Lớp này được sử dụng để giải các bài toán có giá trị trạng thái là liên tục.\nTSPOpt: Lớp này được dùng để giải các bài toán về travelling. Ví dụ bài toán người giao hàng. Bài toán này khác bài toán Discrete ở chỗ chúng ta sẽ phải tìm ra thứ tự tối ưu của các con số.\nBài toán 8 hậu được xếp vào dạng bài toán tối ưu rời rạc. Trong đó, mỗi phần tử trong state vector chỉ mang một con số từ 0 đến 7.\n1 2problem = mlrose.DiscreteOpt(length = 8, fitness_fn = fitness, 3 maximize = False, max_val = 8) length chính là số lượng phần tử trong state vector ( chúng ta có 8 cột nên length = 8), max_val = 8 (đã nói ở trên, giá trị tối ưu là khi 8 con hậu không giáp mặt nhau). Do bài toán của mình là cực tiểu (lý do là fitness = 0 thì không có con hậu nào giáp mặt nhau, nên chúng ta set maximize = False)\nXác định thuật toán tối ưu Thư viện mlrose cung cấp cho chúng ta các thuật toán như leo đồi (hill climbing), leo đồi ngẫu nhiên (stochastic hill climbing),simulated annealing, thuật giải di truyền (genetic algorithm), MIMIC (Mutual-Information-Maximizing Input Clustering). Với dạng bài toán rời rạc và travelling, chúng ta có thể chọn bất kỳ thuật toán tối ưu nào. Với bài toán liên tục, thì thuật toán MIMIC không hỗ trợ.\nVí dụ, chúng ta sẽ sử dụng simulated annealing để mô phỏng hàm tối ưu, với trạng thái init là x = [1,2,3,4,5,6,7], lặp 1000 lần để tìm trạng thái tốt nhất. Có 10 lần thử. để tìm hàng xóm tốt nhất trong mỗi lần lặp.\n1# Define decay schedule 2schedule = mlrose.ExpDecay() 3 4# Define initial state 5init_state = np.array([0, 1, 2, 3, 4, 5, 6, 7]) 6 7# Set random seed 8np.random.seed(1) 9 10# Solve problem using simulated annealing 11best_state, best_fitness = mlrose.simulated_annealing(problem, schedule = schedule, 12 max_attempts = 10, max_iters = 1000, 13 init_state = init_state) 14 15print(\u0026#39;The best state found is: \u0026#39;, best_state) 16print(\u0026#39;The fitness at the best state is: \u0026#39;, best_fitness) Kết quả\n1The best state found is: [0 7 6 4 7 1 3 5] 2The fitness at the best state is: 1.0 Do best state =1 , nên có 2 con hậu có thể nhìn thấy và tấn công nhau, Chúng ta sẽ thử thay dổi số max_attempts =10 thành max_attempts = 50 xem sao.\n1 2The best state found is: [2 0 6 4 7 1 3 5] 3The fitness at the best state is: 0.0 Thử thay bằng bài toán 12 hậu\n1import mlrose 2 3import numpy as np 4 5# Define alternative N-Queens fitness function for maximization problem 6def queens_max(state): 7 8 # Initialize counter 9 fitness = 0 10 11 # For all pairs of queens 12 for i in range(len(state) - 1): 13 for j in range(i + 1, len(state)): 14 15 # Check for horizontal, diagonal-up and diagonal-down attacks 16 if (state[j] == state[i]) \\ 17 or (state[j] == state[i] + (j - i)) \\ 18 or (state[j] == state[i] - (j - i)): 19 20 # If no attacks, then increment counter 21 fitness += 1 22 break 23 24 25 return fitness 26 27fitness_cust = mlrose.CustomFitness(queens_max) 28 29problem = mlrose.DiscreteOpt(length = 12, fitness_fn = fitness_cust, maximize = False, max_val = 12) 30 31 32# Define decay schedule 33schedule = mlrose.ExpDecay() 34 35# Define initial state 36init_state = np.array([0, 1, 2, 3, 4, 5, 6, 7,8,9,10,11]) 37 38# Set random seed 39np.random.seed(1) 40 41# Solve problem using simulated annealing 42best_state, best_fitness = mlrose.simulated_annealing(problem, schedule = schedule, 43 max_attempts = 100, max_iters = 5000, 44 init_state = init_state) 45 46print(\u0026#39;The best state found is: \u0026#39;, best_state) 47print(\u0026#39;The fitness at the best state is: \u0026#39;, best_fitness) 48`` 49 50Kết quả 51 52```python 53The best state found is: [ 8 10 3 6 0 9 1 5 2 11 7 4] 54The fitness at the best state is: 0.0 Tất nhiên, ở trên chỉ là 1 trong số các lời giải của bài toán trên, chúng ta còn có nhiều lời giải khác, do bài toán có nhiều nghiệm.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Feb 8, 2019","img":"","permalink":"/blog/2019-02-08-getting-started-with-randomized-optimization-in-python/","series":null,"tags":["tối ưu hóa ngẫu nhiên","mlrose"],"title":"Tối Ưu Hoá Ngẫu Nhiên"},{"categories":null,"content":" 1. Cấm bay máy bay hoặc đi tàu điện ngầm 2. Điều chỉnh tốc độ internet 3. Cấm bạn, hoặc con cái của bạn được học ở những trường tốt 4. Không cho bạn có một công việc tốt 5. Không được thuê những khách sạn tốt 6. Cấm nuôi chó 7. Bị bêu tên trước công chúng Chính quyền Trung Quốc đang xây dựng một hệ thống xếp hạng có tên là \u0026quot; Hệ thống tín dụng xã hội - social credit system\u0026quot;. Hệ thống được xây dựng nhằm mục đích theo dõi hành vi của công dân và xếp hạng tất cả các hành vi trên.\nTheo một tài liệu cho biết,\u0026ldquo;Hệ thống tín dụng xã hội\u0026rdquo;, lần đầu tiên được công bố vào năm 2014, nhằm mục đích củng cố ý tưởng rằng \u0026ldquo;giữ niềm tin là vinh quang và phá vỡ niềm tin là ô nhục\u0026rdquo;.\nHệ thống sẽ được vận hành hoàn toàn trên toàn quốc vào năm 2020, nhưng đã được thí điểm ở một số vùng trên đất nước, và mang lại kết quả khá khả quan.\nTại thời điểm hiện tại, hệ thống đang được điều hành bởi chính phủ, một số công ty tư nhân cũng được cấp phép tham gia xây dựng và phát triển hệ thống, như alibaba, tencent.\nGiống như điểm tín dụng tư nhân, điểm xã hội của một người có thể đi lên xuống tùy theo hành vi của họ. Cách thức tính điểm và các hành vi được cho là tốt/xấu hiện thời vẫn chưa được công bố. Nhưng các ví dụ về vi phạm đã bị trừ điểm bao gồm lái xe ẩu, hút thuốc trong khu vực cấm hút thuốc, mua quá nhiều trò chơi video và đăng tin tức giả lên mạng.\n1. Cấm bay máy bay hoặc đi tàu điện ngầm Chính phủ Trung Quốc đã bắt đầu trừng phạt người dân bằng cách hạn chế việc đi lại của họ.\nChín triệu người có điểm thấp đã bị chặn mua vé cho các chuyến bay nội địa, Channel News Asia đưa tin vào 16/Mar/2018 nguồn https://www.channelnewsasia.com/news/asia/china-bad-social-credit-barred-from-buying-train-plane-tickets-10050390.\nNgười dân cũng có thể bị giới hạn sử dụng các dịch vụ nâng cao, ví dụ ba triệu người không được mua vé hạng thương gia (trích cùng nguồn trên).\nHere\u0026#39;s a dystopian vision of the future: A real announcement I recorded on the Beijing-Shanghai bullet train. (I\u0026#39;ve subtitled it so you can watch in silence.) pic.twitter.com/ZoRWtdcSMy\n\u0026mdash; James O\u0026#39;Malley (@Psythor) October 29, 2018 video trên, được đăng bởi nhà báo James O\u0026rsquo;Malley, cho thấy một thông báo trên một chuyến tàu cao tốc từ Bắc Kinh đến Thượng Hải cảnh báo mọi người không nên có những hành vi sai trái - nếu không thì \u0026ldquo;hành vi của họ sẽ được ghi lại trong hệ thống thông tin tín dụng cá nhân\u0026rdquo;.\n2. Điều chỉnh tốc độ internet Theo nghiên cứu của Rachel Botsman (nguồn https://www.wired.co.uk/article/chinese-government-social-credit-score-privacy-invasion) chính quyền sẽ giới hạn tốc độ, băng thông của các dịch vụ internet, 3G, 4G, \u0026hellip; của những công dân có điểm tính dụng xã hội thấp.\nTrong nghiên cứu của tác giả, một số hành vi sẽ bị trừng phạt, bao gồm:\nCông dân có thanh toán hóa đơn đúng hạn hay không. Dành quá nhiều thời gian để chơi trò chơi video Lãng phí tiền mua hàng tào lao và đăng lên phương tiện truyền thông xã hội (dạng như tự sướng ở Việt Nam mình á). Truyền bá tin tức giả mạo, cụ thể là về các cuộc tấn công khủng bố hoặc an ninh sân bay. 3. Cấm bạn, hoặc con cái của bạn được học ở những trường tốt Theo Beijing News reported(nguồn http://www.bjnews.com.cn/news/2018/03/19/479533.html), 17 người đã từ chối thực hiện nghĩa vụ quân sự vào năm ngoái (2017) đã bị cấm đăng ký vào giáo dục đại học, nộp đơn vào trường trung học hoặc tiếp tục việc học tập của họ.\nTheo nguồn https://www.businessinsider.com/china-social-credit-affects-childs-university-enrolment-2018-7?r=UK, vào tháng 7/2018, một trường đại học ở Trung Quốc, đã cấm một sinh viên nhập học (dù anh ấy đã thi đậu), vì lý do là điểm tín dụng xã hội của bố anh ấy \u0026ldquo;xấu\u0026rdquo;.\n4. Không cho bạn có một công việc tốt Theo nguồn của Botsman, các cá nhân có điểm tín nhiệm thấp sẽ bị cấm làm quản lý ở các công ty nhà nước, các ngân hàng lớn.\nCác hành vi như gian lận thuế, tham ô, \u0026hellip; cũng ảnh hưởng đến điểm xã hội.\n5. Không được thuê những khách sạn tốt Theo Botsman, những người gian lận nghĩa vụ quân sự sẽ bị cấm thuê khách sạn tốt khi đi du lịch.\nNhững công dân có điểm tín dụng tốt sẽ được thuê khách sạn mà không cần phải đặt cọc, có thể kéo dài thời gian du lịch hơn.\n6. Cấm nuôi chó Thành phố Tế Nam đã bắt đầu thực thi một hệ thống tín dụng xã hội cho các chủ sở hữu chó vào năm 2017. Theo đó, chủ vật nuôi sẽ bị trừ điểm nếu nuôi chó mà không xích, không rọ mõm, hoặc để cho chó đi bậy nơi công cộng.\nNhững người bị zero điểm sẽ bị cấm nuôi chó, con vật sẽ bị tịch thu, người sở hữu phải làm bài kiểm tra. Nguồn http://uk.businessinsider.com/china-dog-owners-social-credit-score-2018-10\n7. Bị bêu tên trước công chúng Chính phủ đã và đang xây dựng một danh sách các cá nhân có điểm tín nhiệm xấu và sẵn sàng đăng tên kèm hình ảnh của họ trên các phương tiện thông tin đại chúng. Các công ty cũng được khuyến khích tham khảo các thông tin của công dân trong hệ thống trước khi thuê họ.\nĐược biết, toà án sẽ thông báo cho công dân về hành vi của họ trước khi tên của họ được đưa vào danh sách đen. Công dân có 10 ngày kháng cáo kể từ khi nhận được thông báo.\nNguồn https://www.hrw.org/news/2017/12/12/chinas-chilling-social-credit-blacklist, http://zxgk.court.gov.cn/\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Feb 7, 2019","img":"","permalink":"/blog/2019-02-07-china-social-creadit-system/","series":null,"tags":["chấm điểm công dân","china","China social credit system","credit system"],"title":"Hệ Thống Tín Dụng Xã Hội Của Trung Quốc - Những Ảnh Hưởng Khi Bạn Có Điểm Xã Hội Thấp"},{"categories":null,"content":" Mở đầu Chẩn bị dữ liệu Xây dựng mô hình Mở đầu Chẩn bị dữ liệu Xây dựng mô hình Mở đầu Việc xây dựng một mô hình machine learning chưa bao giờ thật sự dễ dàng. Rất nhiều bài báo chỉ \u0026ldquo;show hàng\u0026rdquo; những thứ cao siêu, những thứ chỉ nằm trong sự tưởng tượng của chính các nhà báo. Còn khi đọc các bài báo khoa học về machine learning, tác giả công bố cho chúng ta những mô hình rất tốt, giải quyết một domain nhỏ vấn đề của họ. Tuy nhiên, có một thứ họ không/ chưa công bố. Đó là cách thức họ lựa chọn số lượng note ẩn, số lượng layer trong mô hình neural network. Trong bài viết này, chúng ta sẽ xây dựng mô hình LSTM đơn giản để dự đoán giới tính khi biết tên một người, và thử tìm xem công thức để chọn ra tham số \u0026ldquo;đủ tốt\u0026rdquo; là như thế nào.\nChẩn bị dữ liệu Tập dữ liệu ở đây có khoảng 500000 tên kèm giới tính. Đầu tiên mình sẽ làm sạch dữ liệu bằng cách chỉ lấy giới tính là \u0026rsquo;m\u0026rsquo; và \u0026lsquo;f\u0026rsquo;, loại bỏ những tên quá ngắn (có ít hơn 3 ký tự)\n1filepath = \u0026#39;firstnames.csv\u0026#39; 2max_rows = 500000 # Reduction due to memory limitations 3 4df = (pd.read_csv(filepath, usecols=[\u0026#39;name\u0026#39;, \u0026#39;gender\u0026#39;],sep=\u0026#34;;\u0026#34;) 5 .dropna(subset=[\u0026#39;name\u0026#39;, \u0026#39;gender\u0026#39;]) 6 .assign(name = lambda x: x.name.str.strip()) 7 .assign(gender = lambda x: x.gender.str.lower()) 8 .head(max_rows)) 9 10df= df[df.gender.isin([\u0026#39;m\u0026#39;,\u0026#39;f\u0026#39;])] 11 12# In the case of a middle name, we will simply use the first name only 13df[\u0026#39;name\u0026#39;] = df[\u0026#39;name\u0026#39;].apply(lambda x: str(x).split(\u0026#39; \u0026#39;, 1)[0]) 14 15# Sometimes people only but the first letter of their name into the field, so we drop all name where len \u0026lt;3 16df.drop(df[df[\u0026#39;name\u0026#39;].str.len() \u0026lt; 3].index, inplace=True) Tiếp theo, chúng ta sử dụng một kỹ thuật khá cũ trong NLP là one-hot encoding. Mỗi ký tự được biểu diễn bởi một vector nhị phân. Ví dụ có 26 ký tự trong bảng chữ cái tiếng anh, vector đại diện cho chữ a là [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], ký tự b được biểu diễn là [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], \u0026hellip; tương tự cho đến z.\nMột từ được encode là một tập các vector. Ví dụ chữ hello được biểu diễn là\n1[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #h, 2 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #e, 3 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #l, 4 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #l, 5 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #o] Đọc đến đây, chắc các bạn đã mườn tượng ra rằng một từ sẽ được encode như thế nào rồi phải không. Tiếp theo, chúng ta sẽ xây dựng hàm encode cho tập dữ liệu\n1# Define a mapping of chars to integers 2char_to_int = dict((c, i) for i, c in enumerate(accepted_chars)) 3int_to_char = dict((i, c) for i, c in enumerate(accepted_chars)) 4 5# Removes all non accepted characters 6def normalize(line): 7 return [c.lower() for c in line if c.lower() in accepted_chars] 8 9# Returns a list of n lists with n = word_vec_length 10def name_encoding(name): 11 12 # Encode input data to int, e.g. a-\u0026gt;1, z-\u0026gt;26 13 integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i \u0026lt; word_vec_length] 14 15 # Start one-hot-encoding 16 onehot_encoded = list() 17 18 for value in integer_encoded: 19 # create a list of n zeros, where n is equal to the number of accepted characters 20 letter = [0 for _ in range(char_vec_length)] 21 letter[value] = 1 22 onehot_encoded.append(letter) 23 24 # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array 25 for _ in range(word_vec_length - len(name)): 26 onehot_encoded.append([0 for _ in range(char_vec_length)]) 27 28 return onehot_encoded 29 30# Encode the output labels 31def lable_encoding(gender_series): 32 labels = np.empty((0, 2)) 33 for i in gender_series: 34 if i == \u0026#39;m\u0026#39;: 35 labels = np.append(labels, [[1,0]], axis=0) 36 else: 37 labels = np.append(labels, [[0,1]], axis=0) 38 return labels Và tiến hành chia tập dữ liệu thành train, val, và test set\n1 2# Split dataset in 60% train, 20% test and 20% validation 3train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))]) 4 5# Convert both the input names as well as the output lables into the discussed machine readable vector format 6train_x = np.asarray([np.asarray(name_encoding(normalize(name))) for name in train[predictor_col]]) 7train_y = lable_encoding(train.gender) 8 9validate_x = np.asarray([name_encoding(normalize(name)) for name in validate[predictor_col]]) 10validate_y = lable_encoding(validate.gender) 11 12test_x = np.asarray([name_encoding(normalize(name)) for name in test[predictor_col]]) 13test_y = lable_encoding(test.gender) Vậy là chúng ta đã có chuẩn bị xong dữ liệu đầy đủ rồi đó. Bây giờ chúng ta xây dựng mô hình thôi.\nXây dựng mô hình Có rất nhiều cách để chọn tham số cho mô hình, ví dụ như ở https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning liệt kê ra 4 cách là Manual Search, Grid Search, Random Search, Bayesian Optimization. Tuy nhiên, những cách trên đều khá tốn thời gian và đòi hỏi người kỹ sư phải có am hiểu nhất định.\nỞ đây, chúng ta sử dụng một công thức được đưa ra trong link https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw/136542#136542, cụ thể\n$$ N_h = \\frac{N_s}{(\\alpha * (N_i + N_o))}$$\nTrong đó Ni là số lượng input neural, No là số lượng output neural, Ns là số lượng element trong tập dữ liệu train. alpha là một con số trade-off đại diện cho tỷ lệ thuộc đoạn [2-10].\nMột lưu ý ở đây là bạn có thể dựa vào công thức và số alpha mà ước lượng xem rằng bạn đã có đủ dữ liệu mẫu hay chưa. Một ví dụ đơn giản là giả sử bạn có 10,000 mẫu dữ liệu, input số từ 0 đến 9, output là 64, chọn alpha ở mức nhỏ nhất là 2, vậy theo công thức số neural ẩn là 10000/(26410) = 7.8 ~ 8. Nếu bạn tăng số alpha lên thì số hidden layer còn ít nữa. Điều trên chứng tỏ rằng số lượng mẫu của bạn chưa đủ, còn thiếu quá nhiều. Nếu bạn tăng gấp 100 lần số dữ liệu mẫu, thì con số có vẻ hợp lý hơn.\nTrong tập dữ liệu, mình có:\n1The input vector will have the shape {17} x {82} 2Train len: (21883, 17, 82) 36473 Tổng cộng N_s là 21883, Ni là 17, No là 82, chọn alpha là 2 thì mình có 21883/(21782) = 7.8 ~ 8. Một con số khá nhỏ, chứng tỏ dữ liệu của mình còn quá ít.\nĐối với tập dữ liệu nhỏ như thế này, mình thường sẽ áp dụng công thức sau:\n$$ N_h= \\beta* (N_i + N_o) $$\nVới beta là một con số thực thuộc nửa đoạn (0,1]. Thông thường sẽ là 2/3. Kết quả là số lượng neural của mình khoảng 929.333 node. Thông thường, mình sẽ chọn số neural là một con số là bội số của 2, ở đây 929 gần với 2^10 nhất, nên mình chọn số neural là 2^10.\nTóm lại, mình sẽ theo quy tắc\nNếu dữ liệu nhiều:\n$$ N_h = \\frac{N_s}{(\\alpha * (N_i + N_o))}$$\nNếu dữ liệu ít\n$$ N_h= \\frac{2}{3}* (N_i + N_o) $$\nLàm tròn lên bằng với bội số của 2 mũ gần nhất.\nMột lưu ý nhỏ là số lượng node càng nhiều thì tỷ lệ overfit càng cao, và thời gian huấn luyện càng lâu. Do đó, bạn nên trang bị máy có cấu hình kha khá một chút, tốt hơn hết là nên có GPU đi kèm. Ngoài ra, bạn nên chuẩn bị càng nhiều dữ liệu càng tốt. Một kinh nghiệm của mình rút ra trong quá trình làm Machine Learning là nếu không có nhiều dữ liệu, thì đừng cố thử áp dụng các phương pháp ML trên nó.\nMô hình mình xây dựng như sau:\n1 2hidden_nodes = 1024 3 4 5# Build the model 6print(\u0026#39;Build model...\u0026#39;) 7model = Sequential() 8model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length))) 9model.add(Dropout(0.2)) 10model.add(Dense(units=output_labels)) 11model.add(Activation(\u0026#39;softmax\u0026#39;)) 12model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) 13 14batch_size=1000 15model.fit(train_x, train_y, batch_size=batch_size, epochs=50, validation_data=(validate_x, validate_y)) Do bài viết chỉ tập trung vào vấn đề lựa chọn số lượng node, nên mình sẽ bỏ qua những phần phụ như là early stoping, save each epochs \u0026hellip;, Các vấn đề trên ít nhiều mình đã đề cập ở các bài viết trước.\nKết quả của việc huấn luyện mô hình\n121883/21883 [==============================] - 34s 2ms/step - loss: 0.6602 - acc: 0.6171 - val_loss: 0.6276 - val_acc: 0.7199 2Epoch 2/50 321883/21883 [==============================] - 30s 1ms/step - loss: 0.5836 - acc: 0.7056 - val_loss: 0.5625 - val_acc: 0.7193 4Epoch 3/50 521883/21883 [==============================] - 30s 1ms/step - loss: 0.5531 - acc: 0.7353 - val_loss: 0.5506 - val_acc: 0.7389 6Epoch 4/50 721883/21883 [==============================] - 31s 1ms/step - loss: 0.5480 - acc: 0.7446 - val_loss: 0.5664 - val_acc: 0.7313 8Epoch 5/50 921883/21883 [==============================] - 30s 1ms/step - loss: 0.5406 - acc: 0.7420 - val_loss: 0.5247 - val_acc: 0.7613 10Epoch 6/50 1121883/21883 [==============================] - 30s 1ms/step - loss: 0.5077 - acc: 0.7686 - val_loss: 0.4918 - val_acc: 0.7790 12Epoch 7/50 1321883/21883 [==============================] - 30s 1ms/step - loss: 0.4825 - acc: 0.7837 - val_loss: 0.4939 - val_acc: 0.7740 14Epoch 8/50 1521883/21883 [==============================] - 31s 1ms/step - loss: 0.4611 - acc: 0.7887 - val_loss: 0.4407 - val_acc: 0.8037 16Epoch 9/50 1721883/21883 [==============================] - 30s 1ms/step - loss: 0.4421 - acc: 0.7987 - val_loss: 0.4657 - val_acc: 0.8005 18Epoch 10/50 1921883/21883 [==============================] - 30s 1ms/step - loss: 0.4293 - acc: 0.8055 - val_loss: 0.4183 - val_acc: 0.8141 20Epoch 11/50 2121883/21883 [==============================] - 31s 1ms/step - loss: 0.4129 - acc: 0.8128 - val_loss: 0.4171 - val_acc: 0.8212 22Epoch 12/50 2321883/21883 [==============================] - 30s 1ms/step - loss: 0.4153 - acc: 0.8141 - val_loss: 0.4031 - val_acc: 0.8188 24Epoch 13/50 2521883/21883 [==============================] - 30s 1ms/step - loss: 0.3978 - acc: 0.8191 - val_loss: 0.3918 - val_acc: 0.8280 26Epoch 14/50 2721883/21883 [==============================] - 30s 1ms/step - loss: 0.3910 - acc: 0.8268 - val_loss: 0.3831 - val_acc: 0.8276 28Epoch 15/50 2921883/21883 [==============================] - 30s 1ms/step - loss: 0.3848 - acc: 0.8272 - val_loss: 0.3772 - val_acc: 0.8314 30Epoch 16/50 3121883/21883 [==============================] - 30s 1ms/step - loss: 0.3751 - acc: 0.8354 - val_loss: 0.3737 - val_acc: 0.8363 32Epoch 17/50 3321883/21883 [==============================] - 30s 1ms/step - loss: 0.3708 - acc: 0.8345 - val_loss: 0.3717 - val_acc: 0.8374 34Epoch 18/50 3521883/21883 [==============================] - 31s 1ms/step - loss: 0.3688 - acc: 0.8375 - val_loss: 0.3768 - val_acc: 0.8330 36Epoch 19/50 3721883/21883 [==============================] - 30s 1ms/step - loss: 0.3704 - acc: 0.8375 - val_loss: 0.3621 - val_acc: 0.8392 38Epoch 20/50 3921883/21883 [==============================] - 31s 1ms/step - loss: 0.3608 - acc: 0.8444 - val_loss: 0.3656 - val_acc: 0.8422 40Epoch 21/50 4121883/21883 [==============================] - 31s 1ms/step - loss: 0.3548 - acc: 0.8459 - val_loss: 0.3670 - val_acc: 0.8417 42Epoch 22/50 4321883/21883 [==============================] - 30s 1ms/step - loss: 0.3521 - acc: 0.8452 - val_loss: 0.3555 - val_acc: 0.8462 44Epoch 23/50 4521883/21883 [==============================] - 30s 1ms/step - loss: 0.3432 - acc: 0.8504 - val_loss: 0.3591 - val_acc: 0.8402 46Epoch 24/50 4721883/21883 [==============================] - 31s 1ms/step - loss: 0.3415 - acc: 0.8524 - val_loss: 0.3471 - val_acc: 0.8470 48Epoch 25/50 4921883/21883 [==============================] - 30s 1ms/step - loss: 0.3355 - acc: 0.8555 - val_loss: 0.3577 - val_acc: 0.8436 50Epoch 26/50 5121883/21883 [==============================] - 30s 1ms/step - loss: 0.3320 - acc: 0.8552 - val_loss: 0.3602 - val_acc: 0.8430 52Epoch 27/50 5321883/21883 [==============================] - 30s 1ms/step - loss: 0.3294 - acc: 0.8578 - val_loss: 0.3565 - val_acc: 0.8485 54Epoch 28/50 5521883/21883 [==============================] - 30s 1ms/step - loss: 0.3235 - acc: 0.8602 - val_loss: 0.3427 - val_acc: 0.8514 56Epoch 29/50 5721883/21883 [==============================] - 31s 1ms/step - loss: 0.3138 - acc: 0.8651 - val_loss: 0.3523 - val_acc: 0.8470 58Epoch 30/50 5921883/21883 [==============================] - 30s 1ms/step - loss: 0.3095 - acc: 0.8683 - val_loss: 0.3457 - val_acc: 0.8487 60Epoch 31/50 6121883/21883 [==============================] - 31s 1ms/step - loss: 0.3064 - acc: 0.8701 - val_loss: 0.3538 - val_acc: 0.8531 62Epoch 32/50 6321883/21883 [==============================] - 30s 1ms/step - loss: 0.2985 - acc: 0.8717 - val_loss: 0.3555 - val_acc: 0.8455 64Epoch 33/50 6521883/21883 [==============================] - 30s 1ms/step - loss: 0.2930 - acc: 0.8741 - val_loss: 0.3430 - val_acc: 0.8525 66Epoch 34/50 6721883/21883 [==============================] - 30s 1ms/step - loss: 0.2901 - acc: 0.8786 - val_loss: 0.3457 - val_acc: 0.8503 68Epoch 35/50 6921883/21883 [==============================] - 30s 1ms/step - loss: 0.2852 - acc: 0.8776 - val_loss: 0.3458 - val_acc: 0.8510 70Epoch 36/50 7121883/21883 [==============================] - 30s 1ms/step - loss: 0.2817 - acc: 0.8811 - val_loss: 0.3445 - val_acc: 0.8568 72Epoch 37/50 7321883/21883 [==============================] - 30s 1ms/step - loss: 0.2780 - acc: 0.8816 - val_loss: 0.3356 - val_acc: 0.8540 74Epoch 38/50 7521883/21883 [==============================] - 30s 1ms/step - loss: 0.2734 - acc: 0.8852 - val_loss: 0.3442 - val_acc: 0.8559 76Epoch 39/50 7721883/21883 [==============================] - 31s 1ms/step - loss: 0.2579 - acc: 0.8904 - val_loss: 0.3552 - val_acc: 0.8540 78Epoch 40/50 7921883/21883 [==============================] - 30s 1ms/step - loss: 0.2551 - acc: 0.8927 - val_loss: 0.3677 - val_acc: 0.8532 80Epoch 41/50 8121883/21883 [==============================] - 30s 1ms/step - loss: 0.2558 - acc: 0.8921 - val_loss: 0.3496 - val_acc: 0.8588 82Epoch 42/50 8321883/21883 [==============================] - 30s 1ms/step - loss: 0.2472 - acc: 0.8963 - val_loss: 0.3534 - val_acc: 0.8587 84Epoch 43/50 8521883/21883 [==============================] - 31s 1ms/step - loss: 0.2486 - acc: 0.8948 - val_loss: 0.3490 - val_acc: 0.8537 86Epoch 44/50 8721883/21883 [==============================] - 31s 1ms/step - loss: 0.2503 - acc: 0.8965 - val_loss: 0.3594 - val_acc: 0.8552 88Epoch 45/50 8921883/21883 [==============================] - 30s 1ms/step - loss: 0.2391 - acc: 0.8993 - val_loss: 0.3793 - val_acc: 0.8566 90Epoch 46/50 9121883/21883 [==============================] - 31s 1ms/step - loss: 0.2244 - acc: 0.9048 - val_loss: 0.3815 - val_acc: 0.8543 92Epoch 47/50 9321883/21883 [==============================] - 30s 1ms/step - loss: 0.2203 - acc: 0.9095 - val_loss: 0.3848 - val_acc: 0.8554 94Epoch 48/50 9521883/21883 [==============================] - 30s 1ms/step - loss: 0.2221 - acc: 0.9051 - val_loss: 0.3892 - val_acc: 0.8558 96Epoch 49/50 9721883/21883 [==============================] - 30s 1ms/step - loss: 0.2117 - acc: 0.9124 - val_loss: 0.3654 - val_acc: 0.8544 98Epoch 50/50 9921883/21883 [==============================] - 30s 1ms/step - loss: 0.2141 - acc: 0.9118 - val_loss: 0.3726 - val_acc: 0.8547 Độ chính xác trên tập train là hơn 90%, trên tập val là hơn 85%. Nhìn kỹ hơn vào những từ sai ta thấy rằng\n1 name gender predicted_gender 26750 Chiaki f m 328599 Naheed f m 411448 Espiridión m f 5895 Akmaral f m 633778 Ros f m Có một sự nhập nhằng ở ngôn ngữ giữa tên nam và tên nữ ở những từ này. Có lẽ một tập dữ liệu với đầy đủ họ và tên sẽ cho ra một kết quả có độ chính xác cao hơn. Ví dụ, ở Việt Nam, tên Ngọc thì có thể đặt được cho cả Nam lẫn Nữ.\nMình sẽ cố gắng kiếm một bộ dataset tên tiếng việt và thực hiện việc xây dựng mô hình xác định giới tính thông qua tên người dựa vào mô hình LSTM.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Feb 6, 2019","img":"","permalink":"/blog/2019-02-06-choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras/","series":null,"tags":["Machine learning","Deeplearning","python"],"title":"Lựa Chọn Siêu Tham Số Cho Mô Hình LSTM Đơn Giản Sử Dụng Keras"},{"categories":null,"content":" Mở đầu Giảm bộ nhớ tiêu thụ của một đối tượng trong python Mở đầu Bắt đầu bằng một class đơn giản như sau:\n1class DataItem(object): 2 def __init__(self, name, age, address): 3 self.name = name 4 self.age = age 5 self.address = address Bạn nghĩ một đối tượng của class trên sẽ chiếm bao nhiêu bộ nhớ. Chúng ta cùng tiến hành một vài thí nghiệm nho nhỏ bên dưới.\n1dx = DataItem(\u0026#34;Alex Black\u0026#34;, 42, \u0026#34;-\u0026#34;) 2print (\u0026#34;sys.getsizeof(dx):\u0026#34;, sys.getsizeof(dx)) 3\u0026gt;\u0026gt; sys.getsizeof(dx): 56 Kết quả ra là 56 bytes, khá hợp lý phải không các bạn. Thử với một ví dụ khác xem sao nhỉ.\n1dy = DataItem(\u0026#34;Alex Black\u0026#34;, 42, \u0026#34;I am working at MWG\u0026#34;) 2print (\u0026#34;sys.getsizeof(dy):\u0026#34;, sys.getsizeof(dy)) 3\u0026gt;\u0026gt; sys.getsizeof(dy): 56 Kết quả vẫn là 56 bytes. Có cái gì đó sai sai ở đây không nhỉ?\nChúng ta thực nghiệm một vài thí nghiệm khác để chứng thực.\n1print (sys.getsizeof(\u0026#34;\u0026#34;)) 2\u0026gt;\u0026gt; 49 3print (sys.getsizeof(\u0026#34;1\u0026#34;)) 4\u0026gt;\u0026gt; 50 5print (sys.getsizeof(1)) 6\u0026gt;\u0026gt; 28 7print (sys.getsizeof(dict())) 8\u0026gt;\u0026gt; 240 9print (sys.getsizeof({})) 10\u0026gt;\u0026gt; 240 11print (sys.getsizeof(list())) 12\u0026gt;\u0026gt; 64 13print (sys.getsizeof([])) 14\u0026gt;\u0026gt; 64 15print (sys.getsizeof(())) 16\u0026gt;\u0026gt; 48 Một điều cực kỳ bất ngờ đã xuất hiện ở đây. Một chuỗi rỗng chiếm đến tận 49 bytes, một dictionary rỗng, không chứa phần tử nào chiếm đến 240 bytes, và một list rỗng chiếm tới 64 bytes. Rõ ràng, python đã lưu một số thứ gì đó ngoài dữ liệu của mình.\nĐi sâu vào thử tìm hiểu những thứ \u0026rsquo;linh kiện\u0026rsquo; linh tinh mà python đã kèm theo cho chúng ta là gì nhé.\nĐầu tiên, chúng ta sẽ cần một hàm in ra những thứ mà python đã \u0026rsquo;nhúng\u0026rsquo; thêm vào class DataItem chúng ta khai báo ở trên.\n1def dump(obj): 2 for attr in dir(obj): 3 print(\u0026#34; obj.%s = %r\u0026#34; % (attr, getattr(obj, attr))) và dump biến dy ra thôi\n1dump(dy) 2 3obj.__class__ = \u0026lt;class \u0026#39;__main__.DataItem\u0026#39;\u0026gt; 4 obj.__delattr__ = \u0026lt;method-wrapper \u0026#39;__delattr__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 5 obj.__dict__ = {\u0026#39;name\u0026#39;: \u0026#39;Alex Black\u0026#39;, \u0026#39;age\u0026#39;: 42, \u0026#39;address\u0026#39;: \u0026#39;i am working at MWG\u0026#39;} 6 obj.__dir__ = \u0026lt;built-in method __dir__ of DataItem object at 0x000001A64A6DD0F0\u0026gt; 7 obj.__doc__ = None 8 obj.__eq__ = \u0026lt;method-wrapper \u0026#39;__eq__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 9 obj.__format__ = \u0026lt;built-in method __format__ of DataItem object at 0x000001A64A6DD0F0\u0026gt; 10 obj.__ge__ = \u0026lt;method-wrapper \u0026#39;__ge__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 11 obj.__getattribute__ = \u0026lt;method-wrapper \u0026#39;__getattribute__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 12 obj.__gt__ = \u0026lt;method-wrapper \u0026#39;__gt__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 13 obj.__hash__ = \u0026lt;method-wrapper \u0026#39;__hash__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 14 obj.__init__ = \u0026lt;bound method DataItem.__init__ of \u0026lt;__main__.DataItem object at 0x000001A64A6DD0F0\u0026gt;\u0026gt; 15 obj.__init_subclass__ = \u0026lt;built-in method __init_subclass__ of type object at 0x000001A64A5DE738\u0026gt; 16 obj.__le__ = \u0026lt;method-wrapper \u0026#39;__le__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 17 obj.__lt__ = \u0026lt;method-wrapper \u0026#39;__lt__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 18 obj.__module__ = \u0026#39;__main__\u0026#39; 19 obj.__ne__ = \u0026lt;method-wrapper \u0026#39;__ne__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 20 obj.__new__ = \u0026lt;built-in method __new__ of type object at 0x000000005C2DC580\u0026gt; 21 obj.__reduce__ = \u0026lt;built-in method __reduce__ of DataItem object at 0x000001A64A6DD0F0\u0026gt; 22 obj.__reduce_ex__ = \u0026lt;built-in method __reduce_ex__ of DataItem object at 0x000001A64A6DD0F0\u0026gt; 23 obj.__repr__ = \u0026lt;method-wrapper \u0026#39;__repr__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 24 obj.__setattr__ = \u0026lt;method-wrapper \u0026#39;__setattr__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 25 obj.__sizeof__ = \u0026lt;built-in method __sizeof__ of DataItem object at 0x000001A64A6DD0F0\u0026gt; 26 obj.__str__ = \u0026lt;method-wrapper \u0026#39;__str__\u0026#39; of DataItem object at 0x000001A64A6DD0F0\u0026gt; 27 obj.__subclasshook__ = \u0026lt;built-in method __subclasshook__ of type object at 0x000001A64A5DE738\u0026gt; 28 obj.__weakref__ = None 29 obj.address = \u0026#39;i am working at MWG\u0026#39; 30 obj.age = 42 31 obj.name = \u0026#39;Alex Black\u0026#39; Wow, có vẻ khá là đồ sộ nhỉ.\nTrên github, có một hàm có sẵn tính toán số lượng bộ nhớ mà object chiếm được dựa vào cách truy xuất trực tiếp từng trường dữ liệu của đối tượng và tính toán kích thước\n1import sys 2 3def get_size(obj, seen=None): 4 \u0026#34;\u0026#34;\u0026#34;Recursively finds size of objects\u0026#34;\u0026#34;\u0026#34; 5 size = sys.getsizeof(obj) 6 if seen is None: 7 seen = set() 8 obj_id = id(obj) 9 if obj_id in seen: 10 return 0 11 # Important mark as seen *before* entering recursion to gracefully handle 12 # self-referential objects 13 seen.add(obj_id) 14 if isinstance(obj, dict): 15 size += sum([get_size(v, seen) for v in obj.values()]) 16 size += sum([get_size(k, seen) for k in obj.keys()]) 17 elif hasattr(obj, \u0026#39;__dict__\u0026#39;): 18 size += get_size(obj.__dict__, seen) 19 elif hasattr(obj, \u0026#39;__iter__\u0026#39;) and not isinstance(obj, (str, bytes, bytearray)): 20 size += sum([get_size(i, seen) for i in obj]) 21 return size thử với 2 biến dx và dy của chúng ta xem sao\n1\u0026gt;\u0026gt;\u0026gt; print (\u0026#34;get_size(d1):\u0026#34;, get_size(dx)) 2get_size(d1): 466 3\u0026gt;\u0026gt;\u0026gt; print (\u0026#34;get_size(d1):\u0026#34;, get_size(dy)) 4get_size(d1): 484 Chúng tốn lần lượt là 466 và 484 bytes. Có vẻ đúng đó nhỉ.\nĐiều chúng ta quan tâm lúc này là có cách nào để giảm bộ nhớ tiêu thụ của một object hay không?\nGiảm bộ nhớ tiêu thụ của một đối tượng trong python Tất nhiên là sẽ có cách giảm. Python là một ngôn ngữ thông dịch, và nó cho phép chúng ta mở rộng lớp bất kể lúc nào bằng cách thêm một/ nhiều trường dữ liệu.\n1dz = DataItem(\u0026#34;Alex Black\u0026#34;, 42, \u0026#34;-\u0026#34;) 2dz.height = 1.80 3print ( get_size(dz)) 4\u0026gt;\u0026gt; 484 Chính vì lý do này, trình biên dịch sẽ tốn thêm một đống bộ nhớ tạm để chúng ta có thể dễ dàng mở rộng một lớp trong tương lai. Nếu chúng ta \u0026ldquo;ép buộc\u0026rdquo; trình biên dịch, nói rằng chúng ta chỉ có nhiêu đó trường, và bỏ phần dư thừa đi.\n1class DataItem(object): 2 __slots__ = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;address\u0026#39;] 3 def __init__(self, name, age, address): 4 self.name = name 5 self.age = age 6 self.address = address Và thử lại\n1 2dz = DataItem(\u0026#34;Alex Black\u0026#34;, 42, \u0026#34;i am working at MWG\u0026#34;) 3print (\u0026#34;sys.getsizeof(dz):\u0026#34;, get_size(dz)) 4 5\u0026gt;\u0026gt;sys.getsizeof(dz): 64 Các bạn thấy gì không, bộ nhớ tiêu thụ chỉ là \u0026ldquo;64 bytes\u0026rdquo;. Dung lượng đã giảm đi hơn \u0026ldquo;7 lần\u0026rdquo; so với model class ban đầu. Tuy nhiên, chúng ta sẽ không thể mở rộng class dễ dàng như xưa nữa.\n1\u0026gt;\u0026gt;\u0026gt; dz.height = 1.80 2Traceback (most recent call last): 3 File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; 4AttributeError: \u0026#39;DataItem\u0026#39; object has no attribute \u0026#39;height\u0026#39; Thử tạo một đối tượng có 1000 phần tử và kiểm tra thử.\n1class DataItem(object): 2 __slots__ = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;address\u0026#39;] 3 def __init__(self, name, age, address): 4 self.name = name 5 self.age = age 6 self.address = address 7 8 9data = [] 10 11tracemalloc.start() 12start =datetime.datetime.now() 13for p in range(100000): 14 data.append(DataItem(\u0026#34;Alex\u0026#34;, 42, \u0026#34;middle of nowhere\u0026#34;)) 15 16end =datetime.datetime.now() 17snapshot = tracemalloc.take_snapshot() 18top_stats = snapshot.statistics(\u0026#39;lineno\u0026#39;) 19total = sum(stat.size for stat in top_stats) 20print(\u0026#34;Total allocated size: %.1f MB\u0026#34; % (total / (1024*1024))) 21print(\u0026#34;Total execute time:\u0026#34;,(end-start).microseconds) 22 23\u0026gt;\u0026gt; Total allocated size: 6.9 MB 24\u0026gt;\u0026gt; Total execute time: 232565 Bỏ dòng slots = [\u0026rsquo;name\u0026rsquo;, \u0026lsquo;age\u0026rsquo;, \u0026lsquo;address\u0026rsquo;] đi thử\n1 2class DataItem(object): 3 def __init__(self, name, age, address): 4 self.name = name 5 self.age = age 6 self.address = address 7 8 9data = [] 10 11tracemalloc.start() 12start =datetime.datetime.now() 13for p in range(100000): 14 data.append(DataItem(\u0026#34;Alex\u0026#34;, 42, \u0026#34;middle of nowhere\u0026#34;)) 15end =datetime.datetime.now() 16snapshot = tracemalloc.take_snapshot() 17top_stats = snapshot.statistics(\u0026#39;lineno\u0026#39;) 18total = sum(stat.size for stat in top_stats) 19print(\u0026#34;Total allocated size: %.1f MB\u0026#34; % (total / (1024*1024))) 20print(\u0026#34;Total execute time:\u0026#34;,(end-start).microseconds) 21 22\u0026gt;\u0026gt; Total allocated size: 16.8 MB 23\u0026gt;\u0026gt; Total execute time: 240772 So sánh thử, chúng ta thấy rằng số lượng RAM giảm đi khá nhiều, thời gian thực thi khá tương đương nhau (có giảm một chút).\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Feb 6, 2019","img":"","permalink":"/blog/2019-02-06-how-to-reduce-memory-consumption-by-half-by-adding-just-one-line-of-code/","series":null,"tags":["Machine learning","Deeplearning","python"],"title":"Giảm Bộ Nhớ Sử Dụng Trong Python"},{"categories":null,"content":" Mở đầu Mẹo số 1: Sức mạnh của một dòng Mẹo 2: Các thao tác nhanh trên chuỗi Mẹo số 3: Chuỗi lồng nhau Mẹo 4: Cấu trúc dữ liệu đơn giản. Mẹo 5: Xuất dữ liệu ra command line dễ dàng Mở đầu Hiện nay, có rất nhiều thư viện do cộng đồng đóng góp và xây dựng. Ví dụ như biopython trong tin sinh học, pandas (data science), keras/tensorflow (machine learning), astropy ( cho thiên văn học - astronomy). Trước khi bắt đầu đọc bài viết này, bạn đên đọc \u0026ldquo;Python Tricks Book\u0026rdquo; của Dan Bader trước (https://dbader.org/products/python-tricks-book/). Trong sách, anh ấy đã chia sẻ một số lời khuyên và mẹo về các code python hiệu quả hơn.\nMẹo số 1: Sức mạnh của một dòng Khi bạn đọc một đoạn giải thuật với nhiều dòng code, có thể bạn sẽ bị quên thông tin những dòng trước đó đã viết gì, đặc biệt là trong những câu lệnh điều kiện. Ví dụ:\n1 2if alpha \u0026gt; 7: 3 beta = 999 4elif alpha == 7: 5 beta = 99 6else: 7 beta =0 Chóng ta có thể viết đơn giản hơn chỉ với một dòng code như sau.\n1beta = 999 if alpha \u0026gt; 7 else 99 if alpha == 7 else 0 thật đơn giản phải không. Bạn chỉ cần nhìn đúng một dòng là nằm được nội dung ý nghĩa của đoạn code bạn cần. Một ví dụ khác về vòng lặp for.\n1lst = [1, 2, 3, 4] 2lst_double = [] 3 4for num in lst: 5 lst_double.append(num * 2) Đoạn code trên có thể viết lại dưới dạng 1 dòng như sau.\n1lst_double = [num * 2 for num in lst] Tất nhiên, bạn không nên \u0026ldquo;lạm dụng\u0026rdquo; one line một cách thái quá, ví dụ\n1import pprint; pprint.pprint(zip((\u0026#39;Byte\u0026#39;, \u0026#39;KByte\u0026#39;, \u0026#39;MByte\u0026#39;, \u0026#39;GByte\u0026#39;, \u0026#39;TByte\u0026#39;), (1 \u0026lt;\u0026lt; 10*i for i in xrange(5)))) Trông nó có vẻ hơi \u0026ldquo;lố bịch\u0026rdquo; phải không.\nMẹo 2: Các thao tác nhanh trên chuỗi Python cung cấp cho chúng ta một số cách viết ngắn gọn giúp chúng ta có thể dể dàng thao tác trên chuỗi. Để reverse một chuỗi, chúng ta sử dụng toán tử ::-1\n1 2str = \u0026#39;i am alex\u0026#39; 3print(str[::-1]) 4\u0026gt;\u0026gt; xela ma i Mẹo trên cũng có thể sử dụng đối với list số nguyên.\nĐể nối các phần tử trong một list thành một chuỗi, chúng ta có thể dùng hàm join()\n1 2str1 = [\u0026#34;pig\u0026#34;, \u0026#34;year\u0026#34; , \u0026#34;2019\u0026#34;] 3str2 = \u0026#34;happy \u0026#34; 4str3 = \u0026#34;new \u0026#34; 5 6 7print( \u0026#39; \u0026#39;.join(str1)) 8\u0026gt;\u0026gt; pig year 2019 9 10print(str2+str3+\u0026#39; \u0026#39;.join(str1)) 11\u0026gt;\u0026gt; happy new year 2019 Thật tuyệt vời phải không các bạn.\nNgoài ra các bạn có thể sử dụng biếu thức chính quy để tìm kiếm chuỗi và pattern. Về biểu thức chính quy trong python, các bạn có thể tìm hiểu ở https://docs.python.org/3/library/re.html.\nMẹo số 3: Chuỗi lồng nhau Thử tưởng tượng rằng bạn có hàng tá các list, và sau một mớ các thao tác, kết quả của bạn là một list các list. Chúng ta sẽ sử dụng itertools - một thư viện được cung cấp sẵn trong python để giải quyết vấn đề này giúp chúng ta.\n1 2import itertools 3flatten = lambda x: list(itertools.chain.from_iterable(x)) 4s =[[\u0026#34;this\u0026#34;,\u0026#34;is\u0026#34;],[\u0026#34;the\u0026#34;,\u0026#34;year\u0026#34;], [\u0026#34;of\u0026#34;, \u0026#34;pig\u0026#34;], [\u0026#34;in\u0026#34;], [\u0026#34;Việt\u0026#34;, \u0026#34;Nam\u0026#34;]] 5 6print(\u0026#39; \u0026#39;,join(flatten(s))) 7\u0026gt;\u0026gt; this is the year of pig in Việt Nam Nếu bạn chạy dòng code trên bị lỗi, rất có thể là do terminal của bạn không hỗ trợ tiếng việt font unicode. Hãy chuyển qua font unicode trên terminal hoặc dùng terminal của ubuntu, bash (trên window 10).\nNgoài ra, itertools còn hỗ trợ rất nhiều hàm khác để giúp chúng ta thao tác trên chuỗi lồng dễ dàng hơn. Các bạn có thể tham khảo thêm ở https://docs.python.org/2/library/itertools.html.\nMẹo 4: Cấu trúc dữ liệu đơn giản. Chúng ta có thể xây dựng một cây đơn giản chỉ với một dòng mã lệnh:\n1def tree(): return defaultdict(tree) Một ví dụ đơn giản khác là hàm tạo số nguyên chỉ với 1 dòng code ngắn gọn\n1reduce( (lambda r,x: r-set(range(x**2,N,x)) if (x in r) else r), 2 range(2,N), set(range(2,N))) Python có hỗ trợ nhiều thư viện rất mạnh trong việc giải quyết các vấn đề trong thế giới thực. Ví dụ thư viện Collections\n1from collections import Counter 2myList = [1,1,2,3,4,5,3,2,3,4,2,1,2,3] 3print(Counter(myList)) 4Counter({2: 4, 3: 4, 1: 3, 4: 2, 5: 1}) Một lưu ý nhỏ là các thư viện này chỉ nên sử dụng khi tập dữ liệu của bạn nhỏ, nếu tập dữ liệu lớn, ví dụ bạn cần đếm số lần xuất hiện của các từ trong tập văn bản với 100GB dữ liệu. Bạn hãy dùng cách khác, ví dụ hadoop, hoặc tăng bộ nhớ ram của bạn lên, ví dụ 1 Tb chẳng hạn :)\nMẹo 5: Xuất dữ liệu ra command line dễ dàng Để xuất dữ liệu của một list int ra command line, theo như mẹo ở trên, ta sẽ dùng hàm .join() và vòng lặp.\n```python` lst_row = [1,2,3,4,5] print(\u0026rsquo;,\u0026rsquo;.join([str(x) for x in lst_row]) 1,2,3,4,5\n1 2Cách đơn giản hơn chỉ với một dòng code (Ước gì mình biết cách này sớm hơn, hix). 3 4```python 5print(*lst_row, sep=\u0026#39;,\u0026#39;) 61,2,3,4,5 Một mẹo khác là trong một số trường hợp duyệt mảng, bạn cần lấy giá trị và chỉ số của mảng đó để làm một số thao tác khác\n1 2lst_arr = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;] 3 4int_index = 0 5 6for item in lst_arr: 7 print(int_index, item) 8\tint_index = int_index + 1 9 10\u0026gt;\u0026gt; 0 a 111 b 122 c 133 d hoặc cách viết giống c/c++\n1 2for int_index in len(lst_arr): 3 print(int_index, lst_arr[int_index]) 4 5\u0026gt;\u0026gt; 0 a 61 b 72 c 83 d Một cách khác là sử dụng hàm có sẵn enumerate của python\n1for int_index, item in enumerate(lst_arr): 2 print(int_index, item) 3 4\u0026gt;\u0026gt; 0 a 51 b 62 c 73 d Có rất nhiều mẹo hay để đơn giản hoá việc xuất dữ liệu ra terminal. Hãy thông tin cho mình biết nếu bạn có nhiều mẹo hay khác cần chia sẻ nhé.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Feb 5, 2019","img":"","permalink":"/blog/2019-02-05-5-python-tricks-you-need-to-know-today/","series":null,"tags":["Machine learning","Deeplearning","python"],"title":"5 Mẹo Hay Sử Dụng Python"},{"categories":null,"content":" Đặt vấn đề Phân tích dữ liệu Xây dựng chiến lược tiếp cận bài toán 1. Tiền xử lý dữ liệu 2. Xây dựng mô hình Content-Based Filtering a. Xây dựng tập đặc trưng b. Xây dựng mô hình 3. Collaborative Filtering Model a. Xây dựng ma trận donor - project b. Singular Value Decomposition c. Xây dựng Collaborative Filtering Model 4. Hybrid Method 5. Đánh giá mô hình Đặt vấn đề DonorsChoose.org được thành lập vào năm 2000 bởi một giáo viên lịch sử tại Mỹ tên là Bronx và đã huy động được 685 triệu đô la cho các lớp học. 3/4 các giáo viên ở các trường công lập ở Hoa Kỳ đã sử dụng Donor để gửi các yêu cầu bài tập cho học sinh. Từ đó, Donor trở thành nền tảng giáo dục hàng đầu hỗ trợ cho các vấn đề giáo dục công cộng.\nĐến nay, hơn 3 triệu người dùng và đối tác đã đóng góp hơn 1,1 triệu dự án cho Donor. Nhưng các giáo viên vẫn phải tốn hàng tỷ đô tiền túi để chuẩn bị các dụng cụ học tập trên lớp (để truyền tải kiến thức cho học sinh).\nGiải pháp được đưa ra ở đây là xây dựng một chiến dịch gợi ý cho các nhà tại trợ.\nPhân tích dữ liệu Chúng ta có các file sau:\nFile Donations.csv. Với mỗi dự án (Project ID), sẽ có 1 hoặc nhiều nhà quyên góp (Donor ID) mỗi cặp (dự án - nhà quyên góp sẽ định dang bằng 1 mã chung (Donation ID) và có các cột thông tin liên quan đến việc quyên góp đó). File có xấp xỉ 4.67 triệu dòng (chính xác là 4687844 dòng) và 7 cột. (Project ID - Định danh dự án, Donation ID - Định danh khoảng đóng góp (tưởng tượng như khoá tự tăng của bảng này đó các bạn), Donor ID - Mã định danh người đóng góp, Donation Included Option - hỗ trợ website donoschoose 15% giá trị quyên góp, Donation Amount - Số tiền quyên góp, Donor Cart Sequence - Thứ tự của dự án trọng bảng danh sách quyên góp,Donation Received Date - Ngày giờ quyên góp).\nFile Donors.csv. File định danh người quyên góp. Chứa tổng cộng hơn 2 triệu dòng( chính xác là 2122640 dòng) File có kích thước 2122640 x 5 với các thông tin cột là Donor ID (khoá chính, không trùng), Donor City (tên thành phố nhà đầu tư đang sinh sống), Donor State (tiểu bang mà người quyên góp đang sống), Donor is teacher, Donor Zip (3 ký tự đầu của mã bưu điện nhà từ thiện).\nFile Teacher.csv. File có tổng cộng 402900 dòng với các cột TeachId, Teacher Prefix (Mr, Mrs, Ms), Teacher First Project Posted Date.\nFile Schools.csv. File có tổng cộng 72994 dòng với các cột là SchoolID, SchoolName (tên trường có thể trùng nhau), School Metro Type ( phân loại trường thuộc 1 trong 5 nhóm : suburnban - ngoại ô, rural - nông thôn, uban - thành thị, town - thị trấn, unknow), School Percentage Free Lunch ( Số nguyên, mô tả tỷ lệ phần trăm số học sinh đủ điều kiện ăn trưa miễn phí hoặc ăn trưa giảm phí. Dữ liệu thu được cung cấp bởi một đối tác thống kê độc lập là NCES. Nếu trường nào không có giá trị do NCES cung cấp, chúng ta sẽ lấy số phần trăm này là trung bình phần trăm của các trường cùng huyện), School State (Trường đang toạ lạc ở bang nào (vd cali, Florida, Virginia, \u0026hellip;)), School Zip (mã bưu chính), School City, School County\nFile Resources.csv. Với mỗi dự án, chúng ta cần các loại tài nguyên khác nhau. Các cột là Project ID (mã dự án), Resource Item Name (tên tài nguyên cần cho dự án đó vd project 000009891526c0ade7180f8423792063 cần \u0026lsquo;chair move and store cart\u0026rsquo;), Resource Quantity (số lượng tài nguyên cần, vd cần 1 cái ghế, 2 cái bảng v.v), Resource Unit Price (đơn giá cho 1 đơn vị tài nguyên, vd cái ghế giá 7 ngàn, cái bảng giá 10 ngàn, nếu 1 unit là ghế + bảng thì là 17 ngàn), Resource Vendor Name(nhà cung cấp, vd: Amazon Business, Woodwind and Brasswind).\nFile Projects.csv\nXây dựng chiến lược tiếp cận bài toán Hãy xem đây như là bài toán gợi ý. Và Donors chính là hệ thống cung cấp các sản phẩm. Ví dụ đơn giản là bạn có website nghe nhạc mp3.zing.vn, alice vào nghe một hoặc một vài bài nhạc. Chúng ta sẽ xây dựng một hệ gợi ý những bài nhạc tiếp theo alice nên nghe dựa vào những bài nhạc đã nghe trước đó của alice. Tương tự vậy, hệ thống Donor như là website mp3.zing, bài nhạc tương tự như các project đang có, người dùng tương tự như các nhà tự thiện. Một khi một nhà từ thiện đã quyên góp cho 1 hoặc 1 nhón các dự án, chúng ta sẽ lên kế hoạch và gợi ý cho khác hàng dự án tiếp theo khách hàng nên tìm hiểu kỹ để xét xem có nên donate hay không.\nDựa vào các chiến lược trên, chúng ta có 3 cách có thể tiếp cận vấn đề:\nContent-based filltering. Collaborative Filtering Hybrid methods 1. Tiền xử lý dữ liệu Trước khi bắt đầu xây dựng chương trình gợi ý, chúng ta cần phải load dữ liệu lên bộ nhớ chính và làm sạch dữ liệu.\nTrước tiên, chúng ta sẽ import các thư viện cần thiết. Nếu thiếu các thư viện nào, các bạn cứ pip install tên thư viện trong cmd/terminal là được\n1 2import numpy as np 3import scipy 4import pandas as pd 5import math 6import random 7import sklearn 8from nltk.corpus import stopwords 9from sklearn.model_selection import train_test_split 10from sklearn.feature_extraction.text import TfidfVectorizer 11from sklearn.metrics.pairwise import cosine_similarity 12from scipy.sparse.linalg import svds 13import matplotlib.pyplot as plt 14import os Tiếp theo, chúng ta sẽ load 3 file Projects.csv, Donations.csv, Donors.csv lên và merge donations với donors.\n1# Set up test mode to save some time 2test_mode = True 3 4# Read datasets 5projects = pd.read_csv(\u0026#39;../input/Projects.csv\u0026#39;) 6donations = pd.read_csv(\u0026#39;../input/Donations.csv\u0026#39;) 7donors = pd.read_csv(\u0026#39;../input/Donors.csv\u0026#39;) 8 9#this piece of code converts Project_ID which is a 32-bit Hex int digits 10-1010 10# create column \u0026#34;project_id\u0026#34; with sequential integers 11f=len(projects) 12projects[\u0026#39;project_id\u0026#39;] = np.nan 13g = list(range(10,f+10)) 14g = pd.Series(g) 15projects[\u0026#39;project_id\u0026#39;] = g.values 16 17# Merge datasets 18donations = donations.merge(donors, on=\u0026#34;Donor ID\u0026#34;, how=\u0026#34;left\u0026#34;) 19df = donations.merge(projects,on=\u0026#34;Project ID\u0026#34;, how=\u0026#34;left\u0026#34;) 20 21# only load a few lines in test mode 22if test_mode: 23 df = df.head(10000) 24 25donations_df = df Ở giai đoạn xây dựng code và debug, mình chỉ load 10000 dữ liệu lên để test thử (để đảm bảo rằng mình code đúng - bằng cách set test_mode = True). Khi chạy thật mình sẽ set lại test_mode = False.\nThực hiện một vài bước phân tích kỹ thuật đơn giản để nắm rõ hơn về dữ liệu.\nThử đo mối quan hệ giữa các dự án và các \u0026ldquo;mạnh thường quân\u0026rdquo;\n1# Deal with missing values 2donations[\u0026#34;Donation Amount\u0026#34;] = donations[\u0026#34;Donation Amount\u0026#34;].fillna(0) 3 4# Define event strength as the donated amount to a certain project 5donations_df[\u0026#39;eventStrength\u0026#39;] = donations_df[\u0026#39;Donation Amount\u0026#39;] 6 7def smooth_donor_preference(x): 8 return math.log(1+x, 2) 9 10donations_full_df = donations_df \\ 11 .groupby([\u0026#39;Donor ID\u0026#39;, \u0026#39;Project ID\u0026#39;])[\u0026#39;eventStrength\u0026#39;].sum() \\ 12 .apply(smooth_donor_preference).reset_index() 13 14# Update projects dataset 15project_cols = projects.columns 16projects = df[project_cols].drop_duplicates() 17 18print(\u0026#39;# of projects: %d\u0026#39; % len(projects)) 19print(\u0026#39;# of unique user/project donations: %d\u0026#39; % len(donations_full_df)) 1# of projects: 1889 2# of unique user/project donations: 8648 Dựa vào kết quả trên tập test, chúng ta có thể đưa ra một vài nhận xét như sau:\nHầu hết các mạnh thường quân chỉ donate cho 1 project (tỷ lệ 86,48%) Sẽ có trường hợp 1 mạnh thường quân sẽ donate cho nhiều dự án, và cũng có trường hợp 1 mạnh thường quân donate nhiều lần cho 1 dự án. Trường hợp này chiếm phần ít. Để đánh giá mô hình, chúng ta sẽ chia tập dữ liệu thành 2 phần là train và test. Ở đây, chúng ta sẽ set tỷ lệ train/test là 20%.\n2. Xây dựng mô hình Content-Based Filtering Cách tiếp cận đầu tiên, chúng ta sẽ tìm những project gần giống với những project mà donor đã donated. Đơn giản nhất là với mỗi project, chúng ta sẽ định nghĩa các vector đặc trưng của chúng và đo độ giống nhau giữa hai vector đó. Vector đặc trưng chúng ta có thể xây dựng trên các thuộc tính như project type, project catefory, grade level, resource category, cost, school zip code, \u0026hellip; hoặc các bạn có thể từ các vector cơ bản do tập dữ liệu cung cấp bổ sung thêm các vector cấp cao hơn, ví dụ như là rút trích các feature từ tên project hoặc mô tả của project, loại bỏ stopwords \u0026hellip;\nỞ đây, chúng ta sẽ sử dụng kỹ thuật TF-IDF để rút trích thông tin đặc trưng của dự án dựa trên project tittle và description. Về TF-IDF, các bạn có thể đọc ở một bài viết nào đó của google, mình không tiện nhắc đến nó chi tiết ở bài viết này.\na. Xây dựng tập đặc trưng 1 2# Preprocessing of text data 3textfeats = [\u0026#34;Project Title\u0026#34;,\u0026#34;Project Essay\u0026#34;] 4for cols in textfeats: 5 projects[cols] = projects[cols].astype(str) 6 projects[cols] = projects[cols].astype(str).fillna(\u0026#39;\u0026#39;) # FILL NA 7 projects[cols] = projects[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently 8 9text = projects[\u0026#34;Project Title\u0026#34;] + \u0026#39; \u0026#39; + projects[\u0026#34;Project Essay\u0026#34;] 10vectorizer = TfidfVectorizer(strip_accents=\u0026#39;unicode\u0026#39;, 11 analyzer=\u0026#39;word\u0026#39;, 12 lowercase=True, # Convert all uppercase to lowercase 13 stop_words=\u0026#39;english\u0026#39;, # Remove commonly found english words (\u0026#39;it\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;the\u0026#39;) which do not typically contain much signal 14 max_df = 0.9, # Only consider words that appear in fewer than max_df percent of all documents 15 # max_features=5000 # Maximum features to be extracted 16 ) 17project_ids = projects[\u0026#39;Project ID\u0026#39;].tolist() 18tfidf_matrix = vectorizer.fit_transform(text) 19tfidf_feature_names = vectorizer.get_feature_names() 20 21 22# build profile 23 24def get_project_profile(project_id): 25 idx = project_ids.index(project_id) 26 project_profile = tfidf_matrix[idx:idx+1] 27 return project_profile 28 29def get_project_profiles(ids): 30 project_profiles_list = [get_project_profile(x) for x in np.ravel([ids])] 31 project_profiles = scipy.sparse.vstack(project_profiles_list) 32 return project_profiles 33 34def build_donors_profile(donor_id, donations_indexed_df): 35 donations_donor_df = donations_indexed_df.loc[donor_id] 36 donor_project_profiles = get_project_profiles(donations_donor_df[\u0026#39;Project ID\u0026#39;]) 37 donor_project_strengths = np.array(donations_donor_df[\u0026#39;eventStrength\u0026#39;]).reshape(-1,1) 38 #Weighted average of project profiles by the donations strength 39 donor_project_strengths_weighted_avg = np.sum(donor_project_profiles.multiply(donor_project_strengths), axis=0) / (np.sum(donor_project_strengths)+1) 40 donor_profile_norm = sklearn.preprocessing.normalize(donor_project_strengths_weighted_avg) 41 return donor_profile_norm 42 43from tqdm import tqdm 44 45def build_donors_profiles(): 46 donations_indexed_df = donations_full_df[donations_full_df[\u0026#39;Project ID\u0026#39;].isin(projects[\u0026#39;Project ID\u0026#39;])].set_index(\u0026#39;Donor ID\u0026#39;) 47 donor_profiles = {} 48 for donor_id in tqdm(donations_indexed_df.index.unique()): 49 donor_profiles[donor_id] = build_donors_profile(donor_id, donations_indexed_df) 50 return donor_profiles 51 52donor_profiles = build_donors_profiles() 53print(\u0026#34;# of donors with profiles: %d\u0026#34; % len(donor_profiles)) 54 55mydonor1 = \u0026#34;6d5b22d39e68c656071a842732c63a0c\u0026#34; 56mydonor2 = \u0026#34;0016b23800f7ea46424b3254f016007a\u0026#34; 57mydonor1_profile = pd.DataFrame(sorted(zip(tfidf_feature_names, 58 donor_profiles[mydonor1].flatten().tolist()), 59 key=lambda x: -x[1])[:10], 60 columns=[\u0026#39;token\u0026#39;, \u0026#39;relevance\u0026#39;]) 61mydonor2_profile = pd.DataFrame(sorted(zip(tfidf_feature_names, 62 donor_profiles[mydonor2].flatten().tolist()), 63 key=lambda x: -x[1])[:10], 64 columns=[\u0026#39;token\u0026#39;, \u0026#39;relevance\u0026#39;]) 65 66print(\u0026#39;feature of user \u0026#39; + str(mydonor1)) 67print(mydonor1_profile) 68 69print(\u0026#39;feature of user \u0026#39; + str(mydonor2)) 70print(mydonor2_profile) Mã nguồn ở trên cũng có chú thích đầy đủ, và đọc cũng dễ hiểu, nên mình không nói thêm gì nhiều. Mình tóm gọn một chút là chúng ta sẽ convert toàn bộ project tittle và description về dạng chữ thường, tách từ dựa vào khoảng trắng, loại bỏ những english stopwords. Sau đó xây dựng profile cho từng donor.\nKết quả\n1feature of user 6d5b22d39e68c656071a842732c63a0c 2 token relevance 30 music 0.450057 41 auditorium 0.355256 52 cart 0.272809 63 chair 0.223861 74 equipment 0.211338 85 musicians 0.179244 96 time 0.172908 107 moving 0.137749 118 ohms 0.134065 129 prepare 0.131274 13feature of user 0016b23800f7ea46424b3254f016007a 14 token relevance 150 pollinators 0.670222 161 plants 0.305398 172 module 0.223407 183 pollination 0.211870 194 seeds 0.180609 205 writing 0.166816 216 books 0.137455 227 reading 0.115003 238 weaved 0.111704 249 bees 0.101842 Nhìn kết quả trên, ta thấy rằng donor 1 có vẻ thích những thứ liên quan đến âm nhạc (music, auditorim), trong khi đó donor 2 thích những thứ liên quan đến trồng trọt (pollinators - thụ phấn, plants - cây cối)\nb. Xây dựng mô hình Việc xây dựng mô hình đến đây là khá đơn giản. Chúng ta chỉ việc tính khoảng cách cosin giữa vector cần dự đoán và toàn bộ vector có trong tập train rồi show top K prject có liên quan cao nhất\n1 2 3class ContentBasedRecommender: 4 5 MODEL_NAME = \u0026#39;Content-Based\u0026#39; 6 7 def __init__(self, projects_df=None): 8 self.project_ids = project_ids 9 self.projects_df = projects_df 10 11 def get_model_name(self): 12 return self.MODEL_NAME 13 14 def _get_similar_projects_to_donor_profile(self, donor_id, topn=1000): 15 #Computes the cosine similarity between the donor profile and all project profiles 16 cosine_similarities = cosine_similarity(donor_profiles[donor_id], tfidf_matrix) 17 #Gets the top similar projects 18 similar_indices = cosine_similarities.argsort().flatten()[-topn:] 19 #Sort the similar projects by similarity 20 similar_projects = sorted([(project_ids[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1]) 21 return similar_projects 22 23 def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10, verbose=False): 24 similar_projects = self._get_similar_projects_to_donor_profile(donor_id) 25 #Ignores projects the donor has already donated 26 similar_projects_filtered = list(filter(lambda x: x[0] not in projects_to_ignore, similar_projects)) 27 28 recommendations_df = pd.DataFrame(similar_projects_filtered, columns=[\u0026#39;Project ID\u0026#39;, \u0026#39;recStrength\u0026#39;]).head(topn) 29 30 recommendations_df = recommendations_df.merge(self.projects_df, how = \u0026#39;left\u0026#39;, 31 left_on = \u0026#39;Project ID\u0026#39;, 32 right_on = \u0026#39;Project ID\u0026#39;)[[\u0026#39;recStrength\u0026#39;, \u0026#39;Project ID\u0026#39;, \u0026#39;Project Title\u0026#39;, \u0026#39;Project Essay\u0026#39;]] 33 34 35 return recommendations_df 36 37 38cbr_model = ContentBasedRecommender(projects) 39 40 41print(\u0026#39;recommend for user \u0026#39; + str(mydonor1)) 42print(cbr_model.recommend_projects(mydonor1)) 43 44print(\u0026#39;recommend for user \u0026#39; + str(mydonor2)) 45print(cbr_model.recommend_projects(mydonor2)) Kết quả\n1recommend for user 6d5b22d39e68c656071a842732c63a0c 2 recStrength ... Project Essay 30 1.000000 ... the music students in our classes perform freq... 41 0.390997 ... i have spent 12 years as an educator rebuildin... 52 0.338676 ... \u0026#34;music is what feelings sound like.\u0026#34; -g. cates... 63 0.331034 ... true music is created not by the teacher but b... 74 0.324355 ... every morning my first grade students come to ... 85 0.322923 ... in today\u0026#39;s fast paced environment, students ne... 96 0.315910 ... \u0026#34;music is a moral law. it gives soul to the u... 107 0.314845 ... i walk in the door so excited to get the stude... 118 0.310103 ... some students have never put their hands on a ... 129 0.297516 ... my students do not have money, but they do hav... 13 14[10 rows x 4 columns] 15recommend for user 0016b23800f7ea46424b3254f016007a 16 recStrength ... Project Essay 170 1.000000 ... my students are creative, curious, and excited... 181 0.211962 ... our school is a title 1 school. 100% of stude... 192 0.189111 ... my students are active and eager learners who ... 203 0.188095 ... being a small rural school we do a lot of trad... 214 0.173520 ... \u0026#34;science is a way of life...science is the pro... 225 0.159015 ... my second grade students love to come to schoo... 236 0.158071 ... i teach 28 fourth graders in a neighborhood sc... 247 0.150389 ... in my classroom we are working hard to become ... 258 0.144724 ... as a teacher in a diverse, low-income, high-po... 269 0.139937 ... have you ever been told you need to read, but ... 27 28[10 rows x 4 columns] Mình dùng cmd nên bị giới hạn kết quả, các bạn có thể write log vào file hoặc dùng jupiter để show kết quả rõ hơn.\nỞ đây, chúng ta nhận thấy rằng các recommend cho donor 1 thường là những project liên quan tới âm nhạc (nhìn tập feature ta cũng có thể đoán được). Và recommend cho donor 2 là những thứ liên quan đến chủ đề làm vườn và reading.\n3. Collaborative Filtering Model Lý thuyết về Collaborative Filtering Model các bạn có thể xem ở các bài viết khác của mình hoặc tham khảo thêm trên mạng. Ở đây, mình sẽ sử dụng Singular Value Decomposition (SVD) để xây dựng ma trận đặc trưng.\na. Xây dựng ma trận donor - project Đầu tiên, chúng ta sẽ xây dựng ma trận mối quan hệ giữa donor và project. Nếu donor i có donated cho 1 project j thì dòng i cột j của ma trận sẽ được đánh dấu là 1, ngược lại là 0.\n1## create matrix 2#Creating a sparse pivot table with donors in rows and projects in columns 3donors_projects_pivot_matrix_df = donations_full_df.pivot(index=\u0026#39;Donor ID\u0026#39;, 4 columns=\u0026#39;Project ID\u0026#39;, 5 values=\u0026#39;eventStrength\u0026#39;).fillna(0) 6 7# Transform the donor-project dataframe into a matrix 8donors_projects_pivot_matrix = donors_projects_pivot_matrix_df.as_matrix() 9 10# Get donor ids 11donors_ids = list(donors_projects_pivot_matrix_df.index) 12 13print(donors_projects_pivot_matrix[:5]) # print first 5 row 1 2array([[ 0., 0., 0., ..., 0., 0., 0.], 3 [ 0., 0., 0., ..., 0., 0., 0.], 4 [ 0., 0., 0., ..., 0., 0., 0.], 5 [ 0., 0., 0., ..., 0., 0., 0.], 6 [ 0., 0., 0., ..., 0., 0., 0.]]) b. Singular Value Decomposition Sau khi có ma trận trên, ta có một nhận xét rằng nó rất thưa, số lượng 0 thì nhiều mà 1 thì ít. Sau khi áp dụng SVD, ma trận kết quả sẽ ít thưa hơn (có thể đạt được đến mức không còn thưa nữa).\n1# Performs matrix factorization of the original donor-project matrix 2# Here we set k = 20, which is the number of factors we are going to get 3# In the definition of SVD, an original matrix A is approxmated as a product A ≈ UΣV 4# where U and V have orthonormal columns, and Σ is non-negative diagonal. 5U, sigma, Vt = svds(donors_projects_pivot_matrix, k = 20) 6sigma = np.diag(sigma) 7 8# Reconstruct the matrix by multiplying its factors 9all_donor_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 10 11#Converting the reconstructed matrix back to a Pandas dataframe 12cf_preds_df = pd.DataFrame(all_donor_predicted_ratings, 13 columns = donors_projects_pivot_matrix_df.columns, 14 index=donors_ids).transpose() 15 16print(cf_preds_df.head()) 1 0003aba06ccf49f8c44fc2dd3b582411 ... ffff088c35d3455779a30898d1327b76 2Project ID ... 3 4000009891526c0ade7180f8423792063 -3.423182e-34 ...-4.577244e-34 500000ce845c00cbf0686c992fc369df4 -3.061322e-36 ...-6.492305e-36 600002d44003ed46b066607c5455a999a 1.368936e-33 ...-2.239156e-32 700002eb25d60a09c318efbd0797bffb5 1.784576e-33 ...1.163684e-32 80000300773fe015f870914b42528541b 4.314216e-34 ...-4.666110e-34 9 10[5 rows x 8015 columns] c. Xây dựng Collaborative Filtering Model 1 2 3class CFRecommender: 4 5 MODEL_NAME = \u0026#39;Collaborative Filtering\u0026#39; 6 7 def __init__(self, cf_predictions_df, projects_df=None): 8 self.cf_predictions_df = cf_predictions_df 9 self.projects_df = projects_df 10 11 def get_model_name(self): 12 return self.MODEL_NAME 13 14 def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10): 15 # Get and sort the donor\u0026#39;s predictions 16 sorted_donor_predictions = self.cf_predictions_df[donor_id].sort_values(ascending=False) \\ 17 .reset_index().rename(columns={donor_id: \u0026#39;recStrength\u0026#39;}) 18 19 # Recommend the highest predicted projects that the donor hasn\u0026#39;t donated to 20 recommendations_df = sorted_donor_predictions[~sorted_donor_predictions[\u0026#39;Project ID\u0026#39;].isin(projects_to_ignore)] \\ 21 .sort_values(\u0026#39;recStrength\u0026#39;, ascending = False) \\ 22 .head(topn) 23 24 25 recommendations_df = recommendations_df.merge(self.projects_df, how = \u0026#39;left\u0026#39;, 26 left_on = \u0026#39;Project ID\u0026#39;, 27 right_on = \u0026#39;Project ID\u0026#39;)[[\u0026#39;recStrength\u0026#39;, \u0026#39;Project ID\u0026#39;, \u0026#39;Project Title\u0026#39;, \u0026#39;Project Essay\u0026#39;]] 28 29 30 return recommendations_df 31 32cfr_model = CFRecommender(cf_preds_df, projects) 33print(cfr_model.recommend_projects(mydonor1)) 34 35print(cfr_model.recommend_projects(mydonor2)) 1[5 rows x 8015 columns] 2 recStrength ... Project Essay 30 3.015461e-17 ... Our students are some of the hardest working k... 41 2.237275e-17 ... As Service Learning Coordinators at our elemen... 52 2.188501e-17 ... We are trying to engage more students in scien... 63 1.768711e-17 ... We are a brand new charter school that has onl... 74 1.344489e-17 ... Sitting at a desk for a sustained period of ti... 85 9.957278e-18 ... Our students come from a Title I school in Jer... 96 6.932330e-18 ... In my school 50% of the students are socioecon... 107 8.589640e-19 ... Have you ever been told you need to read, but ... 118 6.698040e-19 ... \u0026#34;I cannot say good-bye to those whom I have gr... 129 5.733941e-19 ... I have students in class who are squinting and... 13 14[10 rows x 4 columns] 15 recStrength ... Project Essay 160 3.015461e-17 ... Our students are some of the hardest working k... 171 2.237275e-17 ... As Service Learning Coordinators at our elemen... 182 2.188501e-17 ... We are trying to engage more students in scien... 193 1.768711e-17 ... We are a brand new charter school that has onl... 204 1.344489e-17 ... Sitting at a desk for a sustained period of ti... 215 9.957278e-18 ... Our students come from a Title I school in Jer... 226 6.932330e-18 ... In my school 50% of the students are socioecon... 237 8.589640e-19 ... Have you ever been told you need to read, but ... 248 6.698040e-19 ... \u0026#34;I cannot say good-bye to those whom I have gr... 259 5.733941e-19 ... I have students in class who are squinting and... Kết quả trả về có vẻ không được đẹp như ở phương pháp trên. Ở đây, thuật toán dựa vào hành vi donated của những người khác có điểm tương đồng với user donor 1 và 2. Bởi vậy gợi ý những project sẽ khác những gợi ý ở phương pháp 1.\n4. Hybrid Method Phương pháp lai này kết hợp cả 2 hướng tiếp cận của hai phương pháp ở trên. Ở đây, chúng ta sẽ xây dựng một mô hình nhỏ, nhân điểm của content based và collaborative filtering lại với nhau, sau đó xếp hạng để được điểm hybrid. Đây là 1 cách đơn giản, các bạn có thể tìm đọc nhiều cách tiếp cận khác và ứng dụng vào bài toán.\n1class HybridRecommender: 2 3 MODEL_NAME = \u0026#39;Hybrid\u0026#39; 4 5 def __init__(self, cb_rec_model, cf_rec_model, projects_df): 6 self.cb_rec_model = cb_rec_model 7 self.cf_rec_model = cf_rec_model 8 self.projects_df = projects_df 9 10 def get_model_name(self): 11 return self.MODEL_NAME 12 13 def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10): 14 #Getting the top-1000 Content-based filtering recommendations 15 cb_recs_df = self.cb_rec_model.recommend_projects(donor_id, projects_to_ignore=projects_to_ignore, 16 topn=1000).rename(columns={\u0026#39;recStrength\u0026#39;: \u0026#39;recStrengthCB\u0026#39;}) 17 18 #Getting the top-1000 Collaborative filtering recommendations 19 cf_recs_df = self.cf_rec_model.recommend_projects(donor_id, projects_to_ignore=projects_to_ignore, 20 topn=1000).rename(columns={\u0026#39;recStrength\u0026#39;: \u0026#39;recStrengthCF\u0026#39;}) 21 22 #Combining the results by Project ID 23 recs_df = cb_recs_df.merge(cf_recs_df, 24 how = \u0026#39;inner\u0026#39;, 25 left_on = \u0026#39;Project ID\u0026#39;, 26 right_on = \u0026#39;Project ID\u0026#39;) 27 28 #Computing a hybrid recommendation score based on CF and CB scores 29 recs_df[\u0026#39;recStrengthHybrid\u0026#39;] = recs_df[\u0026#39;recStrengthCB\u0026#39;] * recs_df[\u0026#39;recStrengthCF\u0026#39;] 30 31 #Sorting recommendations by hybrid score 32 recommendations_df = recs_df.sort_values(\u0026#39;recStrengthHybrid\u0026#39;, ascending=False).head(topn) 33 34 recommendations_df = recommendations_df.merge(self.projects_df, how = \u0026#39;left\u0026#39;, 35 left_on = \u0026#39;Project ID\u0026#39;, 36 right_on = \u0026#39;Project ID\u0026#39;)[[\u0026#39;recStrengthHybrid\u0026#39;, 37 \u0026#39;Project ID\u0026#39;, \u0026#39;Project Title\u0026#39;, 38 \u0026#39;Project Essay\u0026#39;]] 39 40 41 return recommendations_df 42 43hybrid_model = HybridRecommender(cbr_model, cfr_model, projects) 44 45 46print(hybrid_model.recommend_projects(mydonor1)) 47 48print(hybrid_model.recommend_projects(mydonor2)) 1 recStrengthHybrid ... Project Essay 20 1.574375e-18 ... we are trying to engage more students in scien... 31 1.221807e-18 ... in my school 50% of the students are socioecon... 42 1.214293e-18 ... our students are some of the hardest working k... 53 4.037232e-19 ... sitting at a desk for a sustained period of ti... 64 6.661794e-20 ... “music expresses that which cannot be put into... 75 4.872264e-20 ... i walk in the door so excited to get the stude... 86 4.410098e-20 ... i have spent 12 years as an educator rebuildin... 97 2.907349e-20 ... \u0026#34;music is what feelings sound like.\u0026#34; -g. cates... 108 2.121616e-20 ... \u0026#34;i cannot say good-bye to those whom i have gr... 119 1.353927e-20 ... our band program is one of the largest in our ... 12 13[10 rows x 4 columns] 14 recStrengthHybrid ... Project Essay 150 2.811124e-18 ... in this modern, digital age, i would like to u... 161 1.249967e-18 ... we are a brand new charter school that has onl... 172 6.055628e-19 ... my students are african american and hispanic.... 183 5.912367e-19 ... the a. community and its students are a very s... 194 2.541749e-19 ... do you want to go on an adventure and learn ab... 205 2.494812e-19 ... the average day in my class involves students ... 216 2.323313e-19 ... i teach ela (reading component) to self-contai... 227 1.271629e-19 ... hi there! do you want to help to instill a lif... 238 1.044990e-19 ... having writing utensils is essential for stude... 249 1.004780e-19 ... there\u0026#39;s no such thing as a kid who hates readi... 25 26[10 rows x 4 columns] Kết quả trả ra tốt hơn nhiều so với cách 2, donor1 có music, donor2 có cây trồng và sách.\n5. Đánh giá mô hình Có rất nhiều cách khác nhau để đánh giá mô hình recommend system. Một trong các cách mình sử dụng ở đây là sử dụng độ đo top K accuracy. Độ đo này được tính như sau:\nVới mỗi user: Với mỗi item user đã pick trong test set Lấy mẫu 1000 item khác mà người dùng chưa bao giờ pick\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo. Cố lên.\n","date":"Dec 11, 2018","img":"","permalink":"/blog/2019-01-03-donor-project-matching-with-recommender-systems/","series":null,"tags":["Machine learning","Deeplearning","recommender system"],"title":"Hệ Thống Gợi Ý Khoá Học Cho Website DonorChoose.org"},{"categories":null,"content":" Lời mở đầu Kiểm tra dữ liệu Lời mở đầu Việc huấn luyện một mô hình neural network khá đơn giản, chỉ việc download code mẫu về, quăng tập data của mình vào, rồi cho chạy, xong. Nhưng khó khăn ở đây là làm cách nào để nâng độ chính xác của mô hình lên. Ở bài viết này, chúng ta sẽ tìm hiểu một số cách giúp tăng độ chính xác của mô hình.\nKiểm tra dữ liệu Thực chất, chúng ta phải hiểu rõ kỹ chúng ta đang có những gì trong tay, thì chúng ta mới dạy cho máy học đủ và đúng được. Các bạn hãy kiểm tra thật kỹ để đảm bảo rằng tập nhãn được gán chính xác, bouding box của đối tượng được vẽ không quá dư thừa, không có missing value, v.v. Một ví dụ nhỏ là tập MNIST, có nhiều hình bị nhập nhằng giữa những con số, chúng ta không thể phân biệt được chính xác hình đó là con số nào bằng mắt thường.\nTiếp theo, các bạn hãy quyết định xem rằng mình có nên sử dụng các pre-train model hay không.\nNếu tập dữ liệu của bạn gần giống với tập dữ liệu ImageNet, hãy dùng pre-train model. Có các mô hình đã được huấn luyện sẵn là VGG net, ResNet, DenseNet, Xception. Với các kiến trúc khác nhau như VGG(16 và 19 layer), ResNet (50, 101, 152 layer), DenseNet(201,169,121 layer). Ban đầu, đừng sử dụng các kiến trúc có số lượng nhiều (ResNet152, DenseNet201) bởi vì nó rất tốn chi phí tính toán. Chúng ta nên bắt đầu bởi các mô hình nhỏ như VGG16, ResNet50. Hãy chọn một mô hình mà bạn nghĩ là sẽ có kết quả tốt. Sau khi huấn luyện, nếu kết quả không được như ý muốn, hãy tăng số lớp lên (ví dụ ban đầu chọn Resnet50, sau đó nâng lên Resnet101, \u0026hellip;).\nNếu bạn có ít dữ liệu, bạn nãy \u0026ldquo;đóng băng\u0026rdquo; lại trọng số của pre-train model, chỉ huấn luyện phần phân lớp. Bạn cũng có thể thêm phần Dropout để tránh overfit.\nNếu tập dữ liệu của bạn không giống một tí nào so với taapk ImageNet, không nên dùng pre-train model.\nLuôn luôn sử dụng lớp chuẩn hoá trong mô hình. Nếu bạn huấn luyện mô hình với batch-size lớn ( ví dụ lớn hơn 10), hãy sử dụng BatchNormalization Layer trong keras. Nếu bạn sử dụng batch-size nhỏ (ví dụ 1), thì hãy sử dụng InstanceNormalization. Hai layer này đã có sẵn trong Keras, trong các framework khác thì mình không rõ lắm. Có nhiều tác giả đã chỉ ra rằng sử dụng BatchNormalization sẽ cho kết quả tốt hơn nếu tăng batch-size và hiệu năng sẽ giảm khi batch-size nhỏ, và trong trường hợp batch-size nhỏ thì kết quả sẽ tốt hơn một tí khi sử dụng InstanceNormalization thay cho BatchNormalization. Ngoài ra, các bạn cũng có thể sử dụng GroupNormalization (mình chưa kiểm chứng GroupNormalization có làm tăng độ chính xác hay không).\nNếu bạn sử dụng concatenation layer để kết hợp các feature từ nhiều convolution layers (Li), và những Li trên rút trích thông tin từ cùng một input (F), thì bạn jay sử dụng SpatialDropout ngay sau concatenation layer trên (Xem hình bên dưới). Khi các convolution layer rút trích thông tin từ cùng một nguồn, các đặc trưng của chúng thường sẽ có mức tương quan với nhau rất lớn. SpatialDropout sẽ loại bỏ những đặc trưng có mức độ liên quan cao này và giúp bạn chống lại hiện tượng overfiting. Thông thường người ta chỉ sử dụng SpatialDropout ở các lớp gần input layer, và không sử dụng chúng ở các lớp cao bên trên.\nTheo andrej Karpathy, để xác định khả năng lưu trữ thông tin của mô hình, hãy rút một phần nhỏ dữ liệu trong tập train của bạn đem đi huấn luyện. Nếu mô hình không overfit, chúng ta tăng số lượng node/layer lên. Nếu mô hình bị overfit, sử dụng các kỹ thuật như L1, L2, Dropout hoăc các kỹ thuật khác để chống lại việc overfit.\nCác kỹ thuật chuẩn hoá thường sẽ ràng buộc hoặc tinh gọn các trọng số của mô hình. Nó cũng đồng thời giúp chúng ta chống lại việc gradient explosion (gradient mang giá trị lớn khi tính backpropagation) (lý do là các trọng số sẽ bị giới hạn trong đoạn nào đó, ví dụ L2 giới hạn căn bậc 2 tổng bình phương các trọng số =1 chẳng hạn). Ví dụ dưới sử dụng kares và giới hạn max của L2 là 2.\n1from keras.constraints import max_norm 2# add to Dense layers 3model.add(Dense(64, kernel_constraint=max_norm(2.))) 4# or add to Conv layers 5model.add(Conv2D(64, kernel_constraint=max_norm(2.))) Việc sử dụng mean subtraction đôi khi cho kết quả khá tệ, đặc biệt là khi sử dụng trong ảnh xám (grayscale image), hoặc các bài toán phân đoạn ảnh.\nLuôn nhớ đến việc xáo trộn dữ liệu (nếu bạn có thể). Nếu được, hãy thực hiện xáo trộn dữ liệu trong quá trình huấn luyện. Việc xáo trộn ảnh sẽ giúp bạn cải thiện độ chính xác.\nNếu bài toán của bạn thuộc nhóm dense prediction (ví dụ phân đoạn ngữ nghĩa - semantic segmentation). Hãy sử dụng pre-train model là Dilated Residual Networks. Mô hình trên cực kỳ hiệu quả cho bài toán này.\nĐể xác định thông tin ngữ cảnh xung quanh các đối tượng, hãy sử dụng module multi-scale feature pooling. Module này sẽ giúp bạn tăng độ chính xác và thường được sử dụng trong bài toán phân đoạn ngữ nghĩa (semantic segmentation) hoặc bài toán phân đoạn nền (foreground segmentation).\nKhi bạn tính độ lỗi hoặc độ chính xác, nếu có vùng nào không trả về nhãn, hoặc nhãn trả về không chắc chắn, hãy bỏ qua việc tính toán chúng đi. Hành động này sẽ giúp mô hình của bạn chắc chắn hơn khi đưa ra quyết định.\nSử dụng trọng số cho từng class trong quá trình training nếu dữ liệu của bạn có tính bất cân bằng cao. Hãy đặt trọng số lớn cho những lớp có ít dữ liệu, và trọng số nhỏ cho những lớp có nhiều dữ liệu. Trọng số của các lớp có thể được tính toán một cách dễ dàng bằng các sử dụng thư viện skearn trong python. Ngoài ra, bạn có thể sử dụng các kỹ thuật như OverSampling hoặc UnderSampling đối với tập dữ liệu nhỏ.\nChọn đúng hàm tối ưu. Có rất nhiều hàm tối ưu như Adam, Adagrad, Adadellta, RMSprop, \u0026hellip; Trong các paper người ta thường sử dụng tổ hợp SGD + momentun. Có hai vấn đề cần được xem xét ở đây: Một là nếu bạn muốn mô hình có độ hội tụ nhanh, hãy dùng Adam ( và có khả năng cao là mô hình sẽ bị kẹt ở điểm cực tiểu cục bộ -\u0026gt; không có tính tổng quát hoá cao). Hai là sử dujg SGD + momentun để tìm cực tiểu toàn cục, mô hình này phụ thuộc rất nhiều vào giá trị khởi tạo ban đầu và mô hình thường sẽ hội tụ rất chậm. (Xem hình bên dưới)\nThông thường, chúng ta sẽ chọn learning-rate là (1e-1, 1e-3, 1e-6). Nếu bạn sử dụng pre-train model, hãy sử dụng learning rate nhỏ hơn 1e-3 (ví dụ 1e-4). Nếu bạn không sử dụng pre-train model, hãy sử dụng learning-rate lớn hơn 1e-3. Bạn có thể grid search giá trị learning-rate và chọn ra mô hình cho kết quả tốt nhất. Bạn có thể sử dụng Learing Rate Schedulers giảm giá trịn learning rate trong quá trình huấn luyện mô hình.\nBên cạnh việc sử dụng Learing Rate Schedulers để giảm giá trị learning rate, bạn có thể sử dụng một kỹ thuật khác để giảm giá trị learning-rate. Ví dụ sau 5 epochs, độ lỗi trên tập validation không thay đổi, bạn giảm learning-rate đi 10 lần (vd từ 1e-3 thành 1e-4). Trong keras, bạn có thể dễ dàng implement công thức trên bằng việc sử dụng callbacs ReduceLROnPlateau.\n1reduce = keras.callbacks.ReduceLROnPlateau(monitor=\u0026#39;val_loss\u0026#39;, factor=0.1, patience=5, mode=\u0026#39;auto\u0026#39;) 2early = keras.callbacks.EarlyStopping(monitor=\u0026#39;val_loss\u0026#39;, min_delta=1e-4, patience=10, mode=\u0026#39;auto\u0026#39;) 3model.fit(X, Y, callbacks=[reduce, early]) Ví dụ trên, chúng ta sẽ giảm learning-rate đi 10 lần khi độ lỗi trên tập validation không thay đổi qua 5 lần lặp liên tiếp, và sẽ dừng việc huấn luyện khi độ lỗi không giảm qua 10 lần lặp liên tiếp.\nNếu bài toán của bạn thuộc nhóm dense prediction như phân đoạn ảnh, phân đoạn ngữ nghĩa, bạn nên sử dụng skip connection để chống lại việc các biên của đối tượng hoặc các thông tin đặc trưng hữu ích của đối tượng bị mất trong max-pooling hoặc strided convolution. Skip connection cũng giúp mô hình học features map từ feature space và image space dễ dàng hơn, và nó cũng giúp cho bạn giảm bị vanish gradient ( giá trị gradient nhỏ dần và gần xấp xỉ bằng 0, nên trọng số không thay đổi nhiều, dẫn đến không hội tụ).\nNên sử dụng data augmentation, như là horizontally flipping, rotating, zoom-croping\u0026hellip; để tăng dữ liệu của bạn lên. Việc có nhiều dữ liệu sẽ giúp mô hình có mức tổng quát hoá cao hơn.\nSử dụng Max-pooling trước Relu để giảm thiểu mức độ tính toán thay vì làm ngược lại. chúng ta biết rằng ReLU trả ra giá trị có ngưỡng cực tiểu là 0 do f(x)=max(0,x), và max-pooling tính max cho các đặc trưng f(x) = max(x1,x2,\u0026hellip;,xi). Nếu ta sử dụng Conv \u0026gt; ReLU \u0026gt; Max-pooling, ta sẽ tốn i lần tính ReLu, và 1 lần tính max. Nếu ta sử dụng Conv -\u0026gt; max-pooling \u0026gt; ReLU, ta tốn 1 lần tính max, 1 lần tính ReLU.\nNếu có thể, hãy thử sử dụng Depthwise Separable Convolution. Nó giúp mô hình giảm số lượng tham số so với các convolution khác, ngoài ra nó giúp mô hình chạy nhanh hơn.\nĐiều cuối cùng là đừng bao giờ từ bỏ. Hãy tin tưởng rằng bạn có thể làm được. Nếu bạn vẫn không thể đạt được độ chính xác như mong đợi, hãy điều chỉnh lại các tham số, kiến trúc mô hình, tập dữ liệu huấn luyện đến khi bạn đạt được mô hình với độ chính xác như bạn đề ra.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo. Cố lên.\n","date":"Dec 11, 2018","img":"","permalink":"/blog/2018-12-11-a-bunch-of-tips-and-tricks-for-training-deep-neural-networks/","series":null,"tags":["Machine learning","Deeplearning","object detector","single shot"],"title":"Một Số Mẹo Để Lựa Chọn Mô Hình Object Detection"},{"categories":null,"content":" Lời mở đầu Box encoding và loss function Feature extraction Feature extractor accuracy Non-max suppression (nms) Data augmentation Feature map strides Speed v.s. accuracy Object size Input image resolution Number of proposals Điểm danh danh lại các bước phát triển của object detection Lesson learned Lời mở đầu Các thuật toán phát hiện đối tượng, như các thuật toán thuộc nhóm region proposal hoặc single shot đầu bắt đầu bởi những ý tưởng khác nhau, nhưng sau qua một vài quá trình cập nhật và nâng cấp cho đến thời điểm hiện tại, mô hình chung của chúng đã gần gần giống nhau hơn. Và hai thuật toán trên là hai thuật toán tiêu biểu cạnh tranh nhau danh hiệu thuật toán phát hiện đối tượng nhanh nhất và thuật toán nhận diện chính xác nhất. Trong bài viết này, chúng ta sẽ đề cập đến một số chiến lược lựa chọn mô hình cho bài toán object detector và một số benchmarks do team Google Research thực hiện.\nBox encoding và loss function Có rất nhiều hàm lỗi và box encoding được sử dụng trong các thuật toán phát hiện đối tượng. Ví dụ, SSD trả ra căn bậc hai của Width và height để giảm tỷ lệ độ lỗi.\nCác bạn có thể để ý kỹ hơn SSD phiên bản custom không sử dụng cặp toạ độ trái trên - phải dưới mà là cặp tâm - căn bậc hai của with, căn bậc hai của height. Một số thuật toán lại dùng log width, log height, một số lại dùng tâm là Wc/Wa, Wy/ha, với Wc và Wy là toạ độ tâm của đối tượng, wa và ha là chiều dài và rộng của anchor khớp nhất (matching anchor). Các bạn có thể tham khảo thêm ở https://arxiv.org/pdf/1611.10012.pdf.\nĐể huấn luyện mô hình tốt hơn, Các nhà nghiên cứu sử dụng các trọng số khác nhau cho các hàm lỗi, YOLO và một ví dụ minh hoạ.\nFeature extraction Trong thực tế, Feature extraction ảnh hưởng lớn trên 2 phần tradeoff là độ chính xác và tốc độ. Nhóm thuật toán ResNet và Inception đi theo tiêu chí là độ chính xác quan trọng hơn tốc độ (và quả thật nhóm thuật toán thuộc họ này có độ chính xác khá cao). MobileNet cung cấp cho chúng ta một mô hình khá nhỏ gọn, sử dụng SSD, mục tiêu của nhóm này là có thể xử lý được trên các thiết bị di động và thời gian xử lý là realtime.\nFeature extractor accuracy Nhìn vào hình trên, chúng ta có thể thấy rõ ràng rằng Faster R-CNN và R-FCN đều cho độ chính xác khá tốt trên feature extraction. Ngược lại SSD có kết quả khá tệ.\nNon-max suppression (nms) Sau khi thu được vị trí của các đối tượng, chúng ta sẽ merge lại các vị trí bị phát hiện trùng lắp. Các thuật toán thuộc nhóm single shot thường cho ra output overlap khá nhiều.\nData augmentation Ngày nay, hầu hết các thuật toán đều sử dụng Data augmentation. Việc augment data bằng cách cắt xét ảnh, quay ảnh một góc ngẫu nhiên nào đó, giúp cho tránh được overfit trong quá trình huấn luyện, do đó gián tiếp tăng độ chính xác của mô hình.\nFeature map strides Thuật toán thuộc nhóm single shot thường có tuỳ chọn layter feature map nào được sử dụng để nhận dạng đối tượng. Feature map có stride là 2 nếu chúng ta thực hiện giảm 2 lần độ phân giải. Feature map có độ phân giải thấp thường giữ lại những thông tin đặc trưng tốt của đối tượng và giúp cho detector thực hiện tốt hơn. Tuy nhiên, những đối tượng có kính thước nhỏ sẽ bị mất thông tin trầm trọng và khó để phát hiện ra chúng.\nSpeed v.s. accuracy Thật khó để trả lời rằng thuật toán nhận dạng đối tượng nào tốt hơn, mà câu trả lời phụ thuộc vào bài toán của bạn đang gặp. Nếu bài toán cần độ chính xác cao, hãy sử dụng ResNet hoặc Inception, nếu bạn cần chạy realtime và độ chính xác tạm chấp nhận, hãy sử dụng MobileNet hoặc YOLO. Không có (chưa có - ít nhất đến thời điểm hiện tại) có thuật toán nào đáp ứng cả 2 tiêu chí là vừa có độ chính xác cao, vừa chạy nhanh cả. Đó là một tradeoff giữa Speed và Accuracy.\nObject size Với những hình ảnh có kích thước lớn, SSD thực hiện rút trích đặc trưng rất tốt (nên nhớ rằng mô hình rút trích đặc trưng của SSD rất đơn giản). Với những hình ảnh dạng này, SSD có thể so sánh với các thuật toán khác khác về độ chính xác.\nVới nhưng hình ảnh có kích thước nhỏ, chúng ta không nên/không bao giờ xài SSD.\nNhìn hình ở trên, chúng ta thấy rõ độ chính xác của SSD và các thuật oán khác trên các tập dữ liệu có kích thước khác nhau. Và phụ thuộc vào kích thước dữ liệu của bạn để chọn ra mô hình tối ưu nhất.\nInput image resolution Nhìn hình trên các bạn cũng có thể nhìn thấy rõ. Ảnh có độ phân giải lớn giúp nhận dạng đối tượng tốt hơn rất nhiều so với ảnh có độ phân giải nhỏ. Khi giảm 2 lần độ phân giải trên mỗi chiều (từ 600x600 xuống còn 300x300), trung bình độ chính xác giảm 15.88% trong quá trình huấn luyện, và trung bình giảm 27.4% trong inference.\nNumber of proposals Số lượng proposal được sinh ra ảnh hưởng trực tiếp đến tốc độ của nhóm R-CNN. Ví dụ, Faster R-CNN có thể tăng tốc độ nhận dạng đối tượng gấp 3 lần nếu ta chỉ sử dụng 50 proposal thay vì 300 proposal. Độ chính xác chỉ giảm 4%\nHình trên, đường nét liền mô tả độ chính xác khi tăng số lượng proposal. Đường nét đứt thể hiện thời gian xử láy tăng khi tăng số lượng proposal.\nĐiểm danh danh lại các bước phát triển của object detection Các thuật toán object detection đã phát triển trong một khoảng thời gian dài. Ý tưởng đầu tiên, đơn giản nhất là chúng ta sẽ sử dụng cửa sổ trượt.\n1 2# Sliding windows 3for window in windows 4 patch = get_patch(image, window) 5 results = detector(patch) Để tăng tốc, chúng ta sẽ\nGiảm số lượng windows (R-CNN giảm còn khoảng 2000) Giảm các phép tính trong việc tìm ROI (Fast R-CNN sử dụng feature map thay vì toàn bộ image patchs). 1# Fast R-CNN 2feature_maps = process(image) 3ROIs = region_proposal(feature_maps) 4for ROI in ROIs 5 patch = roi_pooling(feature_maps, ROI) 6 results = detector2(patch) Việc tìm region_proposal cũng tốn khá nhiều thời gian. Faster R-CNN sử dụng một convolution network thay thế cho region proposal ở bước này (làm giảm thời gian từ 2.3s xuống còn 0.3 giây). Faster R-CNN cũng giới thiệu 1 khái nhiệm là anchor giúp cải thiện độ chính xác và việc huấn luyện trở nên dễ dàng hơn.\nR-FCN đưa ra một điều chỉnh nhỏ, là tiến hành tìm position và sensitive score map trên mỗi ROIS độc lập. Và tính trung bình xác suất xuất hiện đối tượng\n1# R-FCN 2feature_maps = process(image) 3ROIs = region_proposal(feature_maps) 4score_maps = compute_score_map(feature_maps) 5for ROI in ROIs 6 V = pool(score_maps, ROI) 7 class_scores = average(V) 8 class_probabilities = softmax(class_scores) R-FCN chạy khá nhanh, nhưng độ chính xác thì thấp hơn một hút so với Faster R-CNN. Để ý kỹ đoạn mã giả ở trên, chúng ta phải trải qua 2 lần tính toán, một lần là tìm các ROIs, một lần là object detection. Thuật toán Single shot detector được đề xuất để sử dụng 1 lần tính toán.\n1feature_maps = process(image) 2results = detector3(feature_maps) # No more separate step for ROIs Thuật toán SSD và YOLO đều thuộc nhóm single shot detectors. Cả hai đều sử dụng convolution layer để rút trích đặc trưng và một convolution filter để đưa quyết định. Cả hai đều dùng feature map có độ phân giải thấp (low resolution feature map) để dò tìm đối tượng =\u0026gt; chỉ phát hiện được các đối tượng có kích thước lớn. Một cách tiếp cận là sử dụng các feature map có độ phân giải cao (higher resolution feature map). Nhưng độ chính xác sẽ giảm do thông tin đặc trưng của đối tượng quá hỗn loạn. FPN đưa ra ý tưởng sử dụng feature map trung gian merge giữa feature map high resolution và low resolution. Việc này giúp cho chúng ta vẫn giữ được thông tin đặc trưng hữu ích của đối tượng, đồng thời cũng giữ được thông tin của các đối tượng có kích thước nhỏ. Do đó, độ chính xác cũng tăng lên và phát hiện các đối tượng có các tỷ lệ khác nhau (different scale) tốt hơn.\nTrong quá trình huấn luyện, chúng ta sẽ nhận ra 1 vấn đề rằng backgroup sẽ chiếm 1 phần rất lớn trong bức ảnh. Hoặc một đối tượng nào đó có số mẫu nhiều hơn so với các đối tượng khác. Thuật toán Focal loss được sinh ra để giải quyết vấn đề này.\nLesson learned Feature Pyramid Networks sử dụng các feature map nhiều thông tin hơn để cải thiện độ chính xác.\nSử dụng các mô hình như ResNet hoặc Inception ResNet nếu mô hình bạn cần độ chính xác và không quan tâm lắm về tốc độ.\nSử dụng các thuật toán thuộc nhóm Single shot detectors như MobileNet nếu bạn cần tốc độ tính toán và có thể chạy được trên mobilenet, yêu cầu về độ chính xác tạm chấp nhận được.\nSử dụng batch normaliation, nói chung là đều phải chuẩn hoá dữ liệu trước khi sử dụng.\nLựa chọn anchors cẩn thận (Cái này khá khó, đòi hỏi bạn phải am hiểu khá kỹ về dữ liệu, và nếu set nhầm thì sẽ đi tong).\nSử dụng data augmentation.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\nBài viết được lược dịch và tham khảo từ nguồn https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff\n","date":"Dec 10, 2018","img":"","permalink":"/blog/2018-12-10-design-choices-lessons-learned-and-trends-for-object-detections/","series":null,"tags":["Machine learning","Deeplearning","object detector","single shot"],"title":"Lựa Chọn Mô Hình Object Detectors"},{"categories":null,"content":" Single Shot detectors SSD YOLO YOLOv3 Feature Pyramid Networks (FPN) So sánh Feature Pyramid Networks với Region Proposal Network Sử dụng Feature Pyramid Networks trong Fast R-CNN và Faster R-CNN Focal loss (RetinaNet) Single Shot detectors Ở bài trước, chúng ta đã tìm hiểu về region proposal và ứng dụng của nó vào Faster R-CNN. Các thuật toán thuộc nhóm region proposal tuy cho kết quả có độ chính xác cao, nhưng chúng có một nhược điểm rất lớn là thời gian huấn luyện và đưa quyết định rất chậm. Faster R-CNN xử lý khoảng 7 FPS trên tập dữ liệu PASCAL VOC 2007. Một cách để tăng tốc quá trình tính toán là giảm số lượng tính toán trên mỗi ROI.\n1feature_maps = process(image) 2ROIs = region_proposal(feature_maps) 3for ROI in ROIs 4 patch = roi_align(feature_maps, ROI) 5 results = detector2(patch) # Giảm khối lượng tính toán ở đây Một ý tưởng khác, là chúng ta sẽ bỏ qua bước tìm region proposal, mà trực tiếp rút trích boundary boxes và classes trực tiếp từ feature map.\n1feature_maps = process(image) 2results = detector3(feature_maps) # Không cần tìm ROI Dựa trên ý tưởng sử dụng cửa sổ trượt. Chúng ta sẽ trượt trên feature máp để nhận diện các đối tượng. Với mỗi loại đối tượng khác nhau, chúng ta sửa dụng các cửa sổ trượt có kích thước khác nhau. Cách này thoạt đầu trông có vẻ khá tốt, nhưng điểm yếu của nó là đã sử dụng cửa sổ trượt làm final boundary box. Do đó, giả sử chúng ta có nhiều đối tượng, và mỗi đối tượng có kích thước khác nhau, chúng ta sẽ có rất nhiều cửa sổ trượt để bao phủ hết toàn bộ đối tượng.\nMột ý tưởng cải tiến là chúng ta sẽ định nghĩa trước các cửa sổ trượt, sau đó sẽ tiến hành dự đoán lớp và boundary box ( và Ý tưởng này, nhóm nghiên cứu phát triển thuật toán và đặt tên thuật toán là single shot detectors). Ý tưởng này tương tự như việc sử dụng anchors trong Faster R-CNN, nhưng single shot detectors thực hiện dự đoán boundary box và class đồng thời cùng nhau.\nVí dụ, giả sử chúng ta có một feature map 8x8 và chúng ta đưa ra k = 4 dự đoán. Vậy ta có tổng cộng 8x8x4 = 256 dự đoán.\nXét hình bên trên, ta có 4 anchors đã được định nghĩa trước ( màu xanh lá cây), và có 4 prediction( màu xanh nước biển) tương ứng với từng anchor trên.\nVới thuật toán Faster R-CNN, chúng ta sử dụng một convolution filter trả ra 5 kết quả dự đoán: 4 giá trị là toạ độ của boundary box, và giá trị còn lại là xác suất xuất hiện đối tượng. Tổng quát hơn, ta có input là D feature map 8x8, output là 8x8x5, số convolution filter trong Faster R-CNN là 3x3xDx8.\nVới single shot detector, input của ta cũng tương tự là 8x8xD, output là 8x8x (4 + C) ( với 4 tương ứng với 4 điểm boundary box, và C là số lượng lớp đối tượng), vậy ta cần một convolution filter là 3x3xDx(4+C)\nThuật toán Single shot detect chạy khá nhanh, nhưng độ chính xác của nó không cao lắm (không bằng region proposal). Thuật toán có vấn đề về việc nhận dạng các đối tượng có kích thước nhỏ. Ví dụ như hình bên dưới, chúng ta có tổng cộng 9 ông già noel, nhưng thuật toán chỉ nhận diện được có 5 ông.\nSSD SSD là mô hình single shot detector sử dụng mạng VGG16 để rút trích đặc trưng. Mô hình như hình bên dưới. Trong đó, những conv có màu xanh nước biển nhạt là những custom convolution layter (ta có thể thêm bớt bao nhiêu tuỳ thích). Convolutional filter layter (là cục màu xanh lá cây) có nhiệm vụ tổng hợp các thông tin lại để đưa quyết định.\nKhi sử dụng mô hình như hình ở trên, chúng ta thấy rằng các custom convolution layter có nhiệm vụ làm giảm chiều và giảm độ phân giải của bức ảnh. Cho nên, mô hình chỉ có khả năng nhận ra các đối tượng có kích thước lớn. Để giải quyết vấn đề này, chúng ta sẽ sử dụng các object detector khác nhau trên mỗi feature maps (xem output của mỗi custom convolution là một feature map).\nẢnh bên dưới là sơ đồ số chiều của các feature maps.\nSSD sử dụng các layter có kích thước giảm dần theo độ sâu để nhận dạng đối tượng. Nhìn vào hình vẽ sơ đồ bên dưới của SSD, chúng ra dễ dàng nhận thấy rằng độ phân giải giảm đáng kể qua mỗi layer và có lẽ (chắc chắn) sẽ bỏ sót những đối tượng có kích thước nhỏ ở những lớp có độ phân giải thấp. Nếu trong dự án thực tế của bạn có xảy ra vấn đề này, bạn nên tăng độ phân giải của ảnh đầu vào.\nYOLO YOLO cũng là một thuật toán sử dụng single shot detector để dò tìm vị trí của các đối tượng trong ảnh. YOLO sử dụng DarkNet để tạo các feature cho bức ảnh (SSD sử dụng VGG16). Mô hình của YOLLO như ảnh ở bên dưới.\nKhác với kiến trúc mạng SSD ở trên, YOLLO không sử dụng multiple scale feature map (SSD sử dụng các custom convolution layter, qua mỗi layter thì feature maps sẽ có kích thước giảm xuống - các output của custom convolution layer chính là các feature map chúng ta thu được). Thay vào đó, YOLLO sẽ làm phẳng hoá (flatten - vd ma trận 3x3 sẽ biến thành vector 1x9, ma trận 4x5 sẽ biến thành vector 1x20 \u0026hellip;, làm phẳng nghĩa là chúng ta sẽ không dùng bộ lọc nào hết, mà sử dụng các phép biến đổi, nên không làm thay đổi giá trị, chỉ làm thay đổi hình dạng) một phần output của convolution layer và kết hợp với convolution layer ở trong DarkNet tạo thành feature map (Xem hình ở trên sẽ rõ hơn). Ví dụ ở custom convolution layer chúng ta thu được output có kích thước 28x28x512, chúng ta sẽ flatten thành layter có kích thước 14x14x2048, kết hợp với 1 layter có kích thước 14x14x1024 ở trong darknet, chúng ta thu được feature maps có kích thước là 14x14x3072. Đem feature maps này đi đự đoán.\nYOLOv2 đã thêm vào rất nhiều các cải tiền để cải tăng mAP từ 63.4 trong mô hình đầu tiên (YOLOv1) lên 78.6. Các cải tiền bao gồm thêm batch norm, anchor boxes, hi-res classifier \u0026hellip; Các bạn có thể xem ở hình bên dưới. YOLO9000 có thể nhận dạng 9000 đối tượng khác nhau.\nYOLOv2 có thể nhận diện các đối tượng với ảnh đầu vào có độ phân giải bất kỳ. Với ảnh có độ phân giải thấp thì mô hình chạy khá nhanh, có FPS cao nhưng mAP lại thấp (tradeoff giữa FPS và mAP).\nYOLOv3 YOLOv3 sử dụng darknet với kiến trúc phức hơn để rút trích đặc trưng của bức ảnh. YOLOv3 thêm vào đặc trưng Pyramid để dò tìm các đối tượng có kích thước nhỏ.\nHình bên dưới so sánh tradeoff giữa thời gian thực thi và độ chính xác giữa các mô hình. Ta thấy rằng thời gian thực thi của YOLOv3 rất nhanh, cùng phân mức mAP 28.8, thời gian YOLOv3 thực thi chỉ tốn 22ms, trong khi đó SSD321 tốn đến 61ms - gấp 3 lần.\nFeature Pyramid Networks (FPN) Dò tìm các đối tượng có kích thước nhỏ là một vấn đề đáng được giải quyết để nâng cao độ chính xác. Và FPN là mô hình mạng được thiết kế ra dựa trên khái niệm pyramid để giải quyết vấn đề này.\nMô hình FPN kết hợp thông tin của mô hình theo hướng bottom-up kết hợp với top-down để dò tìm đối tượng (trong khi đó, các thuật toán khác chỉ thường sử dụng bottom-up). Khi chúng ta ở bottom và đi lên (up), độ phân giải sẽ giảm, nhưng giá trị ngữ nghĩa sẽ tăng lên. Xem hình mô phỏng bên dưới.\nSSD đưa ra quyết định dựa vào nhiều feature map. Nhưng layer ở bottom không được sử dụng để nhận dạng đối tượng. Vì những layter này có độ phân giải cao nhưng giá trị ngữ nghĩa của chúng lại không đủ cao (thấp) nên những nhà nghiên cứu bỏ chúng đi để tăng tốc độ xử lý. Các nhà nghiêng cứu biện minh rằng các layer ở bottom chưa đủ mức ý nghĩa cần thiết để nâng cao độ chính xác, thêm các layer đó vào sẽ không nâng độ chính xác cao thêm bao nhiêu và họ bỏ chúng đi để có tốc độ tốt hơn. Cho nên, SSD chỉ sử dụng các layer ở lớp trên , và do đó sẽ không nhận dạng được các đối tượng có kích thước nhỏ.\nTrong khi đó, FPN xây dựng thêm mô hình top-down, nhằm mục đích xây dựng các layer có độ phân giải cao từ các layer có ngữ nghĩa cao.\nTrong quá trình xây dựng lại các layer từ top xuống bottom, chúng ta sẽ gặp một vấn đề khá nghiêm trọng là bị mất mát thông tin của các đối tượng. Ví dụ một đối tượng nhỏ khi lên top sẽ không thấy nó, và từ top đi ngược lại sẽ không thể tái tạo lại đối tượng nhỏ đó. Để giải quyết vấn đề này, chúng ta sẽ tạo các kết nối (skip connection) giữa các reconstruction layter và các feature map để giúp quá trình detector dự đoán các vị trí của đối tượng thực hiện tốt hơn (hạn chế tốt nhất việc mất mát thông tin).\nThêm các skip connection giữa feature map và reconstruction layer\nĐồ hình bên dưới diễn ta chi tiết đường đi theo bottom-up và top-down. P2, P3, P4, P5 là các pyramid của các feature map.\nSo sánh Feature Pyramid Networks với Region Proposal Network FPN không phải là mô hình phát hiện đối tượng. Nó là mô hình phát hiện đặc trưng và được sử dụng trong phát hiện đối tượng. Các feature map từ P2 đến P5 trong hình bên dưới độc lập với nhau và các đặc trưng được sử dụng để phát hiện đối tượng.\nSử dụng Feature Pyramid Networks trong Fast R-CNN và Faster R-CNN Chúng ta hoàn toàn có thể sử dụng FPN trong Fast và Faster R-CNN. Chúng ta sẽ tạo ra các feature map sử dụng FPN, kết quả là ta thu được các puramid (feature map). Sau đó, chúng ta sẽ rút trích các ROIs trên các feature map đó. Dựa trên kích thước của các ROI, chúng ta sẽ chọn feature map nào tốt nhất để tạo các feature patches (các hình chữ nhật nhỏ). Các bạn có thể xem chi tiết ở hình bên dưới.\nFocal loss (RetinaNet) Trong thực tế, chúng ta sẽ gặp tình trạng tỷ lệ diện tích của các đối tượng trong ảnh nhỏ hơn nhiều so với phần background còn lại, ví dụ chúng ta cần nhận dạng một quả cam có kích thước 100x100 trong ảnh 1920x1080. Vì phần background quá lớn nên chúng sẽ là thành phần \u0026ldquo;thống trị\u0026rdquo; và làm sai lệch kết quả. SSD sử dụng phương pháp lấy mẫu tỷ lệ của object class và background class trong quá trình train (nên background sẽ không còn thống trị nữa).\nNgoài ra, chúng ta sẽ còn gặp tình trạng là số lượng tỷ lệ object trong ảnh không đều nhau, ví dụ trong tập huấn luyệt có 1000 quả cam và 10 quả táo.\nFocal loss (FL) được sinh ra để giải quyết tình trạng này. Để đi vào chi tiết hơn, chúng ta nhắc lại hàm lỗi cross entropy.\n$$ \\begin{equation} CE(p,y) = \\begin{cases} -\\log(p) \u0026amp; \\text{if y=1} \\\\\\\\ -\\log(1-p) \u0026amp; \\text{otherwise} \\end{cases} \\end{equation} $$\nTrong hàm trên thì y nhận giá trị 1 hoặc -1. Giá trị xác xuất nằm trong khoảng (0,1) là xác suất dự đoán cho lớp có y=1.\nĐể rõ ràng hơn, ta có thể viết lại hàm trên như sau:\n$$ \\begin{equation} p_t = \\begin{cases} p \u0026amp; \\text{if y=1} \\\\\\\\ 1-p \u0026amp; \\text{otherwise} \\end{cases} \\end{equation} $$\n$$ \\begin{equation} CE(p,y) = CE(p_t) = -\\log(p_t) \\end{equation} $$\nTa có nhận xét rằng đối với các trường hợp được phân loại tốt (có xác suất lớn hơn 0.6) thì hàm loss nhận gái trị với độ lớn lớn hơn 0. Và trong trường hợp dữ liệu có tỷ lệ lệch cao thì tổng các giá trị này sẽ cho ra kết quả loss với một con số rất lớn so với loss của các trường hợp khó phâm loại. Và nó ảnh hưởng đến quá trình huấn luyện.\nÝ tưởng chính của focal-lost là đối với các trường hợp được phân loại tốt ( xác suất lớn hơn 0.5) thì focal lost sẽ làm giảm giá trị cross-entropy của nó xuống nhỏ hơn so với thông thường. Do đó, ta sẽ thêm trọng số cho hàm cross-entropy để biến thành hàm focal lost.\n$$ FL(p_t) = -(1-p_t)^\\gamma\\log(p_t) $$\nVới nhân tử được thêm vào được gọi là modulating factor, gamma lớn hơn hoặc bằng 0 được gọi là tham số focusing.\nNhìn hình ở trên, ta thấy rằng khi gamma = 0 thì hàm focal lost chính là cross-entropy.\nĐặc điểm của hàm lost trên như sau:\nKhi mẫu bị phân loại sai, pt nhỏ, nhân tố modulating factor gần với 1 và hàm lost ít bị ảnh hưởng. Khi pt tiến gần tới 1 (mẫu phân loại tốt), moduling factor sẽ tiến gần tới 0 và hàm loss trong trường hợp này sẽ bị giảm trọng số xuống.\nTham số focusing sẽ điều chỉnh tỷ lệ các trường hợp được phân loại tốt được giảm trọng số. Khi gamma càng tăng thì ảnh hưởng của modulating factor cũng tăng. Trong các thí nghiệm cho thấy với gamma = 2 hì kết quả đạt được sẽ tốt nhất.\nHình bên dưới là đồ hình của RetinaNet được xây dựng dựa trên FPN và ResNet sử dung Focal loss.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\nBài viết được lược dịch và tham khảo từ nguồn https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d\n","date":"Dec 6, 2018","img":"","permalink":"/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/","series":null,"tags":["Machine learning","Deeplearning","object detector","single shot"],"title":"Tìm Hiểu Single Shot Object Detectors"},{"categories":null,"content":" Sliding-window detectors Selective Search Mạng R-CNN Mạng Fast R-CNN ROI Pooling Faster R-CNN Region proposal network Hiệu năng của mô hình R-CNN Region-based Fully Convolutional Networks Sliding-window detectors Bắt đầu từ năm 2012, sau khi mạng AlexNet giành giải nhất cuộc thi 2012 ILSVRC, mọi nghiên cứu về phân lớp dữ liệu đều sử dụng mạng CNN. Kể từ đó đến đây, CNN được coi như là thuật toán thống trị trên mọi publish paper về các bài toán phân lớp đối tượng. Trong khi đó, để nhận dạng 1 đối tượng trong ảnh, các đơn giản nhất là thiết lập một cửa sổ trượt có kích thước là window size trượt từ trái qua phải, từ trên xuống dưới, quét qua toàn bộ bức ảnh. Để phát hiện các đối tượng khác nhau ở các góc nhìn khác nhau, chúng ta sẽ sử dụng cửa sổ trượt có kích thước thay đổi và ảnh đầu vào có kích thước thay đổi.\nDựa vào windowsize, chúng ta có thể cắt tấm hình bự thành các tấm hình nhỏ, sau đó sẽ rescale các phần nhỏ của bức ảnh thành các bức ảnh có kích thước cố định.\nCác phần của bức ảnh sau đó sẽ được đem qua bộ phân lớp CNN để rút trích các đặc trưng, sau đó sử dụng một hàm phân lớp (như svm, logictic regression) để xác định lớp của bức hình và sử dụng linear regressor để tìm bao đóng của đối tượng.\nMã giả của mô hình\n1for window in windows 2 patch = get_patch(image, window) 3 results = detector(patch) Cách dễ dàng nhất để cải tiến hiệu năng của phương pháp này là giảm số lượng tấm hình nhỏ xuống (ví dụ tăng kích thước window size). Cách này còn được giang hồ gọi là brute force.\nSelective Search Thay vì hướng tiếp cận brute force ở trên, chúng ta sử dụng phương pháp region proposal để tạo các region of interest (ROIs) để phát hiện đối tượng. Selective search là một phương pháp nằm trong nhóm region proposal. Trong phương pháp selective search(SS), chúng ta bắt đầu bằng cách xem các pixel là mỗi nhóm, các lần lặp tiếp theo, chúng ta sẽ tính khoảng cách ngữ nghĩa (ví dụ như là màu sắc, cường độ ánh sáng) giữa các nhóm và gom các nhóm có khoảng cách gần nhau về chung 1 nhóm để tìm ra phân vùng có khả năng cao nhất chứa đối tượng (ưu tiên gom những nhóm nhỏ trước).\nNhư hình bên dưới, dòng đầu tiên, bức ảnh đâu tiên là ta có một vài nhóm nhỏ ở thời điểm X nào đó, ở hình thứ 2 là thực hiện gom nhớm theo cường độ màu sắc của hình số 1, và ở bước cuối cùng, ta thu được hình số 3. Những hình chữ nhật màu xanh ở dòng thứ 2 là những ROIS mô phỏng quá trình gom nhóm để tìm phân vùng có khả năng chứa đối tượng.\nselective search\nMạng R-CNN Mạng R-CNN sử dụng phương pháp region proposal để tạo ra khoảng 2000 ROIs. Các vùng sau đó sẽ được rescale theo một kích thước cố định nào đó và được đưa vào mô hình CNN có lớp cuối cùng kà một full conected layer để phân lớp đối tượng và để lọc ra boundary box (bao đóng) của đối tượng.\nMô phỏng việc sử dụng region proposal\nMô phỏng việc sử dụng region proposal của RCNN\nMã giả của mô hình\n1ROIs = region_proposal(image) 2for ROI in ROIs 3 patch = get_patch(image, ROI) 4 results = detector(patch) Với việc sử dụng ít tấm ảnh nhỏ hơn, và chất lượng của mỗi tấm ảnh nhỏ tốt hơn, Mạng R-CNN chạy nhanh hơn và có độ chính xác cao hơn so với mô hình sử dụng cửa sổ trượt.\nMạng Fast R-CNN Trong thực tế, các phân vùng của mạng R-CNN bị chồng lấp một phần / toàn bộ với các phân vùng khác. Do đó, việc huấn luyện và thực thi ( inference ) mạng R-CNN diễn ra khá chậm. Nếu chúng ta có 2000 proposal của mạng R-CNN, chúng ta phải thực hiện 2000 lần việc rút trích đặc trưng, một con số khác lớn.\nThay vì phải rút trích đặc trưng của mỗi proposal, chúng ta có thể dùng CNN rút trích đặc trưng của toàn bộ bức ảnh trước (được feature map), đồng thời rút trích các proposal, lấy các proposal tương ứng trên feature map, rescale và cuối cùng là phân lớp và tìm vị trí của object. Với việc không phải lặp lại 2000 lần việc rút trích đặc trưng, Fast R-CNN giảm thời gian xử lý một cách đáng kể.\nMô phỏngviệc sử dụng propoxal trên feature map và các bước tiếp theo của Fast R-CNN\nĐồ hình của Fast R-CNN\nMã giả của mô hình\n1feature_maps = process(image) 2ROIs = region_proposal(image) 3for ROI in ROIs 4 patch = roi_pooling(feature_maps, ROI) 5 results = detector2(patch) Với việc không phải lặp đi lặp lại quá trình tìm ra các proposal, tốc độ của thuật toán tăng lên kha khá. Trong thực nghiệm, mô hình Fast R-CNN chạy nhanh hơn gấp 10 lần so với R-CNN trong quá trình huấn luyện. Và nhanh hơn 150 lần trong inferencing.\nMột khác biệt lớn nhất của Fast R-CNN là toàn bộ network (feature extractior, classifier, boundary box regressor) có thể huấn luyện end-to end (nghĩa là từ đầu đến cuối) với 2 hàm độ lỗi (loss funtion) khác nhau cùng lúc (classification loss và localization loss). Điều này làm tăng độ chính xác của mô hình.\nROI Pooling Vì Fast R-CNN sử dụng full connected layter ở lớp cuối, nên đòi hỏi input của chúng phải có kích thước cố định, nên ta phải resize lại feature về 1 kích thước cố định (do 2000 proposal có kích thước không cố định). Ở đây, các tác giả sử dụng ROI pooling để resize. Thuật toán ở đây được sử dụng như sau:\nGiả sử đơn giản là chúng ta có một proposal có kích thước 5x7, và chúng ta cần resize về hình dạng 2x2. Chúng ta xem kỹ hình bên dưới.\nHình ảnh mô phỏng ROI pooling\nHình ở bên trái là feature map của chúng ta.\nHình số 2, vùng hình chữ nhật xanh là vùng proposal 5x7.\nVì chúng ta cần resize về vùng có kích thước 2x2 (4 phần), nên ta chia vùng proposal 5x7 thành 4 phần (5/2 =2 dư 3, vậy có 1 phần là 2, 1 phần là 3. Tương tự 7/2 = 3 dư 4, vậy có 1 phần 3, một phần 4. Cuối cùng ta có 4 hình chữ nhật có kích thước tương ứng là 2x3, 2x4, 3x3, 3x4) (Hình số 3).\nHình số 4, từ 4 phần của vùng số 3, ta sẽ lấy giá trị lớn nhất của mỗi vùng.\nVậy là ta thu được feature proposal có kích thước 2x2 rồi.\nFaster R-CNN Nhìn kỹ lại vào thuật toán F-CNN, chúng ta cần phải rút rích 2000 ROIs, và nó là nguyên nhân lớn gây nên sự chậm trể của mô hình\n1feature_maps = process(image) 2ROIs = region_proposal(image) # Expensive, slow 3for ROI in ROIs 4 patch = roi_pooling(feature_maps, ROI) 5 results = detector2(patch) Thuật toán Faster R-CNN sử dụng mô hình gần như tương tự Fast R-CNN, ngoài việc sử dụng thuật toán interal deep network thay cho selective search để tìm region proposal. Thuật toán mới chạy hiệu quả hơn khi tìm tất cả các ROIs trên mỗi bức ảnh với tốc độ 10ms/\nMô hình của Faster R-CNN\nĐồ hình của Faster R-CNN\nRegion proposal network Mạng region proposal sử dụng feature map làm input đầu vào (như hình trên đã mô phỏng). Mạng sử dụng 1 bộ lọc 3x3, sau đó là một mô hình CNN như ZF hoặc VGG hoặc ResNet ( mô hình càng phức tạp thì độ chính xác cao, nhưng bù lại thời gian tìm kiếm sẽ lâu hơn) để dự đoán boundary box và object score (để xét xem trong bodary box trên có chứa đối tượng hay không. Trong thực tế, mạng Faster R-CNN trả về 2 lớp, lớp thứ nhất là có chứa object, lớp thứ 2 là không chứa object ( ví dụ lớp màu nền - background, lớp abc gì gì đó)) .\nVí dụ Region proposal network\nMô hình Region proposal network sử dụng ZF network\nGiả sử tại 1 điểm nào đó trên feature map, RPN có k dự đoán, vậy là chúng ta có tổng cộng 4xk toạ độ điểm và 2xk điểm cho điểm đó. Nhìn ví dụ ở hình bên dưới.\nHình 1: ta có feature map với kích thước 8x8, vùng hình vuông được tô là filter đang xét có kích thước 3x3. Hình 2: Giả sử xét điểm có chấm xanh. Tại điểm đó, ta có k=3 sau khi chạy RPN, và ta được 3 hình chữ nhật như hình.\nTuy nhiên, tại mỗi điểm, ta chỉ cần 1 boundary box tốt nhất. Cách đơn giản nhất là chọn ngẫu nhiên 1 cái. Nhưng như vậy thì ngay từ đầu ta chọn k=1 luôn cho khoẻ, mắc công gì phải chọn k=3. Trong thực tế, Faster R-CNN không sử dụng phương pháp random select. Thay vào đó, thuật toán một reference boxs hay còn được gọi với tên là anchors và tìm mức độ liên quan của k boundary box với k reference boxs và chọn ra boundary box có độ liên quan lớn nhất.\nVí dụ anchors box\nCác anchors này được lựa chọn trước đó và được xem là config của mô hình. Faster R-CNN sử dụng 9 anchor boxs (tương ứng với k =3) với 3 box đầu tiên có tỷ lệ width, height khác nhau (ví dụ 2x3, 3x3, 3x2), tiếp đó sẽ scale các box trên với các tỷ lệ khác khau (ví dụ 1.5,3,7) để đạt được 9 anchor boxs.\nVì mỗi điểm sử dụng 9 anchors, nên ta có tổng cộng 2x9 score và 4x9 location (toạ độ)\nAnchor box có thể được goijlaf priors hoặc default boundary boxes trong mỗi bài báo khác nhau.\nHiệu năng của mô hình R-CNN Hình bên dưới mô tả benchmark của các mô hình dẫn xuất từ R-CNN, ta thấy Faster R-CNN có tốc độ tốt nhất.\nRegion-based Fully Convolutional Networks Giả sử chúng ta chỉ có toạ độ của mắt phải trong khuôn mặt, chúng ta có thể nội suy ra được vị trí của khuôn mặt. Vì ta biết rằng mắt phải nằm ở vị trí trái trái trong bức hình, và ta từ đó suy ra vị trí của các phần còn lại (xem hình).\nNếu chúng ta có thêm thông tin khác, ví như toạ độ của mắt trái, mũi, miệng, \u0026hellip; thì chúng ta có thể kết hợp chúng để tăng độ chính xác của phân vùng khuôn mặt.\nTrong Faster R-CNN, chúng ta phải tìm proposal sử dụng một mô hình CNN, với khoảng 2000 ROI, chúng ta sẽ tiêu tốn một khoảng thời gian khá lớn để tìm chúng.\n1feature_maps = process(image) 2ROIs = region_proposal(feature_maps) 3for ROI in ROIs 4 patch = roi_pooling(feature_maps, ROI) 5 class_scores, box = detector(patch) # Expensive, slow 6 class_probabilities = softmax(class_scores) Trong khi đó, với Fast R-CNN, chúng ta chỉ cần phải tính max hoặc average, nên Fast R-CNN nhanh hơn Faster R-CNN ở đây.\n1feature_maps = process(image) 2ROIs = region_proposal(feature_maps) 3score_maps = compute_score_map(feature_maps) 4for ROI in ROIs 5 V = region_roi_pool(score_maps, ROI) 6 class_scores, box = average(V) # Much simpler, faster. 7 class_probabilities = softmax(class_scores) Xét feature map M có kích thước 5x5, trong đó có chứa một hình vuông màu xanh, hình vuông xanh là đối tượng thực tế ta cần tìm.\nTa chia hình vuông thành phân vùng có kích thước 3x3 (hình 2). Sau đó, chúng ta tạo một feature mới để từ M để tìm ra góc trái trên của hình vuông (chỉ tìm góc trái trên) (hình 3). Feature map mới giống hình thứ 3, chỉ có ô được tô màu vàng ở vị trí [2,2] được bật.\nVới mỗi 9 phần của hình vuông, chúng ta có 9 feature map cho mỗi phần, nhận dạng 9 vùng tương ứng cho một đối tượng. Những feature map này được gọi là position sensitive score map, bởi vì chúng detect ra điểm (score) và sub region của một đối tượng (Xem hình bên dưới).\nXét ảnh bên dưới, giả sử vùng được tô gạch đỏ là proposal (hình 1). Chúng ta cũng chia nó thành những phân vùng con có kích thước 3x3 (hình 2). Và tìm xem mức độ giống nhau của mỗi vùng con của proposal và vùng con của feature map như thế nào. Kết quả sẽ được lưu vào một ma trận 3x3 như hình số 3.\nQuá trình ánh xạ điểm từ score maps và ROIS vào mảng vote_array được gọi là position sensitive ROI pool.\nSau khi tính toán hết các giá trị của position-sensitive ROI pool, chúng ta sẽ tính trung bình của vote_array để lấy điểm của lớp (class score).\nGiả sử mô hình chúng ta phải nhận dạng k lớp, do có thêm lớp background nên chúng ta có tổng cộng k+1 lớp. Với mỗi lớp chúng ta có 3x3 score map, suy ra chúng ta có tổng cộng là (k+1)x3x3 score maps, (k+1) điểm, và dùng softmax ta sẽ thu được xác suất của mỗi lớp.\nLuồng dữ liệu của mô hình\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\nBài viết được lược dịch và tham khảo từ nguồn https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9\n","date":"Dec 5, 2018","img":"","permalink":"/blog/2018-12-05-what-do-we-learn-from-object-detection-p1/","series":null,"tags":["Machine learning","Deeplearning","object detector","region base"],"title":"Tìm Hiểu Region Based Object Detectors"},{"categories":null,"content":" Lời mở đầu Dẫn nhập Phân tích dữ liệu Thực hành Xây dựng tập đặc trưng Đặc trưng sản phẩm Đặc trưng người dùng Đặc trưng mối tương quan giữa người dùng và sản phẩm Bổ sung thêm đặc trưng Huấn luyện mô hình Lời mở đầu Instacart là một startup cung ứng đồ tạp hóa qua website và ứng dụng di động. Người dùng chỉ cần chọn đồ muốn mua tại các chuỗi bán lẻ và đặt đồ, Instacart sẽ đi mua và giao đến tận tay họ. Đến nay, Instacart hoạt động tại 15.000 cửa hàng tạp hoá tại 4.000 thành phố với khoảng 50.000 “trợ lý mua sắm”. Team data science của instacart đóng vai trò rất quan trọng trong việc cung cấp trải nghiệm người dùng trong việc sử dụng app để mua hàng. Hiện tại, họ đang sử dụng các dữ liệu của khách hàng để tạo nên mô hình dự đoán sản phẩm nào người dùng sẽ mua lại, sẽ mua thử lần đầu tiên, hoặc sẽ thêm vào giỏ hàng. Hiện họ đã publish khoảng 3 triệu đơn hàng của họ để các nhà khoa học dữ liệu khác sử dụng và nghiên cứu.\nDẫn nhập Phân tích dữ liệu Các bạn có thể download dữ liệu ở https://www.instacart.com/datasets/grocery-shopping-2017.\nCác file bao gồm:\nFile aisles.csv (134 dòng) có 2 cột là aisle_id,aisle\n1aisle_id,aisle 21,prepared soups salads 32,specialty cheeses 43,energy granola bars 5 ... File departments.csv (21 dòng) gồm 2 cột là department_id,department\n1department_id,department 21,frozen 32,other 43,bakery 5 ... File order_products__(prior|train).csv (trên 30 triệu dòng)\nTập này chứa danh sách sản phẩm được mua trong mỗi đơn hàng. File order_products__prior.csv chứa sản phẩm của đơn hàng trước đó của khách hàng. \u0026lsquo;reordered\u0026rsquo; nói rằng sản phẩm này trong đơn hàng hiện tại đã được mua ở đơn hàng trước đó. Vì vậy, sẽ có đơn hàng không được gán là \u0026lsquo;reordered\u0026rsquo; (chúng ta có thể gán nhãn là None hoặc cái gì đó cũng được để chỉ các sản phẩm này). \u0026lsquo;add_to_cart_order\u0026rsquo; là thứ tự của sp được thêm vào giỏ hàng.\n1order_id,product_id,add_to_cart_order,reordered 2 1,49302,1,1 3 1,11109,2,1 4 1,10246,3,0 5 ... File orders.csv (3.4 triệu dòng, 206k users): chứa thông tin của đơn hàng, trong đó, order_dow là ngày trong tuần, eval_set thuộc một trong 3 loại là prior, train, test. order_number là thứ tự của đơn hàng của user này.\n1order_id,user_id,eval_set,order_number,order_dow,order_hour_of_day,days_since_prior_order 2 2539329,1,prior,1,2,08, 3 2398795,1,prior,2,3,07,15.0 4 473747,1,prior,3,3,12,21.0 5 ... File products.csv ((50k dòng) chứa thông tin sản phẩm:\n1 product_id,product_name,aisle_id,department_id 2 1,Chocolate Sandwich Cookies,61,19 3 2,All-Seasons Salt,104,13 4 3,Robust Golden Unsweetened Oolong Tea,94,7 5 ... Với mỗi order_id trong tập test ở file orders.csv, chúng ta phải dự đoán các sản phẩm nào người dùng sẽ mua lại (\u0026ldquo;reorder\u0026rdquo;) thuộc đơn hàng đó. Nếu bạn dự đoán đó là đơn hàng không có sản phẩm nào được mua lại, thì ta sẽ điền vào giá trị \u0026lsquo;None\u0026rsquo;\nVí dụ về kết quả dự đoán:\n1order_id,products 217,1 2 334,None 4137,1 2 3 Thực hành Đầu tiên, ta sẽ import một số thư viện cơ bản để sử dụng, và load tất cả các file lên. Lưu ý một chút là ở đây, mình để tất cả các file trong thư mục data\n1import pandas as pd 2import numpy as np 3from collections import OrderedDict 4 5from sklearn.linear_model import LogisticRegression 6from sklearn.metrics import f1_score 7 8from sklearn import metrics, cross_validation 9from sklearn.metrics import f1_score 10from sklearn.preprocessing import MinMaxScaler 11 12#Import the files 13aisles_df = pd.read_csv(\u0026#39;data/aisles.csv\u0026#39;) 14products_df = pd.read_csv(\u0026#39;data/products.csv\u0026#39;) 15orders_df = pd.read_csv(\u0026#39;data/orders.csv\u0026#39;) 16order_products_prior_df = pd.read_csv(\u0026#39;data/order_products__prior.csv\u0026#39;) 17departments_df = pd.read_csv(\u0026#39;data/departments.csv\u0026#39;) 18order_products_train_df = pd.read_csv(\u0026#39;data/order_products__train.csv\u0026#39;) Sau đó, mình sẽ merge đơn hàng vào chi tiết đơn hàng của tập train và tập prior\n1order_products_train_df = order_products_train_df.merge(orders_df.drop(\u0026#39;eval_set\u0026#39;, axis=1), on=\u0026#39;order_id\u0026#39;) 2order_products_prior_df = order_products_prior_df.merge(orders_df.drop(\u0026#39;eval_set\u0026#39;, axis=1), on=\u0026#39;order_id\u0026#39;) show ra 5 dòng đầu tiên của order_products_train_df\n1print(order_products_train_df.head()) 1 order_id product_id add_to_cart_order reordered user_id order_number order_dow order_hour_of_day days_since_prior_order 20 1 49302 1 1 112108 4 4 10 9.0 31 1 11109 2 1 112108 4 4 10 9.0 42 1 10246 3 0 112108 4 4 10 9.0 53 1 49683 4 0 112108 4 4 10 9.0 64 1 43633 5 1 112108 4 4 10 9.0 7 8[5 rows x 9 columns] Tổng cộng mình có 9 cột, ý nghĩa các cột mình có giải thích ở trên rồi nha.\nTiếp theo, chúng ta tạo tập tập dữ liệu đếm số lượng sản phẩm của từng người mua\n1user_product_df = (order_products_prior_df.groupby([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;],as_index=False) 2 .agg({\u0026#39;order_id\u0026#39;:\u0026#39;count\u0026#39;}) 3 .rename(columns={\u0026#39;order_id\u0026#39;:\u0026#39;user_product_total_orders\u0026#39;})) 4 5train_ids = order_products_train_df[\u0026#39;user_id\u0026#39;].unique() 6df_X = user_product_df[user_product_df[\u0026#39;user_id\u0026#39;].isin(train_ids)] 7print(df_X.head()) 1 product_id user_id user_product_total_orders 20 1 138 2 31 1 709 1 43 1 777 1 56 1 1052 2 69 1 1494 3 Ở đây, người 138 mua sản phẩm 1 2 lần, người 709 mua sản phẩm 1 1 lần, \u0026hellip; tương tự như vậy cho các user và product khác.\nBước tiếp theo, chúng ta sẽ liệt kê các sản phẩm người dùng đã mua:\n1train_carts = (order_products_train_df.groupby(\u0026#39;user_id\u0026#39;,as_index=False) 2 .agg({\u0026#39;product_id\u0026#39;:(lambda x: set(x))}) 3 .rename(columns={\u0026#39;product_id\u0026#39;:\u0026#39;latest_cart\u0026#39;})) print(train_carts.head())\n1 user_id latest_cart 20 1 {196, 26405, 27845, 46149, 13032, 39657, 26088... 31 2 {24838, 11913, 45066, 31883, 48523, 38547, 248... 42 5 {40706, 21413, 20843, 48204, 21616, 19057, 201... 53 7 {17638, 29894, 47272, 45066, 13198, 37999, 408... 64 8 {27104, 15937, 5539, 41540, 31717, 48230, 2224... Mối tương quan giữa sản phẩm được add to card và sản phẩm được mua\n1df_X = df_X.merge(train_carts, on=\u0026#39;user_id\u0026#39;) 2df_X[\u0026#39;in_cart\u0026#39;] = (df_X.apply(lambda row: row[\u0026#39;product_id\u0026#39;] in row[\u0026#39;latest_cart\u0026#39;], axis=1).astype(int)) 3 4print(df_X.head()) 5 6print(df_X[\u0026#39;in_cart\u0026#39;].value_counts()) 1# df_X.head() 2 product_id user_id user_product_total_orders latest_cart in_cart 30 1 138 2 {42475} 0 41 907 138 2 {42475} 0 52 1000 138 1 {42475} 0 63 3265 138 1 {42475} 0 74 4913 138 1 {42475} 0 8 9# df_X[\u0026#39;in_cart\u0026#39;].value_counts() 100 7645837 111 828824 12Name: in_cart, dtype: int64 Tỷ lệ khoảng 9.7%. Điều này nói lên rằng, người dùng trong 1 phiên mua hàng có thể add rất nhiều sản phẩm vào giỏ, nhưng chỉ khoảng 10% sản phẩm họ mua thật sự, hơn 90% sản phẩm còn lại sẽ bị remove trước khi nọ nhấn nút thanh toán.\nXây dựng tập đặc trưng Đặc trưng sản phẩm Với đặc trưng sản phẩm, chúng ta sẽ rút trích 2 đặc trưng đơn giản là tổng số lượng đơn hàng của một sản phẩm và trung bình số lượng đơn hàng có chứa sản phẩm.\n1prod_features = [\u0026#39;product_total_orders\u0026#39;,\u0026#39;product_avg_add_to_cart_order\u0026#39;] 2 3prod_features_df = (order_products_prior_df.groupby([\u0026#39;product_id\u0026#39;],as_index=False) 4 .agg(OrderedDict( 5 [(\u0026#39;order_id\u0026#39;,\u0026#39;nunique\u0026#39;), 6 (\u0026#39;add_to_cart_order\u0026#39;,\u0026#39;mean\u0026#39;)]))) 7prod_features_df.columns = [\u0026#39;product_id\u0026#39;] + prod_features 8print(prod_features_df.head()) 1 2 product_id product_total_orders product_avg_add_to_cart_order 30 1 1852 5.801836 41 2 90 9.888889 52 3 277 6.415162 63 4 329 9.507599 74 5 15 6.466667 Add thêm đặc trưng sản phẩm vào trong tập huấn luyện\n1 2df_X = df_X.merge(prod_features_df, on=\u0026#39;product_id\u0026#39;) 3 4#note that dropping rows with NA product_avg_days_since_prior_order is likely a naive choice 5df_X = df_X.dropna() 6print(df_X.head()) 1 product_id user_id ... product_total_orders product_avg_add_to_cart_order 20 1 138 ... 1852 5.801836 31 1 709 ... 1852 5.801836 42 1 777 ... 1852 5.801836 53 1 1052 ... 1852 5.801836 64 1 1494 ... 1852 5.801836 Đặc trưng người dùng Với người dùng, chúng sa sử dụng các đặc trưng là: Tổng số lượng đơn hàng, trung bình số sản phẩm trong 1 đơn hàng, tổng số lượng sản phẩm người dùng mua, Trung bình số ngày user sẽ mua đơn hàng tiếp theo\n1user_features = [\u0026#39;user_total_orders\u0026#39;,\u0026#39;user_avg_cartsize\u0026#39;,\u0026#39;user_total_products\u0026#39;,\u0026#39;user_avg_days_since_prior_order\u0026#39;] 2 3user_features_df = (order_products_prior_df.groupby([\u0026#39;user_id\u0026#39;],as_index=False) 4 .agg(OrderedDict( 5 [(\u0026#39;order_id\u0026#39;,[\u0026#39;nunique\u0026#39;, (lambda x: x.shape[0] / x.nunique())]), 6 (\u0026#39;product_id\u0026#39;,\u0026#39;nunique\u0026#39;), 7 (\u0026#39;days_since_prior_order\u0026#39;,\u0026#39;mean\u0026#39;)]))) 8 9user_features_df.columns = [\u0026#39;user_id\u0026#39;] + user_features 10print(user_features_df.head()) Và chúng ta merge tiếp đặc trưng user vào trong tập huấn luyện.\n1 2df_X = df_X.merge(user_features_df, on=\u0026#39;product_id\u0026#39;) 3 4#note that dropping rows with NA product_avg_days_since_prior_order is likely a naive choice 5df_X = df_X.dropna() Đặc trưng mối tương quan giữa người dùng và sản phẩm Ở đây, chúng ta sử dụng đặc trưng trung bình số sản phẩm của 1 người được thêm vào đơn hàng và tần suất 1 sản phẩm 1 user add vào đơn hàng.\n1user_prod_features = [\u0026#39;user_product_avg_add_to_cart_order\u0026#39;] 2 3user_prod_features_df = (order_products_prior_df.groupby([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;],as_index=False) \\ 4 .agg(OrderedDict( 5 [(\u0026#39;add_to_cart_order\u0026#39;,\u0026#39;mean\u0026#39;)]))) 6 7user_prod_features_df.columns = [\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;] + user_prod_features 8df_X = df_X.merge(user_prod_features_df,on=[\u0026#39;user_id\u0026#39;,\u0026#39;product_id\u0026#39;]) 9df_X[\u0026#39;user_product_order_freq\u0026#39;] = df_X[\u0026#39;user_product_total_orders\u0026#39;] / df_X[\u0026#39;user_total_orders\u0026#39;] Bổ sung thêm đặc trưng Ngoài các đặc trưng cơ bản ở trên, ta sẽ bổ sung thêm một số đặc trưng khác:\nĐặc trưng sản phẩm: bổ sung thêm 3 đặc trưng trung bình ngày trong tuần được đặt hàng (cột order_down), trung bình giờ đặt hàng (cột order_hour_of_day), trung bình ngày đặt hàng kể từ lần đặt trước đó (cột days_since_prior_order) theo sản phẩm.\n1prod_features = [\u0026#39;product_avg_order_dow\u0026#39;, \u0026#39;product_avg_order_hour_of_day\u0026#39;, \u0026#39;product_avg_days_since_prior_order\u0026#39;] 2 3prod_features_df = (order_products_prior_df.groupby([\u0026#39;product_id\u0026#39;], as_index=False) 4 .agg(OrderedDict( 5 [(\u0026#39;order_dow\u0026#39;,\u0026#39;mean\u0026#39;), 6 (\u0026#39;order_hour_of_day\u0026#39;, \u0026#39;mean\u0026#39;), 7 (\u0026#39;days_since_prior_order\u0026#39;, \u0026#39;mean\u0026#39;)]))) 8 9prod_features_df.columns = [\u0026#39;product_id\u0026#39;] + prod_features 10 11df_X = df_X.merge(prod_features_df, on=\u0026#39;product_id\u0026#39;) 12df_X = df_X.dropna() Đặc trưng người dùng: bổ sung thêm 2 cột đặc trung trung bình ngày trong tuần được đặt hàng (cột order_down) và trung bình giờ đặt hàng (cột order_hour_of_day) theo người dùng\n1user_features = [\u0026#39;user_avg_order_dow\u0026#39;,\u0026#39;user_avg_order_hour_of_day\u0026#39;] 2 3user_features_df = (order_products_prior_df.groupby([\u0026#39;user_id\u0026#39;],as_index=False) 4 .agg(OrderedDict( 5 [(\u0026#39;order_dow\u0026#39;,\u0026#39;mean\u0026#39;), 6 (\u0026#39;order_hour_of_day\u0026#39;,\u0026#39;mean\u0026#39;)]))) 7 8user_features_df.columns = [\u0026#39;user_id\u0026#39;] + user_features 9df_X = df_X.merge(user_features_df, on=\u0026#39;user_id\u0026#39;) 10df_X = df_X.dropna() Đặc trung người dùng - sản phẩm: Bổ sung thêm đặc trưng tung bình trên cột order_down, order_hour_of_day, days_since_prior_order theo người dùng và sản phẩm\n1 2user_prod_features = [\u0026#39;user_product_avg_days_since_prior_order\u0026#39;, 3 \u0026#39;user_product_avg_order_dow\u0026#39;, 4 \u0026#39;user_product_avg_order_hour_of_day\u0026#39;] 5 6user_prod_features_df = (order_products_prior_df.groupby([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;],as_index=False) \\ 7 .agg(OrderedDict( 8 [(\u0026#39;days_since_prior_order\u0026#39;,\u0026#39;mean\u0026#39;), 9 (\u0026#39;order_dow\u0026#39;,\u0026#39;mean\u0026#39;), 10 (\u0026#39;order_hour_of_day\u0026#39;,\u0026#39;mean\u0026#39;)]))) 11 12user_prod_features_df.columns = [\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;] + user_prod_features 13 14df_X = df_X.merge(user_prod_features_df, on=[\u0026#39;user_id\u0026#39;, \u0026#39;product_id\u0026#39;]) 15df_X = df_X.dropna() Đặc trưng độ lệch: Tính độ lệch của của một số đặc trưng so với trung bình của chúng\n1#Create delta columns to compare how users perform against averages 2df_X[\u0026#39;product_total_orders_delta_per_user\u0026#39;] = df_X[\u0026#39;product_total_orders\u0026#39;] - df_X[\u0026#39;user_product_total_orders\u0026#39;] 3 4df_X[\u0026#39;product_avg_add_to_cart_order_delta_per_user\u0026#39;] = df_X[\u0026#39;product_avg_add_to_cart_order\u0026#39;] - \\ 5 df_X[\u0026#39;user_product_avg_add_to_cart_order\u0026#39;] 6 7df_X[\u0026#39;product_avg_order_dow_per_user\u0026#39;] = df_X[\u0026#39;product_avg_order_dow\u0026#39;] - df_X[\u0026#39;user_product_avg_order_dow\u0026#39;] 8 9df_X[\u0026#39;product_avg_order_hour_of_day_per_user\u0026#39;] = df_X[\u0026#39;product_avg_order_hour_of_day\u0026#39;] - \\ 10 df_X[\u0026#39;user_product_avg_order_hour_of_day\u0026#39;] 11 12df_X[\u0026#39;product_avg_days_since_prior_order_per_user\u0026#39;] = df_X[\u0026#39;product_avg_days_since_prior_order\u0026#39;] - \\ 13 df_X[\u0026#39;user_product_avg_days_since_prior_order\u0026#39;] Bổ sung thêm đặc trưng department name\n1f_departments_df = products_df.merge(departments_df, on = \u0026#39;department_id\u0026#39;) 2f_departments_df = f_departments_df[[\u0026#39;product_id\u0026#39;, \u0026#39;department\u0026#39;]] 3 4df_X = df_X.merge(f_departments_df, on = \u0026#39;product_id\u0026#39;) 5df_X = df_X.dropna() 6df_X = pd.concat([df_X, pd.get_dummies(df_X[\u0026#39;department\u0026#39;])], axis=1) 7del df_X[\u0026#39;department\u0026#39;] Chúng ta có tổng cộng 21 department name, vậy chúng ta thêm 21 cột, một cột tương ứng với một department name, ví dụ: alcohol,babies ,bakery, \u0026hellip; Sản phẩm thuộc department name thì sẽ được đánh số 1, không thuộc department name thì đánh số 0.\nHuấn luyện mô hình Chia tập dữ liệu thành 80/20 trong đó 80% là tập train, 20% là tập test. Sử dụng k-fold-cross_validation với k=10\n1 2np.random.seed(99) 3total_users = df_X[\u0026#39;user_id\u0026#39;].unique() 4test_users = np.random.choice(total_users, size=int(total_users.shape[0] * .20), replace=False) 5 6 7 8test_user_sets = [] 9length = len(test_users) 10cv = 10 11 12 13for x in range (0, cv): 14 start = int(x/cv*length) 15 finish = int((x+1)/cv*length) 16 test_user_sets.append(test_users[start:finish]) 17 18cv_f1_scores = [] 19cv_f1_scores_balanced = [] 20cv_f1_scores_10fit = [] 21 22for test_user_set in test_user_sets: 23 df_X_tr, df_X_te = df_X[~df_X[\u0026#39;user_id\u0026#39;].isin(test_user_set)], df_X[df_X[\u0026#39;user_id\u0026#39;].isin(test_user_set)] 24 25 y_tr, y_te = df_X_tr[\u0026#39;in_cart\u0026#39;], df_X_te[\u0026#39;in_cart\u0026#39;] 26 X_tr, X_te = df_X_tr.drop([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;,\u0026#39;latest_cart\u0026#39;,\u0026#39;in_cart\u0026#39;],axis=1), \\ 27 df_X_te.drop([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;,\u0026#39;latest_cart\u0026#39;,\u0026#39;in_cart\u0026#39;],axis=1), \\ 28 29 scaler = MinMaxScaler() 30 X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=X_tr.columns) 31 X_te = pd.DataFrame(scaler.fit_transform(X_te), columns=X_te.columns) 32 33 lr = LogisticRegression(C=10000000) 34 lr_balanced = LogisticRegression(class_weight=\u0026#39;balanced\u0026#39;, C=10000000) 35 lr_10x = LogisticRegression(class_weight={1 : 6, 0 : 1}, C=10000000) 36 37 lr.fit(X_tr, y_tr) 38 cv_f1_scores.append(f1_score(lr.predict(X_te), y_te)) 39 40 lr_balanced.fit(X_tr, y_tr) 41 cv_f1_scores_balanced.append(f1_score(lr_balanced.predict(X_te), y_te)) 42 43 lr_10x.fit(X_tr, y_tr) 44 cv_f1_scores_10fit.append(f1_score(lr_10x.predict(X_te), y_te)) 45 46print(\u0026#34;cv_f1_scores: \u0026#34; +str( np.mean(cv_f1_scores))) 47print(\u0026#34;cv_f1_scores_balanced: \u0026#34;+str(np.mean(cv_f1_scores_balanced))) 48print(\u0026#34;cv_f1_scores_10fit: \u0026#34;+str(np.mean(cv_f1_scores_10fit))) 49 50df_X_tr, df_X_te = df_X[~df_X[\u0026#39;user_id\u0026#39;].isin(test_users)], df_X[df_X[\u0026#39;user_id\u0026#39;].isin(test_users)] 51 52y_tr, y_te = df_X_tr[\u0026#39;in_cart\u0026#39;], df_X_te[\u0026#39;in_cart\u0026#39;] 53X_tr, X_te = df_X_tr.drop([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;,\u0026#39;latest_cart\u0026#39;,\u0026#39;in_cart\u0026#39;],axis=1), \\ 54 df_X_te.drop([\u0026#39;product_id\u0026#39;,\u0026#39;user_id\u0026#39;,\u0026#39;latest_cart\u0026#39;,\u0026#39;in_cart\u0026#39;],axis=1), \\ 55 56lr_10x = LogisticRegression(class_weight={1 : 6, 0 : 1}, C=10000000) 57lr_10x.fit(X_tr, y_tr) 58print(\u0026#34;F1 store all: \u0026#34;+str(f1_score(lr_10x.predict(X_te), y_te))) 1cv_f1_scores: 0.2026889989037295 2cv_f1_scores_balanced: 0.3816810646496983 3cv_f1_scores_10fit: 0.3899595078917494 4 5F1 store all: 0.3808374055616213 Thử in ra hệ số của hàm hồi quy\n1coefficients = pd.DataFrame(lr_10x.coef_, columns = X_tr.columns) 2coefficients = np.exp(coefficients) 3print(coefficients.T) 1user_product_total_orders 1.160475 2product_total_orders 1.077254 3product_avg_add_to_cart_order 0.915343 4user_total_orders 0.983272 5user_avg_cartsize 1.059655 6user_total_products 0.993839 7user_avg_days_since_prior_order 0.993513 8user_product_avg_add_to_cart_order 0.950418 9user_product_order_freq 1.051246 10product_avg_order_dow 0.994744 11product_avg_order_hour_of_day 1.010971 12product_avg_days_since_prior_order 0.994498 13user_avg_order_dow 0.997298 14user_avg_order_hour_of_day 1.012958 15user_product_avg_days_since_prior_order 1.003382 16user_product_avg_order_dow 0.994477 17user_product_avg_order_hour_of_day 1.003457 18product_total_orders_delta_per_user 0.928288 19product_avg_add_to_cart_order_delta_per_user 0.963095 20product_avg_order_dow_per_user 1.000268 21product_avg_order_hour_of_day_per_user 1.007489 22product_avg_days_since_prior_order_per_user 0.991147 23alcohol 0.998866 24babies 1.000313 25bakery 1.003098 26beverages 1.007733 27breakfast 1.000117 28bulk 0.999980 29canned goods 0.995017 30dairy eggs 1.018069 31deli 1.002720 32dry goods pasta 0.997379 33frozen 1.000752 34household 0.992164 35international 0.996822 36meat seafood 1.000340 37missing 1.001953 38other 0.999607 39pantry 0.972038 40personal care 0.992072 41pets 1.000466 42produce 1.017809 43snacks 1.004893 Thử show confusion matrix của dữ liệu:\n1from sklearn.metrics import confusion_matrix 2import seaborn as sns 3import matplotlib.pyplot as plt 4%matplotlib inline 5plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) 6 7def plot_confusion_matrix(cm,title=\u0026#39;Confusion matrix\u0026#39;, cmap=plt.cm.Reds): 8 plt.imshow(cm, interpolation=\u0026#39;nearest\u0026#39;,cmap=cmap) 9 plt.title(title) 10 plt.colorbar() 11 plt.tight_layout() 12 plt.ylabel(\u0026#39;True label\u0026#39;) 13 plt.xlabel(\u0026#39;Predicted label\u0026#39;) 14 15#y_tr=np.ravel(y_tr) 16 17train_acc=lr_10x.score(X_tr, y_tr) 18test_acc=lr_10x.score(X_te, y_te) 19print(\u0026#34;Training Data Accuracy: %0.2f\u0026#34; %(train_acc)) 20print(\u0026#34;Test Data Accuracy: %0.2f\u0026#34; %(test_acc)) 21 22y_true = y_te 23y_pred = lr_10x.predict(X_te) 24 25 26conf = confusion_matrix(y_true, y_pred) 27print(conf) 28 29print (\u0026#39;\\n\u0026#39;) 30print (\u0026#34;Precision: %0.2f\u0026#34; %(conf[1, 1] / (conf[1, 1] + conf[0, 1]))) 31print (\u0026#34;Recall: %0.2f\u0026#34;% (conf[1, 1] / (conf[1, 1] + conf[1, 0]))) 32 33cm=confusion_matrix(y_true, y_pred, labels=[0, 1]) 34 35plt.figure() 36plot_confusion_matrix(cm) Kết quả\n1Training Data Accuracy: 0.83 2Test Data Accuracy: 0.83 3[[1236979 190126] 4 [ 78107 82493]] 5 6 7Precision: 0.30 8Recall: 0.51 Show đường cong ROC của dữ liệu\n1 2from sklearn.metrics import roc_curve, auc 3 4y_score = lr_10x.predict_proba(X_te)[:,1] 5 6fpr, tpr,_ = roc_curve(y_te, y_score) 7roc_auc = auc(fpr, tpr) 8 9plt.figure() 10# Plotting our Baseline.. 11plt.plot([0,1],[0,1], linestyle=\u0026#39;--\u0026#39;, color = \u0026#39;black\u0026#39;) 12plt.plot(fpr, tpr, color = \u0026#39;green\u0026#39;) 13plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) 14plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) 15plt.gca().set_aspect(\u0026#39;equal\u0026#39;, adjustable=\u0026#39;box\u0026#39;) Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Nov 13, 2018","img":"","permalink":"/blog/2018-11-13-instacart-market-basket-analysis/","series":null,"tags":["Machine learning","Deeplearning","instacart","Giỏ hàng","Đơn hàng"],"title":"Phân Tích Giỏ Hàng Của Website Instacart"},{"categories":null,"content":" Lời mở đầu Dẫn nhập Phân tích dữ liệu Scale dữ liệu Phân chia tập train và test. Xây dựng mô hình sử dụng keras Lời mở đầu Ở bài viết này, mình sẽ xây dựng mô hình hơn giản để áp dụng vào tập dữ liệu giá chứng khoáng. Mục tiêu của bài này là chúng ta sẽ dự đoán chỉ số S\u0026amp;P 500 sử dụng LSTM. Các bạn có nhu cầu tìm hiểu thêm về chỉ số sp 500 có thể đọc thêm ở https://vi.wikipedia.org/wiki/S%26P_500. Đây là một ứng dụng nhỏ, không có ý nghĩa nhiều ở thực tế do khi phân tích chứng khoán, ta còn xét thêm rất nhiều yếu tố phụ nữa. Mô hình này thực chất chỉ là một trong những mô hình chơi chơi.\nDẫn nhập Phân tích dữ liệu Các bạn có thể download dữ liệu ở https://github.com/AlexBlack2202/alexmodel/blob/master/GSPC.csv\nĐầu tiên, như thường lệ, chúng ta sẽ import các thư viện cần thiết để sử dụng.\n1import numpy as np # linear algebra 2import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) 3 4from subprocess import check_output 5from keras.layers.core import Dense, Activation, Dropout 6from keras.layers.recurrent import LSTM 7from keras.models import Sequential 8from sklearn.cross_validation import train_test_split 9import time #helper libraries 10from sklearn.preprocessing import MinMaxScaler 11import matplotlib.pyplot as plt 12from numpy import newaxis Đọc dữ liệu lên:\n1 2file_name =\u0026#39;GSPC.csv\u0026#39; 3 4prices_dataset = pd.read_csv(file_name, header=0) 5 6`` 7 8Xem kích thước của dữ liệu: 9 10```python 11print(prices_dataset.shape) 1(17114, 7) Kết quả là ta có 17114 ngàn dòng và 7 cột. Thử show 10 row đầu tiên của dữ liệu lên xem như thế nào.\n1print(prices_dataset.head()) 1 Date Open High Low Close Adj Close Volume 20 1950-11-09 19.790001 19.790001 19.790001 19.790001 19.790001 1760000 31 1950-11-10 19.940001 19.940001 19.940001 19.940001 19.940001 1640000 42 1950-11-13 20.010000 20.010000 20.010000 20.010000 20.010000 1630000 53 1950-11-14 19.860001 19.860001 19.860001 19.860001 19.860001 1780000 64 1950-11-15 19.820000 19.820000 19.820000 19.820000 19.820000 1620000 Cột đầu tiên là ngày, sau đó là giá mở cửa, giá giao dịch cao nhất, giá giao dịch thấp nhât, giá đóng cử, giá đóng cửa đã điều chỉnh, khối lượng giao dịch.\nPlot đồ thị của mã SP500 lên:\n1import matplotlib.pyplot as plt 2 3plt.plot(prices_dataset.Open.values, color=\u0026#39;red\u0026#39;, label=\u0026#39;open\u0026#39;) 4plt.plot(prices_dataset.Close.values, color=\u0026#39;green\u0026#39;, label=\u0026#39;close\u0026#39;) 5plt.plot(prices_dataset.Low.values, color=\u0026#39;blue\u0026#39;, label=\u0026#39;low\u0026#39;) 6plt.plot(prices_dataset.High.values, color=\u0026#39;black\u0026#39;, label=\u0026#39;high\u0026#39;) 7plt.title(\u0026#39;stock price\u0026#39;) 8plt.xlabel(\u0026#39;time [days]\u0026#39;) 9plt.ylabel(\u0026#39;price\u0026#39;) 10plt.legend(loc=\u0026#39;best\u0026#39;) 11plt.show() Hình với số lượng hơi nhiều nên khó phân biệt được giá trị của dữ liệu, chúng ta thử show đồ thị của 50 ngày cuối cùng trong dữ liệu.\n1prices_dataset_tail_50 = prices_dataset.tail(50) 2 3plt.plot(prices_dataset_tail_50.Open.values, color=\u0026#39;red\u0026#39;, label=\u0026#39;open\u0026#39;) 4plt.plot(prices_dataset_tail_50.Close.values, color=\u0026#39;green\u0026#39;, label=\u0026#39;close\u0026#39;) 5plt.plot(prices_dataset_tail_50.Low.values, color=\u0026#39;blue\u0026#39;, label=\u0026#39;low\u0026#39;) 6plt.plot(prices_dataset_tail_50.High.values, color=\u0026#39;black\u0026#39;, label=\u0026#39;high\u0026#39;) 7plt.title(\u0026#39;stock price\u0026#39;) 8plt.xlabel(\u0026#39;time [days]\u0026#39;) 9plt.ylabel(\u0026#39;price\u0026#39;) 10plt.legend(loc=\u0026#39;best\u0026#39;) 11plt.show() Hình ảnh trông khá rõ ràng và trực quan hơn rất nhiều.\nChúng ta sẽ bỏ đi cột DATE,Adj Close,Volume đi. Các cột đó không cần thiết cho quá trình dự đoán.\n1 2prices_dataset_dropout = prices_dataset.drop([\u0026#39;Date\u0026#39;,\u0026#39;Adj Close\u0026#39;,\u0026#39;Volume\u0026#39;], 1) Scale dữ liệu Khi sử dụng ANN, chúng ta thông thường sẽ scale dữ liệu input về đoạn [-1,1]. Trong python, thư viện sklearn đã hỗ trợ cho chúng ta sẵn các hàm scale dữ liệu cần thiết.\n1# Scale data 2def normalize_data(df): 3 min_max_scaler = MinMaxScaler() 4 df[\u0026#39;Open\u0026#39;] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1)) 5 df[\u0026#39;High\u0026#39;] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1)) 6 df[\u0026#39;Low\u0026#39;] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1)) 7 df[\u0026#39;Close\u0026#39;] = min_max_scaler.fit_transform(df.Close.values.reshape(-1,1)) 8 return df 9 10prices_dataset_norm = normalize_data(prices_dataset_dropout) Phân chia tập train và test. Chúng ta sẽ chia dữ liệu thành 2 phần với 80% là train và 20% còn lại là test. Chọn seq_len=20, các bạn có thể test với các seq len khác, và sau đó chuyển dữ liệu về dạng numpy array để dễ dàng thực hiện các phép chuyển đổi.\n1 2def generate_data(stock_ds, seq_len): 3 data_raw = stock_ds.as_matrix() 4 data = [] 5 6 # create all possible sequences of length seq_len 7 for index in range(len(data_raw) - seq_len): 8 data.append(data_raw[index: index + seq_len]) 9 return data 10 11#data as numpy array 12def generate_train_test(data_ds,split_percent=0.8): 13 print(len(data_ds)) 14 data = np.asarray(data_ds) 15 16 data_size = len(data) 17 train_end = int(np.floor(split_percent*data_size)) 18 19 x_train = data[:train_end,:-1,:] 20 y_train = data[:train_end,-1,:] 21 22 23 24 x_test = data[train_end:,:-1,:] 25 y_test = data[train_end:,-1,:] 26 27 return [x_train, y_train, x_test, y_test] 28 29 30 31seq_len = 20 # choose sequence length 32 33seq_prices_dataset = generate_data(prices_dataset_norm,seq_len) 34 35x_train, y_train, x_test, y_test = generate_train_test(seq_prices_dataset, 0.8) 36 37print(\u0026#39;x_train.shape = \u0026#39;,x_train.shape) 38print(\u0026#39;y_train.shape = \u0026#39;, y_train.shape) 39print(\u0026#39;x_test.shape = \u0026#39;, x_test.shape) 40print(\u0026#39;y_test.shape = \u0026#39;,y_test.shape) Kết quả:\n1x_train.shape = (13675, 19, 4) 2y_train.shape = (13675, 4) 3x_test.shape = (3419, 19, 4) 4y_test.shape = (3419, 4) Xây dựng mô hình sử dụng keras Ở đây mình sử dụng keras xây dựng mô hình ANN. Mô hình của mình xây dựng gồm:\n1model = Sequential() 2 3model.add(LSTM( 4 input_dim=4, 5 output_dim=50, 6 return_sequences=True)) 7model.add(Dropout(0.2)) 8 9model.add(LSTM( 10 100, 11 return_sequences=False)) 12model.add(Dropout(0.2)) 13 14model.add(Dense( 15 output_dim=4)) 16model.add(Activation(\u0026#39;linear\u0026#39;)) 17 18 19 20model.compile(loss=\u0026#39;mean_squared_error\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) 21checkpoint = ModelCheckpoint(filepath=\u0026#39;sp500_stockperdict.h5\u0026#39;, verbose=1, save_best_only=True) 22hist = model.fit(x_train, y_train, epochs=300, batch_size=128, verbose=1, callbacks=[checkpoint], validation_split=0.2) Sau một thời gian chạy, mình cũng thu được model. Các bạn quan tâm có thể download model của mình huấn luyện được tại https://drive.google.com/open?id=1ImHQM9yWmOjpF5tjmSI9oqAi5BORa9Rs . Tiến hành plot dữ liệu tập test lên xem kết quả như thế nào.\n1 2model =load_model(\u0026#39;sp500_stockperdict.h5\u0026#39;) 3 4 5y_hat = model.predict(x_test) 6 7ft = 3 # 0 = open, 1 = highest, 2 =lowest , 3 = close 8 9plt.plot( y_test[:,ft], color=\u0026#39;blue\u0026#39;, label=\u0026#39;target\u0026#39;) 10 11plt.plot( y_hat[:,ft], color=\u0026#39;red\u0026#39;, label=\u0026#39;prediction\u0026#39;) 12 13plt.title(\u0026#39;future stock prices\u0026#39;) 14plt.xlabel(\u0026#39;time [days]\u0026#39;) 15plt.ylabel(\u0026#39;normalized price\u0026#39;) 16plt.legend(loc=\u0026#39;best\u0026#39;) 17 18plt.show() 19 20from sklearn.metrics import mean_squared_error 21 22# 0 = open, 1 = highest, 2 =lowest , 3 = close 23print(\u0026#34;open error: \u0026#34;) 24print(mean_squared_error(y_test[:,0], y_hat[ :,0])) 25 26print(\u0026#34;highest error: \u0026#34;) 27print(mean_squared_error(y_test[:,1], y_hat[ :,1])) 28 29print(\u0026#34;lowest error: \u0026#34;) 30print(mean_squared_error(y_test[:,2], y_hat[ :,2])) 31 32print(\u0026#34;close error: \u0026#34;) 33print(mean_squared_error(y_test[:,3], y_hat[ :,3])) 1open error: 20.0009739211460315127 3highest error: 40.0010539412808401607 5lowest error: 60.0010066509540756113 7close error: 80.0010840500965408758 Hiện đã có bản tensorflow 2 có tích hợp keras, mình update lại code\n1 2from re import T 3import numpy as np 4# linear algebra 5import pandas as pd 6from tensorflow.keras.models import Sequential 7from tensorflow.keras.layers import Dense 8from tensorflow.keras.layers import LSTM 9from sklearn.preprocessing import MinMaxScaler 10import tensorflow as tf 11import joblib 12 13import matplotlib 14matplotlib.use(\u0026#39;TkAgg\u0026#39;) 15import matplotlib.pyplot as plt 16 17 18file_name =\u0026#39;GSPC.csv\u0026#39; 19 20 21prices_dataset = pd.read_csv(file_name, header=0) 22 23 24# prices_dataset_dropout = prices_dataset.drop([\u0026#39;Date\u0026#39;,\u0026#39;Adj Close\u0026#39;,\u0026#39;Volume\u0026#39;], 1) 25prices_dataset_dropout=prices_dataset.reset_index()[\u0026#39;Close\u0026#39;] 26 27 28scaler=MinMaxScaler(feature_range=(0,1)) 29prices_dataset_norm=scaler.fit_transform(np.array(prices_dataset_dropout).reshape(-1,1)) 30joblib.dump(scaler, \u0026#39;scaler.alex\u0026#39;) 31 32 33print(prices_dataset_norm[:10]) 34 35 36def generate_data(stock_ds, seq_len,predict_next_t): 37 dataX, dataY = [], [] 38 for i in range(len(stock_ds)-seq_len-1): 39 dataX.append(stock_ds[i:(i+seq_len)]) 40 dataY.append(stock_ds[i + seq_len+predict_next_t]) 41 return np.array(dataX), np.array(dataY) 42 43#data as numpy array 44def generate_train_test(data_x,data_y,split_percent=0.8): 45 46 train_end = int(np.floor(split_percent*data_x.shape[0])) 47 48 x_train,x_test=data_x[:train_end,:],data_x[train_end:,:] 49 y_train,y_test = data_y[:train_end],data_y[train_end:] 50 return x_train,y_train,x_test,y_test 51 52 53 54seq_len = 100 # choose sequence length 55predict_next_t = 1 # 0 is next date, 1 is 2 next date 56 57data_x, data_y = generate_data(prices_dataset_norm,seq_len,predict_next_t) 58 59x_train,y_train,x_test,y_test = generate_train_test(data_x,data_y, 0.8) 60 61 62x_train =x_train.reshape(x_train.shape[0],x_train.shape[1] , 1) 63x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1) 64print(\u0026#39;x_train.shape = \u0026#39;,x_train.shape) 65print(\u0026#39;y_train.shape = \u0026#39;, y_train.shape) 66print(\u0026#39;x_test.shape = \u0026#39;, x_test.shape) 67print(\u0026#39;y_test.shape = \u0026#39;,y_test.shape) 68 69 70 71model = Sequential() 72 73 # input_dim=4, 74 # output_dim=50, 75model.add(LSTM(units=100,input_shape=x_train.shape[1:], 76 return_sequences=True)) 77 78model.add(LSTM( 79 100, 80 return_sequences=False)) 81model.add(Dense(1)) 82 83 84model.compile(loss=\u0026#39;mean_squared_error\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) 85checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\u0026#39;my_model_stock.h5\u0026#39;, verbose=1, save_best_only=True) 86hist = model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=1, callbacks=[checkpoint], validation_split=0.2) 87 88from tensorflow.keras.models import load_model 89print(\u0026#39;load model\u0026#39;) 90model =load_model(\u0026#39;my_model_stock.h5\u0026#39;) 91 92print(\u0026#39;predict\u0026#39;) 93y_test = y_test.reshape(y_test.shape[0]) 94# train_predict=model.predict(x_train) 95test_predict=model.predict(x_test) 96print(\u0026#39;invert\u0026#39;) 97print(y_test.shape) 98# train_predict=scaler.inverse_transform(train_predict) 99 100# scaler = joblib.load(\u0026#39;scaler.alex\u0026#39;) 101 102y_hat=scaler.inverse_transform(test_predict) 103y_test=scaler.inverse_transform(y_test.reshape(-1, 1)) 104print(y_hat.shape) 105# y_hat = model.predict(x_test) 106# import matplotlib 107# matplotlib.use(\u0026#39;GTKAgg\u0026#39;) 108# print(\u0026#39;plot\u0026#39;) 109 110plt.plot( y_test, color=\u0026#39;blue\u0026#39;, label=\u0026#39;target\u0026#39;) 111 112plt.plot( y_hat, color=\u0026#39;red\u0026#39;, label=\u0026#39;prediction\u0026#39;) 113print(\u0026#39;plot complete\u0026#39;) 114plt.title(\u0026#39;future stock prices\u0026#39;) 115plt.xlabel(\u0026#39;time [days]\u0026#39;) 116plt.ylabel(\u0026#39;normalized price\u0026#39;) 117plt.legend(loc=\u0026#39;best\u0026#39;) 118print(\u0026#39;plot show\u0026#39;) 119plt.savefig(\u0026#34;mygraph.png\u0026#34;) 120plt.show() Kết quả của mô hình trông khá tốt, về hình dạng thì khá tương đồng với kết quả. Chúng ta có thể cải tiến model bằng cách nâng số lượng layer/ hidden node.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Nov 10, 2018","img":"","permalink":"/blog/2018-11-10-stock-prediction_lsmt/","series":null,"tags":["Machine learning","Deeplearning","stock prediction","chứng khoán"],"title":"Dự Đoán Giá Chứng Khoán SP500 Sử Dụng LSTM"},{"categories":null,"content":" Lời mở đầu Dẫn nhập Phân tích dữ liệu Phân chia tập train và test. Scale dữ liệu Xây dựng mô hình sử dụng keras Lời mở đầu Ở bài viết này, mình sẽ xây dựng mô hình hơn giản để áp dụng vào tập dữ liệu giá chứng khoán. Mục tiêu của bài này là chúng ta sẽ dự đoán chỉ số S\u0026amp;P 500 dựa trên chỉ số của 500 mã chứng khoán. Các bạn có nhu cầu tìm hiểu thêm về chỉ số sp 500 có thể đọc thêm ở https://vi.wikipedia.org/wiki/S%26P_500. Đây là một ứng dụng nhỏ, không có ý nghĩa nhiều ở thực tế do khi phân tích chứng khoán, ta còn xét thêm rất nhiều yếu tố phụ nữa. Mô hình này thực chất chỉ là một trong những mô hình chơi chơi.\nDẫn nhập Phân tích dữ liệu Các bạn có thể download dữ liệu ở https://drive.google.com/open?id=1UTlj5Ced-yj6RBRVc6bBM6IWMjfQR3GR.\nĐầu tiên, chúng ta sẽ dùng pandas để load mô hình lên:\n1import pandas as pd 2 3# Import data 4data = pd.read_csv(\u0026#39;data_stocks.csv\u0026#39;) Xem kích thước của dữ liệu:\n1print(data.shape) 1(41266, 502) Kết quả là ta có hơn 40 ngàn dòng và 502 cột. Thử show 10 row đầu tiên của dữ liệu lên xem như thế nào.\n1print(data.head()) 1 DATE SP500 NASDAQ.AAL NASDAQ.AAPL NASDAQ.ADBE NASDAQ.ADI \\ 20 1491226200 2363.6101 42.3300 143.6800 129.6300 82.040 31 1491226260 2364.1001 42.3600 143.7000 130.3200 82.080 42 1491226320 2362.6799 42.3100 143.6901 130.2250 82.030 53 1491226380 2364.3101 42.3700 143.6400 130.0729 82.000 64 1491226440 2364.8501 42.5378 143.6600 129.8800 82.035 7 8 NASDAQ.ADP NASDAQ.ADSK NASDAQ.AKAM NASDAQ.ALXN ... NYSE.WYN \\ 90 102.2300 85.2200 59.760 121.52 ... 84.370 101 102.1400 85.6500 59.840 121.48 ... 84.370 112 102.2125 85.5100 59.795 121.93 ... 84.585 123 102.1400 85.4872 59.620 121.44 ... 84.460 134 102.0600 85.7001 59.620 121.60 ... 84.470 14 15 NYSE.XEC NYSE.XEL NYSE.XL NYSE.XOM NYSE.XRX NYSE.XYL NYSE.YUM \\ 160 119.035 44.40 39.88 82.03 7.36 50.22 63.86 171 119.035 44.11 39.88 82.03 7.38 50.22 63.74 182 119.260 44.09 39.98 82.02 7.36 50.12 63.75 193 119.260 44.25 39.99 82.02 7.35 50.16 63.88 204 119.610 44.11 39.96 82.03 7.36 50.20 63.91 21 22 NYSE.ZBH NYSE.ZTS 230 122.000 53.350 241 121.770 53.350 252 121.700 53.365 263 121.700 53.380 274 121.695 53.240 Cột đầu tiên là ngày, sau đó là mã chứng khoán. Chúng ta có tổng cộng 500 mã chứng khoán và 1 chỉ số. Để ý cột Date, ta thấy giá trị đầu tiên là 1491226200, giá trị thứ 2 là 1491226260, giá trị thứ 3 là 1491226320, mỗi giá trị cách nhau 60. Chuyển đổi số 1491226200 sang dạng datetime thì ra giá trị Monday, April 3, 2017 1:30:00 PM giờ GMT, tương tự số 1491226260 ra Monday, April 3, 2017 1:31:00 PM giờ GMT. Ta có thể suy luận ra là giá trị giao dịch lưu theo từng phút một (khoảng interval là 60 giây), và dữ liệu chúng ta có bắt đầu vào 3 tháng 4 năm 2017.\nPlot đồ thị của mã SP500 lên:\n1import matplotlib.pyplot as plt 2 3plt.plot(data[\u0026#39;SP500\u0026#39;]) 4plt.show() 1Notes: Ở đây có một lưu ý nhỏ nhưng rất quan trọng. Đó là tại thời điểm phút thứ t lưu trữ giá trị sp500 của thời điểm phút thứ t+1. Ví dụ với chỉ số sp500, dòng đầu tiên ta thấy là 1491226200 2363.6101, nghĩa là giá thực tế của thời điểm 1491226260 là 2363.6101. Do bài toán của chúng ta là dữ đoán giá tương lại, nên tại thời điểm hiện tại ta sẽ dự đoán giá 1 phút sau sẽ bằng bao nhiêu. Và tập dữ liệu đã tự động dịch chuyển giá trị lên 1 phút cho chúng ta đỡ mất công làm. Còn giá của 500 cỗ phiếu còn lại vẫn là giá tại thời điểm t Phân chia tập train và test. Chúng ta sẽ chia dữ liệu thành 2 phần với 80% là train và 20% còn lại là test. Do tích chất của dữ liệu là time serial nên chúng ta không thể làm thay đổi thứ tự dữ liệu.\nChúng ta sẽ bỏ đi cột DATE đầu tiên, và sau đó chuyển dữ liệu về dạng numpy array để dễ dàng thực hiện các phép chuyển đổi.\n1data_ = data_raw.drop([\u0026#39;DATE\u0026#39;], 1) 2 3data = data_.values 4# Training and test data 5train_start = 0 6train_end = int(np.floor(0.8*n)) 7test_start = train_end 8test_end = n 9data_train = data[ :train_end] 10data_test = data[train_end:] Scale dữ liệu Khi sử dụng ANN, chúng ta thông thường sẽ scale dữ liệu input về đoạn [-1,1]. Trong python, thư viện sklearn đã hỗ trợ cho chúng ta sẵn các hàm scale dữ liệu cần thiết.\n1# Scale data 2from sklearn.preprocessing import MinMaxScaler 3scaler = MinMaxScaler() 4data_train = scaler.fit_transform(data_train) 5data_test = scaler.transform(data_test) 6# Build X and y 7X_train = data_train[:, 1:] 8y_train = data_train[:, 0] 9X_test = data_test[:, 1:] 10y_test = data_test[:, 0] Mình cần dự đoán giá trị của chỉ số sp 500, nên giá trị của sp500 sẽ là cái mình cần dự đoán, chính là cột đầu tiên, còn 500 cái còn lại là input của mình.\nXây dựng mô hình sử dụng keras Ở đây mình sử dụng keras xây dựng mô hình ANN. Mô hình của mình xây dựng gồm\n1from keras.models import Sequential 2from keras.layers.core import Dense, Dropout, Activation 3from keras.callbacks import ModelCheckpoint 4from keras.optimizers import SGD 5 6import os 7os.environ[\u0026#34;CUDA_DEVICE_ORDER\u0026#34;]=\u0026#34;PCI_BUS_ID\u0026#34; 8# The GPU id to use, usually either \u0026#34;0\u0026#34; or \u0026#34;1\u0026#34; 9os.environ[\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;]=\u0026#34;0\u0026#34; 10# create model 11model = Sequential() 12model.add(Dense(2048, input_dim=input_dim,kernel_initializer=\u0026#39;normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)) 13model.add(Dense(1024,kernel_initializer=\u0026#39;normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)) 14model.add(Dense(512,kernel_initializer=\u0026#39;normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)) 15model.add(Dense(256,kernel_initializer=\u0026#39;normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)) 16model.add(Dense(128,kernel_initializer=\u0026#39;normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)) 17model.add(Dense(1,kernel_initializer=\u0026#39;normal\u0026#39;)) 18 19 20 21model.compile(loss=\u0026#39;mse\u0026#39;, optimizer=\u0026#39;rmsprop\u0026#39;) 22checkpoint = ModelCheckpoint(filepath=\u0026#39;my_model3.h5\u0026#39;, verbose=1, save_best_only=True) 23model.fit(X_train, y_train, epochs=100, batch_size=256, verbose=1, callbacks=[checkpoint], validation_split=0.2) Sau một thời gian chạy, mình cũng thu được model. Các bạn quan tâm có thể download model của mình huấn luyện được tại https://drive.google.com/open?id=1BLQZbcADfnLqzIHlkgpsqZBlhljBp1Eb . Tiến hành plot dữ liệu tập test lên xem kết quả như thế nào.\n1 2yhat = model.predict(X_test) 3 4 5x = np.arange(len(yhat)) 6 7plt.plot(x, y_test) 8plt.plot(x, yhat) 9plt.legend([\u0026#39;real\u0026#39;, \u0026#39;test\u0026#39;], loc=\u0026#39;upper right\u0026#39;) 10plt.show() 11 12 13from sklearn.metrics import mean_squared_error 14 15print(\u0026#34;mse: \u0026#34;+ str(mean_squared_error(y_test, yhat))) 1mse: 0.0014582120695331884 Kết quả của mô hình tạm chấp nhận được, về hình dạng thì khá tương đồng với kết quả. Chúng ta có thể cải tiến model bằng cách nâng số lượng layter/ hidden node, hoặc thêm dropout. Hoặc có thể thay thế mô hình bằng RNN. Chúng ta sẽ đề cập đến mô hình RNN trong bài viết sau.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Nov 3, 2018","img":"","permalink":"/blog/2018-11-03-stock-prediction/","series":null,"tags":["Machine learning","Deep learning","stock prediction"],"title":"Dự Đoán Chứng Khoán Sử Dụng Tensorflow"},{"categories":null,"content":" Lời mở đầu Dẫn nhập Lời mở đầu Sau khi thực hiện bài phân loại chó mèo bằng keras, mình phát hiện rằng keras có hỗ trợ rất nhiều thuật toán tối ưu hoá https://keras.io/optimizers/. Nhân dịp rãnh rỗi, mình sẽ tổng hợp lại một vài thuật toán mà keras hỗ trợ.\nDẫn nhập Tại thời điểm hiện tại, Gradient descent là một trong những thuật toán phổ biến được sử dụng để tối ưu hoá mạng neural networks. Các thư viện DNN sẽ implement kèm theo một vài biến thể của gradient descent giúp người dùng dễ dàng sử dụng công cụ hơn.\nBài viết này mình sẽ cập nhật dần đến khi hoàn thiện.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Oct 29, 2018","img":"","permalink":"/blog/2018-11-01-overview-of-gradient-descent-optimization-algorithm/","series":null,"tags":["Machine learning","Deeplearning"],"title":"Overview of Gradient Descent Optimization Algorithm"},{"categories":null,"content":" Lời mở đầu Thực hiện Quậy phá mô hình Quậy phá 1: Mở đóng băng một số lớp cuối và train trên chúng. Quậy phá 2: Chỉ sử dụng 72 lớp đầu tiên của inception. Lời mở đầu Bài toán phân loại chó mèo là bài toán khá cũ tại thời điểm hiện tại. Tuy nhiên, đối với các bạn mới bước chân vào con đường machine learning thì đây là một trong những bài toán cơ bản để các bạn thực hành sử dụng và tìm hiểu thư viện mà mình đang có. Ở đây, chúng ta sẽ sử dụng pretrain model có sẵn của kares áp dụng trên tập dữ liệu. Các bạn có thể download tập dữ liệu train và test ở địa chỉ https://www.kaggle.com/c/dogs-vs-cats/download/train.zip và https://www.kaggle.com/c/dogs-vs-cats/download/test1.zip để bắt đầu thực hiện.\nThực hiện Sau khi giải nén dữ liệu, ta thấy rằng thư mục train có cấu trúc đặt trên sẽ là label.số thứ tự.jpg. Trong đó label có thể là dog hoặc cat, số thứ tự tăng dần từ 0 đến \u0026hellip;. 12499. Để đảm bảo đúng với mô hình, ta phải cấu trúc lại dữ liệu thành dạng.\n1data_dir/classname1/*.* 2data_dir/classname2/*.* 3... Vì vậy, ta tạo ra thư mục cat và copy những file bắt đầu bằng cat.* vào thư mục cat. Làm tương tự với thư mục dog.\nĐầu tiên, các bạn download file pretrain model, giải nén ra và để ở đâu đó trong ổ cứng của máy bạn. Đường dẫn file pretrain model các bạn có thể download ở http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz. Các bạn có thể download các file pretrain khác nếu có hứng thú tìm hiểu.\nTiếp theo, chúng ta sẽ load dataset lên và tranform nó để đưa vào huấn luyện.\n1import sys 2import os 3from collections import defaultdict 4import numpy as np 5import scipy.misc 6 7 8def preprocess_input(x0): 9 x = x0 / 255. 10 x -= 0.5 11 x *= 2. 12 return x 13 14 15def reverse_preprocess_input(x0): 16 x = x0 / 2.0 17 x += 0.5 18 x *= 255. 19 return x 20 21 22def dataset(base_dir, n): 23 print(\u0026#34;base dir: \u0026#34;+base_dir) 24 print(\u0026#34;n: \u0026#34;+str(n)) 25 n = int(n) 26 d = defaultdict(list) 27 for root, subdirs, files in os.walk(base_dir): 28 for filename in files: 29 file_path = os.path.join(root, filename) 30 assert file_path.startswith(base_dir) 31 32 suffix = file_path[len(base_dir):] 33 34 suffix = suffix.lstrip(\u0026#34;/\u0026#34;) 35 suffix = suffix.lstrip(\u0026#34;\\\\\u0026#34;) 36 if(suffix.find(\u0026#39;/\u0026#39;)\u0026gt;-1): #linux 37 label = suffix.split(\u0026#34;/\u0026#34;)[0] 38 else: #window 39 label = suffix.split(\u0026#34;\\\\\u0026#34;)[0] 40 d[label].append(file_path) 41 print(\u0026#34;walk directory complete\u0026#34;) 42 tags = sorted(d.keys()) 43 44 processed_image_count = 0 45 useful_image_count = 0 46 47 X = [] 48 y = [] 49 50 for class_index, class_name in enumerate(tags): 51 filenames = d[class_name] 52 for filename in filenames: 53 processed_image_count += 1 54 if processed_image_count%100 ==0: 55 print(class_name+\u0026#34;\\tprocess: \u0026#34;+str(processed_image_count)+\u0026#34;\\t\u0026#34;+str(len(d[class_name]))) 56 img = scipy.misc.imread(filename) 57 height, width, chan = img.shape 58 assert chan == 3 59 aspect_ratio = float(max((height, width))) / min((height, width)) 60 if aspect_ratio \u0026gt; 2: 61 continue 62 # We pick the largest center square. 63 centery = height // 2 64 centerx = width // 2 65 radius = min((centerx, centery)) 66 img = img[centery-radius:centery+radius, centerx-radius:centerx+radius] 67 img = scipy.misc.imresize(img, size=(n, n), interp=\u0026#39;bilinear\u0026#39;) 68 X.append(img) 69 y.append(class_index) 70 useful_image_count += 1 71 print(\u0026#34;processed %d, used %d\u0026#34; % (processed_image_count, useful_image_count)) 72 73 X = np.array(X).astype(np.float32) 74 #X = X.transpose((0, 3, 1, 2)) 75 X = preprocess_input(X) 76 y = np.array(y) 77 78 perm = np.random.permutation(len(y)) 79 X = X[perm] 80 y = y[perm] 81 82 print(\u0026#34;classes:\u0026#34;,end=\u0026#34; \u0026#34;) 83 for class_index, class_name in enumerate(tags): 84 print(class_name, sum(y==class_index),end=\u0026#34; \u0026#34;) 85 print(\u0026#34;X shape: \u0026#34;,X.shape) 86 87 return X, y, tags Đoạn code trên khá đơn giản và dễ hiểu. Lưu ý ở đây là với những bức ảnh có tỷ lệ width và height \u0026gt; 2 thì mình sẽ loại chúng ra khỏi tập dữ liệu.\nTiếp theo, chúng ta sẽ xây dựng mô hình dựa trên mô hình InceptionV3 có sẵn, thêm một lớp softmax ở cuối để phân lớp dữ liệu, chúng ta sẽ huấn luyện lớp softmax này. Các lớp trước lớp softmax này sẽ bị đóng băng (không cập nhật trọng số trong quá trình huấn luyện ).\n1 2# create the base pre-trained model 3def build_model(nb_classes): 4 base_model = InceptionV3(weights=\u0026#39;imagenet\u0026#39;, include_top=False) 5 6 # add a global spatial average pooling layer 7 x = base_model.output 8 x = GlobalAveragePooling2D()(x) 9 # let\u0026#39;s add a fully-connected layer 10 x = Dense(1024, activation=\u0026#39;relu\u0026#39;)(x) 11 # and a logistic layer 12 predictions = Dense(nb_classes, activation=\u0026#39;softmax\u0026#39;)(x) 13 14 # this is the model we will train 15 model = Model(inputs=base_model.input, outputs=predictions) 16 17 # first: train only the top layers (which were randomly initialized) 18 # i.e. freeze all convolutional InceptionV3 layers 19 for layer in base_model.layers: 20 layer.trainable = False 21 22 # compile the model (should be done *after* setting layers to non-trainable) 23 print(\u0026#34;starting model compile\u0026#34;) 24 compile(model) 25 print(\u0026#34;model compile done\u0026#34;) 26 return model Visualize một chút xíu về kiến trúc inceptionV3 mình đang dùng.\n1__________________________________________________________________________________________________ 2Layer (type) Output Shape Param # Connected to 3================================================================================================== 4input_1 (InputLayer) (None, None, None, 3 0 5__________________________________________________________________________________________________ 6conv2d_1 (Conv2D) (None, None, None, 3 864 input_1[0][0] 7__________________________________________________________________________________________________ 8batch_normalization_1 (BatchNor (None, None, None, 3 96 conv2d_1[0][0] 9__________________________________________________________________________________________________ 10activation_1 (Activation) (None, None, None, 3 0 batch_normalization_1[0][0] 11__________________________________________________________________________________________________ 12conv2d_2 (Conv2D) (None, None, None, 3 9216 activation_1[0][0] 13__________________________________________________________________________________________________ 14batch_normalization_2 (BatchNor (None, None, None, 3 96 conv2d_2[0][0] 15__________________________________________________________________________________________________ 16activation_2 (Activation) (None, None, None, 3 0 batch_normalization_2[0][0] 17__________________________________________________________________________________________________ 18conv2d_3 (Conv2D) (None, None, None, 6 18432 activation_2[0][0] 19__________________________________________________________________________________________________ 20batch_normalization_3 (BatchNor (None, None, None, 6 192 conv2d_3[0][0] 21__________________________________________________________________________________________________ 22activation_3 (Activation) (None, None, None, 6 0 batch_normalization_3[0][0] 23__________________________________________________________________________________________________ 24max_pooling2d_1 (MaxPooling2D) (None, None, None, 6 0 activation_3[0][0] 25__________________________________________________________________________________________________ 26conv2d_4 (Conv2D) (None, None, None, 8 5120 max_pooling2d_1[0][0] 27__________________________________________________________________________________________________ 28batch_normalization_4 (BatchNor (None, None, None, 8 240 conv2d_4[0][0] 29__________________________________________________________________________________________________ 30activation_4 (Activation) (None, None, None, 8 0 batch_normalization_4[0][0] 31__________________________________________________________________________________________________ 32conv2d_5 (Conv2D) (None, None, None, 1 138240 activation_4[0][0] 33__________________________________________________________________________________________________ 34batch_normalization_5 (BatchNor (None, None, None, 1 576 conv2d_5[0][0] 35__________________________________________________________________________________________________ 36activation_5 (Activation) (None, None, None, 1 0 batch_normalization_5[0][0] 37__________________________________________________________________________________________________ 38max_pooling2d_2 (MaxPooling2D) (None, None, None, 1 0 activation_5[0][0] 39__________________________________________________________________________________________________ 40conv2d_9 (Conv2D) (None, None, None, 6 12288 max_pooling2d_2[0][0] 41__________________________________________________________________________________________________ 42batch_normalization_9 (BatchNor (None, None, None, 6 192 conv2d_9[0][0] 43__________________________________________________________________________________________________ 44activation_9 (Activation) (None, None, None, 6 0 batch_normalization_9[0][0] 45__________________________________________________________________________________________________ 46conv2d_7 (Conv2D) (None, None, None, 4 9216 max_pooling2d_2[0][0] 47__________________________________________________________________________________________________ 48conv2d_10 (Conv2D) (None, None, None, 9 55296 activation_9[0][0] 49__________________________________________________________________________________________________ 50batch_normalization_7 (BatchNor (None, None, None, 4 144 conv2d_7[0][0] 51__________________________________________________________________________________________________ 52batch_normalization_10 (BatchNo (None, None, None, 9 288 conv2d_10[0][0] 53__________________________________________________________________________________________________ 54activation_7 (Activation) (None, None, None, 4 0 batch_normalization_7[0][0] 55__________________________________________________________________________________________________ 56activation_10 (Activation) (None, None, None, 9 0 batch_normalization_10[0][0] 57__________________________________________________________________________________________________ 58average_pooling2d_1 (AveragePoo (None, None, None, 1 0 max_pooling2d_2[0][0] 59__________________________________________________________________________________________________ 60conv2d_6 (Conv2D) (None, None, None, 6 12288 max_pooling2d_2[0][0] 61__________________________________________________________________________________________________ 62conv2d_8 (Conv2D) (None, None, None, 6 76800 activation_7[0][0] 63__________________________________________________________________________________________________ 64conv2d_11 (Conv2D) (None, None, None, 9 82944 activation_10[0][0] 65__________________________________________________________________________________________________ 66conv2d_12 (Conv2D) (None, None, None, 3 6144 average_pooling2d_1[0][0] 67__________________________________________________________________________________________________ 68batch_normalization_6 (BatchNor (None, None, None, 6 192 conv2d_6[0][0] 69__________________________________________________________________________________________________ 70batch_normalization_8 (BatchNor (None, None, None, 6 192 conv2d_8[0][0] 71__________________________________________________________________________________________________ 72batch_normalization_11 (BatchNo (None, None, None, 9 288 conv2d_11[0][0] 73__________________________________________________________________________________________________ 74batch_normalization_12 (BatchNo (None, None, None, 3 96 conv2d_12[0][0] 75__________________________________________________________________________________________________ 76activation_6 (Activation) (None, None, None, 6 0 batch_normalization_6[0][0] 77__________________________________________________________________________________________________ 78activation_8 (Activation) (None, None, None, 6 0 batch_normalization_8[0][0] 79__________________________________________________________________________________________________ 80activation_11 (Activation) (None, None, None, 9 0 batch_normalization_11[0][0] 81__________________________________________________________________________________________________ 82activation_12 (Activation) (None, None, None, 3 0 batch_normalization_12[0][0] 83__________________________________________________________________________________________________ 84mixed0 (Concatenate) (None, None, None, 2 0 activation_6[0][0] 85 activation_8[0][0] 86 activation_11[0][0] 87 activation_12[0][0] 88__________________________________________________________________________________________________ 89conv2d_16 (Conv2D) (None, None, None, 6 16384 mixed0[0][0] 90__________________________________________________________________________________________________ 91batch_normalization_16 (BatchNo (None, None, None, 6 192 conv2d_16[0][0] 92__________________________________________________________________________________________________ 93activation_16 (Activation) (None, None, None, 6 0 batch_normalization_16[0][0] 94__________________________________________________________________________________________________ 95conv2d_14 (Conv2D) (None, None, None, 4 12288 mixed0[0][0] 96__________________________________________________________________________________________________ 97conv2d_17 (Conv2D) (None, None, None, 9 55296 activation_16[0][0] 98__________________________________________________________________________________________________ 99batch_normalization_14 (BatchNo (None, None, None, 4 144 conv2d_14[0][0] 100__________________________________________________________________________________________________ 101batch_normalization_17 (BatchNo (None, None, None, 9 288 conv2d_17[0][0] 102__________________________________________________________________________________________________ 103activation_14 (Activation) (None, None, None, 4 0 batch_normalization_14[0][0] 104__________________________________________________________________________________________________ 105activation_17 (Activation) (None, None, None, 9 0 batch_normalization_17[0][0] 106__________________________________________________________________________________________________ 107average_pooling2d_2 (AveragePoo (None, None, None, 2 0 mixed0[0][0] 108__________________________________________________________________________________________________ 109conv2d_13 (Conv2D) (None, None, None, 6 16384 mixed0[0][0] 110__________________________________________________________________________________________________ 111conv2d_15 (Conv2D) (None, None, None, 6 76800 activation_14[0][0] 112__________________________________________________________________________________________________ 113conv2d_18 (Conv2D) (None, None, None, 9 82944 activation_17[0][0] 114__________________________________________________________________________________________________ 115conv2d_19 (Conv2D) (None, None, None, 6 16384 average_pooling2d_2[0][0] 116__________________________________________________________________________________________________ 117batch_normalization_13 (BatchNo (None, None, None, 6 192 conv2d_13[0][0] 118__________________________________________________________________________________________________ 119batch_normalization_15 (BatchNo (None, None, None, 6 192 conv2d_15[0][0] 120__________________________________________________________________________________________________ 121batch_normalization_18 (BatchNo (None, None, None, 9 288 conv2d_18[0][0] 122__________________________________________________________________________________________________ 123batch_normalization_19 (BatchNo (None, None, None, 6 192 conv2d_19[0][0] 124__________________________________________________________________________________________________ 125activation_13 (Activation) (None, None, None, 6 0 batch_normalization_13[0][0] 126__________________________________________________________________________________________________ 127activation_15 (Activation) (None, None, None, 6 0 batch_normalization_15[0][0] 128__________________________________________________________________________________________________ 129activation_18 (Activation) (None, None, None, 9 0 batch_normalization_18[0][0] 130__________________________________________________________________________________________________ 131activation_19 (Activation) (None, None, None, 6 0 batch_normalization_19[0][0] 132__________________________________________________________________________________________________ 133mixed1 (Concatenate) (None, None, None, 2 0 activation_13[0][0] 134 activation_15[0][0] 135 activation_18[0][0] 136 activation_19[0][0] 137__________________________________________________________________________________________________ 138conv2d_23 (Conv2D) (None, None, None, 6 18432 mixed1[0][0] 139__________________________________________________________________________________________________ 140batch_normalization_23 (BatchNo (None, None, None, 6 192 conv2d_23[0][0] 141__________________________________________________________________________________________________ 142activation_23 (Activation) (None, None, None, 6 0 batch_normalization_23[0][0] 143__________________________________________________________________________________________________ 144conv2d_21 (Conv2D) (None, None, None, 4 13824 mixed1[0][0] 145__________________________________________________________________________________________________ 146conv2d_24 (Conv2D) (None, None, None, 9 55296 activation_23[0][0] 147__________________________________________________________________________________________________ 148batch_normalization_21 (BatchNo (None, None, None, 4 144 conv2d_21[0][0] 149__________________________________________________________________________________________________ 150batch_normalization_24 (BatchNo (None, None, None, 9 288 conv2d_24[0][0] 151__________________________________________________________________________________________________ 152activation_21 (Activation) (None, None, None, 4 0 batch_normalization_21[0][0] 153__________________________________________________________________________________________________ 154activation_24 (Activation) (None, None, None, 9 0 batch_normalization_24[0][0] 155__________________________________________________________________________________________________ 156average_pooling2d_3 (AveragePoo (None, None, None, 2 0 mixed1[0][0] 157__________________________________________________________________________________________________ 158conv2d_20 (Conv2D) (None, None, None, 6 18432 mixed1[0][0] 159__________________________________________________________________________________________________ 160conv2d_22 (Conv2D) (None, None, None, 6 76800 activation_21[0][0] 161__________________________________________________________________________________________________ 162conv2d_25 (Conv2D) (None, None, None, 9 82944 activation_24[0][0] 163__________________________________________________________________________________________________ 164conv2d_26 (Conv2D) (None, None, None, 6 18432 average_pooling2d_3[0][0] 165__________________________________________________________________________________________________ 166batch_normalization_20 (BatchNo (None, None, None, 6 192 conv2d_20[0][0] 167__________________________________________________________________________________________________ 168batch_normalization_22 (BatchNo (None, None, None, 6 192 conv2d_22[0][0] 169__________________________________________________________________________________________________ 170batch_normalization_25 (BatchNo (None, None, None, 9 288 conv2d_25[0][0] 171__________________________________________________________________________________________________ 172batch_normalization_26 (BatchNo (None, None, None, 6 192 conv2d_26[0][0] 173__________________________________________________________________________________________________ 174activation_20 (Activation) (None, None, None, 6 0 batch_normalization_20[0][0] 175__________________________________________________________________________________________________ 176activation_22 (Activation) (None, None, None, 6 0 batch_normalization_22[0][0] 177__________________________________________________________________________________________________ 178activation_25 (Activation) (None, None, None, 9 0 batch_normalization_25[0][0] 179__________________________________________________________________________________________________ 180activation_26 (Activation) (None, None, None, 6 0 batch_normalization_26[0][0] 181__________________________________________________________________________________________________ 182mixed2 (Concatenate) (None, None, None, 2 0 activation_20[0][0] 183 activation_22[0][0] 184 activation_25[0][0] 185 activation_26[0][0] 186__________________________________________________________________________________________________ 187conv2d_28 (Conv2D) (None, None, None, 6 18432 mixed2[0][0] 188__________________________________________________________________________________________________ 189batch_normalization_28 (BatchNo (None, None, None, 6 192 conv2d_28[0][0] 190__________________________________________________________________________________________________ 191activation_28 (Activation) (None, None, None, 6 0 batch_normalization_28[0][0] 192__________________________________________________________________________________________________ 193conv2d_29 (Conv2D) (None, None, None, 9 55296 activation_28[0][0] 194__________________________________________________________________________________________________ 195batch_normalization_29 (BatchNo (None, None, None, 9 288 conv2d_29[0][0] 196__________________________________________________________________________________________________ 197activation_29 (Activation) (None, None, None, 9 0 batch_normalization_29[0][0] 198__________________________________________________________________________________________________ 199conv2d_27 (Conv2D) (None, None, None, 3 995328 mixed2[0][0] 200__________________________________________________________________________________________________ 201conv2d_30 (Conv2D) (None, None, None, 9 82944 activation_29[0][0] 202__________________________________________________________________________________________________ 203batch_normalization_27 (BatchNo (None, None, None, 3 1152 conv2d_27[0][0] 204__________________________________________________________________________________________________ 205batch_normalization_30 (BatchNo (None, None, None, 9 288 conv2d_30[0][0] 206__________________________________________________________________________________________________ 207activation_27 (Activation) (None, None, None, 3 0 batch_normalization_27[0][0] 208__________________________________________________________________________________________________ 209activation_30 (Activation) (None, None, None, 9 0 batch_normalization_30[0][0] 210__________________________________________________________________________________________________ 211max_pooling2d_3 (MaxPooling2D) (None, None, None, 2 0 mixed2[0][0] 212__________________________________________________________________________________________________ 213mixed3 (Concatenate) (None, None, None, 7 0 activation_27[0][0] 214 activation_30[0][0] 215 max_pooling2d_3[0][0] 216__________________________________________________________________________________________________ 217conv2d_35 (Conv2D) (None, None, None, 1 98304 mixed3[0][0] 218__________________________________________________________________________________________________ 219batch_normalization_35 (BatchNo (None, None, None, 1 384 conv2d_35[0][0] 220__________________________________________________________________________________________________ 221activation_35 (Activation) (None, None, None, 1 0 batch_normalization_35[0][0] 222__________________________________________________________________________________________________ 223conv2d_36 (Conv2D) (None, None, None, 1 114688 activation_35[0][0] 224__________________________________________________________________________________________________ 225batch_normalization_36 (BatchNo (None, None, None, 1 384 conv2d_36[0][0] 226__________________________________________________________________________________________________ 227activation_36 (Activation) (None, None, None, 1 0 batch_normalization_36[0][0] 228__________________________________________________________________________________________________ 229conv2d_32 (Conv2D) (None, None, None, 1 98304 mixed3[0][0] 230__________________________________________________________________________________________________ 231conv2d_37 (Conv2D) (None, None, None, 1 114688 activation_36[0][0] 232__________________________________________________________________________________________________ 233batch_normalization_32 (BatchNo (None, None, None, 1 384 conv2d_32[0][0] 234__________________________________________________________________________________________________ 235batch_normalization_37 (BatchNo (None, None, None, 1 384 conv2d_37[0][0] 236__________________________________________________________________________________________________ 237activation_32 (Activation) (None, None, None, 1 0 batch_normalization_32[0][0] 238__________________________________________________________________________________________________ 239activation_37 (Activation) (None, None, None, 1 0 batch_normalization_37[0][0] 240__________________________________________________________________________________________________ 241conv2d_33 (Conv2D) (None, None, None, 1 114688 activation_32[0][0] 242__________________________________________________________________________________________________ 243conv2d_38 (Conv2D) (None, None, None, 1 114688 activation_37[0][0] 244__________________________________________________________________________________________________ 245batch_normalization_33 (BatchNo (None, None, None, 1 384 conv2d_33[0][0] 246__________________________________________________________________________________________________ 247batch_normalization_38 (BatchNo (None, None, None, 1 384 conv2d_38[0][0] 248__________________________________________________________________________________________________ 249activation_33 (Activation) (None, None, None, 1 0 batch_normalization_33[0][0] 250__________________________________________________________________________________________________ 251activation_38 (Activation) (None, None, None, 1 0 batch_normalization_38[0][0] 252__________________________________________________________________________________________________ 253average_pooling2d_4 (AveragePoo (None, None, None, 7 0 mixed3[0][0] 254__________________________________________________________________________________________________ 255conv2d_31 (Conv2D) (None, None, None, 1 147456 mixed3[0][0] 256__________________________________________________________________________________________________ 257conv2d_34 (Conv2D) (None, None, None, 1 172032 activation_33[0][0] 258__________________________________________________________________________________________________ 259conv2d_39 (Conv2D) (None, None, None, 1 172032 activation_38[0][0] 260__________________________________________________________________________________________________ 261conv2d_40 (Conv2D) (None, None, None, 1 147456 average_pooling2d_4[0][0] 262__________________________________________________________________________________________________ 263batch_normalization_31 (BatchNo (None, None, None, 1 576 conv2d_31[0][0] 264__________________________________________________________________________________________________ 265batch_normalization_34 (BatchNo (None, None, None, 1 576 conv2d_34[0][0] 266__________________________________________________________________________________________________ 267batch_normalization_39 (BatchNo (None, None, None, 1 576 conv2d_39[0][0] 268__________________________________________________________________________________________________ 269batch_normalization_40 (BatchNo (None, None, None, 1 576 conv2d_40[0][0] 270__________________________________________________________________________________________________ 271activation_31 (Activation) (None, None, None, 1 0 batch_normalization_31[0][0] 272__________________________________________________________________________________________________ 273activation_34 (Activation) (None, None, None, 1 0 batch_normalization_34[0][0] 274__________________________________________________________________________________________________ 275activation_39 (Activation) (None, None, None, 1 0 batch_normalization_39[0][0] 276__________________________________________________________________________________________________ 277activation_40 (Activation) (None, None, None, 1 0 batch_normalization_40[0][0] 278__________________________________________________________________________________________________ 279mixed4 (Concatenate) (None, None, None, 7 0 activation_31[0][0] 280 activation_34[0][0] 281 activation_39[0][0] 282 activation_40[0][0] 283__________________________________________________________________________________________________ 284conv2d_45 (Conv2D) (None, None, None, 1 122880 mixed4[0][0] 285__________________________________________________________________________________________________ 286batch_normalization_45 (BatchNo (None, None, None, 1 480 conv2d_45[0][0] 287__________________________________________________________________________________________________ 288activation_45 (Activation) (None, None, None, 1 0 batch_normalization_45[0][0] 289__________________________________________________________________________________________________ 290conv2d_46 (Conv2D) (None, None, None, 1 179200 activation_45[0][0] 291__________________________________________________________________________________________________ 292batch_normalization_46 (BatchNo (None, None, None, 1 480 conv2d_46[0][0] 293__________________________________________________________________________________________________ 294activation_46 (Activation) (None, None, None, 1 0 batch_normalization_46[0][0] 295__________________________________________________________________________________________________ 296conv2d_42 (Conv2D) (None, None, None, 1 122880 mixed4[0][0] 297__________________________________________________________________________________________________ 298conv2d_47 (Conv2D) (None, None, None, 1 179200 activation_46[0][0] 299__________________________________________________________________________________________________ 300batch_normalization_42 (BatchNo (None, None, None, 1 480 conv2d_42[0][0] 301__________________________________________________________________________________________________ 302batch_normalization_47 (BatchNo (None, None, None, 1 480 conv2d_47[0][0] 303__________________________________________________________________________________________________ 304activation_42 (Activation) (None, None, None, 1 0 batch_normalization_42[0][0] 305__________________________________________________________________________________________________ 306activation_47 (Activation) (None, None, None, 1 0 batch_normalization_47[0][0] 307__________________________________________________________________________________________________ 308conv2d_43 (Conv2D) (None, None, None, 1 179200 activation_42[0][0] 309__________________________________________________________________________________________________ 310conv2d_48 (Conv2D) (None, None, None, 1 179200 activation_47[0][0] 311__________________________________________________________________________________________________ 312batch_normalization_43 (BatchNo (None, None, None, 1 480 conv2d_43[0][0] 313__________________________________________________________________________________________________ 314batch_normalization_48 (BatchNo (None, None, None, 1 480 conv2d_48[0][0] 315__________________________________________________________________________________________________ 316activation_43 (Activation) (None, None, None, 1 0 batch_normalization_43[0][0] 317__________________________________________________________________________________________________ 318activation_48 (Activation) (None, None, None, 1 0 batch_normalization_48[0][0] 319__________________________________________________________________________________________________ 320average_pooling2d_5 (AveragePoo (None, None, None, 7 0 mixed4[0][0] 321__________________________________________________________________________________________________ 322conv2d_41 (Conv2D) (None, None, None, 1 147456 mixed4[0][0] 323__________________________________________________________________________________________________ 324conv2d_44 (Conv2D) (None, None, None, 1 215040 activation_43[0][0] 325__________________________________________________________________________________________________ 326conv2d_49 (Conv2D) (None, None, None, 1 215040 activation_48[0][0] 327__________________________________________________________________________________________________ 328conv2d_50 (Conv2D) (None, None, None, 1 147456 average_pooling2d_5[0][0] 329__________________________________________________________________________________________________ 330batch_normalization_41 (BatchNo (None, None, None, 1 576 conv2d_41[0][0] 331__________________________________________________________________________________________________ 332batch_normalization_44 (BatchNo (None, None, None, 1 576 conv2d_44[0][0] 333__________________________________________________________________________________________________ 334batch_normalization_49 (BatchNo (None, None, None, 1 576 conv2d_49[0][0] 335__________________________________________________________________________________________________ 336batch_normalization_50 (BatchNo (None, None, None, 1 576 conv2d_50[0][0] 337__________________________________________________________________________________________________ 338activation_41 (Activation) (None, None, None, 1 0 batch_normalization_41[0][0] 339__________________________________________________________________________________________________ 340activation_44 (Activation) (None, None, None, 1 0 batch_normalization_44[0][0] 341__________________________________________________________________________________________________ 342activation_49 (Activation) (None, None, None, 1 0 batch_normalization_49[0][0] 343__________________________________________________________________________________________________ 344activation_50 (Activation) (None, None, None, 1 0 batch_normalization_50[0][0] 345__________________________________________________________________________________________________ 346mixed5 (Concatenate) (None, None, None, 7 0 activation_41[0][0] 347 activation_44[0][0] 348 activation_49[0][0] 349 activation_50[0][0] 350__________________________________________________________________________________________________ 351conv2d_55 (Conv2D) (None, None, None, 1 122880 mixed5[0][0] 352__________________________________________________________________________________________________ 353batch_normalization_55 (BatchNo (None, None, None, 1 480 conv2d_55[0][0] 354__________________________________________________________________________________________________ 355activation_55 (Activation) (None, None, None, 1 0 batch_normalization_55[0][0] 356__________________________________________________________________________________________________ 357conv2d_56 (Conv2D) (None, None, None, 1 179200 activation_55[0][0] 358__________________________________________________________________________________________________ 359batch_normalization_56 (BatchNo (None, None, None, 1 480 conv2d_56[0][0] 360__________________________________________________________________________________________________ 361activation_56 (Activation) (None, None, None, 1 0 batch_normalization_56[0][0] 362__________________________________________________________________________________________________ 363conv2d_52 (Conv2D) (None, None, None, 1 122880 mixed5[0][0] 364__________________________________________________________________________________________________ 365conv2d_57 (Conv2D) (None, None, None, 1 179200 activation_56[0][0] 366__________________________________________________________________________________________________ 367batch_normalization_52 (BatchNo (None, None, None, 1 480 conv2d_52[0][0] 368__________________________________________________________________________________________________ 369batch_normalization_57 (BatchNo (None, None, None, 1 480 conv2d_57[0][0] 370__________________________________________________________________________________________________ 371activation_52 (Activation) (None, None, None, 1 0 batch_normalization_52[0][0] 372__________________________________________________________________________________________________ 373activation_57 (Activation) (None, None, None, 1 0 batch_normalization_57[0][0] 374__________________________________________________________________________________________________ 375conv2d_53 (Conv2D) (None, None, None, 1 179200 activation_52[0][0] 376__________________________________________________________________________________________________ 377conv2d_58 (Conv2D) (None, None, None, 1 179200 activation_57[0][0] 378__________________________________________________________________________________________________ 379batch_normalization_53 (BatchNo (None, None, None, 1 480 conv2d_53[0][0] 380__________________________________________________________________________________________________ 381batch_normalization_58 (BatchNo (None, None, None, 1 480 conv2d_58[0][0] 382__________________________________________________________________________________________________ 383activation_53 (Activation) (None, None, None, 1 0 batch_normalization_53[0][0] 384__________________________________________________________________________________________________ 385activation_58 (Activation) (None, None, None, 1 0 batch_normalization_58[0][0] 386__________________________________________________________________________________________________ 387average_pooling2d_6 (AveragePoo (None, None, None, 7 0 mixed5[0][0] 388__________________________________________________________________________________________________ 389conv2d_51 (Conv2D) (None, None, None, 1 147456 mixed5[0][0] 390__________________________________________________________________________________________________ 391conv2d_54 (Conv2D) (None, None, None, 1 215040 activation_53[0][0] 392__________________________________________________________________________________________________ 393conv2d_59 (Conv2D) (None, None, None, 1 215040 activation_58[0][0] 394__________________________________________________________________________________________________ 395conv2d_60 (Conv2D) (None, None, None, 1 147456 average_pooling2d_6[0][0] 396__________________________________________________________________________________________________ 397batch_normalization_51 (BatchNo (None, None, None, 1 576 conv2d_51[0][0] 398__________________________________________________________________________________________________ 399batch_normalization_54 (BatchNo (None, None, None, 1 576 conv2d_54[0][0] 400__________________________________________________________________________________________________ 401batch_normalization_59 (BatchNo (None, None, None, 1 576 conv2d_59[0][0] 402__________________________________________________________________________________________________ 403batch_normalization_60 (BatchNo (None, None, None, 1 576 conv2d_60[0][0] 404__________________________________________________________________________________________________ 405activation_51 (Activation) (None, None, None, 1 0 batch_normalization_51[0][0] 406__________________________________________________________________________________________________ 407activation_54 (Activation) (None, None, None, 1 0 batch_normalization_54[0][0] 408__________________________________________________________________________________________________ 409activation_59 (Activation) (None, None, None, 1 0 batch_normalization_59[0][0] 410__________________________________________________________________________________________________ 411activation_60 (Activation) (None, None, None, 1 0 batch_normalization_60[0][0] 412__________________________________________________________________________________________________ 413mixed6 (Concatenate) (None, None, None, 7 0 activation_51[0][0] 414 activation_54[0][0] 415 activation_59[0][0] 416 activation_60[0][0] 417__________________________________________________________________________________________________ 418conv2d_65 (Conv2D) (None, None, None, 1 147456 mixed6[0][0] 419__________________________________________________________________________________________________ 420batch_normalization_65 (BatchNo (None, None, None, 1 576 conv2d_65[0][0] 421__________________________________________________________________________________________________ 422activation_65 (Activation) (None, None, None, 1 0 batch_normalization_65[0][0] 423__________________________________________________________________________________________________ 424conv2d_66 (Conv2D) (None, None, None, 1 258048 activation_65[0][0] 425__________________________________________________________________________________________________ 426batch_normalization_66 (BatchNo (None, None, None, 1 576 conv2d_66[0][0] 427__________________________________________________________________________________________________ 428activation_66 (Activation) (None, None, None, 1 0 batch_normalization_66[0][0] 429__________________________________________________________________________________________________ 430conv2d_62 (Conv2D) (None, None, None, 1 147456 mixed6[0][0] 431__________________________________________________________________________________________________ 432conv2d_67 (Conv2D) (None, None, None, 1 258048 activation_66[0][0] 433__________________________________________________________________________________________________ 434batch_normalization_62 (BatchNo (None, None, None, 1 576 conv2d_62[0][0] 435__________________________________________________________________________________________________ 436batch_normalization_67 (BatchNo (None, None, None, 1 576 conv2d_67[0][0] 437__________________________________________________________________________________________________ 438activation_62 (Activation) (None, None, None, 1 0 batch_normalization_62[0][0] 439__________________________________________________________________________________________________ 440activation_67 (Activation) (None, None, None, 1 0 batch_normalization_67[0][0] 441__________________________________________________________________________________________________ 442conv2d_63 (Conv2D) (None, None, None, 1 258048 activation_62[0][0] 443__________________________________________________________________________________________________ 444conv2d_68 (Conv2D) (None, None, None, 1 258048 activation_67[0][0] 445__________________________________________________________________________________________________ 446batch_normalization_63 (BatchNo (None, None, None, 1 576 conv2d_63[0][0] 447__________________________________________________________________________________________________ 448batch_normalization_68 (BatchNo (None, None, None, 1 576 conv2d_68[0][0] 449__________________________________________________________________________________________________ 450activation_63 (Activation) (None, None, None, 1 0 batch_normalization_63[0][0] 451__________________________________________________________________________________________________ 452activation_68 (Activation) (None, None, None, 1 0 batch_normalization_68[0][0] 453__________________________________________________________________________________________________ 454average_pooling2d_7 (AveragePoo (None, None, None, 7 0 mixed6[0][0] 455__________________________________________________________________________________________________ 456conv2d_61 (Conv2D) (None, None, None, 1 147456 mixed6[0][0] 457__________________________________________________________________________________________________ 458conv2d_64 (Conv2D) (None, None, None, 1 258048 activation_63[0][0] 459__________________________________________________________________________________________________ 460conv2d_69 (Conv2D) (None, None, None, 1 258048 activation_68[0][0] 461__________________________________________________________________________________________________ 462conv2d_70 (Conv2D) (None, None, None, 1 147456 average_pooling2d_7[0][0] 463__________________________________________________________________________________________________ 464batch_normalization_61 (BatchNo (None, None, None, 1 576 conv2d_61[0][0] 465__________________________________________________________________________________________________ 466batch_normalization_64 (BatchNo (None, None, None, 1 576 conv2d_64[0][0] 467__________________________________________________________________________________________________ 468batch_normalization_69 (BatchNo (None, None, None, 1 576 conv2d_69[0][0] 469__________________________________________________________________________________________________ 470batch_normalization_70 (BatchNo (None, None, None, 1 576 conv2d_70[0][0] 471__________________________________________________________________________________________________ 472activation_61 (Activation) (None, None, None, 1 0 batch_normalization_61[0][0] 473__________________________________________________________________________________________________ 474activation_64 (Activation) (None, None, None, 1 0 batch_normalization_64[0][0] 475__________________________________________________________________________________________________ 476activation_69 (Activation) (None, None, None, 1 0 batch_normalization_69[0][0] 477__________________________________________________________________________________________________ 478activation_70 (Activation) (None, None, None, 1 0 batch_normalization_70[0][0] 479__________________________________________________________________________________________________ 480mixed7 (Concatenate) (None, None, None, 7 0 activation_61[0][0] 481 activation_64[0][0] 482 activation_69[0][0] 483 activation_70[0][0] 484__________________________________________________________________________________________________ 485conv2d_73 (Conv2D) (None, None, None, 1 147456 mixed7[0][0] 486__________________________________________________________________________________________________ 487batch_normalization_73 (BatchNo (None, None, None, 1 576 conv2d_73[0][0] 488__________________________________________________________________________________________________ 489activation_73 (Activation) (None, None, None, 1 0 batch_normalization_73[0][0] 490__________________________________________________________________________________________________ 491conv2d_74 (Conv2D) (None, None, None, 1 258048 activation_73[0][0] 492__________________________________________________________________________________________________ 493batch_normalization_74 (BatchNo (None, None, None, 1 576 conv2d_74[0][0] 494__________________________________________________________________________________________________ 495activation_74 (Activation) (None, None, None, 1 0 batch_normalization_74[0][0] 496__________________________________________________________________________________________________ 497conv2d_71 (Conv2D) (None, None, None, 1 147456 mixed7[0][0] 498__________________________________________________________________________________________________ 499conv2d_75 (Conv2D) (None, None, None, 1 258048 activation_74[0][0] 500__________________________________________________________________________________________________ 501batch_normalization_71 (BatchNo (None, None, None, 1 576 conv2d_71[0][0] 502__________________________________________________________________________________________________ 503batch_normalization_75 (BatchNo (None, None, None, 1 576 conv2d_75[0][0] 504__________________________________________________________________________________________________ 505activation_71 (Activation) (None, None, None, 1 0 batch_normalization_71[0][0] 506__________________________________________________________________________________________________ 507activation_75 (Activation) (None, None, None, 1 0 batch_normalization_75[0][0] 508__________________________________________________________________________________________________ 509conv2d_72 (Conv2D) (None, None, None, 3 552960 activation_71[0][0] 510__________________________________________________________________________________________________ 511conv2d_76 (Conv2D) (None, None, None, 1 331776 activation_75[0][0] 512__________________________________________________________________________________________________ 513batch_normalization_72 (BatchNo (None, None, None, 3 960 conv2d_72[0][0] 514__________________________________________________________________________________________________ 515batch_normalization_76 (BatchNo (None, None, None, 1 576 conv2d_76[0][0] 516__________________________________________________________________________________________________ 517activation_72 (Activation) (None, None, None, 3 0 batch_normalization_72[0][0] 518__________________________________________________________________________________________________ 519activation_76 (Activation) (None, None, None, 1 0 batch_normalization_76[0][0] 520__________________________________________________________________________________________________ 521max_pooling2d_4 (MaxPooling2D) (None, None, None, 7 0 mixed7[0][0] 522__________________________________________________________________________________________________ 523mixed8 (Concatenate) (None, None, None, 1 0 activation_72[0][0] 524 activation_76[0][0] 525 max_pooling2d_4[0][0] 526__________________________________________________________________________________________________ 527conv2d_81 (Conv2D) (None, None, None, 4 573440 mixed8[0][0] 528__________________________________________________________________________________________________ 529batch_normalization_81 (BatchNo (None, None, None, 4 1344 conv2d_81[0][0] 530__________________________________________________________________________________________________ 531activation_81 (Activation) (None, None, None, 4 0 batch_normalization_81[0][0] 532__________________________________________________________________________________________________ 533conv2d_78 (Conv2D) (None, None, None, 3 491520 mixed8[0][0] 534__________________________________________________________________________________________________ 535conv2d_82 (Conv2D) (None, None, None, 3 1548288 activation_81[0][0] 536__________________________________________________________________________________________________ 537batch_normalization_78 (BatchNo (None, None, None, 3 1152 conv2d_78[0][0] 538__________________________________________________________________________________________________ 539batch_normalization_82 (BatchNo (None, None, None, 3 1152 conv2d_82[0][0] 540__________________________________________________________________________________________________ 541activation_78 (Activation) (None, None, None, 3 0 batch_normalization_78[0][0] 542__________________________________________________________________________________________________ 543activation_82 (Activation) (None, None, None, 3 0 batch_normalization_82[0][0] 544__________________________________________________________________________________________________ 545conv2d_79 (Conv2D) (None, None, None, 3 442368 activation_78[0][0] 546__________________________________________________________________________________________________ 547conv2d_80 (Conv2D) (None, None, None, 3 442368 activation_78[0][0] 548__________________________________________________________________________________________________ 549conv2d_83 (Conv2D) (None, None, None, 3 442368 activation_82[0][0] 550__________________________________________________________________________________________________ 551conv2d_84 (Conv2D) (None, None, None, 3 442368 activation_82[0][0] 552__________________________________________________________________________________________________ 553average_pooling2d_8 (AveragePoo (None, None, None, 1 0 mixed8[0][0] 554__________________________________________________________________________________________________ 555conv2d_77 (Conv2D) (None, None, None, 3 409600 mixed8[0][0] 556__________________________________________________________________________________________________ 557batch_normalization_79 (BatchNo (None, None, None, 3 1152 conv2d_79[0][0] 558__________________________________________________________________________________________________ 559batch_normalization_80 (BatchNo (None, None, None, 3 1152 conv2d_80[0][0] 560__________________________________________________________________________________________________ 561batch_normalization_83 (BatchNo (None, None, None, 3 1152 conv2d_83[0][0] 562__________________________________________________________________________________________________ 563batch_normalization_84 (BatchNo (None, None, None, 3 1152 conv2d_84[0][0] 564__________________________________________________________________________________________________ 565conv2d_85 (Conv2D) (None, None, None, 1 245760 average_pooling2d_8[0][0] 566__________________________________________________________________________________________________ 567batch_normalization_77 (BatchNo (None, None, None, 3 960 conv2d_77[0][0] 568__________________________________________________________________________________________________ 569activation_79 (Activation) (None, None, None, 3 0 batch_normalization_79[0][0] 570__________________________________________________________________________________________________ 571activation_80 (Activation) (None, None, None, 3 0 batch_normalization_80[0][0] 572__________________________________________________________________________________________________ 573activation_83 (Activation) (None, None, None, 3 0 batch_normalization_83[0][0] 574__________________________________________________________________________________________________ 575activation_84 (Activation) (None, None, None, 3 0 batch_normalization_84[0][0] 576__________________________________________________________________________________________________ 577batch_normalization_85 (BatchNo (None, None, None, 1 576 conv2d_85[0][0] 578__________________________________________________________________________________________________ 579activation_77 (Activation) (None, None, None, 3 0 batch_normalization_77[0][0] 580__________________________________________________________________________________________________ 581mixed9_0 (Concatenate) (None, None, None, 7 0 activation_79[0][0] 582 activation_80[0][0] 583__________________________________________________________________________________________________ 584concatenate_1 (Concatenate) (None, None, None, 7 0 activation_83[0][0] 585 activation_84[0][0] 586__________________________________________________________________________________________________ 587activation_85 (Activation) (None, None, None, 1 0 batch_normalization_85[0][0] 588__________________________________________________________________________________________________ 589mixed9 (Concatenate) (None, None, None, 2 0 activation_77[0][0] 590 mixed9_0[0][0] 591 concatenate_1[0][0] 592 activation_85[0][0] 593__________________________________________________________________________________________________ 594conv2d_90 (Conv2D) (None, None, None, 4 917504 mixed9[0][0] 595__________________________________________________________________________________________________ 596batch_normalization_90 (BatchNo (None, None, None, 4 1344 conv2d_90[0][0] 597__________________________________________________________________________________________________ 598activation_90 (Activation) (None, None, None, 4 0 batch_normalization_90[0][0] 599__________________________________________________________________________________________________ 600conv2d_87 (Conv2D) (None, None, None, 3 786432 mixed9[0][0] 601__________________________________________________________________________________________________ 602conv2d_91 (Conv2D) (None, None, None, 3 1548288 activation_90[0][0] 603__________________________________________________________________________________________________ 604batch_normalization_87 (BatchNo (None, None, None, 3 1152 conv2d_87[0][0] 605__________________________________________________________________________________________________ 606batch_normalization_91 (BatchNo (None, None, None, 3 1152 conv2d_91[0][0] 607__________________________________________________________________________________________________ 608activation_87 (Activation) (None, None, None, 3 0 batch_normalization_87[0][0] 609__________________________________________________________________________________________________ 610activation_91 (Activation) (None, None, None, 3 0 batch_normalization_91[0][0] 611__________________________________________________________________________________________________ 612conv2d_88 (Conv2D) (None, None, None, 3 442368 activation_87[0][0] 613__________________________________________________________________________________________________ 614conv2d_89 (Conv2D) (None, None, None, 3 442368 activation_87[0][0] 615__________________________________________________________________________________________________ 616conv2d_92 (Conv2D) (None, None, None, 3 442368 activation_91[0][0] 617__________________________________________________________________________________________________ 618conv2d_93 (Conv2D) (None, None, None, 3 442368 activation_91[0][0] 619__________________________________________________________________________________________________ 620average_pooling2d_9 (AveragePoo (None, None, None, 2 0 mixed9[0][0] 621__________________________________________________________________________________________________ 622conv2d_86 (Conv2D) (None, None, None, 3 655360 mixed9[0][0] 623__________________________________________________________________________________________________ 624batch_normalization_88 (BatchNo (None, None, None, 3 1152 conv2d_88[0][0] 625__________________________________________________________________________________________________ 626batch_normalization_89 (BatchNo (None, None, None, 3 1152 conv2d_89[0][0] 627__________________________________________________________________________________________________ 628batch_normalization_92 (BatchNo (None, None, None, 3 1152 conv2d_92[0][0] 629__________________________________________________________________________________________________ 630batch_normalization_93 (BatchNo (None, None, None, 3 1152 conv2d_93[0][0] 631__________________________________________________________________________________________________ 632conv2d_94 (Conv2D) (None, None, None, 1 393216 average_pooling2d_9[0][0] 633__________________________________________________________________________________________________ 634batch_normalization_86 (BatchNo (None, None, None, 3 960 conv2d_86[0][0] 635__________________________________________________________________________________________________ 636activation_88 (Activation) (None, None, None, 3 0 batch_normalization_88[0][0] 637__________________________________________________________________________________________________ 638activation_89 (Activation) (None, None, None, 3 0 batch_normalization_89[0][0] 639__________________________________________________________________________________________________ 640activation_92 (Activation) (None, None, None, 3 0 batch_normalization_92[0][0] 641__________________________________________________________________________________________________ 642activation_93 (Activation) (None, None, None, 3 0 batch_normalization_93[0][0] 643__________________________________________________________________________________________________ 644batch_normalization_94 (BatchNo (None, None, None, 1 576 conv2d_94[0][0] 645__________________________________________________________________________________________________ 646activation_86 (Activation) (None, None, None, 3 0 batch_normalization_86[0][0] 647__________________________________________________________________________________________________ 648mixed9_1 (Concatenate) (None, None, None, 7 0 activation_88[0][0] 649 activation_89[0][0] 650__________________________________________________________________________________________________ 651concatenate_2 (Concatenate) (None, None, None, 7 0 activation_92[0][0] 652 activation_93[0][0] 653__________________________________________________________________________________________________ 654activation_94 (Activation) (None, None, None, 1 0 batch_normalization_94[0][0] 655__________________________________________________________________________________________________ 656mixed10 (Concatenate) (None, None, None, 2 0 activation_86[0][0] 657 mixed9_1[0][0] 658 concatenate_2[0][0] 659 activation_94[0][0] 660__________________________________________________________________________________________________ 661global_average_pooling2d_1 (Glo (None, 2048) 0 mixed10[0][0] 662__________________________________________________________________________________________________ 663dense_1 (Dense) (None, 1024) 2098176 global_average_pooling2d_1[0][0] 664__________________________________________________________________________________________________ 665dense_2 (Dense) (None, 2) 2050 dense_1[0][0] 666================================================================================================== 667Total params: 23,903,010 668Trainable params: 2,100,226 669Non-trainable params: 21,802,784 670__________________________________________________________________________________________________ Phần train lại sẽ có khoảng hơn 2 triệu tham số, phần layter ở trước đó không train là khoảng 21 triệu tham số.\nĐồ hình của model (các bạn có thể download về rồi zoom bự lên để xem rõ hơn).\nChia tập dữ liệu ra thành 5 phần, 4 phần làm tập train, 1 phần làm tập validation.\n1X, y, tags = dataset.dataset(data_directory, n) 2nb_classes = len(tags) 3 4 5sample_count = len(y) 6train_size = sample_count * 4 // 5 7X_train = X[:train_size] 8y_train = y[:train_size] 9Y_train = np_utils.to_categorical(y_train, nb_classes) 10X_test = X[train_size:] 11y_test = y[train_size:] 12Y_test = np_utils.to_categorical(y_test, nb_classes) Để chống overfit, chúng ta sẽ thêm một số yếu tố như thực hiện các phép biến đổi affine trên ảnh gốc.\n1datagen = ImageDataGenerator( 2 featurewise_center=False, 3 samplewise_center=False, 4 featurewise_std_normalization=False, 5 samplewise_std_normalization=False, 6 zca_whitening=False, 7 rotation_range=45, 8 width_shift_range=0.25, 9 height_shift_range=0.25, 10 horizontal_flip=True, 11 vertical_flip=False, 12 zoom_range=0.5, 13 channel_shift_range=0.5, 14 fill_mode=\u0026#39;nearest\u0026#39;) 15 16datagen.fit(X_train) Cuối cùng, chúng ta sẽ xây dựng mô hình và tiến hành huấn luyện, lưu mô hình. Quá trình này tốn hơi nhiều thời gian.\n1 2model = net.build_model(nb_classes) 3model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#34;accuracy\u0026#34;]) 4 5# train the model on the new data for a few epochs 6 7print(\u0026#34;training the newly added dense layers\u0026#34;) 8 9samples_per_epoch = X_train.shape[0]//batch_size*batch_size 10steps_per_epoch = samples_per_epoch//batch_size 11validation_steps = X_test.shape[0]//batch_size*batch_size 12 13model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True), 14 samples_per_epoch=samples_per_epoch, 15 epochs=nb_epoch, 16 steps_per_epoch = steps_per_epoch, 17 validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size), 18 validation_steps=validation_steps, 19 ) 20 21 22net.save(model, tags, model_file_prefix) Độ chính xác trên tập train.\n1Y_pred = model.predict(X_test, batch_size=batch_size) 2y_pred = np.argmax(Y_pred, axis=1) 3 4accuracy = float(np.sum(y_test==y_pred)) / len(y_test) 5print(\u0026#34;accuracy: \u0026#34;, accuracy) 6 7confusion = np.zeros((nb_classes, nb_classes), dtype=np.int32) 8for (predicted_index, actual_index, image) in zip(y_pred, y_test, X_test): 9 confusion[predicted_index, actual_index] += 1 10 11print(\u0026#34;rows are predicted classes, columns are actual classes\u0026#34;) 12for predicted_index, predicted_tag in enumerate(tags): 13 print(predicted_tag[:7], end=\u0026#39;\u0026#39;, flush=True) 14 for actual_index, actual_tag in enumerate(tags): 15 print(\u0026#34;\\t%d\u0026#34; % confusion[predicted_index, actual_index], end=\u0026#39;\u0026#39;) 16 print(\u0026#34;\u0026#34;, flush=True) 1accuracy: 0.9907213167661771 2rows are predicted classes, columns are actual classes 3cat 12238 106 4dog 124 12320 Kết quả đạt 0.99 trên tập train, khá tốt phải không các bạn.\nCác bạn có thể download mô hình mình đã huấn luyện ở https://drive.google.com/open?id=1qQo8gj3KA6c1rPmJMVS_FZkVDcDmRgSf.\nThử show ra kết quả trên tập test xem như thế nào.\n1Y_pred = model.predict(X_test, batch_size=batch_size) 2y_pred = np.argmax(Y_pred, axis=1) 3 4lst_img = [] 5 6columns = 5 7rows = 5 8# fig,= plt.figure(rows) 9for idx, val in enumerate(X_test): 10 pred =y_pred[idx] 11 label = \u0026#34;{}: {:.2f}%\u0026#34;.format(tags[pred], Y_pred[idx][pred] * 100) 12 image = dataset.reverse_preprocess_input(val) 13 image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) 14 cv2.putText(image,label , (10, 25), cv2.FONT_HERSHEY_SIMPLEX,0.7, (255, 000, 0), 2) 15 16 plt.subplot(rows,rows,idx+1) 17 plt.imshow(image) 18 plt.title(label) 19 plt.axis(\u0026#39;off\u0026#39;) 20 21plt.show() Kết quả có một số hình mèo bị nhận nhầm là chó, và một số hình không phải mèo, không phải chó. Nhìn chung kết quả cũng không đến nỗi nào quá tệ.\nQuậy phá mô hình Mô hình InceptionV3 chúng ta đang xài có tổng cộng 311 lớp, chúng ta sẽ tiến hành một số pha quậy phá mô hình xem kết quả như trả ra như thế nào\nQuậy phá 1: Mở đóng băng một số lớp cuối và train trên chúng. Nếu các bạn để ý kỹ, trong đoạn mã nguồn của mình có đoạn\n1# first: train only the top layers (which were randomly initialized) 2 # i.e. freeze all convolutional InceptionV3 layers 3 for layer in base_model.layers: 4 layer.trainable = False Nghĩa là mình đóng băng toàn bộ 311 lớp, không cho nó train mà chỉ lấy kết quả của nó train lớp softmax cuối cùng. Bây giờ mình sẽ thử nghiệm với việc là để 299 lớp ban đầu vẫn đóng băng, và train lại toàn bộ các lớp còn lại (Các bạn đừng thắc mắc vì sao lại là 299 nha, do mình thích thôi).\n1for layer in model.layers[:299]: 2 layer.trainable = False 3for layer in model.layers[299:]: 4 layer.trainable = True Đồ hình của mô đồ khá giống ở trên, mình chỉ post lại kết quả của số param.\n1================================================================================================== 2Total params: 23,903,010 3Trainable params: 2,493,954 4Non-trainable params: 21,409,056 5__________________________________________________________________________________________________ Như vậy là có khoảng 2 triệu 5 tham số được huấn luyện lại\nModel của mình huấn luyện được các bạn có thể download ở https://drive.google.com/open?id=1Ts18LICUAh6gcOnXcmuVr7PUG5IxpCdt.\nKết quả đạt được:\n1accuracy: 0.9834610730133119 2rows are predicted classes, columns are actual classes 3cat 2429 69 4dog 13 2447 Kết quả 25 hình ngẫu nhiên cũng khá giống kết quả ở trước đó. Một số hình không có con vật bị nhận nhầm như hình còn mèo ở góc phải trên bị nhận nhầm là chó. Tuy nhiên, với chất lượng hình ảnh như thế này thì mình thấy kết quả như vậy là khá tuyệt vời.\nQuậy phá 2: Chỉ sử dụng 72 lớp đầu tiên của inception. Ở lần thí nghiệm này, mình sẽ chỉ sử dụng 72 lớp đầu tiên của inception để huấn luyện. Mình sẽ sửa lại một xíu ở hàm build model như sau:\n1x = base_model.layers[72].output Một lưu ý nhỏ là do inception không có tính tuần tự giữa các lớp (các bạn có thể nhìn hình ở trên sẽ thấy rõ), nên index sẽ không phải là 72 như thông thường.\nTiếp theo, chúng ta sẽ thực hiện việc huấn luyện lại mô hình và kết quả là:\n1accuracy: 0.5494150867285196 2rows are predicted classes, columns are actual classes 3cat 339 131 4dog 2103 2385 Kết quả khá tệ, lý do là mô hình các layer không theo sequence, mình lấy ngẫu nhiêu 72 lớp làm thông tin feature của các hình bị mất mát nhiều (ví dụ trường hợp layey 80 là tổng hợp thông tin của layter 79 + layter 4 + layer 48, mà mình chỉ lấy 72 layter đầu, nên sẽ mất đi phần đóng góp cực kỳ quan trọng của layter 4 và 48 ở lớp cao hơn).\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Oct 29, 2018","img":"","permalink":"/blog/2018-10-29-phan-loai-cho-meo/","series":null,"tags":["Machine learning","Deeplearning","dog cat"],"title":"Phân Loại Chó Mèo Sử Dụng Pretrain Model"},{"categories":null,"content":" Lời mở đầu Sử dụng pretrain model Lời mở đầu Phân vùng đối tượng là một bài toán khá phổ biến trong lĩnh vực computer vision. Trong open cv có hỗ trợ cho chúng ta một số hàm để phân vùng đối tượng rất dễ sử dụng. Đặc điểm chung của các hàm này là độ chính xác không được cao cho lắm. Ở bài viết này, chúng ta sẽ tìm hiểu cách sử dụng mô hình pretrain của DNN để phân vùng các đối tượng trong ảnh.\nSử dụng pretrain model Đầu tiên, các bạn download file pretrain model, giải nén ra và để ở đâu đó trong ổ cứng của máy bạn. Đường dẫn file pretrain model các bạn có thể download ở http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz. Các bạn có thể download các file pretrain khác nếu có hứng thú tìm hiểu.\nTiếp theo, chúng ta sẽ load mô hình lên:\n1import numpy as np 2import os 3import sys 4import tarfile 5import tensorflow as tf 6 7from collections import defaultdict 8from io import StringIO 9from matplotlib import pyplot as plt 10from PIL import Image 11import PIL.ImageDraw as ImageDraw 12import PIL.ImageFont as ImageFont 13import cv2 14 15import pprint 16 17import PIL.Image as Image 18import PIL.ImageColor as ImageColor 19 20# Model preparation 21 22 23# Path to frozen detection graph. This is the actual model that is used for the object detection. 24PATH_TO_CKPT = \u0026#39;mask_rcnn_inception_v2_coco_2018_01_28\u0026#39; + \u0026#39;/frozen_inference_graph.pb\u0026#39; 25 26# List of the strings that is used to add correct label for each box. 27#PATH_TO_LABELS = \u0026#39;mscoco_label_map.pbtxt\u0026#39; 28 29NUM_CLASSES = 1 30 31 32# categories 33 34category_index = {1: {\u0026#39;id\u0026#39;: 1, \u0026#39;name\u0026#39;: \u0026#39;person\u0026#39;}, 35# 3: {\u0026#39;id\u0026#39;: 3, \u0026#39;name\u0026#39;: \u0026#39;car\u0026#39;}, 36 } 37 38detection_graph = tf.Graph() 39with detection_graph.as_default(): 40 od_graph_def = tf.GraphDef() 41 with tf.gfile.GFile(PATH_TO_CKPT, \u0026#39;rb\u0026#39;) as fid: 42 serialized_graph = fid.read() 43 od_graph_def.ParseFromString(serialized_graph) 44 tf.import_graph_def(od_graph_def, name=\u0026#39;\u0026#39;) Ở đây, mình chỉ demo detect người trong hình, nên mình chỉ để category_index chỉ là \u0026ldquo;person\u0026rdquo;. Thực tế, mô hình COCO hỗ trợ cho chúng ta nhận dạng 90 loại đối tượng khác nhau, các bạn có nhu cầu tìm hiểu thì thay bằng đoạn mã sau:\n1category_index = {1: {\u0026#39;id\u0026#39;: 1, \u0026#39;name\u0026#39;: \u0026#39;person\u0026#39;}, 2 2: {\u0026#39;id\u0026#39;: 2, \u0026#39;name\u0026#39;: \u0026#39;bicycle\u0026#39;}, 3 3: {\u0026#39;id\u0026#39;: 3, \u0026#39;name\u0026#39;: \u0026#39;car\u0026#39;}, 4 4: {\u0026#39;id\u0026#39;: 4, \u0026#39;name\u0026#39;: \u0026#39;motorcycle\u0026#39;}, 5 5: {\u0026#39;id\u0026#39;: 5, \u0026#39;name\u0026#39;: \u0026#39;airplane\u0026#39;}, 6 6: {\u0026#39;id\u0026#39;: 6, \u0026#39;name\u0026#39;: \u0026#39;bus\u0026#39;}, 7 7: {\u0026#39;id\u0026#39;: 7, \u0026#39;name\u0026#39;: \u0026#39;train\u0026#39;}, 8 8: {\u0026#39;id\u0026#39;: 8, \u0026#39;name\u0026#39;: \u0026#39;truck\u0026#39;}, 9 9: {\u0026#39;id\u0026#39;: 9, \u0026#39;name\u0026#39;: \u0026#39;boat\u0026#39;}, 10 10: {\u0026#39;id\u0026#39;: 10, \u0026#39;name\u0026#39;: \u0026#39;traffic light\u0026#39;}, 11 11: {\u0026#39;id\u0026#39;: 11, \u0026#39;name\u0026#39;: \u0026#39;fire hydrant\u0026#39;}, 12 13: {\u0026#39;id\u0026#39;: 13, \u0026#39;name\u0026#39;: \u0026#39;stop sign\u0026#39;}, 13 14: {\u0026#39;id\u0026#39;: 14, \u0026#39;name\u0026#39;: \u0026#39;parking meter\u0026#39;}, 14 15: {\u0026#39;id\u0026#39;: 15, \u0026#39;name\u0026#39;: \u0026#39;bench\u0026#39;}, 15 16: {\u0026#39;id\u0026#39;: 16, \u0026#39;name\u0026#39;: \u0026#39;bird\u0026#39;}, 16 17: {\u0026#39;id\u0026#39;: 17, \u0026#39;name\u0026#39;: \u0026#39;cat\u0026#39;}, 17 18: {\u0026#39;id\u0026#39;: 18, \u0026#39;name\u0026#39;: \u0026#39;dog\u0026#39;}, 18 19: {\u0026#39;id\u0026#39;: 19, \u0026#39;name\u0026#39;: \u0026#39;horse\u0026#39;}, 19 20: {\u0026#39;id\u0026#39;: 20, \u0026#39;name\u0026#39;: \u0026#39;sheep\u0026#39;}, 20 21: {\u0026#39;id\u0026#39;: 21, \u0026#39;name\u0026#39;: \u0026#39;cow\u0026#39;}, 21 22: {\u0026#39;id\u0026#39;: 22, \u0026#39;name\u0026#39;: \u0026#39;elephant\u0026#39;}, 22 23: {\u0026#39;id\u0026#39;: 23, \u0026#39;name\u0026#39;: \u0026#39;bear\u0026#39;}, 23 24: {\u0026#39;id\u0026#39;: 24, \u0026#39;name\u0026#39;: \u0026#39;zebra\u0026#39;}, 24 25: {\u0026#39;id\u0026#39;: 25, \u0026#39;name\u0026#39;: \u0026#39;giraffe\u0026#39;}, 25 27: {\u0026#39;id\u0026#39;: 27, \u0026#39;name\u0026#39;: \u0026#39;backpack\u0026#39;}, 26 28: {\u0026#39;id\u0026#39;: 28, \u0026#39;name\u0026#39;: \u0026#39;umbrella\u0026#39;}, 27 31: {\u0026#39;id\u0026#39;: 31, \u0026#39;name\u0026#39;: \u0026#39;handbag\u0026#39;}, 28 32: {\u0026#39;id\u0026#39;: 32, \u0026#39;name\u0026#39;: \u0026#39;tie\u0026#39;}, 29 33: {\u0026#39;id\u0026#39;: 33, \u0026#39;name\u0026#39;: \u0026#39;suitcase\u0026#39;}, 30 34: {\u0026#39;id\u0026#39;: 34, \u0026#39;name\u0026#39;: \u0026#39;frisbee\u0026#39;}, 31 35: {\u0026#39;id\u0026#39;: 35, \u0026#39;name\u0026#39;: \u0026#39;skis\u0026#39;}, 32 36: {\u0026#39;id\u0026#39;: 36, \u0026#39;name\u0026#39;: \u0026#39;snowboard\u0026#39;}, 33 37: {\u0026#39;id\u0026#39;: 37, \u0026#39;name\u0026#39;: \u0026#39;sports ball\u0026#39;}, 34 38: {\u0026#39;id\u0026#39;: 38, \u0026#39;name\u0026#39;: \u0026#39;kite\u0026#39;}, 35 39: {\u0026#39;id\u0026#39;: 39, \u0026#39;name\u0026#39;: \u0026#39;baseball bat\u0026#39;}, 36 40: {\u0026#39;id\u0026#39;: 40, \u0026#39;name\u0026#39;: \u0026#39;baseball glove\u0026#39;}, 37 41: {\u0026#39;id\u0026#39;: 41, \u0026#39;name\u0026#39;: \u0026#39;skateboard\u0026#39;}, 38 42: {\u0026#39;id\u0026#39;: 42, \u0026#39;name\u0026#39;: \u0026#39;surfboard\u0026#39;}, 39 43: {\u0026#39;id\u0026#39;: 43, \u0026#39;name\u0026#39;: \u0026#39;tennis racket\u0026#39;}, 40 44: {\u0026#39;id\u0026#39;: 44, \u0026#39;name\u0026#39;: \u0026#39;bottle\u0026#39;}, 41 46: {\u0026#39;id\u0026#39;: 46, \u0026#39;name\u0026#39;: \u0026#39;wine glass\u0026#39;}, 42 47: {\u0026#39;id\u0026#39;: 47, \u0026#39;name\u0026#39;: \u0026#39;cup\u0026#39;}, 43 48: {\u0026#39;id\u0026#39;: 48, \u0026#39;name\u0026#39;: \u0026#39;fork\u0026#39;}, 44 49: {\u0026#39;id\u0026#39;: 49, \u0026#39;name\u0026#39;: \u0026#39;knife\u0026#39;}, 45 50: {\u0026#39;id\u0026#39;: 50, \u0026#39;name\u0026#39;: \u0026#39;spoon\u0026#39;}, 46 51: {\u0026#39;id\u0026#39;: 51, \u0026#39;name\u0026#39;: \u0026#39;bowl\u0026#39;}, 47 52: {\u0026#39;id\u0026#39;: 52, \u0026#39;name\u0026#39;: \u0026#39;banana\u0026#39;}, 48 53: {\u0026#39;id\u0026#39;: 53, \u0026#39;name\u0026#39;: \u0026#39;apple\u0026#39;}, 49 54: {\u0026#39;id\u0026#39;: 54, \u0026#39;name\u0026#39;: \u0026#39;sandwich\u0026#39;}, 50 55: {\u0026#39;id\u0026#39;: 55, \u0026#39;name\u0026#39;: \u0026#39;orange\u0026#39;}, 51 56: {\u0026#39;id\u0026#39;: 56, \u0026#39;name\u0026#39;: \u0026#39;broccoli\u0026#39;}, 52 57: {\u0026#39;id\u0026#39;: 57, \u0026#39;name\u0026#39;: \u0026#39;carrot\u0026#39;}, 53 58: {\u0026#39;id\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;hot dog\u0026#39;}, 54 59: {\u0026#39;id\u0026#39;: 59, \u0026#39;name\u0026#39;: \u0026#39;pizza\u0026#39;}, 55 60: {\u0026#39;id\u0026#39;: 60, \u0026#39;name\u0026#39;: \u0026#39;donut\u0026#39;}, 56 61: {\u0026#39;id\u0026#39;: 61, \u0026#39;name\u0026#39;: \u0026#39;cake\u0026#39;}, 57 62: {\u0026#39;id\u0026#39;: 62, \u0026#39;name\u0026#39;: \u0026#39;chair\u0026#39;}, 58 63: {\u0026#39;id\u0026#39;: 63, \u0026#39;name\u0026#39;: \u0026#39;couch\u0026#39;}, 59 64: {\u0026#39;id\u0026#39;: 64, \u0026#39;name\u0026#39;: \u0026#39;potted plant\u0026#39;}, 60 65: {\u0026#39;id\u0026#39;: 65, \u0026#39;name\u0026#39;: \u0026#39;bed\u0026#39;}, 61 67: {\u0026#39;id\u0026#39;: 67, \u0026#39;name\u0026#39;: \u0026#39;dining table\u0026#39;}, 62 70: {\u0026#39;id\u0026#39;: 70, \u0026#39;name\u0026#39;: \u0026#39;toilet\u0026#39;}, 63 72: {\u0026#39;id\u0026#39;: 72, \u0026#39;name\u0026#39;: \u0026#39;tv\u0026#39;}, 64 73: {\u0026#39;id\u0026#39;: 73, \u0026#39;name\u0026#39;: \u0026#39;laptop\u0026#39;}, 65 74: {\u0026#39;id\u0026#39;: 74, \u0026#39;name\u0026#39;: \u0026#39;mouse\u0026#39;}, 66 75: {\u0026#39;id\u0026#39;: 75, \u0026#39;name\u0026#39;: \u0026#39;remote\u0026#39;}, 67 76: {\u0026#39;id\u0026#39;: 76, \u0026#39;name\u0026#39;: \u0026#39;keyboard\u0026#39;}, 68 77: {\u0026#39;id\u0026#39;: 77, \u0026#39;name\u0026#39;: \u0026#39;cell phone\u0026#39;}, 69 78: {\u0026#39;id\u0026#39;: 78, \u0026#39;name\u0026#39;: \u0026#39;microwave\u0026#39;}, 70 79: {\u0026#39;id\u0026#39;: 79, \u0026#39;name\u0026#39;: \u0026#39;oven\u0026#39;}, 71 80: {\u0026#39;id\u0026#39;: 80, \u0026#39;name\u0026#39;: \u0026#39;toaster\u0026#39;}, 72 81: {\u0026#39;id\u0026#39;: 81, \u0026#39;name\u0026#39;: \u0026#39;sink\u0026#39;}, 73 82: {\u0026#39;id\u0026#39;: 82, \u0026#39;name\u0026#39;: \u0026#39;refrigerator\u0026#39;}, 74 84: {\u0026#39;id\u0026#39;: 84, \u0026#39;name\u0026#39;: \u0026#39;book\u0026#39;}, 75 85: {\u0026#39;id\u0026#39;: 85, \u0026#39;name\u0026#39;: \u0026#39;clock\u0026#39;}, 76 86: {\u0026#39;id\u0026#39;: 86, \u0026#39;name\u0026#39;: \u0026#39;vase\u0026#39;}, 77 87: {\u0026#39;id\u0026#39;: 87, \u0026#39;name\u0026#39;: \u0026#39;scissors\u0026#39;}, 78 88: {\u0026#39;id\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;teddy bear\u0026#39;}, 79 89: {\u0026#39;id\u0026#39;: 89, \u0026#39;name\u0026#39;: \u0026#39;hair drier\u0026#39;}, 80 90: {\u0026#39;id\u0026#39;: 90, \u0026#39;name\u0026#39;: \u0026#39;toothbrush\u0026#39;}} Tiếp theo, chúng ta sẽ load một số hàm giúp hỗ trợ việc hậu xử lý ảnh để vẽ các mask cho chúng ta xem trực quan hơn.\n1 2 draw = ImageDraw.Draw(image) 3 im_width, im_height = image.size 4 if use_normalized_coordinates: 5 (left, right, top, bottom) = (xmin * im_width, xmax * im_width, 6 ymin * im_height, ymax * im_height) 7 else: 8 (left, right, top, bottom) = (xmin, xmax, ymin, ymax) 9 draw.line([(left, top), (left, bottom), (right, bottom), 10 (right, top), (left, top)], width=thickness, fill=color) 11 try: 12 font = ImageFont.truetype(\u0026#39;arial.ttf\u0026#39;, 24) 13 except IOError: 14 font = ImageFont.load_default() 15 16 # If the total height of the display strings added to the top of the bounding 17 # box exceeds the top of the image, stack the strings below the bounding box 18 # instead of above. 19 display_str_heights = [font.getsize(ds)[1] for ds in display_str_list] 20 # Each display_str has a top and bottom margin of 0.05x. 21 total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights) 22 23 if top \u0026gt; total_display_str_height: 24 text_bottom = top 25 else: 26 text_bottom = bottom + total_display_str_height 27 # Reverse list and print from bottom to top. 28 for display_str in display_str_list[::-1]: 29 text_width, text_height = font.getsize(display_str) 30 margin = np.ceil(0.05 * text_height) 31 draw.rectangle( 32 [(left, text_bottom - text_height - 2 * margin), (left + text_width, 33 text_bottom)], 34 fill=color) 35 draw.text( 36 (left + margin, text_bottom - text_height - margin), 37 display_str, 38 fill=\u0026#39;black\u0026#39;, 39 font=font) 40 text_bottom -= text_height - 2 * margin 41 42 43 44def visualize_boxes_and_labels_on_image_array( 45 image, 46 boxes, 47 classes, 48 scores, 49 category_index, 50 instance_masks=None, 51 instance_boundaries=None, 52 keypoints=None, 53 use_normalized_coordinates=False, 54 max_boxes_to_draw=20, 55 min_score_thresh=.5, 56 agnostic_mode=False, 57 line_thickness=4, 58 groundtruth_box_visualization_color=\u0026#39;black\u0026#39;, 59 skip_scores=False, 60 skip_labels=False): 61 62 box_to_display_str_map = collections.defaultdict(list) 63 box_to_color_map = collections.defaultdict(str) 64 box_to_instance_masks_map = {} 65 box_to_instance_boundaries_map = {} 66 box_to_keypoints_map = collections.defaultdict(list) 67 if not max_boxes_to_draw: 68 max_boxes_to_draw = boxes.shape[0] 69 #print(boxes) 70 for i in range(min(max_boxes_to_draw, boxes.shape[0])): 71 if scores is None or scores[i] \u0026gt; min_score_thresh: 72 box = tuple(boxes[i].tolist()) 73 if instance_masks is not None: 74 box_to_instance_masks_map[box] = instance_masks[i] 75 if instance_boundaries is not None: 76 box_to_instance_boundaries_map[box] = instance_boundaries[i] 77 if keypoints is not None: 78 box_to_keypoints_map[box].extend(keypoints[i]) 79 if scores is None: 80 box_to_color_map[box] = groundtruth_box_visualization_color 81 else: 82 display_str = \u0026#39;\u0026#39; 83 if not skip_labels: 84 if not agnostic_mode: 85 if classes[i] in category_index.keys(): 86 class_name = category_index[classes[i]][\u0026#39;name\u0026#39;] 87 else: 88 class_name = \u0026#39;N/A\u0026#39; 89 display_str = str(class_name) 90 if not skip_scores: 91 if not display_str: 92 display_str = \u0026#39;{}%\u0026#39;.format(int(100 * scores[i])) 93 else: 94 display_str = \u0026#39;{}: {}%\u0026#39;.format( 95 display_str, int(100 * scores[i])) 96 box_to_display_str_map[box].append(display_str) 97 if agnostic_mode: 98 box_to_color_map[box] = \u0026#39;DarkOrange\u0026#39; 99 else: 100 box_to_color_map[box] = STANDARD_COLORS[classes[i] % 101 len(STANDARD_COLORS)] 102 103 # Draw all boxes onto image. 104 for box, color in box_to_color_map.items(): 105 ymin, xmin, ymax, xmax = box 106 if instance_masks is not None: 107 draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color) 108 109 draw_bounding_box_on_image_array( 110 image, 111 ymin, 112 xmin, 113 ymax, 114 xmax, 115 color=color, 116 thickness=line_thickness, 117 display_str_list=box_to_display_str_map[box], 118 use_normalized_coordinates=use_normalized_coordinates) 119 120 return image 121 122 123def reframe_box_masks_to_image_masks(box_masks, boxes, image_height, 124 image_width): 125 \u0026#34;\u0026#34;\u0026#34;Transforms the box masks back to full image masks. 126 127 Embeds masks in bounding boxes of larger masks whose shapes correspond to 128 image shape. 129 130 Args: 131 box_masks: A tf.float32 tensor of size [num_masks, mask_height, mask_width]. 132 boxes: A tf.float32 tensor of size [num_masks, 4] containing the box 133 corners. Row i contains [ymin, xmin, ymax, xmax] of the box 134 corresponding to mask i. Note that the box corners are in 135 normalized coordinates. 136 image_height: Image height. The output mask will have the same height as 137 the image height. 138 image_width: Image width. The output mask will have the same width as the 139 image width. 140 141 Returns: 142 A tf.float32 tensor of size [num_masks, image_height, image_width]. 143 \u0026#34;\u0026#34;\u0026#34; 144 # TODO(rathodv): Make this a public function. 145 def reframe_box_masks_to_image_masks_default(): 146 \u0026#34;\u0026#34;\u0026#34;The default function when there are more than 0 box masks.\u0026#34;\u0026#34;\u0026#34; 147 def transform_boxes_relative_to_boxes(boxes, reference_boxes): 148 boxes = tf.reshape(boxes, [-1, 2, 2]) 149 min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1) 150 max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1) 151 transformed_boxes = (boxes - min_corner) / \\ 152 (max_corner - min_corner) 153 return tf.reshape(transformed_boxes, [-1, 4]) 154 155 box_masks_expanded = tf.expand_dims(box_masks, axis=3) 156 num_boxes = tf.shape(box_masks_expanded)[0] 157 unit_boxes = tf.concat( 158 [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1) 159 reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes) 160 return tf.image.crop_and_resize( 161 image=box_masks_expanded, 162 boxes=reverse_boxes, 163 box_ind=tf.range(num_boxes), 164 crop_size=[image_height, image_width], 165 extrapolation_value=0.0) 166 image_masks = tf.cond( 167 tf.shape(box_masks)[0] \u0026gt; 0, 168 reframe_box_masks_to_image_masks_default, 169 lambda: tf.zeros([0, image_height, image_width, 1], dtype=tf.float32)) 170 return tf.squeeze(image_masks, axis=3) Cho hình ảnh vào và rút ra kết quả.\n1 2def detect_frame(image_np, sess, detection_graph): 3 4 with detection_graph.as_default(): 5 6 ops = tf.get_default_graph().get_operations() 7 all_tensor_names = {output.name for op in ops for output in op.outputs} 8 tensor_dict = {} 9 for key in [ 10 \u0026#39;num_detections\u0026#39;, \u0026#39;detection_boxes\u0026#39;, \u0026#39;detection_scores\u0026#39;, 11 \u0026#39;detection_classes\u0026#39;, \u0026#39;detection_masks\u0026#39; 12 ]: 13 tensor_name = key + \u0026#39;:0\u0026#39; 14 if tensor_name in all_tensor_names: 15 tensor_dict[key] = tf.get_default_graph( 16 ).get_tensor_by_name(tensor_name) 17 if \u0026#39;detection_masks\u0026#39; in tensor_dict: 18 # The following processing is only for single image 19 detection_boxes = tf.squeeze(tensor_dict[\u0026#39;detection_boxes\u0026#39;], [0]) 20 detection_masks = tf.squeeze(tensor_dict[\u0026#39;detection_masks\u0026#39;], [0]) 21 # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size. 22 real_num_detection = tf.cast( 23 tensor_dict[\u0026#39;num_detections\u0026#39;][0], tf.int32) 24 25 detection_boxes = tf.slice(detection_boxes, [0, 0], [ 26 real_num_detection, -1]) 27 detection_masks = tf.slice(detection_masks, [0, 0, 0], [ 28 real_num_detection, -1, -1]) 29 detection_masks_reframed = reframe_box_masks_to_image_masks( 30 detection_masks, detection_boxes, image_np.shape[0], image_np.shape[1]) 31 detection_masks_reframed = tf.cast( 32 tf.greater(detection_masks_reframed, 0.5), tf.uint8) 33 # Follow the convention by adding back the batch dimension 34 tensor_dict[\u0026#39;detection_masks\u0026#39;] = tf.expand_dims( 35 detection_masks_reframed, 0) 36 image_tensor = tf.get_default_graph().get_tensor_by_name(\u0026#39;image_tensor:0\u0026#39;) 37 38 # Run inference 39 output_dict = sess.run(tensor_dict, 40 feed_dict={image_tensor: np.expand_dims(image_np, 0)}) 41 42 # all outputs are float32 numpy arrays, so convert types as appropriate 43 output_dict[\u0026#39;num_detections\u0026#39;] = int(output_dict[\u0026#39;num_detections\u0026#39;][0]) 44 #print(\u0026#34;num detect \u0026#34;+str(output_dict[\u0026#39;num_detections\u0026#39;])) 45 output_dict[\u0026#39;detection_classes\u0026#39;] = output_dict[\u0026#39;detection_classes\u0026#39;][0].astype( 46 np.uint8) 47 output_dict[\u0026#39;detection_boxes\u0026#39;] = output_dict[\u0026#39;detection_boxes\u0026#39;][0] 48 output_dict[\u0026#39;detection_scores\u0026#39;] = output_dict[\u0026#39;detection_scores\u0026#39;][0] 49 if \u0026#39;detection_masks\u0026#39; in output_dict: 50 output_dict[\u0026#39;detection_masks\u0026#39;] = output_dict[\u0026#39;detection_masks\u0026#39;][0] 51 52 visualize_boxes_and_labels_on_image_array( 53 image_np, 54 output_dict[\u0026#39;detection_boxes\u0026#39;], 55 output_dict[\u0026#39;detection_classes\u0026#39;], 56 output_dict[\u0026#39;detection_scores\u0026#39;], 57 category_index, 58 instance_masks=output_dict.get(\u0026#39;detection_masks\u0026#39;), 59 use_normalized_coordinates=True, 60 line_thickness=1, 61 max_boxes_to_draw=min(output_dict[\u0026#39;num_detections\u0026#39;],20) 62 ) 63 64 return image_np 1image = cv2.imread(\u0026#39;img2.jpg\u0026#39;) 2with detection_graph.as_default(): 3 with tf.Session(graph=detection_graph) as sess: 4 image_np = detect_frame(image, sess, detection_graph) 5 6cv2.imwrite(\u0026#39;output.jpg\u0026#39;, image) Kết quả file output.jpg của chúng ta là:\nThử với bức ảnh người và xe hơi.\nCảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.\n","date":"Oct 8, 2018","img":"","permalink":"/blog/2018-10-08-mask-rnn/","series":null,"tags":["Machine learning","Deeplearning","Spark"],"title":"Mask R-CNN Trong Bài Toán Nhận Dạng Và Phân Vùng Đối Tượng"},{"categories":null,"content":"Lời mở đầu Lưu ý: Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv \u0026gt; 3.4.1.\nỞ bài viết trước, chúng ta đã tìm hiểu cách thức rút trích khung xương sử dụng DNN và đã áp dụng thành công trên ảnh có chứa 1 đối tượng người. Trong bài viết này, chúng ta sẽ thực hiện áp dụng mô hình cho bài toán có nhiều người trong cùng 1 bức ảnh.\nSử dụng pretrain model trong bài toán multiple Pose Estimation Trong bài viết này, chúng ta tiếp tục sử dụng mô hình MPI để dò tìm các điểm đặc trưng của con người và rút ra mô hình khung xương. Kết quả trả về của thuật toán gồm 15 đặc trưng như bên dưới.\n1Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4, 2Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8, 3Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12, 4Left Ankle – 13, Chest – 14, Background – 15 Áp dụng mô hình với ảnh của nhóm T-ARA.\n1import cv2 2 3nPoints = 15 4POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ] 5 6protoFile = \u0026#34;pose/mpi/pose_deploy_linevec.prototxt\u0026#34; 7weightsFile = \u0026#34;pose/mpi/pose_iter_160000.caffemodel\u0026#34; 8 9net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile) 10 11frame = cv2.imread(\u0026#34;tara1.jpg\u0026#34;) 12 13inWidth = 368 14inHeight = 368 15 16# Prepare the frame to be fed to the network 17inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False) 18 19# Set the prepared object as the input blob of the network 20net.setInput(inpBlob) 21 22output = net.forward() Thử show lên vị trí vùng cổ trong hình.\n1 2i = 0 3probMap = output[0, i, :, :] 4probMap = cv2.resize(probMap, (frameWidth, frameHeight)) 5 6import matplotlib.pyplot as plt 7 8plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB)) 9plt.imshow(probMap, alpha=0.5) 10plt.show() Thử show lên hình điểm đặc trưng vùng cổ\n1i = 1 2probMap = output[0, i, :, :] 3probMap = cv2.resize(probMap, (frameWidth, frameHeight)) 4 5import matplotlib.pyplot as plt 6 7plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB)) 8plt.imshow(probMap, alpha=0.5) 9plt.show() Bằng một số phép biến đổi quen thuộc có sẵn trong opencv, chúng ta hoàn toàn có thể lấy được toạ độ của các điểm keypoint một cách dễ dàng.\n1 2# Find the Keypoints using Non Maximum Suppression on the Confidence Map 3def getKeypoints(probMap, threshold=0.1): 4 5 mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0) 6 7 mapMask = np.uint8(mapSmooth\u0026gt;threshold) 8 keypoints = [] 9 10 #find the blobs 11 _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) 12 13 #for each blob find the maxima 14 for cnt in contours: 15 blobMask = np.zeros(mapMask.shape) 16 blobMask = cv2.fillConvexPoly(blobMask, cnt, 1) 17 maskedProbMap = mapSmooth * blobMask 18 _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap) 19 keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],)) 20 21 return keypoints 22 23 24detected_keypoints = [] 25keypoints_list = np.zeros((0,3)) 26keypoint_id = 0 27threshold = 0.1 28for i in range(nPoints): 29 probMap = output[0, i, :, :] 30 probMap = cv2.resize(probMap, (frameWidth, frameHeight)) 31 32 keypoints = getKeypoints(probMap, threshold) 33 keypoints_with_id = [] 34 for j in range(len(keypoints)): 35 keypoints_with_id.append(keypoints[j] + (keypoint_id,)) 36 keypoints_list = np.vstack([keypoints_list, keypoints[j]]) 37 keypoint_id += 1 38 39 detected_keypoints.append(keypoints_with_id) 40 41 42 43frameClone = cv2.cvtColor(frameCopy,cv2.COLOR_BGR2RGB) 44for i in range(nPoints): 45 for j in range(len(detected_keypoints[i])): 46 cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA) 47 48plt.imshow(frameClone) 49plt.show() Cuối cùng, chúng ta sẽ nối các điểm đặc trưng của các nhân vật thông qua thuật toán Part Affinity Heatmaps. Thuật toán này được đề xuất bởi nhóm tác giả Zhe Cao, Tomas Simon,Shih-En Wei, Yaser Sheikh thuộc phòng thí nghiệm The Robotics Institute trường đại học Carnegie Mellon. Các bạn có nhu cầu có thể tìm hiểu ở https://arxiv.org/pdf/1611.08050.pdf.\n1 2mapIdx = [[16,17], [18,19], [20,21], [22,23], [24,25], [26,27], [28,29], [30,31], [32,33], [34,35], [36,37], [38,39], [40,41], [42,43]] 3 4 5 6colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255], 7 [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255], 8 [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]] 9# Find valid connections between the different joints of a all persons present 10def getValidPairs(output): 11 valid_pairs = [] 12 invalid_pairs = [] 13 n_interp_samples = 10 14 paf_score_th = 0.1 15 conf_th = 0.5 16 # loop for every POSE_PAIR 17 for k in range(len(mapIdx)): 18 # A-\u0026gt;B constitute a limb 19 pafA = output[0, mapIdx[k][0], :, :] 20 pafB = output[0, mapIdx[k][1], :, :] 21 pafA = cv2.resize(pafA, (frameWidth, frameHeight)) 22 pafB = cv2.resize(pafB, (frameWidth, frameHeight)) 23 24 25 # Find the keypoints for the first and second limb 26 candA = detected_keypoints[POSE_PAIRS[k][0]] 27 candB = detected_keypoints[POSE_PAIRS[k][1]] 28 nA = len(candA) 29 nB = len(candB) 30 31 # fig=plt.figure(figsize=(8, 8)) 32 33 # interp_coord = list(zip(np.linspace(candA[0][0], candB[0][0], num=n_interp_samples), 34 # np.linspace(candA[0][1], candB[0][1], num=n_interp_samples))) 35 36 # frameClone1 = frameClone.copy() 37 # fig.add_subplot(1, 2, 1) 38 39 # for xx in interp_coord: 40 # cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA) 41 42 43 # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB)) 44 # plt.imshow(pafA, alpha=0.5) 45 46 # frameClone1 = frameClone.copy() 47 # fig.add_subplot(1, 2, 2) 48 49 50 51 52 # for xx in interp_coord: 53 # cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA) 54 55 # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB)) 56 # plt.imshow(pafB, alpha=0.5) 57 # plt.show() 58 59 60 61 62 63 # If keypoints for the joint-pair is detected 64 # check every joint in candA with every joint in candB 65 # Calculate the distance vector between the two joints 66 # Find the PAF values at a set of interpolated points between the joints 67 # Use the above formula to compute a score to mark the connection valid 68 69 if( nA != 0 and nB != 0): 70 valid_pair = np.zeros((0,3)) 71 for i in range(nA): 72 max_j=-1 73 maxScore = -1 74 found = 0 75 for j in range(nB): 76 # Find d_ij 77 d_ij = np.subtract(candB[j][:2], candA[i][:2]) 78 norm = np.linalg.norm(d_ij) 79 if norm: 80 d_ij = d_ij / norm 81 else: 82 continue 83 # Find p(u) 84 interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples), 85 np.linspace(candA[i][1], candB[j][1], num=n_interp_samples))) 86 # Find L(p(u)) 87 paf_interp = [] 88 for k in range(len(interp_coord)): 89 paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))], 90 pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) 91 # Find E 92 paf_scores = np.dot(paf_interp, d_ij) 93 avg_paf_score = sum(paf_scores)/len(paf_scores) 94 95 # Check if the connection is valid 96 # If the fraction of interpolated vectors aligned with PAF is higher then threshold -\u0026gt; Valid Pair 97 if ( len(np.where(paf_scores \u0026gt; paf_score_th)[0]) / n_interp_samples ) \u0026gt; conf_th : 98 if avg_paf_score \u0026gt; maxScore: 99 max_j = j 100 maxScore = avg_paf_score 101 found = 1 102 # Append the connection to the list 103 if found: 104 valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0) 105 106 # Append the detected connections to the global list 107 valid_pairs.append(valid_pair) 108 109 pprint(valid_pair) 110 else: # If no keypoints are detected 111 print(\u0026#34;No Connection : k = {}\u0026#34;.format(k)) 112 invalid_pairs.append(k) 113 valid_pairs.append([]) 114 pprint(valid_pairs) 115 return valid_pairs, invalid_pairs 116 117# This function creates a list of keypoints belonging to each person 118# For each detected valid pair, it assigns the joint(s) to a person 119# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint 120def getPersonwiseKeypoints(valid_pairs, invalid_pairs): 121 # the last number in each row is the overall score 122 personwiseKeypoints = -1 * np.ones((0, 19)) 123 124 for k in range(len(mapIdx)): 125 if k not in invalid_pairs: 126 partAs = valid_pairs[k][:,0] 127 partBs = valid_pairs[k][:,1] 128 indexA, indexB = np.array(POSE_PAIRS[k]) 129 130 for i in range(len(valid_pairs[k])): 131 found = 0 132 person_idx = -1 133 for j in range(len(personwiseKeypoints)): 134 if personwiseKeypoints[j][indexA] == partAs[i]: 135 person_idx = j 136 found = 1 137 break 138 139 if found: 140 personwiseKeypoints[person_idx][indexB] = partBs[i] 141 personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2] 142 143 # if find no partA in the subset, create a new subset 144 elif not found and k \u0026lt; 17: 145 row = -1 * np.ones(19) 146 row[indexA] = partAs[i] 147 row[indexB] = partBs[i] 148 # add the keypoint_scores for the two keypoints and the paf_score 149 row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2] 150 personwiseKeypoints = np.vstack([personwiseKeypoints, row]) 151 return personwiseKeypoints 152 153valid_pairs, invalid_pairs = getValidPairs(output) 154 155personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs) 156 157 158for i in range(nPoints-1): 159 for n in range(len(personwiseKeypoints)): 160 161 index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])] 162 if -1 in index: 163 continue 164 B = np.int32(keypoints_list[index.astype(int), 0]) 165 A = np.int32(keypoints_list[index.astype(int), 1]) 166 cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA) 167 168 169 170plt.imshow(frameClone) 171 # plt.imshow(mapMask, alpha=0.5) 172plt.show() Hẹn gặp lại các bạn ở những bài viết tiếp theo.\nBài viết này được viết dựa vào nguồn https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/ của tác giả VIKAS GUPTA. Tôi sử dụng tập model và hình ảnh khác với bài viết nguyên gốc của tác giả.\n","date":"Oct 5, 2018","img":"","permalink":"/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/","series":null,"tags":["Machine learning","Deeplearning","multiple pose estimation"],"title":"Deep Learning Based Multiple Human Pose Estimation Using OpenCV"},{"categories":null,"content":"Lời mở đầu Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv \u0026gt; 3.4.1.\nPose Estimation là gì? Post Estimation ( đôi khi được dùng với thuật ngữ Keypoint Detection) là một vấn đề khá phổ biến trong lĩnh vực xử lý ảnh khi chúng ta cần xác định vị trí và hướng của một đối tượng. Mức ý nghĩa ở đây là chúng ta phải rút ra được những đặc điểm chính, những đặc điểm đó là những đặc trưng của đối tượng ( có thể mô tả được đối tượng).\nVí dụ, trong bài toán face pose estimation ( có tên khác là facial landmark detection), chúng ta cần xác định được đâu là vị trí của những điểm landmark trên khuôn mặt người.\nMột bài toán có liên quan đến bài toán trên là head pose estimation. Chúng ta cần xác định những điểm landmark để mô hình hoá lại được mô hình 3D của đầu người.\nỞ trong bài viết này, chúng ta đề cập đến bài toán human pose estimation, công việc chính là xác định và chỉ ra được một phần/ toàn bộ các phần chính của cơ thể con người (vd vai, khuỷu tay, cổ tay, đầu gối v.v).\nTrong bài viết này, chúng ta sẽ sử dụng mô hình được huấn luyện sẵn để chỉ ra các phần chính của cơ thể con người. Kết quả cơ bản của phần nhận diện này sẽ gần giống như hình bên dưới.\nSử dụng pretrain model trong bài toán Pose Estimation Vào nằm 2016, 2017, Phòng thí nghiệm Perceptual Computing của trường đại học Carnegie Mellon University đã công bố một bài báo có liên quan đến chủ đề Multi-Person Pose Estimation. Và đến nay, họ đã công bố mô hình huấn luyện cho chúng ta sử dụng. Các bạn có nhu cầu tìm hiểu sâu hơn có thể đọc kỹ nguồn dữ liệu của họ công bố ở link https://github.com/CMU-Perceptual-Computing-Lab/openpose.\nTrong bài post này, mình sẽ không đề cập kỹ đến phần kiến trúc mạng neural net họ sử dụng bên dưới, thay vào đó, mình sẽ tập trung hơn vào cách thức sử dụng mô hình để thu được kết quả cần thiết.\nTrước khi bắt đầu vào thực hành, mình sẽ mô tả một chút về mô hình pretrain có sẵn. Ở đây, họ cung cấp cho chúng ta 2 mô hình là MPII model và COCO model. Đó chính là tên của hai bộ database mà họ sử dụng để đào tạo mô hình. Kết quả trả về của mỗ bộ database là khác nhau hoàn toàn.\nVới bộ COCO dataset, kết quả trả về là 18 đặc trưng gồm các thông tin:\n1Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4, 2Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8, 3Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12, 4LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16, 5Left Ear – 17, Background – 18 Với bộ MPII, kết quả trả về là 15 đặc trưng gồm các thông tin:\n1Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4, 2Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8, 3Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12, 4Left Ankle – 13, Chest – 14, Background – 15 Trong phần này, chúng ta sẽ tập trung vào mô hình MPII, mô hình COCO sử dụng tương tự, chỉ việc thay lại đường dẫn file mô hình là được.\nBắt đầu code. Bước 1: Download mô hình.\nNhóm tác giả sử dụng caffe để huấn luyện mô hình, do đó, để sử dụng được, chúng ta cần download file mô hình ở đường dẫn http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel và file cấu hình ở đường dẫn http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_deploy_linevec.prototxt. Các bạn có thể để đâu đó tuỳ thích, ở đây tôi để trong thư mục pose/mpi để dễ dàng nhận biết với các mô hình khác.\nBước 2: Load mô hình.\nĐể load mô hình lên bộ nhớ chính, đơn giản là chúng ta thực hiện câu lệnh sau trong python\n1import cv2 2# Specify the paths for the 2 files 3protoFile = \u0026#34;pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\u0026#34; 4weightsFile = \u0026#34;pose/mpi/pose_iter_160000.caffemodel\u0026#34; 5 6# Read the network into Memory 7net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile) Đơn giản quá phải không các bạn :).\nBước 3: Đọc ảnh và đưa ảnh vào trong mô hình.\n1 2# Read image 3frame = cv2.imread(\u0026#34;img2.jpg\u0026#34;) 4 5frameCopy = np.copy(frame) 6frameWidth = frame.shape[1] 7frameHeight = frame.shape[0] 8t = time.time() 9# Specify the input image dimensions 10inWidth = 368 11inHeight = 368 12 13# Prepare the frame to be fed to the network 14inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False) 15 16# Set the prepared object as the input blob of the network 17net.setInput(inpBlob) Chắc không cần phải nói gì thêm, phần comment chú thích đã mô tả khá đầy đủ chức năng của từng phần trong này rồi.\nBước 4: Thu thập kết quả và trích xuất điểm đặc trưng\n1 2frameCopy = frame.copy() 3 4output = net.forward() 5print(\u0026#34;time taken by network : {:.3f}\u0026#34;.format(time.time() - t)) 6H = output.shape[2] 7W = output.shape[3] 8 9nPoints = 15 10POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ] 11 12 13threshold = 0.01 14# Empty list to store the detected keypoints 15points = [] 16for i in range(nPoints): 17 # confidence map of corresponding body\u0026#39;s part. 18 probMap = output[0, i, :, :] 19 20 # Find global maxima of the probMap. 21 minVal, prob, minLoc, point = cv2.minMaxLoc(probMap) 22 23 # Scale the point to fit on the original image 24 x = (frameWidth * point[0]) / W 25 y = (frameHeight * point[1]) / H 26 27 print(prob) 28 29 if prob \u0026gt; threshold : 30 cv2.circle(frame, (int(x), int(y)), 15, (0, 255, 255), thickness=-1, lineType=cv2.FILLED) 31 cv2.putText(frame, \u0026#34;{}\u0026#34;.format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 2, lineType=cv2.LINE_AA) 32 33 # Add the point to the list if the probability is greater than the threshold 34 points.append((int(x), int(y))) 35 else : 36 points.append(None) 37 38# cv2.imshow(\u0026#34;Output-Keypoints\u0026#34;,frame) 39# cv2.waitKey(0) 40# cv2.destroyAllWindows() 41 42cv2.imwrite(\u0026#34;dot_keypoint.png\u0026#34;,frame) 43 44# Draw Skeleton 45for pair in POSE_PAIRS: 46 partA = pair[0] 47 partB = pair[1] 48 49 if points[partA] and points[partB]: 50 cv2.line(frameCopy, points[partA], points[partB], (0, 255, 255), 2) 51 cv2.circle(frameCopy, points[partA], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED) 52 cv2.circle(frameCopy, points[partB], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED) 53 54 55cv2.imwrite(\u0026#34;line_keypoint.png\u0026#34;,frameCopy) Kết quả của giá trị output là một ma trận 4D, với ý nghĩa của mỗi chiều như sau:\nChiều đầu tiên là image ID (định danh ảnh trong trường hợp bạn truyền nhiều ảnh vào mạng) Chiều thứ 2 là chỉ số của các điểm đặc trưng. Tập MPI trả về tập gồm 44 điểm dữ liệu, ta chỉ sử dụng một vài điểm dữ liệu tương ứng với vị trí các điểm đặc trưng mà chúng ta quan tâm. Chiều thứ 3 là height của output map. Chiều thứ 4 là width của output map. Một lưu ý ở đây là tôi có sử dụng đặt giá trị chặn dưới threshold để giảm thiểu sự sai sót do nhận diện sai. Và kết quả đạt được là hai hình bên dưới: Hẹn gặp lại các bạn ở những bài viết tiếp theo.\n","date":"Oct 4, 2018","img":"","permalink":"/blog/2018-10-04-deep-learning-base-human-pose-estimation/","series":null,"tags":["Machine learning","Deeplearning","pose estimation"],"title":"Deep Learning Based Human Pose Estimation Using OpenCV"},{"categories":null,"content":" Lời mở đầu Khái niện Epoch Batch Size Iterations Tại sao phải dùng hơn 1 Epoch. Số lần lặp tối ưu là bao nhiêu? Repeat Regularization Images Lời mở đầu Khi mới bắt đầu bước vào thế giới của ML/DL chúng ta sẽ bắt gặp các thuật ngữ Epoch - Batch size và Iterations. Và sẽ cảm thấy bối rối vì chúng khá giống nhau, nhưng thực tế là chúng khác xa nhau.\nĐể cho dễ hình dung, mình lấy ví dụ về việc ăn cơm. Chúng ta không thể ăn một lần hết một chén cơm được, mà phải mỗi lần ăn phải xúc từng muỗn ăn. Xúc lần lượt khi hết bát thứ nhất, chúng ta lại ăn tiếp bát thứ 2, bát thứ 3 \u0026hellip; đến khi no, kết thúc bữa ăn.\nLiên tưởng giữa việt ăn cơm và các thuật ngữ epoch, batch size, iteration như sau:\nbatch size: Số hạt cơm trong 1 lần xúc.\nIteration : Số lần xúc cơm hết 1 bát.\nepoch : Số bát cơm bạn ăn trong 1 bữa ăn.\nHết phần diễn giải bằng ví dụ. Đến phần viết hàn lâm bên dưới, nếu bạn nào đã hiểu rồi thì có thể bỏ qua, bạn nào muốn đào sâu thêm lý do thì xem mình diễn giải bên dưới.\nĐể hiểu rõ sự khác biệt giữa chúng, các bạn cần tìm hiểu một khái niệm vô cùng quan trọng trong machine learning - Gradient Descent.\nĐịnh nghĩa ngắn gọn của Gradient Descent:\nGradient Descent là thuật toán lặp tối ưu (iteractive optimization algorithm) được sử dụng trong machine learning để tìm kết quả tốt nhất (minima of a curve).\nTrong đó:\n..* Gradient có nghĩa là tỷ lệ của độ nghiêng của đường dốc.\n..* Descent là từ viết tắt của decending - nghĩa là giảm.\nThuật toán sẽ lặp đi lặp lại nhiều lần để tìm ra được cực tiểu.\nhttps://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a Nguồn ảnh\nCác bạn quan sát hình phía trên bên trái, ban đầu, bước nhảy khá lớn, nghĩa là giá trị cost lớn, và sau một vài lần lặp, điểm chấm đen đi xuống dần, và giá trị cost nhỏ dần theo. Mô hình hội tụ dần dần đến khi cost \u0026lt;= epselon\nChúng ta sử dụng thuật ngữ epochs, batch size, iterations khi chúng ta cần phải trainning mô hình machine learning, mà tập trainset của chúng ta quá (rất) lớn (vd 10 triệu mẫu, ví dụ train mô hình nhận dạng khuôn mặt với tập ms-celeb-1m). Lúc này các khái niệm trên mới trở nên rõ ràng, còn với trường hợp dữ liệu nhỏ thì chúng khá tương tự nhau.\nKhái niện Epoch Một Epoch được tính là khi chúng ta đưa tất cả dữ liệu trong tập train vào mạng neural network 1 lần. Ví dụ, bạn có 10 bức hình trong tập train, bạn đem hết toàn bộ 10 bức hình đó cho mô hình học ở lần thứ nhất, bạn đã train được một epoch. Sau đó, bạn lại quay lại vị trí hình ban đầu rồi cho toàn bộ 10 bức hình đó học, bạn có thêm 1 epoch nữa, vậy là bạn đã train 2 epoch, bạn lặp lại việc này 100 lần , suy ra bạn đã train 100 epoch.\nKhi dữ liệu quá lớn, chúng ta không thể đưa hết tất cả tập dữ liệu vào để huấn luyện trong 1 lần train được, vì bạn cần một siêu máy tính có lượng RAM và GPU RAM cực lớn để lưu trữ toàn bộ hình ảnh trên, điều này là bất khả thi đối với người dùng bình thường, phòng lab nhỏ, hoặc các hệ thống máy tính hiện tại. Buộc lòng chúng ta phải chia nhỏ tập dữ liệu ra, và khái niệm batch hình thành.\nBatch Size Batch size là số lượng mẫu dữ liệu trong một lần train. Ví dụ, trong bài toán phân loại chó mèo, chọn batch size =32, nghĩa là 1 lần train ta sẽ cho ngẫu nhiên 32 bức nhìn chó hoặc mèo chạy lan truyền tiến trong mạng neural network. Tiếp theo bạn quăng tiếp 32 hình ngẫu nhiên, không lặp với các hình trước đó, vào mạng, quăng đến khi nào không còn hình nào có thể quăng vào nữa -\u0026gt; bạn hoàn thành 1 epoch.\nIterations Iterations là số lượng batchs cần để hoàn thành 1 epoch.\n$$Iterations = data size / batch size$$\nVí dụ chúng ta có tập dữ liệu có 20,000 mẫu, batch size là 500, vậy chúng ta cần 40 lần lặp (iteration) để hoàn thành 1 epoch.\nTại sao phải dùng hơn 1 Epoch. Câu trả lời ở đây là tại vì chúng ta đang dùng thuật toán tối ưu là Gradient Descent. Thuật toán này đòi hỏi chúng ta phải đem toàn bộ dữ liệu qua mạng một vài lần để tìm được kết quả tối ưu. Vì vậy, dùng 1 epoch thật sự không đủ để tìm được kết quả tốt nhất.\nVới việc chỉ sử dụng 1 lần lặp, xác suất rất cao là dữ liệu sẽ bị underfitting(như hình mô tả bên dưới).\nKhi số lần lặp tăng dần, trạng thái của mô hình sẽ chuyển dần từ underfitting sang optimal và sau đó là overfitting (thông thường là vậy, trừ khi mô hình huấn luyện của bạn đang sử dụng quá đơn giản, quá ít trọng số thì chúng không thể nào overfitting nổi).\nChúng ta có thể dùng 1 epoch để huấn luyện mô hình, với điều kiện là ta sử dụng thuật toán tối ưu không phải là gradient descent.\nSố lần lặp tối ưu là bao nhiêu? Tiếc rằng không có câu trả lời cho câu hỏi này. Phụ thuộc hoàn toàn vào nhiều yếu tố. Mục tiêu chung là ta sẽ lặp đến khi nào hội tụ. Có một số phương pháp giúp chúng ta xác định mô hình đã đứng ở ngưỡng cực tiểu cục bộ rồi, không thể xuống hơn được nữa.\nCác bạn có thể tìm hiểu với từ khóa early stopping.\nRepeat update 13/01/2025\nGần đây, với sự phát triển của stable diffusion và flux, chúng ta có thêm tham số Repeat, được hiểu là số lần 1 hình sẽ được lặp lại trong quá trình train.\nHiểu đơn giản là khi train lora, trong 1 batch, 1 hình trong batch đó sẽ Repeat x lần\nMục tiêu của Repeat trong stable diffusion\nBalance the number of training images to the regularization images. The number of regularization images is larger than the training, so it is required to repeat training images for using all regularization images in the epoch.\nControl \u0026lsquo;weight\u0026rsquo; over folders. If you have high quality images and low quality images, you can set higher number of repeats for high quality images, and lower for low quality.\nRegularization Images Hình ảnh điều chỉnh (Regularization images) là các hình ảnh được sử dụng như một phần của quá trình điều chỉnh nhằm cải thiện sự ổn định và hiệu suất của các mô hình học sâu (deep learning).\nQuá trình này giúp ngăn mô hình học quá mức từ dữ liệu huấn luyện, duy trì sự cân bằng giữa các lớp, và bảo toàn tính linh hoạt trong việc tạo ra hình ảnh mới.\nRegularization giúp giải quyết hai vấn đề chính: quá khớp (overfitting) và bảo toàn lớp (class preservation).\nBảo Toàn Lớp (Class Preservation):\nKhi tạo ra các hình ảnh điều chỉnh, bạn đang định nghĩa một \u0026ldquo;lớp\u0026rdquo; của những gì bạn muốn nghịch đảo. Ví dụ, nếu bạn đang cố nghịch đảo một chiếc máy bay mới, bạn có thể tạo một loạt hình ảnh về máy bay để làm hình ảnh điều chỉnh. Điều này giúp đảm bảo rằng quá trình huấn luyện không bị lệch sang một lớp khác, chẳng hạn như \u0026ldquo;ô tô\u0026rdquo; hoặc \u0026ldquo;xe đạp\u0026rdquo;. Thậm chí, nó còn giúp ngăn việc mô hình bị nghiêng về hướng \u0026ldquo;máy bay đồ chơi\u0026rdquo; nếu bạn sử dụng các tham chiếu thực tế thay vì các diễn giải trừu tượng. Chống Quá Khớp (Overfitting):\nNhững hình ảnh điều chỉnh này cũng được sử dụng trong quá trình huấn luyện để đảm bảo rằng các hình ảnh bạn đang cố nghịch đảo không bị quá khớp. Nếu quá khớp xảy ra, các hình ảnh được tạo ra có thể giống hệt với tập huấn luyện, làm mất khả năng chỉnh sửa của chúng. Một trong những vấn đề của nghịch đảo văn bản (textual inversion) là bạn có thể mất khả năng chỉnh sửa hình ảnh trong quá trình nghịch đảo, đặc biệt khi huấn luyện quá lâu. Việc thêm hình ảnh điều chỉnh vào quá trình huấn luyện giúp ngăn chặn vấn đề này. Hiện Trạng Của Dreambooth:\nVới cách triển khai hiện tại của Dreambooth, một số hiện tượng lệch hướng (drifting) vẫn có thể xảy ra. Ví dụ: nếu bạn nghịch đảo hình ảnh một con ếch, các thế hệ mới có thể có đặc điểm giống ếch. Tuy nhiên, mô hình vẫn hoạt động khá tốt miễn là bạn giữ mọi thứ hợp lý với dữ liệu đã huấn luyện. Tóm lại, hình ảnh điều chỉnh không chỉ giúp duy trì tính nguyên bản của lớp mà bạn đang làm việc, mà còn giảm nguy cơ quá khớp, cải thiện khả năng chỉnh sửa và tính linh hoạt của các hình ảnh được tạo ra.\nNguồn của phần Regularization Images https://www.reddit.com/r/StableDiffusion/comments/xu1ill/comment/iqu81m7/\nCảm ơn các bạn đã theo dõi bài viết.\nNguồn: https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n","date":"Oct 2, 2018","img":"https://unsplash.it/1920/1080?image=99","permalink":"/blog/2018-10-02-understanding-epoch-batchsize-iterations/","series":null,"tags":["Machine learning","Deeplearning","Epoch","Batch Size","Iteration"],"title":"Phân Biệt Epoch - Batch Size Và Iterations"},{"categories":null,"content":" Lời mở đầu Chuẩn bị dữ liệu Loading và parsing dữ liệu. Collaborative Filtering Chọn các tham số cho ALS Xây dựng mô hình với tập dữ liệu large Xây dựng mô hình dự đoán phim Dự đoán rating của 1 cá nhân Lưu trữ mô hình Lời mở đầu MovieLens là một tập dữ liệu được sử dụng rộng rãi cách đây nhiều năm. Hôm nay, mình sẽ sử dụng tập dữ liệu này và mô hình ALS của spark để xây dựng chương trình dự đoán phim cho người dùng.\nChuẩn bị dữ liệu Các bạn có thể download tập dữ liệu MovieLens ở link https://grouplens.org/datasets/movielens/. Các bạn có thể download trực tiếp 2 file nén ở link http://files.grouplens.org/datasets/movielens/ml-latest-small.zip và link http://files.grouplens.org/datasets/movielens/ml-latest.zip.\nỞ trên bao gồm 2 tập dữ liệu. chúng ta tạo thư mục datasets và download rồi bỏ chúng vào trong thư mục đấy.\n1complete_dataset_url = \u0026#39;http://files.grouplens.org/datasets/movielens/ml-latest.zip\u0026#39; 2small_dataset_url = \u0026#39;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\u0026#39; 3 4import os 5 6datasets_path = \u0026#39;datasets\u0026#39; 7if not os.path.exists(datasets_path): 8 os.makedirs(datasets_path)) 9 10complete_dataset_path = os.path.join(datasets_path, \u0026#39;ml-latest.zip\u0026#39;) 11small_dataset_path = os.path.join(datasets_path, \u0026#39;ml-latest-small.zip\u0026#39;) 12 13import urllib 14import zipfile 15 16if not os.path.exists(small_dataset_url): 17\tsmall_f = urllib.urlretrieve (small_dataset_url, small_dataset_path)#Download 18\twith zipfile.ZipFile(small_dataset_path, \u0026#34;r\u0026#34;) as z:#Giải nén 19\tz.extractall(datasets_path) 20if not os.path.exists(small_dataset_url): 21\tcomplete_f = urllib.urlretrieve (complete_dataset_url, complete_dataset_path)#Download 22\twith zipfile.ZipFile(complete_dataset_path, \u0026#34;r\u0026#34;) as z:#Giải nén 23\tz.extractall(datasets_path) Trong thư mục giải nén, chúng ta sẽ có các file ratings.csv, movies.csv, tags.csv, links.csv, README.txt.\nLoading và parsing dữ liệu. Mỗi dòng trong tập ratings.csv có định dạng \u0026quot;userId,movieId,rating,timestamp\u0026quot;.\nMỗi dòng trong tập movies.csv có định dạng \u0026quot;movieId,title,genres\u0026quot;.\nMỗi dòng trong tập tags.csv có định dạng \u0026quot;userId,movieId,tag,timestamp\u0026quot;.\nMỗi dòng trong tập links.csv có định dạng \u0026quot;movieId,imdbId,tmdbId\u0026quot;.\nTóm lại, các trường dữ liệu trong các file csv đều ngăn cách nhau bởi dấu phẩy (,). Trong python, ta có thể dùng hàm split để cắt chúng ra. Sau đó sẽ load toàn bộ dữ liệu lên RDDs.\nLưu ý nhỏ:\nỞ tập dữ liệu ratings, chúng ta chỉ giữ lại các trường (UserID, MovieID, Rating) bỏ đi trường timestamp vì không cần thiết. Ở tập dữ liệu movies chúng ta giữ lại trường (MovieID, Title) và bỏ đi trường genres vì lý do tương tự. 1small_ratings_file = os.path.join(datasets_path, \u0026#39;ml-latest-small\u0026#39;, \u0026#39;ratings.csv\u0026#39;) 2small_ratings_raw_data = sc.textFile(small_ratings_file) 3small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0] 4small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header).map(lambda line: line.split(\u0026#34;,\u0026#34;)).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache() 5print(small_ratings_data.take(3)) #Hiện thị top 3 ratting đầu tiên 6 7small_movies_file = os.path.join(datasets_path, \u0026#39;ml-latest-small\u0026#39;, \u0026#39;movies.csv\u0026#39;) 8 9small_movies_raw_data = sc.textFile(small_movies_file) 10small_movies_raw_data_header = small_movies_raw_data.take(1)[0] 11 12small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\ 13 .map(lambda line: line.split(\u0026#34;,\u0026#34;)).map(lambda tokens: (tokens[0],tokens[1])).cache() 14 15small_movies_data.take(3) #Hiện thị top 3 movie đầu tiên Phần tiếp theo, chúng ta sẽ tìm hiểu lọc cộng tác (Collaborative Filtering) và cách sử dụng Spark MLlib để xây dựng mô hình dự báo.\nCollaborative Filtering Ở đây, tôi sẽ không đề cập đến lọc cộng tác là gì, các bạn có nhu cầu tìm hiểu có thể xem ở bài post khác hoặc tham khảo trên wiki. Chúng ta sẽ tập trung vào tìm hiểu cách sử dụng ALS trong thư viện MLlib của Spark. Các tham số của thuật toán này bao gồm:\nnumBlocks: số lượng block được sử dụng trong tính toán song song (-1 với ý nghĩa là auto configure).\nrank: số lượng nhân tố ẩn (latent factor) trong mô hình.\niterations: số lần lặp.\nlambda: tham số của chuẩn hoá(regularization ) trong ALS.\nimplicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.\nalpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.\nChọn các tham số cho ALS Để chọn được các tham số tốt nhất cho mô hình ALS, chúng ta sẽ sử dụng tập small để grid search. Đầu tiên, chúng ta chia tập dữ liệu thành 3 phần là tập train, tập vali và tập test. Sau đó tiến hành huấn luyện trên tập train và predict trên tập valid để tìm được tham số tốt nhất. Cuối cùng đánh giá kết quả đạt được trên tập test.\n1training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0) 2validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1])) 3test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1])) 4 5from pyspark.mllib.recommendation import ALS 6import math 7 8seed = 5L 9iterations = 10 10regularization_parameter = 0.1 11ranks = [4, 8, 12] 12errors = [0, 0, 0] 13err = 0 14tolerance = 0.02 15 16min_error = float(\u0026#39;inf\u0026#39;) 17best_rank = -1 18best_iteration = -1 19for rank in ranks: 20 model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations, 21 lambda_=regularization_parameter) 22 predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2])) 23 rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) 24 error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) 25 errors[err] = error 26 err += 1 27 print(\u0026#39;For rank %s the RMSE is %s\u0026#39; % (rank, error)) 28 if error \u0026lt; min_error: 29 min_error = error 30 best_rank = rank 31 32print(\u0026#39;The best model was trained with rank %s\u0026#39; % best_rank) Kết quả sau khi thực hiện đoạn code trên là:\n1For rank 4 the RMSE is 0.963681878574 2For rank 8 the RMSE is 0.96250475933 3For rank 12 the RMSE is 0.971647563632 4The best model was trained with rank 8 Tiến hành thực hiện test.\n1model_test = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations, 2 lambda_=regularization_parameter) 3predictions = model_test.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2])) 4rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) 5error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) 6 7print(\u0026#39;For testing data the RMSE is %s\u0026#39; % (error)) 1For testing data the RMSE is 0.972342381898 Xem kỹ hơn một chút về dữ liệu mà spark trả về cho chúng ta. Với predictions và rates_and_preds, ta có:\n1print(predictions.take(3)) 1[((32, 4018), 3.280114696166238), 2 ((375, 4018), 2.7365714977314086), 3 ((674, 4018), 2.510684514310653)] Tập dữ liệu trả về bao gồm cặp (UserID, MovieID) và Rating (tương ứng với colum 0, column 1 và column 2 ở trên),được hiểu ở đây là với người dùng UserID và phim MovieID thì mô hình sẽ dự đoán người dùng sẽ rating kết quả Rating.\nSau đó chúng ta sẽ nối(join) chúng với tập valid tương ứng theo cặp (UserID, MovieID), kết quả đạt được là:\n1rates_and_preds.take(3) 1[((558, 788), (3.0, 3.0419325487471403)), 2 ((176, 3550), (4.5, 3.3214065001580986)), 3 ((302, 3908), (1.0, 2.4728711204440765))] Việc còn lại là chúng ta sẽ tính trung bình độ lỗi bằng hàm mean() và sqlt().\nXây dựng mô hình với tập dữ liệu large Tiếp theo, chúng ta sẽ sử dụng tập dự liệu bự hơn để xây dựng mô hình. Cách thực hiện y chang như tập dữ liệu nhỏ đã được trình bày ở trên, nên tôi sẽ bỏ qua một số giải thích không cần thiết để tránh lặp lại.\n1# Load the complete dataset file 2complete_ratings_file = os.path.join(datasets_path, \u0026#39;ml-latest\u0026#39;, \u0026#39;ratings.csv\u0026#39;) 3complete_ratings_raw_data = sc.textFile(complete_ratings_file) 4complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0] 5 6# Parse 7complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\ 8 .map(lambda line: line.split(\u0026#34;,\u0026#34;)).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache() 9 10print(\u0026#34;There are %s recommendations in the complete dataset\u0026#34; % (complete_ratings_data.count())) 1There are 21063128 recommendations in the complete dataset Tiến hành train và test.\n1training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0) 2 3complete_model = ALS.train(training_RDD, best_rank, seed=seed,iterations=iterations, lambda_=regularization_parameter) 4 5test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1])) 6 7predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2])) 8rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) 9error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) 10 11print(\u0026#39;For testing data the RMSE is %s\u0026#39; % (error)) 1For testing data the RMSE is 0.82183583368 Xây dựng mô hình dự đoán phim 1complete_movies_file = os.path.join(datasets_path, \u0026#39;ml-latest\u0026#39;, \u0026#39;movies.csv\u0026#39;) 2complete_movies_raw_data = sc.textFile(complete_movies_file) 3complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0] 4 5# Parse 6complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\ 7 .map(lambda line: line.split(\u0026#34;,\u0026#34;)).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache() 8 9complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1])) 10 11print(\u0026#34;There are %s movies in the complete dataset\u0026#34; % (complete_movies_titles.count())) 1There are 27303 movies in the complete dataset 1def get_counts_and_averages(ID_and_ratings_tuple): 2 nratings = len(ID_and_ratings_tuple[1]) 3 return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings) 4 5movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey()) 6movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages) 7movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0])) Giả sử chúng ta có 1 người dùng mới, với các ratting như sau:\n1new_user_ID = 0 2 3# The format of each line is (userID, movieID, rating) 4new_user_ratings = [ 5 (0,260,4), # Star Wars (1977) 6 (0,1,3), # Toy Story (1995) 7 (0,16,3), # Casino (1995) 8 (0,25,4), # Leaving Las Vegas (1995) 9 (0,32,4), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995) 10 (0,335,1), # Flintstones, The (1994) 11 (0,379,1), # Timecop (1994) 12 (0,296,3), # Pulp Fiction (1994) 13 (0,858,5) , # Godfather, The (1972) 14 (0,50,4) # Usual Suspects, The (1995) 15 ] 16new_user_ratings_RDD = sc.parallelize(new_user_ratings) 17print(\u0026#39;New user ratings: %s\u0026#39; % new_user_ratings_RDD.take(10)) 1New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)] Chúng ta tiến hành huấn luyện lại mô hình khi có thêm người mới:\n1complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD) 2 3from time import time 4 5t0 = time() 6new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, 7 iterations=iterations, lambda_=regularization_parameter) 8tt = time() - t0 9 10print(\u0026#34;New model trained in %s seconds\u0026#34; % round(tt,3)) 1New model trained in 56.61 seconds Tiến hành dự đoán ratting của người dùng mới cho toàn bộ các phim người dùng đó chưa xem.\n1new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs 2# keep just those not on the ID list (thanks Lei Li for spotting the error!) 3new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0]))) 4 5# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies 6new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD) Và show ra top 3 kết quả :\n1# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating) 2new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating)) 3new_user_recommendations_rating_title_and_count_RDD = \\ 4 new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD) 5new_user_recommendations_rating_title_and_count_RDD.take(3) Hiển thị top recommend (Ở đây sẽ flat dữ liệu hiển thị thành dàng ((Title, Rating, Ratings Count)) ra cho dễ nhìn).\n1new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1])) 2 3top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]\u0026gt;=25).takeOrdered(25, key=lambda x: -x[1]) 4 5print (\u0026#39;TOP recommended movies (with more than 25 reviews):\\n%s\u0026#39; % 6 \u0026#39;\\n\u0026#39;.join(map(str, top_movies))) 1TOP recommended movies (with more than 25 reviews): 2 (u\u0026#39;\u0026#34;Godfather: Part II\u0026#39;, 8.503749129186701, 29198) 3 (u\u0026#39;\u0026#34;Civil War\u0026#39;, 8.386497469089297, 257) 4 (u\u0026#39;Frozen Planet (2011)\u0026#39;, 8.372705479107108, 31) 5 (u\u0026#39;\u0026#34;Shawshank Redemption\u0026#39;, 8.258510064442426, 67741) 6 (u\u0026#39;Cosmos (1980)\u0026#39;, 8.252254825768972, 948) 7 (u\u0026#39;Band of Brothers (2001)\u0026#39;, 8.225114960311624, 4450) 8 (u\u0026#39;Generation Kill (2008)\u0026#39;, 8.206487040524653, 52) 9 (u\u0026#34;Schindler\u0026#39;s List (1993)\u0026#34;, 8.172761674773625, 53609) 10 (u\u0026#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)\u0026#39;, 8.166229786764168, 23915) 11 (u\u0026#34;One Flew Over the Cuckoo\u0026#39;s Nest (1975)\u0026#34;, 8.15617022970577, 32948) 12 (u\u0026#39;Casablanca (1942)\u0026#39;, 8.141303207981174, 26114) 13 (u\u0026#39;Seven Samurai (Shichinin no samurai) (1954)\u0026#39;, 8.139633165142612, 11796) 14 (u\u0026#39;Goodfellas (1990)\u0026#39;, 8.12931139039048, 27123) 15 (u\u0026#39;Star Wars: Episode V - The Empire Strikes Back (1980)\u0026#39;, 8.124225700242096, 47710) 16 (u\u0026#39;Jazz (2001)\u0026#39;, 8.078538221315313, 25) 17 (u\u0026#34;Long Night\u0026#39;s Journey Into Day (2000)\u0026#34;, 8.050176820606127, 34) 18 (u\u0026#39;Lawrence of Arabia (1962)\u0026#39;, 8.041331489948814, 13452) 19 (u\u0026#39;Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\u0026#39;, 8.0399424815528, 45908) 20 (u\u0026#39;12 Angry Men (1957)\u0026#39;, 8.011389274280754, 13235) 21 (u\u0026#34;It\u0026#39;s Such a Beautiful Day (2012)\u0026#34;, 8.007734839026181, 35) 22 (u\u0026#39;Apocalypse Now (1979)\u0026#39;, 8.005094327199552, 23905) 23 (u\u0026#39;Paths of Glory (1957)\u0026#39;, 7.999379786394267, 3598) 24 (u\u0026#39;Rear Window (1954)\u0026#39;, 7.9860865203540214, 17996) 25 (u\u0026#39;State of Play (2003)\u0026#39;, 7.981582126801772, 27) 26 (u\u0026#39;Chinatown (1974)\u0026#39;, 7.978673289692703, 16195) Dự đoán rating của 1 cá nhân Một trường hợp khác là chúng ta cần dự đoán giá trị ratting của 1 người dùng với 1 bộ phim cụ thể nào đó.\n1my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994) 2individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD) 3individual_movie_rating_RDD.take(1) 1[Rating(user=0, product=122880, rating=4.955831875971526)] Lưu trữ mô hình Sau khi có được mô hình. Chúng ta cần phải lưu trữ chúng lại để sau này dùng.\n1from pyspark.mllib.recommendation import MatrixFactorizationModel 2 3model_path = os.path.join(\u0026#39;models\u0026#39;, \u0026#39;movie_lens_als\u0026#39;) 4 5# Save and load model 6model.save(sc, model_path) 7same_model = MatrixFactorizationModel.load(sc, model_path) ","date":"Oct 1, 2018","img":"https://unsplash.it/1920/1080?image=100","permalink":"/blog/2018-10-01-buiding-a-movie-model/","series":null,"tags":["Machine learning","Deeplearning","Spark"],"title":"Xây Dựng Chương Trình Gợi Ý Phim Dựa Vào Tập Dữ Liệu Movie Len"},{"categories":null,"content":" Lời mở đầu Đầu vào Kiến trúc AlexNet Overlapping Max Pooling ReLu Nonlinearity Reducing overfitting Overfitting là gì? Data Augmentation Dropout Lời mở đầu Tỷ phú Peter Thiel đã từng đưa ra câu hỏi tréo ngoe như thế này: \u0026ldquo;What important truth do very few people agree with you on?\u0026rdquo;\nNếu bạn đem câu này hỏi giáo sư Geoffrey Hinton vào năm 2010, ông ấy sẽ trả lời rằng mạng Convolutional Neural Networks (CNN) sẽ có bước đột phá lớn và giúp chúng ta giải quyết hoàn toàn bài toán phân loại ảnh. Tại thời điểm năm 2010, các nhà nghiên cứu trong lĩnh vực phân loại ảnh đều không nghĩ như giáo sư Geoffrey Hinton. Và Deep Learning tại thời điểm đó chưa thật sự giải quyết được bài toán này.\nNăm 2010 cũng là năm ra đời của cuộc thi ImageNet Large Scale Visual Recognition Challenge. Tập dữ liệu ảnh trong cuộc thi bao gồm khoảng 1.2 triệu ảnh thuộc 1000 lớp khác nhau, người thắng cuộc là người tạo ra mô hình làm cho độ lỗi trên tập dữ liệu trên là nhỏ nhất.\nHai năm sau, trong bài báo \u0026ldquo;ImageNet Classification with Deep Convolutional Neural Networks\u0026rdquo; của nhóm tác giả Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, Geoffrey và các cộng sự của mình đã chứng minh điều ông ấy nói hai năm trước là hoàn toàn chính xác. Ở bài báo này, nhóm tác giả đã huấn luyện mạng CNN và và đạt độ lỗi top-5 error rate là 15.3% (nhóm tác giả đã giành hạng nhất), cách biệt khá xa so với kết quả của nhóm đứng thứ hai(độ lỗi 26.2%). Trong các năm tiếp theo, rất nhiều nhóm đã nghiên cứu, cải tiến kiến trúc của mô hình CNN để đạt được kết quả tốt hơn, thậm chí hơn luôn khả năng nhận biết của con người.\nKiến trúc mạng CNN được sử dụng vào năm 2012 được cộng đồng nghiên cứu gọi với tên gọi thân thương là AlexNet do tác giả chính của nhóm nghiên cứu là Alex Krizhevsky. Ở trong bài viết này, chúng ta sẽ đi sâu vào tìm hiểu kiến trúc AlexNet và đóng góp chính của nó trong CNN.\nĐầu vào Như đã đề cập ở phần trên, mạng AlexNet đã thắng hạng nhất trong cuộc thi ILSVRC năm 2012. Mô hình giải quyết bài toán phân lớp một bức ảnh vào 1 lớp trong 1000 lớp khác nhau (vd gà, chó, mèo \u0026hellip; ). Đầu ra của mô hình là một vector có 1000 phần tử. Phần tử thứ i của vector đại diện cho xác suất bức ảnh thuộc về lớp thứ i. Do đó, tổng của các phần tử trong vector là 1.\nĐầu vào của mạng AlexNet là một bức ảnh RGB có kích thước 256x256 pixel. Toàn bộ các bức ảnh của tập train và tập test đều có cùng kích thước là 256x256. Nếu một bức ảnh nào đó không có kích thước 256x256, bức ảnh đó sẽ được chuyển về kích thước đúng 256x256. Những bức hình có kích thước nhỏ hơn 256 thì sẽ được phóng bự lên đến kích thước 256, những bức hình nào có kích thước lớn hơn 256 thì sẽ được cắt loại phần thừa để nhận được bức hình có kích thước 256x256. Hình ảnh ở dưới là một ví dụ về việc điều chỉnh bức ảnh về kích thước 256x256.\nNếu ảnh đầu vào là ảnh xám (grayscale), bức ảnh trên sẽ được chuyển đổi thành định dạng RGB bằng cách tạo ra 3 layer kênh màu giống nhau từ ảnh xám.\nSau khi chuẩn hoá hết tất cả các ảnh về dạng 256x256x3, nhóm tác giả chỉ sử dụng một phần của bức ảnh có kích thước 227x227x3 của một bức ảnh làm đầu vào cho mạng neural network. Trong bài báo nhóm tác giả ghi là 224x224, nhưng đây là một lỗi nhỏ của nhóm tác giả, và kích thước thực tế đầu vào của bức ảnh là 227x227.\nKiến trúc AlexNet Kiến trúc AlexNet lớn hơn nhiều so với các kiến trúc CNNs được sử dụng trong thị giác máy tính trước kia (trước năm 2010), vd kiến trúc LeNet của Yann LeCun năm 1998. Nó có 60 triệu tham số và 650000 neural và tốn khoảng từ năm đến sáu ngày huấn luyện trên hai GPU GTX 580 3GB. Ngày nay, với sự tiến bộ vượt bật của GPU, chúng ta có nhiều kiến trúc CNN có cấu trúc phức tạp hơn, và hoạt động rất hiệu quả trên những tập dữ liệu phức tạp. Nhưng tại thời điểm năm 2012 thì việc huấn luyện mô hình với lượng tham số và neural lớn như vậy là một vấn đề cực kỳ khó khăn. Nhìn kỹ vào hình bên dưới để hiểu rõ hơn về kiến trúc AlexNet. AlexNet bao gồm 5 convolution Layer và 3 Fully connected Layers.\nNhững convolution layer ( hay còn gọi với tên khác là các filter) rút trích các thông tin hữu ích trong các bức ảnh. Trong một convolution layer bất kỳ thường bao gồm nhiều kernel có cùng kích thước. Ví dụ như convolution layer đầu tiên của AlexNet chứa 96 kernel có kích thước 11x11x3. Thông thường thì width và height của một kernel bằng nhau, và độ sâu (depth) thường bằng số lượng kênh màu.\nConvolutional 1 và convolution 2 kết nối với nhau qua một Overlapping Max Pooling ở giữa. Tương tự như vậy giữa convolution 2 và convolution 3. Convolutional 3, convolution 4, convolution 5 kết nối trực tiếp với nhau, không thông qua trung gian. Convolutional 5 kết nối fully connected layter 1 thông qua một Overlapping Max pooling, tiếp theo mà một fully connected layter nữa. Và cuối cùng là một bộ phân lớp softmax với 1000 lớp nhãn (các bạn có thể xem hình kiến trúc mạng AlexNet ở trên để có cái nhìn tổng quát hơn).\nReLU nonlinerity được sử dụng sau tất các các convolution và fully connected layer. Trước đây, ReLU nonlinerity của lớp convolution 1 và 2 thường theo sau bởi một bước chuẩn hoá cục bộ (local normalization) rồi mới thực hiện pooling. Tuy nhiên, các nghiên cứu sau đó nhận thấy rằng việc sử dụng normalization không thật sự hữu ích. Do vậy chúng ta sẽ không đi chi tiết về vấn đề đó.\nOverlapping Max Pooling Max Pooling layer thường được sử dụng để giảm chiều rộng và chiều dài của một tensor nhưng vẫn giữ nguyên chiều sâu. Overlapping Max Pool layter cũng tương tự như Max Pool layter, ngoại trừ việc là một window của bước này sẽ có một phần chồng lên window của bước tiếp theo. Tác giả sử dụng pooling có kích thước 3x3 và bước nhảy là 2 giữa các pooling. Nghĩa là giữa pooling này và pooling khác sẽ overlapping với nhau 1 pixel. Các thí nghiệm thực tế đã chứng minh rằng việc sử dụng overlapping giữa các pooling giúp giảm độ lỗi top-1 error 0.4% và top-5 error là 0.3% khi so với việc sử dụng pooling có kích thước 2x2 và bước nhảy 2 (vector output của cả hai đều có số chiều bằng nhau).\nReLu Nonlinearity Một cải tiến quan trọng khác của AlexNet là việc sử dụng hàm phi tuyến ReLU. Trước đây, các nhóm nghiên cứu khác thường sử dụng hàm kích hoạt là hàm Tanh hoặc hàm Sigmoid để huấn luyên mô hình neural network. AlexNet chỉ ra rằng, khi sử dụng ReLU, mô hình deep CNN sẽ huấn luyện nhanh hơn so với viêc sử dụng tanh hoặc sigmoid. Hình bên dưới được rút ra từ bài báo chỉ ra rằng với việc sử dụng ReLU (đường nét liền trong hình), AlexNet đạt độ lỗi 25% trên tập huấn luyện và nhanh hơn gấp 6 lần so với mô hình tương tự nhưng sử dụng Tanh (đường nét đứt trong hình). Thí nghiệm trên sử dụng tập dữ liệu CIFAR-10 để huấn luyện.\nĐể hiểu rõ hơn lý do vì sao ReLU lại nhanh hơn so với các hàm khác, chúng ta hãy đối sánh hình dạng giá trị output của các hàm trên.\nCông thức của ReLU là: f(X) = max(0,x)\nNhìn kỹ vào hình trên, ta có nhận xét rằng: hàm tanh đạt giá trị bão hoà khi giá trị z \u0026gt;2.5 và z \u0026lt; -2.5 (số 2.5 là số cảm tính của mình). Và tại vùng |z|\u0026gt;2.5, thì độ dốc của hàm hầu như gần như bằng 0, |z| càng lớn thì độ dốc càng gần 0 hơn. Vì lý do này nên gradient descent sẽ hội tụ chậm. Còn đối với hàm ReLU, với giá trị z dương thì độ dốc của hàm không gần bằng 0 như hàm tanh. Điều này giúp cho việc hội tụ xảy ra nhanh hơn. Với giá trị z âm, độ dốc bằng 0, tuy nhiên, hầu hết các giá trị của các neural trong mạng thường có giá trị dương, nên trường hợp âm ít (hiếm) khi xảy ra. ReLU huấn luyện nhanh hơn so với sigmoid cũng bởi lý do tương tự.\nReducing overfitting Overfitting là gì? Khi bạn dạy một đứa trẻ từ 2-5 tuổi về việc cộng hai số, chúng sẽ học rất nhanh và trả lời đúng hầu hết các câu hỏi mà chúng ta đã dạy chúng. Tuy nhiên, chúng sẽ trả lời sai đối với những câu hỏi hơi lắc léo một chút (câu hỏi tương tự câu chúng ta đã dạy, nhưng thêm một xíu thông tin đòi hỏi trẻ phải suy nghĩ), hoặc các câu hỏi chưa được dạy. Lý do chúng trả lời sai những câu hỏi đó là khi trả lời những câu hỏi được dạy, chúng thường nhớ lại câu trả lời, chứ không thực sự hiểu câu hỏi. Cái này ở Việt Nam ta gọi là học vẹt.\nTương tự vậy, Neural network chính bản thân nó có khả năng học được những gì được dạy, tuy nhiên, nếu quá trình huấn luyện của bạn không tốt, mô hình có khả năng sẽ giống như những đứa trẻ trên kia, hồi tưởng lại những gì đã dạy cho chúng mà không hiểu bản chất. Và kết quả Neural Network sẽ hoạt động tốt trên tập huấn luyện ( nhưng chúng không rút ra được bản chất chính của vấn đề), và kết quả trên tập test tệ. Người ta gọi trường hợp trên là overfitting.\nNhóm nghiên cứu AlexNet sử dụng nhiều phương pháp khác nhau để giảm overfitting.\nData Augmentation Việc sử dụng nhiều biến thể khác nhau của một bức hình có thể giúp ngăn mô hình không bị overfitting. Với việc sử dụng nhiều biến thể của 1 bức hình, bạn bắt ép mô hình không học vẹt dữ liệu. Có nhiều cách khác nhau để sinh ra dữ liệu mới dựa vào dữ liệu có sẵn. Một vài các mà nhóm AlexNet đã sử dụng là.\nData Augmentation by Mirroring Ý tưởng của việc này là lấy ảnh trong gương của một bức hình (ảnh ảo). Nhìn vào ảnh bên dưới, bên trái là hình gốc của con mèo trong tập huấn luyện, bên phải là ảnh của con mèo khi thêm hiệu ứng hình qua gương (đơn giản là xoay qua trục y là được ) Data Augmentation by Random Crops Việc lựa chọn vị trí ảnh gốc một cách ngẫu nhiên cũng giúp chúng ta có thêm một ảnh khác so với ảnh gốc ban đầu.\nNhóm tác giả của AlexNet rút trích ngẫu nhiên bức ảnh có kích thước 227x227 từ bức ảnh 256x256 ban đầu làm input dầu vào cho mô hình. Bằng cách này, chúng ta có thể tăng số lượng dữ liệu lên gấp 2048 lần bằng việc sử dụng cách này.\nBốn bức ảnh được crop ngẫu nhiên ở trên thoạt nhìn có vẻ giống nhau, nhưng thực chất không phải như vậy.\nVới việc sử dụng Data Augmentation, chúng ta đang bố gắng dạy cho mô hình rằng với việc nhìn hình con mèo qua gương, nó vẫn là con mèo, hoặc hình hình con mèo ở bất kỳ góc độ nào thì nó vẫn là nó.\nDropout Với gần 60 triệu tham số trong tập huấn luyện, việc overfitting xảy ra là điều dễ hiểu. Các tác giả của AlexNet đã thực nghiệm nhiều cách nữa để giảm overfitting. Họ sử dụng một kỹ thuật gọi là dropout - kỹ thuật này được giới thiệu ở bài báo khác của G.E. Hintol vào năm 2012. Kỹ thuật này khá đơn giản, một neural sẽ có xác suất bị loại khỏi mô hình là 0.5. Khi một neural bị loại khỏi mô hình, nó sẽ không được tham qia vào quá trình lan truyền tiến hoặc lan truyền ngược. Cho nên, mỗi giá trị input sẽ đi qua một kiến trúc mạng khác nhau. Như mô tả ở hình động ở dưới, kết quả là giá trị của tham số trọng số sẽ tốt hơn và khó bị overfitting hơn. Trong quá trình test, toàn bộ network được sử dụng, không có dropout, tuy nhiên, giá trị output sẽ scaled bởi tham số 0.5 tương ứng với những neural không sử dụng trong quá trình trainning. Với việc sử dụng dropout, chúng ta sẽ tăng gấp đôi lần lặp cần thiết để đạt được độ hội tụ, nhưng khi không sử dụng dropout, mạng AlexNet rất dễ bị overfitting.\nNgày nay, chuẩn hoá dropout là một yếu tố không thể thiếu và các mô hình sử dụng nó thường có kết quả tốt hơn so với mô hình tương tự không sử dụng dropout. Chúng ta sẽ bàn sâu hơn về dropout ở một bài khác trong tương lai.\nTham khảo\nImageNet Classification with Deep Convolutional Neural Networks by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, 2012\nhttps://www.learnopencv.com/understanding-alexnet/\n","date":"Jun 15, 2018","img":"https://unsplash.it/1920/1080?image=35","permalink":"/blog/2018-06-15-understanding-alexnet/","series":null,"tags":["Machine learning","Deeplearning","AlexNet"],"title":"Tìm Hiểu Về Mạng Neural Network AlexNet"},{"categories":null,"content":"Chapter 9: References Introduction to References Declaring and using references Comparing pointers and references References and const\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/10_references/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 10: Character Manipulation and Strings Introduction to Strings Character Manipulation C-string manipulation C-String concatenation and copy Introducing std::string Declaring and using std::string\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/11_string/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 11: Functions The One Definition Rule First Hand on C++ Functions Function Declaration and Function Definitions Multiple Files - Compilation Model Revisited Pass by value Pass by pointer Pass by reference\nChapter 12: Getting Things out of functions Introduction to getting things out of functions Input and output parameters Returning from functions by value\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/12_function/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 13: Function Overloading Function Overloading Introduction Overloading with different parameters\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/13_function_overloading/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 14: Lambda functions Introduction to Lambda Functions Declaring and using lambda functions Capture lists Capture all in context Summary\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/14_lambda_function/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 15: Function Templates Introduction to function templates Trying out function templates Template type deduction and explicit arguments Template parameters by reference Template specialization\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/15_function_template/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 16: C++20 Concepts Crash course Introduction to C++20 Concepts Using C++20 Concepts Building your own C++20 Concepts Zooming in on the requires clause Combining C++20 Concepts C++20 Concepts and auto\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/16_concept/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 17: Classes Introduction to classes Your First Class C++ Constructors Defaulted constructors Setters and Getters Class Across Multiple Files Arrow pointer call notation Destructors Order of Constructor Destructor Calls The this Pointer struct Size of objects\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/17_class/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 18: Inheritance Introduction to Inheritance First try on Inheritance Protected members Base class access specifiers : Zooming in Base class access specifiers - A demo Closing in on Private Inheritance Resurrecting Members Back in Context Default Constructors with Inheritance Custom Constructors With Inheritance Copy Constructors with Inheritance Inheriting Base Constructors Inheritance and Destructors Reused Symbols in Inheritance\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/18_inheritance/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 19: Polymorphism Introduction to Polymorphism Static Binding with Inheritance Dynamic binding with virtual functions Size of polymorphic objects and slicing Polymorphic objects stored in collections (array) Override Overloading, overriding and function hiding Inheritance and Polymorphism at different levels Inheritance and polymorphism with static members Final Virtual functions with default arguments Virtual Destructors Dynamic casts Polymorphic Functions and Destructors Pure virtual functions and abstract classes Abstract Classes as Interfaces\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/19_polymorphism/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 3: Variables and data types Variables and data types Introduction Number Systems Integer types : Decimals and Integers Integer Modifiers Fractional Numbers Booleans Characters And Text Auto Assignments Variables and data types summary\nBài 3: Xây dựng chương trình C++ đầu tiên với Visual Studio 2015\nMột số kiến thức cần lưu ý Cách tạo và biên dịch chương trình C++ đầu tiên trên Visual Studio Một số vấn đề thường gặp đối với lập trình viên mới Bài 4: Cấu trúc một chương trình C++ (Structure of a program)\nCấu trúc của một chương trình C++ Cú pháp và lỗi cú pháp trong C++ (Syntax and syntax errors) Bài 5: Ghi chú trong C++ (Comments in C++)\nCú pháp comment trong C++ Một số kinh nghiệm khi comment trong lập trình Bài 6: Biến trong C++ (Variables in C++)\nBiến trong C++ Khởi tạo biến trong C++ (Defining a variable) Định nghĩa biến ở đâu (Where to define variables) Bài 7: Số tự nhiên và Số chấm động trong C++ (Integer, Floating point)\nTổng quan về kiểu dữ liệu cơ bản trong C++ Kiểu số nguyên (Integer) Số chấm động (Floating point numbers) Bài 8: Kiểu ký tự trong C++ (Character)\nTổng quan về kiểu ký tự (Character) Khai báo, khởi tạo và gán giá trị một biến ký tự In ký tự ra màn hình In ký tự từ số nguyên và ngược lại (Casting) Escape sequences Newline ‘\\n’ và std::endl Dấu nháy đơn ‘K’ và dấu nháy kép “Kteam” Bài 9: Kiểu luận lý và cơ bản về Câu điều kiện If (Boolean and If statements)\nTổng quan về kiểu luận lý (Boolean) Cơ bản về câu điều kiện If và Boolean Bài 10: Nhập, Xuất và Định dạng dữ liệu trong C++ (Input and Output)\nXuất dữ liệu với std::cout trong C++ Nhập dữ liệu với std::cin trong C++ Định dạng dữ liệu nhập xuất trong C++ Bài 11: Hằng số trong C++ (Constants)\nTổng quan hằng số (Constants) Hằng số với từ khóa const Hằng số với chỉ thị tiền xử lý #define Nên định nghĩa hằng số ở đâu Bài 12: Toán tử số học, toán tử tăng giảm, toán tử gán số học trong C++ (Operators)\nTổng quan về toán tử Toán tử số học trong C++ (Arithmetic operators) Toán tử gán số học trong C++ (Arithmetic assignment operators) Bài 13: Toán tử quan hệ, logic, bitwise, misc và độ ưu tiên toán tử trong C++\nToán tử quan hệ trong C++ (Relational operators) Toán tử logic trong C++ (Logical operators) Toán tử trên bit trong C++ (Bitwise operators) Các toán tử hỗn hợp trong C++ (Misc Operators) Độ ưu tiên và quy tắc kết hợp toán tử trong C++ Bài 14: Cơ bản về chuỗi ký tự trong C++ (An introduction to std::string)\nTổng quan về chuỗi ký tự (std::string) Khai báo, khởi tạo và gán giá trị một chuỗi ký tự Xuất một chuỗi ký tự (string output): Nhập một chuỗi ký tự (string input) Một số thao tác cơ bản với chuỗi ký tự Bài 15: Biến cục bộ trong C++ (Local variables in C++)\nTổng quan về tầm vực của biến Biến cục bộ (Local variables) Bài 16: Biến toàn cục trong C++ (Global variables in C++)\nTổng quan về tầm vực của biến Biến toàn cục (Global variables) Sử dụng biến toàn cục là nguy hiểm Khi nào cần sử dụng biến toàn cục (non-const) Bài 17: Biến tĩnh trong C++ (Static variables in C++)\nTổng quan về biến tĩnh (static variables) Khi nào nên sử dụng biến tĩnh Bài 18: Ép kiểu ngầm định trong C++ (Implicit type conversion in C++)\nTổng quan về ép kiểu dữ liệu Ép kiểu ngầm định trong C++ (Implicit type conversion) Bài 19: Ép kiểu tường minh trong C++ (Explicit type conversion in C++)\nÉp kiểu tường minh trong C++ (Explicit type conversion) Bài 20: Cơ bản về Hàm và Giá trị trả về (Basic of functions and return values)\nTổng quan về hàm (functions overview) Giá trị trả về (return values) Giá trị trả về của kiểu void (return values of type void) Bài 21: Truyền Giá Trị cho Hàm (Passing Arguments by Value)\nTham số và đối số của hàm (Function parameters and arguments) Truyền giá trị cho hàm (Passing arguments by value) Tổng kết về phương pháp truyền giá trị cho hàm (Passing argument by value) Bài 22: Truyền Tham Chiếu cho Hàm (Passing Arguments by Reference)\nTruyền tham chiếu cho hàm (Passing arguments by reference) Truyền tham chiếu hằng (Pass by const reference) Tổng kết về phương pháp truyền tham chiếu cho hàm (Passing arguments by reference) Bài 23: Tiền khai báo và Định nghĩa Hàm (Forward declarations and Definitions of Functions)\nLỗi “identifier not found” Tiền khai báo và nguyên mẫu hàm (Forward declaration and function prototypes) Khai báo và định nghĩa trong C++ (Declarations and definitions in C++) Bài 24: Giới thiệu về cấu trúc điều khiển (Control flow introduction)\nTổng quan về cấu trúc điều khiển trong C++ Câu lệnh dừng (halt) Câu lệnh nhảy (Jumps) Cấu trúc rẽ nhánh có điều kiện (Conditional branches) Cấu trúc vòng lặp (Loops) Xử lý ngoại lệ (Exceptions handling) Bài 25: Câu điều kiện If và Toán tử điều kiện (If statements and Conditional operator)\nCâu điều kiện If Toán tử điều kiện (Conditional operator) Bài 26: Câu điều kiện Switch trong C++ (Switch statements)\nCâu điều kiện Switch (Switch statements) Khai báo và khởi tạo biến bên trong case statement Bài 27: Câu lệnh Goto trong C++ (Goto statements)\nTổng quan về câu lệnh Goto trong C++ Một số vấn đề của câu lệnh Goto Bài 28: Vòng lặp While trong C++ (While statements)\nTổng quan về cấu trúc vòng lặp Vòng lặp while (while statements) Bài 29: Vòng lặp Do while trong C++ (Do while statements)\nVòng lặp do while (do while statements) Bài 30: Vòng lặp For trong C++ (For statements)\nVòng lặp for (for statements) Bài 31: Từ khóa Break and continue trong C++\nTừ khóa break Từ khóa continue Bài 32: Phát sinh số ngẫu nhiên trong C++ (Random number generation)\nTổng quan về phát sinh số ngẫu nhiên Phát sinh số ngẫu nhiên trong C++ Phát sinh số ngẫu nhiên trong C++ 11 Bài 33: Mảng 1 chiều trong C++ (Arrays)\nTại sao lại sử dụng mảng? Tổng quan về mảng 1 chiều Khai báo và khởi tạo mảng 1 chiều Xuất các phần tử mảng 1 chiều Nhập dữ liệu cho mảng 1 chiều Phát sinh dữ liệu ngẫu nhiên cho mảng 1 chiều Bài 34: Các thao tác trên Mảng một chiều\nTruyền mảng vào hàm (passing arrays to functions) Nhập và xuất mảng 1 chiều Sao chép mảng 1 chiều Tìm kiếm phần tử trong mảng Sắp xếp mảng 1 chiều Thêm và xóa một phần tử trong mảng Bài 35: Mảng 2 chiều trong C++ (Two-dimensional arrays)\nMảng 2 chiều là gì? Khai báo và khởi tạo mảng 2 chiều Xuất các phần tử mảng 2 chiều Nhập các phần tử mảng 2 chiều Bài 36: Các thao tác trên Mảng 2 chiều\nTruyền mảng vào hàm (passing arrays to functions) Nhập và xuất mảng 2 chiều Tính tổng các phần tử trong mảng Tìm giá trị lớn nhất của mảng 2 chiều Bài 37: Mảng ký tự trong C++ (C-style strings)\nMảng ký tự (C-style strings) là gì? Khai báo và khởi tạo mảng ký tự (C-style strings) Xuất mảng ký tự (C-style strings) với std::cout Nhập mảng ký tự (C-style strings) với std::cin Bài 38: Các thao tác trên Mảng ký tự (C-style strings)\nMột số thao tác với mảng ký tự (C-style strings)\nhttps://www.freecodecamp.org/news/learn-c-with-free-31-hour-course/\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/4_variable_and_datatype/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 4: Operations on Data Introduction on Data operations Basic Operations Precedence and Associativity Prefix/Postfix Increment \u0026amp; Decrement Compound Assignment Operators Relational Operators Logical Operators Output formatting Numeric Limits Math Functions Weird Integral Types Data Operations Summary\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/5_operator_on_data/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 5: Flow Control Flow Control Introduction If Statements Else If Switch Ternary Operators Flow Control Summary\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/6_flow_control/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 6: Loops Loops Introduction For Loop While Loop Do While Loop\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/7_loop/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 7: Arrays Introduction to Arrays Declaring and using arrays Size of an array Arrays of characters Array Bounds\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/8_array/","series":null,"tags":null,"title":""},{"categories":null,"content":"Chapter 8: Pointers Introduction to Pointers Declaring and using pointers Pointer to char Program Memory Map Revisited Dynamic Memory Allocation Dangling Pointers When new Fails Null Pointer Safety Memory Leaks Dynamically allocated arrays\n","date":"Jan 1, 0001","img":"","permalink":"/courses/cplusplus/9_array/","series":null,"tags":null,"title":""},{"categories":null,"content":"⌨️ (0:00:00) Introduction to data structures ⌨️ (0:06:33) Data Structures: List as abstract data type ⌨️ (0:19:40) Introduction to linked list ⌨️ (0:36:50) Arrays vs Linked Lists ⌨️ (0:49:05) Linked List - Implementation in C/C++ ⌨️ (1:03:02) Linked List in C/C++ - Inserting a node at beginning ⌨️ (1:15:50) Linked List in C/C++ - Insert a node at nth position ⌨️ (1:31:04) Linked List in C/C++ - Delete a node at nth position ⌨️ (1:43:32) Reverse a linked list - Iterative method ⌨️ (1:57:21) Print elements of a linked list in forward and reverse order using recursion ⌨️ (2:11:43) Reverse a linked list using recursion ⌨️ (2:20:38) Introduction to Doubly Linked List ⌨️ (2:27:50) Doubly Linked List - Implementation in C/C++ ⌨️ (2:43:09) Introduction to stack ⌨️ (2:51:34) Array implementation of stacks ⌨️ (3:04:42) Linked List implementation of stacks ⌨️ (3:15:39) Reverse a string or linked list using stack. ⌨️ (3:32:03) Check for balanced parentheses using stack ⌨️ (3:46:14) Infix, Prefix and Postfix ⌨️ (3:59:14) Evaluation of Prefix and Postfix expressions using stack ⌨️ (4:14:00) Infix to Postfix using stack ⌨️ (4:32:17) Introduction to Queues ⌨️ (4:41:35) Array implementation of Queue ⌨️ (4:56:33) Linked List implementation of Queue ⌨️ (5:10:48) Introduction to Trees ⌨️ (5:26:37) Binary Tree ⌨️ (5:42:51) Binary Search Tree ⌨️ (6:02:17) Binary search tree - Implementation in C/C++ ⌨️ (6:20:52) BST implementation - memory allocation in stack and heap ⌨️ (6:33:55) Find min and max element in a binary search tree ⌨️ (6:39:41) Find height of a binary tree ⌨️ (6:46:50) Binary tree traversal - breadth-first and depth-first strategies ⌨️ (6:58:43) Binary tree: Level Order Traversal ⌨️ (7:10:05) Binary tree traversal: Preorder, Inorder, Postorder ⌨️ (7:24:33) Check if a binary tree is binary search tree or not ⌨️ (7:41:01) Delete a node from Binary Search Tree ⌨️ (7:59:27) Inorder Successor in a binary search tree ⌨️ (8:17:23) Introduction to graphs ⌨️ (8:34:05) Properties of Graphs ⌨️ (8:49:19) Graph Representation part 01 - Edge List ⌨️ (9:03:03) Graph Representation part 02 - Adjacency Matrix ⌨️ (9:17:46) Graph Representation part 03 - Adjacency List\n","date":"Jan 1, 0001","img":"","permalink":"/courses/data_structures/","series":null,"tags":null,"title":""},{"categories":null,"content":"Thông thường, chúng ta đọc nội dung của file sau khi mở file.\nGiả sử chúng ta có file \u0026ldquo;test.txt\u0026rdquo; có nội dung như bên dưới:\n1This file is testing. 2Good Luck! Hàm mở file ra được viết như thế này:\nf = open(\u0026ldquo;test.txt\u0026rdquo;,\u0026lsquo;r\u0026rsquo;,encoding = \u0026lsquo;utf-8\u0026rsquo;)\nĐể đọc nội dung file, python hỗ trợ các hàm là read, readline, readlines, mỗi hàm sẽ có tác dụng khác nhau\nhàm read sẽ trả về toàn bộ nội dung của file f.read()\n'This file is testing.\\nGood Luck!\\n'\nhàm readline, mỗi lần đọc sẽ trả về 1 dòng trong file f.readline()\n\u0026lsquo;This file is testing.\\n\u0026rsquo;\nKhi gọi readline một lần nữa, chương trình sẽ trả về dòng tiếp theo\nf.readline()\n\u0026lsquo;Good Luck!\\n\u0026rsquo;\nhàm readlines sẽ trả về một mảng các chuỗi, mỗi chuỗi tương ứng một dòng trong file f.readlines()\n[\u0026lsquo;This file is testing.\\n\u0026rsquo;, \u0026lsquo;Good Luck!\\n\u0026rsquo;]\n","date":"Jan 1, 0001","img":"","permalink":"/courses/python/io/","series":null,"tags":null,"title":""},{"categories":null,"content":"Phạm Duy Tùng.\nR\u0026amp;D at MWG.\n","date":"Jan 1, 0001","img":"","permalink":"/about/","series":null,"tags":null,"title":"About"},{"categories":null,"content":"","date":"Jan 1, 0001","img":"","permalink":"/contact/","series":null,"tags":null,"title":"Contact Us"},{"categories":null,"content":"","date":"Jan 1, 0001","img":"","permalink":"/faq/","series":null,"tags":null,"title":"FAQs"},{"categories":null,"content":"","date":"Jan 1, 0001","img":"","permalink":"/offline/","series":null,"tags":null,"title":"Offline"},{"categories":null,"content":"Tools sinh password\nTools sinh số ngẫu nhiên\n","date":"Jan 1, 0001","img":"","permalink":"/tools/","series":null,"tags":null,"title":"Tools"}]