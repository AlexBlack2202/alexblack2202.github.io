<!DOCTYPE HTML>

<html>
    <head>
        <script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "/"
        },
        "articleSection" : "blog",
        "name" : "Deep Learning based Multiple Human Pose Estimation using OpenCV",
        "headline" : "Deep Learning based Multiple Human Pose Estimation using OpenCV",
        "description" : "Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation.",
        "inLanguage" : "en",
        "author" : "",
        "creator" : "",
        "publisher": "",
        "accountablePerson" : "",
        "copyrightHolder" : "",
        "copyrightYear" : "2018",
        "datePublished": "2018-10-05 00:19:00 &#43;0300 &#43;0300",
        "dateModified" : "2018-10-05 00:19:00 &#43;0300 &#43;0300",
        "url" : "/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/",
        "wordCount" : "1263",
        "keywords" : [ "Machine learning","Deeplearning","multiple pose estimation","Blog" ]
    }
    </script>
        
            
                <title>Deep Learning based Multiple Human Pose Estimation using OpenCV</title>
            
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        
		<meta name="generator" content="Phạm Duy Tùng" />
        
  
    
  

  

  
  
  
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content='/favicon/mstile.png'>
  <meta name="application-name" content="Phạm Duy Tùng Machine Learning Blog">
  <meta name="msapplication-tooltip" content="Blog ML của Phạm Duy Tùng và Đặng Thị Hằng">
   
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">



        
        
            <meta name="description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation.">
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning based Multiple Human Pose Estimation using OpenCV"/>
<meta name="twitter:description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation."/>
<meta name="twitter:site" content="@example"/>

        <meta property="og:title" content="Deep Learning based Multiple Human Pose Estimation using OpenCV" />
<meta property="og:description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/" />



<meta property="article:published_time" content="2018-10-05T00:19:00&#43;03:00"/>

<meta property="article:modified_time" content="2018-10-05T00:19:00&#43;03:00"/>











        <meta property="og:image" content="//images/logo.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="512">
        
<meta itemprop="name" content="Deep Learning based Multiple Human Pose Estimation using OpenCV">
<meta itemprop="description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation.">


<meta itemprop="datePublished" content="2018-10-05T00:19:00&#43;03:00" />
<meta itemprop="dateModified" content="2018-10-05T00:19:00&#43;03:00" />
<meta itemprop="wordCount" content="1263">



<meta itemprop="keywords" content="Machine learning,Deeplearning,multiple pose estimation," />

        

        
            
        

        
        
          
			
			 <link rel="stylesheet" href="/css/font-awesome.min.css">
			 <link rel="stylesheet" href="/css/bootstrap.min.css">


          
            <link rel="stylesheet" href="/css/academicons.min.css">
        

        
            
                
            
        


  
    
    <link href='//cdn.bootcss.com/highlight.js/9.15.8/styles/github.min.css' rel='stylesheet' type='text/css' />
  


      
<script>
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-114911596-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

	  
	  <style>
	  
	  body{
font-family: Helvetica,Arial,sans-serif;
}

.card{
	margin-bottom: 10px;
}

#disqus_thread{
padding: 0 5px;
}

.item-header{
padding: 0;
}

.single-content-img{
width: 100%;
    max-height: 450px !important;
    background-size: cover;
    display: block;
    background-position: center;
}

.thumbnail {
    position: relative;
}

.caption {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
	background: rgba(0, 0, 0, 0.25);
	text-align:left;
}

.caption .title{
	font-size: 1.6em;
    line-height: 1.4em;
    top: 0;
	margin-left:20px;
	margin-top:20px;
	
}

.caption .title-caption{
margin-left:10px;
}

#content p{
text-align: justify;
    line-height: 1.9;
    font-size: 12pt;
}

#content img{
	display: block;
    margin-left: auto;
margin-right: auto;
max-width:98%;
}

img + strong {
    font-style: normal;
    display: inherit;
    text-align: center;
}
.img-news{
max-height:150px;
width:100%;
}

.news-tittle{
	padding-top:15px;
	text-align:justify;
}

.author{
	color: orange;
	display:block;
}
.author-inline{
	color: orange;
}

.adv{
height:18px;
}

h1, h2, h3, h4{
padding-bottom: 10px;
    font-weight: bold;
}

h1{
color: #d04764}


h2{
color: orange}

h3{
color: #d04764}

h4{
color: #8dec89}

.hljs{
    white-space: pre-wrap;
    white-space: -moz-pre-wrap;
    white-space: -pre-wrap;
    white-space: -o-pre-wrap;
    word-wrap: break-word;}
	  .titledetail {
    display: block;
    overflow: hidden;
    line-height: 53px;
    font-size: 45px;
    color: #333;
    font-family: 'Roboto Condensed',sans-serif;
    font-weight: 600;
    margin: auto;
	padding:25px 0;
	text-align:justify;
}

.newsrelate {
    display: block;
    overflow: hidden;
	 list-style:none;
}
a ,a:hover{
    text-decoration: none;
}
.newsrelate li {
    float: left;
    overflow: hidden;
    width: 30%;
    margin-left: 2.5%;
    margin-bottom: 15px;
}

.newsrelate li a {
    display: block;
    overflow: hidden;
}

.newsrelate li h3 {
    display: block;
    overflow: hidden;
    line-height: 1.3em;
    font-size: 16pt;
    color: #333;
    line-height: 22px;
    font-weight: 300;
    font-family: Arial,Helvetica,sans-serif;
    width: auto;
    margin: 5px auto;
}

.titlerelate {
    display: block;
    overflow: hidden;
    font-size: 18px;
    font-weight: 600;
    color: #333;
    font-family: 'Roboto Condensed',sans-serif;
    line-height: 26px;
    text-transform: uppercase;
    padding: 30px 20px 10px;
    margin: 0 20px;
    border-top: 1px solid #eee;
}


	  </style>
	  
    </head>
    <body>
<script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '1546237302193677',
      xfbml      : true,
      version    : 'v5.0'
    });
    FB.AppEvents.logPageView();
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "https://connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v5.0&appId=1853483258232756&autoLogAppEvents=1"></script>
      
      

    
    
<header id="header"  style="background: #790014; color: hsla(0,0%,100%,1);">
<div class="container">
    <nav class="navbar navbar-expand-md navbar-dark">
	
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav mr-auto">
            
                <li class="nav-item">
                    <a class='nav-link' href="/blog">
                            <i class="fa fa-home active">&nbsp;</i>Home
                    </a>
                </li>
            
                <li class="nav-item">
                    <a class='nav-link' href="/news/">
                            <i class="fa fa-list">&nbsp;</i>News
                    </a>
                </li>
            
                <li class="nav-item">
                    <a class='nav-link' href="/xem-truyen/">
                            <i class="fa fa-id-card-o">&nbsp;</i>Truyện
                    </a>
                </li>
            
        </ul>
    </nav>
    </div></div>
</header>


   
    
	<div class="container">
	<div class="adv"></div>
	<div>
    <main role="main" >
	
        
        
        <article class="post">
  


        
		 <div>
            <h1 class="titledetail">Deep Learning based Multiple Human Pose Estimation using OpenCV</h1>
			
			<div class="col-md-12">
			 
			  <time class="published"
            datetime='2018-10-05'>
            05/10/2018</time>
		 - 
			   <span style="font-weight: 600; vertical-align: middle;"></span>
			   <div
  class="fb-like"
  data-share="true"
  data-width="450"
  data-show-faces="true">
</div>
<div class="fb-share-button" data-href="" data-layout="button_count" data-size="small"><a target="_blank" href="" class="fb-xfbml-parse-ignore">Chia sẻ</a></div>
			   </p>
			</div>
			</div>
			<hr/>
		<div class="thumbnail text-center">
		 <img class="img-fluid single-content-img lazy" src="../../post_image/post_estimator_shark.gif" style="width:100%" />
		
			</div>
			 
            
        
       



  

  

  <div id="content" class="col-md-8 mx-auto">
    

<h2 id="lời-mở-đầu">Lời mở đầu</h2>

<p>Lưu ý: Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &gt; 3.4.1.</p>

<p>Ở bài viết trước, chúng ta đã tìm hiểu cách thức rút trích khung xương sử dụng DNN và đã áp dụng thành công trên ảnh có chứa 1 đối tượng người. Trong bài viết này, chúng ta sẽ thực hiện áp dụng mô hình cho bài toán có nhiều người trong cùng 1 bức ảnh.</p>

<h2 id="sử-dụng-pretrain-model-trong-bài-toán-multiple-pose-estimation">Sử dụng pretrain model trong bài toán multiple Pose Estimation</h2>

<p>Trong bài viết này, chúng ta tiếp tục sử dụng mô hình MPI để dò tìm các điểm đặc trưng của con người và rút ra mô hình khung xương. Kết quả trả về của thuật toán gồm 15 đặc trưng như bên dưới.</p>

<pre><code class="language-python">Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
</code></pre>

<p>Áp dụng mô hình với ảnh của nhóm T-ARA.</p>

<pre><code class="language-python">import cv2

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]

protoFile = &quot;pose/mpi/pose_deploy_linevec.prototxt&quot;
weightsFile = &quot;pose/mpi/pose_iter_160000.caffemodel&quot;

net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

frame = cv2.imread(&quot;tara1.jpg&quot;)

inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)

output = net.forward()
</code></pre>

<p>Thử show lên vị trí vùng cổ trong hình.</p>

<pre><code class="language-python">
i = 0
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_head_heatmap.png" alt="Hình với điểm đặc trưng vùng đầu" /></p>

<p>Thử show lên hình điểm đặc trưng vùng cổ</p>

<pre><code class="language-python">i = 1
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_neck_heatmap.png" alt="Hình với điểm đặc trưng vùng cổ" /></p>

<p>Bằng một số phép biến đổi quen thuộc có sẵn trong opencv, chúng ta hoàn toàn có thể lấy được toạ độ của các điểm keypoint một cách dễ dàng.</p>

<pre><code class="language-python">
# Find the Keypoints using Non Maximum Suppression on the Confidence Map
def getKeypoints(probMap, threshold=0.1):
    
    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)

    mapMask = np.uint8(mapSmooth&gt;threshold)
    keypoints = []
    
    #find the blobs
    _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    #for each blob find the maxima
    for cnt in contours:
        blobMask = np.zeros(mapMask.shape)
        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
        maskedProbMap = mapSmooth * blobMask
        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))

    return keypoints


detected_keypoints = []
keypoints_list = np.zeros((0,3))
keypoint_id = 0
threshold = 0.1
for i in range(nPoints):
    probMap = output[0, i, :, :]
    probMap = cv2.resize(probMap, (frameWidth, frameHeight))

    keypoints = getKeypoints(probMap, threshold)
    keypoints_with_id = []
    for j in range(len(keypoints)):
        keypoints_with_id.append(keypoints[j] + (keypoint_id,))
        keypoints_list = np.vstack([keypoints_list, keypoints[j]])
        keypoint_id += 1

    detected_keypoints.append(keypoints_with_id)



frameClone = cv2.cvtColor(frameCopy,cv2.COLOR_BGR2RGB)
for i in range(nPoints):
    for j in range(len(detected_keypoints[i])):
        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA)

plt.imshow(frameClone) 
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_keypoint.png" alt="Hình toàn bộ điểm đặc trưng" /></p>

<p>Cuối cùng, chúng ta sẽ nối các điểm đặc trưng của các nhân vật thông qua thuật toán Part Affinity Heatmaps. Thuật toán này được đề xuất bởi nhóm tác giả Zhe Cao, Tomas Simon,Shih-En Wei, Yaser Sheikh thuộc phòng thí nghiệm The Robotics Institute trường đại học Carnegie Mellon. Các bạn có nhu cầu có thể tìm hiểu ở <a href="https://arxiv.org/pdf/1611.08050.pdf">https://arxiv.org/pdf/1611.08050.pdf</a>.</p>

<pre><code class="language-python">
mapIdx = [[16,17], [18,19], [20,21], [22,23], [24,25], [26,27], [28,29], [30,31], [32,33], [34,35], [36,37], [38,39], [40,41], [42,43]]



colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],
         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],
         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]
# Find valid connections between the different joints of a all persons present
def getValidPairs(output):
    valid_pairs = []
    invalid_pairs = []
    n_interp_samples = 10
    paf_score_th = 0.1
    conf_th = 0.5
    # loop for every POSE_PAIR
    for k in range(len(mapIdx)):
        # A-&gt;B constitute a limb
        pafA = output[0, mapIdx[k][0], :, :]
        pafB = output[0, mapIdx[k][1], :, :]
        pafA = cv2.resize(pafA, (frameWidth, frameHeight))
        pafB = cv2.resize(pafB, (frameWidth, frameHeight))


        # Find the keypoints for the first and second limb
        candA = detected_keypoints[POSE_PAIRS[k][0]]
        candB = detected_keypoints[POSE_PAIRS[k][1]]
        nA = len(candA)
        nB = len(candB)

        # fig=plt.figure(figsize=(8, 8))

        # interp_coord = list(zip(np.linspace(candA[0][0], candB[0][0], num=n_interp_samples),
        #                                     np.linspace(candA[0][1], candB[0][1], num=n_interp_samples)))

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 1)
        
        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)


        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafA, alpha=0.5)

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 2)
        

        

        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)
        
        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafB, alpha=0.5)
        # plt.show()



        

        # If keypoints for the joint-pair is detected
        # check every joint in candA with every joint in candB 
        # Calculate the distance vector between the two joints
        # Find the PAF values at a set of interpolated points between the joints
        # Use the above formula to compute a score to mark the connection valid
        
        if( nA != 0 and nB != 0):
            valid_pair = np.zeros((0,3))
            for i in range(nA):
                max_j=-1
                maxScore = -1
                found = 0
                for j in range(nB):
                    # Find d_ij
                    d_ij = np.subtract(candB[j][:2], candA[i][:2])
                    norm = np.linalg.norm(d_ij)
                    if norm:
                        d_ij = d_ij / norm
                    else:
                        continue
                    # Find p(u)
                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),
                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))
                    # Find L(p(u))
                    paf_interp = []
                    for k in range(len(interp_coord)):
                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],
                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) 
                    # Find E
                    paf_scores = np.dot(paf_interp, d_ij)
                    avg_paf_score = sum(paf_scores)/len(paf_scores)
                    
                    # Check if the connection is valid
                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -&gt; Valid Pair  
                    if ( len(np.where(paf_scores &gt; paf_score_th)[0]) / n_interp_samples ) &gt; conf_th :
                        if avg_paf_score &gt; maxScore:
                            max_j = j
                            maxScore = avg_paf_score
                            found = 1
                # Append the connection to the list
                if found:            
                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)

            # Append the detected connections to the global list
            valid_pairs.append(valid_pair)

            pprint(valid_pair)
        else: # If no keypoints are detected
            print(&quot;No Connection : k = {}&quot;.format(k))
            invalid_pairs.append(k)
            valid_pairs.append([])
    pprint(valid_pairs)
    return valid_pairs, invalid_pairs

# This function creates a list of keypoints belonging to each person
# For each detected valid pair, it assigns the joint(s) to a person
# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint
def getPersonwiseKeypoints(valid_pairs, invalid_pairs):
    # the last number in each row is the overall score 
    personwiseKeypoints = -1 * np.ones((0, 19))

    for k in range(len(mapIdx)):
        if k not in invalid_pairs:
            partAs = valid_pairs[k][:,0]
            partBs = valid_pairs[k][:,1]
            indexA, indexB = np.array(POSE_PAIRS[k])

            for i in range(len(valid_pairs[k])): 
                found = 0
                person_idx = -1
                for j in range(len(personwiseKeypoints)):
                    if personwiseKeypoints[j][indexA] == partAs[i]:
                        person_idx = j
                        found = 1
                        break

                if found:
                    personwiseKeypoints[person_idx][indexB] = partBs[i]
                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]

                # if find no partA in the subset, create a new subset
                elif not found and k &lt; 17:
                    row = -1 * np.ones(19)
                    row[indexA] = partAs[i]
                    row[indexB] = partBs[i]
                    # add the keypoint_scores for the two keypoints and the paf_score 
                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]
                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])
    return personwiseKeypoints

valid_pairs, invalid_pairs = getValidPairs(output)

personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)


for i in range(nPoints-1):
    for n in range(len(personwiseKeypoints)):
       
        index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]
        if -1 in index:
            continue
        B = np.int32(keypoints_list[index.astype(int), 0])
        A = np.int32(keypoints_list[index.astype(int), 1])
        cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)
       
        
        
plt.imshow(frameClone)
    # plt.imshow(mapMask, alpha=0.5)
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_t-ara_finalresult.png" alt="Hình kết quả cuối cùng" /></p>

<p>Hẹn gặp lại các bạn ở những bài viết tiếp theo.</p>

<p>Bài viết này được viết dựa vào nguồn <a href="https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/">https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/</a> của tác giả VIKAS GUPTA. Tôi sử dụng tập model và hình ảnh khác với bài viết nguyên gốc của tác giả.</p>

  </div>
  <footer>
  <div class="col-md-12">
    <ul class="stats list-unstyled">
 
    
  <li class="tags">
    <ul class="list-inline">
       
            
            
                <i class="fa fa-tags"></i>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/machine-learning">Machine learning</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/deeplearning">Deeplearning</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/multiple-pose-estimation">multiple pose estimation</a></li>
                
            
        
    </ul>
  </li>
  
</ul>

	</div>
  </footer>
</article>
<div class="col-md-12">
<div class="titlerelate">Bài viết khác</div>
<ul class="newsrelate">
    
        <li class="page-item">
		<a href="/blog/2018-10-04-deep-learning-base-human-pose-estimation/"
                class="button big previous">
		<div>
		<img class="img-fluid single-content-img lazy" src="../../post_image/post_estimator_shark.gif" width="100" />
		</div>
		<h3>
		Deep Learning based Human Pose Estimation using OpenCV
				</h3></a>
				</li>
    

    
        <li class="page-item">
		<a href="/blog/2018-10-08-mask-rnn/"
                class="button big previous">
		<div>
		<img class="img-fluid single-content-img lazy" src="../../post_image/maxresdefault.jpg" width="100" />
		</div>
		<h3>
		Mask R-CNN trong bài toán nhận dạng và phân vùng đối tượng
				</h3></a>
				</li>
    
</ul>
</div>


<div class="fb-comments" data-href="" data-width="" data-numposts="5"></div>

    <article class="post">
        <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "phamduytung" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </article>




		
    </main>
    
	</div>
	</div>
    
	<hr>
  <footer class="footer">
  <div class="container text-center">
    
    <p class="copyright">
      
        &copy; 2019
        
          Phạm Duy Tùng Machine Learning Blog
        
      
     
    </p>
	</div>
  </footer>
    
    

    
      
    

    
      
      
      
        <script src="//cdn.bootcss.com/highlight.js/9.15.8/highlight.min.js"></script>
        
        
        
        <script src="//cdn.bootcss.com/highlight.js/9.15.8/languages/python.min.js"></script>
        <script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
      
    
    
    
     

   <script src="/js/jquery-3.3.1.min.js"></script>
   
    <script src="/js/bootstrap.min.js"></script>
      <script src="/js/util.js"></script>
      <script src="/js/main.js"></script>
     
    

    
      
        
      
    
	
    
    <script>hljs.initHighlightingOnLoad();</script>
      
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


	  
	  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114911596-1"  data-cfasync="false"></script>
<script  data-cfasync="false">
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114911596-1');
</script>
<script src="https://cdn.jsdelivr.net/npm/intersection-observer@0.5.1/intersection-observer.js"  data-cfasync="false"></script>
<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@12.0.0/dist/lazyload.min.js"  data-cfasync="false"></script>
	   <script  type="text/javascript"  data-cfasync="false">

  function getcontent(){
 

   

             var myLazyLoad = new LazyLoad({
    elements_selector: ".lazy"
});
                }
            
			
			$(document).ready(function(){
			getcontent();
			}); 
</script>
<script type="text/javascript" src="https://code.responsivevoice.org/responsivevoice.js?key=kIfnlvoe" async></script>
  </body>
</html>

