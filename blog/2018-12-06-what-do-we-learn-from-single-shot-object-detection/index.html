<!DOCTYPE HTML>

<html>
    <head>
        <script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "/"
        },
        "articleSection" : "blog",
        "name" : "Tìm hiểu single shot object detectors",
        "headline" : "Tìm hiểu single shot object detectors",
        "description" : "Ở bài viết này, chúng ta sẽ tìm hiểu single shot object detectors và ứng dụng của nó trên các thuật toán SSD, Yollov3, FPN và RetinaNet.",
        "inLanguage" : "en",
        "author" : "",
        "creator" : "",
        "publisher": "",
        "accountablePerson" : "",
        "copyrightHolder" : "",
        "copyrightYear" : "2018",
        "datePublished": "2018-12-06 00:19:00 &#43;0300 &#43;0300",
        "dateModified" : "2018-12-06 00:19:00 &#43;0300 &#43;0300",
        "url" : "/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/",
        "wordCount" : "2392",
        "keywords" : [ "Machine learning","Deeplearning","object detector","single shot","Blog" ]
    }
    </script>
        
            
                <title>Tìm hiểu single shot object detectors</title>
            
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        
		<meta name="generator" content="Phạm Duy Tùng" />
        
  
    
  

  

  
  
  
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content='/favicon/mstile.png'>
  <meta name="application-name" content="Phạm Duy Tùng Machine Learning Blog">
  <meta name="msapplication-tooltip" content="Blog ML của Phạm Duy Tùng và Đặng Thị Hằng">
   
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">



        
            <meta name="author" content="Phạm Duy Tùng">
        
        
            <meta name="description" content="Ở bài viết này, chúng ta sẽ tìm hiểu single shot object detectors và ứng dụng của nó trên các thuật toán SSD, Yollov3, FPN và RetinaNet.">
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tìm hiểu single shot object detectors"/>
<meta name="twitter:description" content="Ở bài viết này, chúng ta sẽ tìm hiểu single shot object detectors và ứng dụng của nó trên các thuật toán SSD, Yollov3, FPN và RetinaNet."/>
<meta name="twitter:site" content="@example"/>

        <meta property="og:title" content="Tìm hiểu single shot object detectors" />
<meta property="og:description" content="Ở bài viết này, chúng ta sẽ tìm hiểu single shot object detectors và ứng dụng của nó trên các thuật toán SSD, Yollov3, FPN và RetinaNet." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/" /><meta property="article:published_time" content="2018-12-06T00:19:00&#43;03:00"/>
<meta property="article:modified_time" content="2018-12-06T00:19:00&#43;03:00"/>

        <meta property="og:image" content="//images/logo.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="512">
        
<meta itemprop="name" content="Tìm hiểu single shot object detectors">
<meta itemprop="description" content="Ở bài viết này, chúng ta sẽ tìm hiểu single shot object detectors và ứng dụng của nó trên các thuật toán SSD, Yollov3, FPN và RetinaNet.">


<meta itemprop="datePublished" content="2018-12-06T00:19:00&#43;03:00" />
<meta itemprop="dateModified" content="2018-12-06T00:19:00&#43;03:00" />
<meta itemprop="wordCount" content="2392">



<meta itemprop="keywords" content="Machine learning,Deeplearning,object detector,single shot," />

        

        
            
        

        
        
          
			
			 <link rel="stylesheet" href="/css/font-awesome.min.css">
			 <link rel="stylesheet" href="/css/bootstrap.min.css">


          
            <link rel="stylesheet" href="/css/academicons.min.css">
        

        
            
                
            
        


  
    
    <link href='//cdn.bootcss.com/highlight.js/9.15.8/styles/xcode.min.css' rel='stylesheet' type='text/css' />
  


      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-114911596-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

	  
	  <style>
	  
	  body{
font-family: Helvetica,Arial,sans-serif;
}

.card{
	margin-bottom: 10px;
}

#disqus_thread{
padding: 0 5px;
}

.item-header{
padding: 0;
}

.single-content-img{
width: 100%;
    max-height: 450px !important;
    background-size: cover;
    display: block;
    background-position: center;
}

.thumbnail {
    position: relative;
}

.caption {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
	background: rgba(0, 0, 0, 0.25);
	text-align:left;
}

.caption .title{
	font-size: 1.6em;
    line-height: 1.4em;
    top: 0;
	margin-left:20px;
	margin-top:20px;
	
}

.caption .title-caption{
margin-left:10px;
}

#content p{
text-align: justify;
    line-height: 1.9;
    font-size: 12pt;
}

#content img{
	display: block;
    margin-left: auto;
margin-right: auto;
max-width:98%;
}

img + strong {
    font-style: normal;
    display: inherit;
    text-align: center;
}
.img-news{
max-height:150px;
width:100%;
}

.news-tittle{
	padding-top:15px;
	text-align:justify;
}

.author{
	color: orange;
}
.author-inline{
	color: orange;
}

.adv{
height:18px;
}


.hljs{
    white-space: pre-wrap;
    white-space: -moz-pre-wrap;
    white-space: -pre-wrap;
    white-space: -o-pre-wrap;
    word-wrap: break-word;}
.titledetail {
    display: block;
    overflow: hidden;
    line-height: 53px;
    font-size: 45px;
    font-family: 'Roboto Condensed',sans-serif;
    font-weight: 600;
    width: 800px;
    margin: auto;
	padding: 0 ;
}

.newsrelate {
    display: block;
    overflow: hidden;
	 list-style:none;
}
a ,a:hover{
    text-decoration: none;
}
.newsrelate li {
    float: left;
    overflow: hidden;
    width: 30%;
    margin-left: 2.5%;
    margin-bottom: 15px;
}

.newsrelate li a {
    display: block;
    overflow: hidden;
}

.userdetail {
    display: block;
    overflow: hidden;
    margin: 0 10px 0 0;
    padding: 15px 0;
}
.newsrelate li h3 {
    display: block;
    overflow: hidden;
    line-height: 1.3em;
    font-size: 16pt;
    line-height: 22px;
    font-weight: 300;
    font-family: Arial,Helvetica,sans-serif;
    width: auto;
    margin: 5px auto;
}

.titlerelate {
    overflow: hidden;
    font-size: 18px;
    font-weight: 600;
    font-family: 'Roboto Condensed',sans-serif;
    line-height: 32px;
    text-transform: uppercase;
}
article .captionnews {
    color: #999;
    font-size: 14px;
    font-style: italic;
    padding: 10px;
    text-align: center;
    margin-bottom: 15px;
}

	  </style>

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P62ZZPB');</script>


<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P62ZZPB"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    </head>
    <body>
<script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '1546237302193677',
      xfbml      : true,
      version    : 'v5.0'
    });
    FB.AppEvents.logPageView();
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "https://connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v5.0&appId=1853483258232756&autoLogAppEvents=1"></script>
      
      

    
    
<header id="header"  style="background: #790014; color: hsla(0,0%,100%,1);">
<div class="container">
    <nav class="navbar navbar-expand-md navbar-dark">
	
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav mr-auto">
            
                <li class="nav-item">
                    <a class='nav-link' href="/blog">
                            <i class="fa fa-home active">&nbsp;</i>Home
                    </a>
                </li>
            
                <li class="nav-item">
                    <a class='nav-link' href="/news/">
                            <i class="fa fa-list">&nbsp;</i>News
                    </a>
                </li>
            
                <li class="nav-item">
                    <a class='nav-link' href="/xem-truyen/">
                            <i class="fa fa-id-card-o">&nbsp;</i>Truyện
                    </a>
                </li>
            
        </ul>
    </nav>
    </div></div>
</header>


   
    
	<div class="container">
	<div class="adv"></div>
	<div>
    <main role="main" >
	
        
        
        <article class="col-md-10 col-lg-9 mx-auto">
  


        
		 <div class="">
            <h1 class="titledetail">Tìm hiểu single shot object detectors</h1>
			
			
			<div class="userdetail">
			 
			  <time class="published"
            datetime='2018-12-06'>
            06/12/2018</time>
		 - 
			   <span class="author">Phạm Duy Tùng</span>
			   
			</div>
		<div class="thumbnail text-center">
		 <img class="img-fluid single-content-img lazy" src="/post_image/single-shot-object-detectors%20.jpg" />
		<div class="captionnews">Ở bài viết này, chúng ta sẽ tìm hiểu single shot object detectors và ứng dụng của nó trên các thuật toán SSD, Yollov3, FPN và RetinaNet.</div>
			</div>
			 <div class="col-md-12">
            
        
       



  

  

  <div id="content" class="mx-auto">
    

<h2 id="single-shot-detectors">Single Shot detectors</h2>

<p>Ở bài trước, chúng ta đã tìm hiểu về region proposal và ứng dụng của nó vào Faster R-CNN. Các thuật toán thuộc nhóm region proposal tuy cho kết quả có độ chính xác cao, nhưng chúng có một nhược điểm rất lớn là thời gian huấn luyện và đưa quyết định rất chậm. Faster R-CNN xử lý khoảng 7 <em>FPS</em> trên tập dữ liệu PASCAL VOC 2007. Một cách để tăng tốc quá trình tính toán là giảm số lượng tính toán trên mỗi ROI.</p>

<pre><code class="language-python">feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_align(feature_maps, ROI)
    results = detector2(patch)    # Giảm khối lượng tính toán ở đây
</code></pre>

<p>Một ý tưởng khác, là chúng ta sẽ bỏ qua bước tìm region proposal, mà trực tiếp rút trích boundary boxes và classes trực tiếp từ feature map.</p>

<pre><code class="language-python">feature_maps = process(image)
results = detector3(feature_maps) # Không cần tìm ROI
</code></pre>

<p>Dựa trên ý tưởng sử dụng cửa sổ trượt. Chúng ta sẽ trượt trên feature máp để nhận diện các đối tượng. Với mỗi loại đối tượng khác nhau, chúng ta sửa dụng các cửa sổ trượt có kích thước khác nhau. Cách này thoạt đầu trông có vẻ khá tốt, nhưng điểm yếu của nó là đã sử dụng cửa sổ trượt làm final boundary box. Do đó, giả sử chúng ta có nhiều đối tượng, và mỗi đối tượng có kích thước khác nhau, chúng ta sẽ có rất nhiều cửa sổ trượt để bao phủ hết toàn bộ đối tượng.</p>

<p>Một ý tưởng cải tiến là chúng ta sẽ định nghĩa trước các cửa sổ trượt, sau đó sẽ tiến hành dự đoán lớp và boundary box ( và Ý tưởng này, nhóm nghiên cứu phát triển thuật toán và đặt tên thuật toán là single shot detectors). Ý tưởng này tương tự như việc sử dụng anchors trong Faster R-CNN, nhưng single shot detectors thực hiện dự đoán boundary box và class đồng thời cùng nhau.</p>

<p>Ví dụ, giả sử chúng ta có một feature map 8x8 và chúng ta đưa ra k = 4 dự đoán.  Vậy ta có tổng cộng 8x8x4 = 256 dự đoán.</p>

<p><img src="/post_image/single-shot-object-detectors-img-1.jpg" alt="Hình ảnh" /></p>

<p>Xét hình bên trên, ta có 4 anchors đã được định nghĩa trước ( màu xanh lá cây), và có 4 prediction( màu xanh nước biển) tương ứng với từng anchor trên.</p>

<p>Với thuật toán Faster R-CNN, chúng ta sử dụng một convolution filter trả ra 5 kết quả dự đoán: 4 giá trị là toạ độ của boundary box, và giá trị còn lại là xác suất xuất hiện đối tượng. Tổng quát hơn, ta có input là D feature map 8x8, output là 8x8x5, số convolution filter trong Faster R-CNN là 3x3xDx8.</p>

<p>Với single shot detector, input của ta cũng tương tự là 8x8xD, output là 8x8x (4 + C) ( với 4 tương ứng với 4 điểm boundary box, và C là số lượng lớp đối tượng), vậy ta cần một convolution filter là 3x3xDx(4+C)</p>

<p><img src="/post_image/single-shot-object-detectors-architech.png" alt="Hình ảnh" /></p>

<p>Thuật toán Single shot detect chạy khá nhanh, nhưng độ chính xác của nó không cao lắm (không bằng region proposal). Thuật toán có vấn đề về việc nhận dạng các đối tượng có kích thước nhỏ. Ví dụ như hình bên dưới, chúng ta có tổng cộng 9 ông già noel, nhưng thuật toán chỉ nhận diện được có 5 ông.</p>

<p><img src="/post_image/single-shot-object-detectors-img-2.jpg" alt="Hình ảnh" /></p>

<h2 id="ssd">SSD</h2>

<p>SSD là mô hình single shot detector sử dụng mạng VGG16 để rút trích đặc trưng. Mô hình như hình bên dưới. Trong đó, những conv có màu xanh nước biển nhạt là những custom convolution layter (ta có thể thêm bớt bao nhiêu tuỳ thích). Convolutional filter layter (là cục màu xanh lá cây) có nhiệm vụ tổng hợp các thông tin lại để đưa quyết định.</p>

<p><img src="/post_image/single-shot-object-detectors-vgg19-model.jpg" alt="Hình ảnh" /></p>

<p>Khi sử dụng mô hình như hình ở trên, chúng ta thấy rằng các custom convolution layter có nhiệm vụ làm giảm chiều và giảm độ phân giải của bức ảnh. Cho nên, mô hình chỉ có khả năng nhận ra các đối tượng có kích thước lớn. Để giải quyết vấn đề này, chúng ta sẽ sử dụng các object detector khác nhau trên mỗi feature maps (xem output của mỗi custom convolution là một feature map).</p>

<p><img src="/post_image/single-shot-object-detectors-vgg19-model1.jpg" alt="Hình ảnh" /></p>

<p>Ảnh bên dưới là sơ đồ số chiều của các feature maps.</p>

<p><img src="/post_image/single-shot-object-detectors-vgg19-diagram.jpg" alt="Hình ảnh" /></p>

<p>SSD sử dụng các layter có kích thước giảm dần theo độ sâu để nhận dạng đối tượng. Nhìn vào hình vẽ sơ đồ bên dưới của SSD, chúng ra dễ dàng nhận thấy rằng độ phân giải giảm đáng kể qua mỗi layer và có lẽ (chắc chắn) sẽ bỏ sót những đối tượng có kích thước nhỏ ở những lớp có độ phân giải thấp. Nếu trong dự án thực tế của bạn có xảy ra vấn đề này, bạn nên tăng độ phân giải của ảnh đầu vào.</p>

<p><img src="/post_image/single-shot-object-detectors-SSD1-diagram.jpg" alt="Hình ảnh" /></p>

<h2 id="yolo">YOLO</h2>

<p>YOLO cũng là một thuật toán sử dụng single shot detector để dò tìm vị trí của các đối tượng trong ảnh. YOLO sử dụng DarkNet để tạo các feature cho bức ảnh (SSD sử dụng VGG16). Mô hình của YOLLO như ảnh ở bên dưới.</p>

<p><img src="/post_image/single-shot-object-detectors-darknet-diagram.jpg" alt="Hình ảnh" /></p>

<p>Khác với kiến trúc mạng SSD ở trên, YOLLO không sử dụng multiple scale feature map (SSD sử dụng các custom convolution layter, qua mỗi layter thì feature maps sẽ có kích thước giảm xuống - các output của custom convolution layer chính là các feature map chúng ta thu được). Thay vào đó, YOLLO sẽ làm phẳng hoá (flatten - vd ma trận 3x3  sẽ biến thành vector 1x9, ma trận 4x5 sẽ biến thành vector 1x20 &hellip;, làm phẳng nghĩa là chúng ta sẽ không dùng bộ lọc nào hết, mà sử dụng các phép biến đổi, nên không làm thay đổi giá trị, chỉ làm thay đổi hình dạng) một phần output của convolution layer và kết hợp với  convolution layer ở trong DarkNet tạo thành feature map (Xem hình ở trên sẽ rõ hơn). Ví dụ ở custom convolution layer chúng ta thu được output có kích thước 28x28x512, chúng ta sẽ flatten thành layter có kích thước 14x14x2048, kết hợp với 1 layter có kích thước 14x14x1024 ở trong darknet, chúng ta thu được feature maps có kích thước là 14x14x3072. Đem feature maps này đi đự đoán.</p>

<p>YOLOv2 đã thêm vào rất nhiều các cải tiền để cải tăng mAP từ 63.4 trong mô hình đầu tiên (YOLOv1) lên 78.6. Các cải tiền bao gồm thêm batch norm, anchor boxes,  hi-res classifier &hellip; Các bạn có thể xem ở hình bên dưới. YOLO9000 có thể nhận dạng 9000 đối tượng khác nhau.</p>

<p><img src="/post_image/yollo-v2-improment.jpg" alt="Hình ảnh" /></p>

<p>YOLOv2 có thể nhận diện các đối tượng với ảnh đầu vào có độ phân giải bất kỳ. Với ảnh có độ phân giải thấp thì mô hình chạy khá nhanh, có FPS cao nhưng mAP lại thấp (tradeoff giữa FPS và mAP).</p>

<p><img src="/post_image/yollo-v2-acc.jpg" alt="Hình ảnh" /></p>

<h2 id="yolov3">YOLOv3</h2>

<p>YOLOv3 sử dụng darknet với kiến trúc phức hơn để rút trích đặc trưng của bức ảnh. YOLOv3 thêm vào đặc trưng Pyramid để dò tìm các đối tượng có kích thước nhỏ.</p>

<p>Hình bên dưới so sánh tradeoff giữa thời gian thực thi và độ chính xác giữa các mô hình. Ta thấy rằng thời gian thực thi của YOLOv3 rất nhanh, cùng phân mức mAP 28.8, thời gian YOLOv3 thực thi chỉ tốn 22ms, trong khi đó SSD321 tốn đến 61ms - gấp 3 lần.</p>

<p><img src="/post_image/single-shot-object-detectors-compare.jpg" alt="Hình ảnh" /></p>

<h2 id="feature-pyramid-networks-fpn">Feature Pyramid Networks (FPN)</h2>

<p>Dò tìm các đối tượng có kích thước nhỏ là một vấn đề đáng được giải quyết để nâng cao độ chính xác. Và FPN là mô hình mạng được thiết kế ra dựa trên khái niệm pyramid để giải quyết vấn đề này.</p>

<p><img src="/post_image/feature-pyramid-network-model1.jpg" alt="Hình ảnh" /></p>

<p>Mô hình FPN kết hợp thông tin của mô hình theo hướng <em>bottom-up</em> kết hợp với <em>top-down</em> để dò tìm đối tượng (trong khi đó, các thuật toán khác chỉ thường sử dụng <em>bottom-up</em>). Khi chúng ta ở bottom và đi lên (up), độ phân giải sẽ giảm, nhưng giá trị ngữ nghĩa sẽ tăng lên. Xem hình mô phỏng bên dưới.</p>

<p><img src="/post_image/feature-pyramid-network-model2.jpg" alt="Hình ảnh" /></p>

<p>SSD đưa ra quyết định dựa vào nhiều feature map. Nhưng layer ở bottom không được sử dụng để nhận dạng đối tượng. Vì những layter này có độ phân giải cao nhưng giá trị ngữ nghĩa của chúng lại không đủ cao (thấp) nên những nhà nghiên cứu bỏ chúng đi để tăng tốc độ xử lý. Các nhà nghiêng cứu biện minh rằng các layer ở bottom chưa đủ mức ý nghĩa cần thiết để nâng cao độ chính xác, thêm các layer đó vào sẽ không nâng độ chính xác cao thêm bao nhiêu và họ bỏ chúng đi để có tốc độ tốt hơn. Cho nên, SSD chỉ sử dụng các layer ở lớp trên , và do đó sẽ không nhận dạng được các đối tượng có kích thước nhỏ.</p>

<p><img src="/post_image/ssd-model-bottom-up.jpg" alt="Hình ảnh" /></p>

<p>Trong khi đó, FPN xây dựng thêm mô hình top-down, nhằm mục đích xây dựng các layer có độ phân giải cao từ các layer có ngữ nghĩa cao.</p>

<p><img src="/post_image/fpn-top-down-model.jpg" alt="Hình ảnh" /></p>

<p>Trong quá trình xây dựng lại các layer từ top xuống bottom, chúng ta sẽ gặp một vấn đề khá nghiêm trọng là bị mất mát thông tin của các đối tượng. Ví dụ một đối tượng nhỏ khi lên top sẽ không thấy nó, và từ top đi ngược lại sẽ không thể tái tạo lại đối tượng nhỏ đó. Để giải quyết vấn đề này, chúng ta sẽ tạo các kết nối (skip connection) giữa các reconstruction layter và các feature map để giúp quá trình detector dự đoán các vị trí của đối tượng thực hiện tốt hơn (hạn chế tốt nhất việc mất mát thông tin).</p>

<p><img src="/post_image/fpn-top-down-model-with-skip-connection.jpg" alt="Hình ảnh" />
<em>Thêm các skip connection giữa feature map và reconstruction layer</em></p>

<p>Đồ hình bên dưới diễn ta chi tiết đường đi theo bottom-up và top-down. P2, P3, P4, P5 là các pyramid  của các feature map.</p>

<p><img src="/post_image/fpn-top-down-with-bottom-up.jpg" alt="Hình ảnh" /></p>

<h2 id="so-sánh-feature-pyramid-networks-với-region-proposal-network">So sánh Feature Pyramid Networks với Region Proposal Network</h2>

<p>FPN không phải là mô hình phát hiện đối tượng. Nó là mô hình phát hiện đặc trưng và được sử dụng trong phát hiện đối tượng. Các feature map từ P2 đến P5 trong hình bên dưới độc lập với nhau và các đặc trưng được sử dụng để phát hiện đối tượng.</p>

<p><img src="/post_image/fpn-detail.jpg" alt="Hình ảnh" /></p>

<h2 id="sử-dụng-feature-pyramid-networks-trong-fast-r-cnn-và-faster-r-cnn">Sử dụng Feature Pyramid Networks trong Fast R-CNN và Faster R-CNN</h2>

<p>Chúng ta hoàn toàn có thể sử dụng FPN trong Fast và Faster R-CNN. Chúng ta sẽ tạo ra các feature map sử dụng FPN, kết quả là ta thu được các puramid (feature map). Sau đó, chúng ta sẽ rút trích các ROIs trên các feature map đó. Dựa trên kích thước của các ROI, chúng ta sẽ chọn feature map nào tốt nhất để tạo các feature patches (các hình chữ nhật nhỏ). Các bạn có thể xem chi tiết ở hình bên dưới.</p>

<p><img src="/post_image/fpn-in-faster-r-cnn.jpg" alt="Hình ảnh" /></p>

<h2 id="focal-loss-retinanet">Focal loss (RetinaNet)</h2>

<p>Trong thực tế, chúng ta sẽ gặp tình trạng tỷ lệ diện tích của các đối tượng trong ảnh nhỏ hơn nhiều so với phần background còn lại, ví dụ chúng ta cần nhận dạng một quả cam có kích thước 100x100 trong ảnh 1920x1080. Vì phần background quá lớn nên chúng sẽ là thành phần &ldquo;thống trị&rdquo; và làm sai lệch kết quả. SSD sử dụng phương pháp lấy mẫu tỷ lệ của object class và background class trong quá trình train (nên background sẽ không còn thống trị nữa).</p>

<p>Ngoài ra, chúng ta sẽ còn gặp tình trạng là số lượng tỷ lệ object trong ảnh không đều nhau, ví dụ trong tập huấn luyệt có 1000 quả cam và 10 quả táo.</p>

<p>Focal loss (FL) được sinh ra để giải quyết tình trạng này. Để đi vào chi tiết hơn, chúng ta nhắc lại hàm lỗi cross entropy.</p>

<p>$$
\begin{equation}
  CE(p,y) =
    \begin{cases}
      -\log(p) &amp; \text{if y=1} \\\<br />
      -\log(1-p) &amp; \text{otherwise}
    \end{cases}<br />
\end{equation}
$$</p>

<p>Trong hàm trên thì y nhận giá trị 1 hoặc -1. Giá trị xác xuất nằm trong khoảng (0,1) là xác suất dự đoán cho lớp có y=1.</p>

<p>Để rõ ràng hơn, ta có thể viết lại hàm trên như sau:</p>

<p>$$
\begin{equation}
  p_t =
    \begin{cases}
      p &amp; \text{if y=1} \\\<br />
      1-p &amp; \text{otherwise}
    \end{cases}<br />
\end{equation}
$$</p>

<p>$$
\begin{equation}
    CE(p,y) = CE(p_t) = -\log(p_t)
\end{equation}
$$</p>

<p>Ta có nhận xét rằng đối với các trường hợp được phân loại tốt (có xác suất lớn hơn 0.6) thì hàm loss nhận gái trị với độ lớn lớn hơn 0. Và trong trường hợp dữ liệu có tỷ lệ lệch cao thì tổng các giá trị này sẽ cho ra kết quả loss với một con số rất lớn so với loss của các trường hợp khó phâm loại. Và nó ảnh hưởng đến quá trình huấn luyện.</p>

<p><img src="/post_image/focal-lost.jpg" alt="Hình ảnh" /></p>

<p>Ý tưởng chính của focal-lost là đối với các trường hợp được phân loại tốt ( xác suất lớn hơn 0.5) thì focal lost sẽ làm giảm giá trị cross-entropy của nó xuống nhỏ hơn so với thông thường. Do đó, ta sẽ thêm trọng số cho hàm cross-entropy để biến thành hàm focal lost.</p>

<p>$$
FL(p_t) = -(1-p_t)^\gamma\log(p_t)
$$</p>

<p>Với nhân tử được thêm vào được gọi là modulating factor, gamma lớn hơn hoặc bằng 0 được gọi là tham số focusing.</p>

<p>Nhìn hình ở trên, ta thấy rằng khi gamma = 0 thì hàm focal lost chính là cross-entropy.</p>

<p>Đặc điểm của hàm lost trên như sau:</p>

<p>Khi mẫu bị phân loại sai, pt nhỏ, nhân tố modulating factor gần với 1 và hàm lost ít bị ảnh hưởng. Khi pt tiến gần tới 1 (mẫu phân loại tốt), moduling factor sẽ tiến gần tới 0 và hàm loss trong trường hợp này sẽ bị giảm trọng số xuống.</p>

<p>Tham số focusing sẽ điều chỉnh tỷ lệ các trường hợp được phân loại tốt được giảm trọng số. Khi gamma càng tăng thì ảnh hưởng của modulating factor cũng tăng. Trong các thí nghiệm cho thấy với gamma = 2 hì kết quả đạt được sẽ tốt nhất.</p>

<p>Hình bên dưới là đồ hình của RetinaNet được xây dựng dựa trên FPN và ResNet sử dung Focal loss.</p>

<p><img src="/post_image/retina-net-fpn-resnet.jpg" alt="Hình ảnh" /></p>

<p>Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.</p>

<p>Bài viết được lược dịch và tham khảo từ nguồn <a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d">https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d</a></p>

  </div>
  
			</div>
  <footer class="col-md-10  mx-auto">
  <div >
  <div class="fb-like" data-share="true"  data-width="450"  data-show-faces="true">
</div> </p></div>
    <ul class="stats list-unstyled">
 
    
  <li class="tags">
    <ul class="list-inline">
       
            
            
                <i class="fa fa-tags"></i>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/machine-learning">Machine learning</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/deeplearning">Deeplearning</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/object-detector">object detector</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/single-shot">single shot</a></li>
                
            
        
    </ul>
  </li>
  
</ul>

	</div>
  </footer>
  <hr/>
<div class="titlerelate">Bài viết khác</div>
<div class="infinite-container featured-task">
<div class="card-deck card-break infinite-item">



    
        <div class="card">
		<a href="/blog/2018-12-05-what-do-we-learn-from-object-detection-p1/"
                class="button big previous">
		
		<img class="card-img-top lazy" src="/post_image/sliding-window.gif" width="100" />
		<div class="card-body">
		<h5 class="card-title">
		Tìm hiểu region based object detectors
				</h5>
				</div>
				</a>
				</div>
    

    
        <div class="card">
		<a href="/blog/2018-12-10-design-choices-lessons-learned-and-trends-for-object-detections/"
                class="button big previous">
		
		<img class="card-img-top lazy" src="/post_image/Object-Detection-for-outdoor-cv-tricks.jpg" width="100" />
		
		<div class="card-body">
		<h5 class="card-title">
		Lựa chọn mô hình object detectors
				</h5>
				</div>
				</a>
				</div>
    

</div>
</div>



<div class="fb-comments" data-href="" data-width="" data-numposts="5"></div>

    <article class="post">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "phamduytung" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </article>



</article>


		
    </main>
    
	</div>
	</div>
    
	<hr>
  <footer class="footer">
  <div class="container text-center">
    
    <p class="copyright">
      
        &copy; 2020
        
          Phạm Duy Tùng Machine Learning Blog
        
      
     
    </p>
	</div>
  </footer>
    
    

    
      
    

    
      
      
      
        <script src="//cdn.bootcss.com/highlight.js/9.15.8/highlight.min.js"></script>
        
        
        
        <script src="//cdn.bootcss.com/highlight.js/9.15.8/languages/python.min.js"></script>
        <script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
      
    
    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/skel/3.0.1/skel.min.js"></script>
     

   <script src="/js/jquery-3.3.1.min.js"></script>
   
    <script src="/js/bootstrap.min.js"></script>
      <script src="/js/util.js"></script>
      <script src="/js/main.js"></script>
     
    

    
      
        
      
    
	
    
    
      
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


	  
	  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114911596-1"  data-cfasync="false"></script>
<script  data-cfasync="false">
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114911596-1');
</script>
<script src="https://cdn.jsdelivr.net/npm/intersection-observer@0.5.1/intersection-observer.js"  data-cfasync="false"></script>
<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@12.0.0/dist/lazyload.min.js"  data-cfasync="false"></script>
	   <script  type="text/javascript"  data-cfasync="false">

  function getcontent(){
 

   

             var myLazyLoad = new LazyLoad({
    elements_selector: ".lazy"
});
                }
            
			
			$(document).ready(function(){
      getcontent();
      
      const calander = document.querySelector('#calander');
      if (window.Worker) {
if (calander){
        var calanderworker = new Worker('/js/worker.js');
        calanderworker.onmessage = (event) => { calander.innerHTML = event.data; }
        calanderworker.postMessage(calander.textContent);
      }
}
			}); 
</script>


  </body>
</html>

